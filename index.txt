
&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&

Input 1x1x32x32
L-->R current op 140372191117808
after conv2d 1x1x32x32
L-->R current op 140372191117952
after maxpool2d 1x1x16x16
L-->R current op 140372191067488
after conv2d 1x1x16x16
L-->R current op 140372191067248
after maxpool2d 1x1x8x8
coord [0, 0]
bwd_out_shape  (3, 3)
fwd_out_shape  (2, 2)
Yes, fwd is smaller
------------------------------
f_info {-11: fake[-11,-11] PI( <0,0,>,
 <otileshape 3,3,>,
 <padding >,
 <inpslidx 0,2,0,2,>, 
 <internal >, 
 <realidx 0,2,0,2,>, 
 <ndtsize >, 
  local_first False
 next_id -11)
, 140372191067248: maxpool2d140372191067248[0,-1] PI( <0,0,>,
 <otileshape 3,3,>,
 <padding >,
 <inpslidx 0,5,0,5,>, 
 <internal >, 
 <realidx 0,5,0,5,>, 
 <ndtsize 2,2,>, 
  local_first False
 next_id -99)
, 140372191067488: conv2d140372191067488[1,0] PI( <0,0,>,
 <otileshape 6,6,>,
 <padding 1,0,1,0,>,
 <inpslidx 0,6,0,6,>, 
 <internal 0,1,0,1,>, 
 <realidx -1,6,-1,6,>, 
 <ndtsize 4,4,>, 
  local_first True
 next_id 140372191067248)
, 140372191117952: maxpool2d140372191117952[2,-1] PI( <0,0,>,
 <otileshape 7,7,>,
 <padding >,
 <inpslidx 0,13,0,13,>, 
 <internal >, 
 <realidx 0,13,0,13,>, 
 <ndtsize 4,4,>, 
  local_first False
 next_id 140372191067488)
, 140372191117808: conv2d140372191117808[3,0] PI( <0,0,>,
 <otileshape 14,14,>,
 <padding 1,0,1,0,>,
 <inpslidx 0,14,0,14,>, 
 <internal 0,1,0,1,>, 
 <realidx -1,14,-1,14,>, 
 <ndtsize 8,8,>, 
  local_first True
 next_id 140372191117952)
}
------------------------------
b_info {140372191117808: bk-conv2d140372191117808[0,0] PI( <0,0,>,
 <otileshape 9,9,>,
 <padding 1,0,1,0,>,
 <inpslidx 0,8,0,8,>, 
 <internal 0,1,0,1,>, 
 <realidx -1,8,-1,8,>, 
 <ndtsize >, 
  local_first False
 next_id -99)
, 140372191117952: bk-maxpool2d140372191117952[1,-1] PI( <0,0,>,
 <otileshape 5,5,>,
 <padding >,
 <inpslidx 0,4,0,4,>, 
 <internal >, 
 <realidx 0,4,0,4,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191117808)
, 140372191067488: bk-conv2d140372191067488[2,0] PI( <0,0,>,
 <otileshape 6,6,>,
 <padding 1,0,1,0,>,
 <inpslidx 0,5,0,5,>, 
 <internal 0,1,0,1,>, 
 <realidx -1,5,-1,5,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191117952)
, 140372191067248: bk-maxpool2d140372191067248[3,-1] PI( <0,0,>,
 <otileshape 3,3,>,
 <padding >,
 <inpslidx 0,2,0,2,>, 
 <internal >, 
 <realidx 0,2,0,2,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191067488)
}
++++++++++++++++++++++++++++++++++++++++++++++++
***input tile torch.Size([1, 1, 15, 15])
id 140372191117808
== tiled conv2d forward
input shape torch.Size([1, 1, 15, 15])
shape input_tile_for_next
 torch.Size([1, 1, 14, 14])
1 out_temp torch.Size([1, 1, 14, 14])
max 1 torch.Size([1, 1, 7, 7])
id 140372191067488
== tiled conv2d forward
input shape torch.Size([1, 1, 7, 7])
shape input_tile_for_next
 torch.Size([1, 1, 6, 6])
2 out_temp torch.Size([1, 1, 6, 6])
max 2 torch.Size([1, 3, 3])
(3, 3) [3, 3] [0, 2, 0, 2]
coord [0, 2, 0, 2]
out_temp torch.Size([1, 1, 3, 3])
coord [0, 1]
bwd_out_shape  (4, 3)
fwd_out_shape  (2, 2)
Yes, fwd is smaller
------------------------------
f_info {-11: fake[-11,-11] PI( <0,1,>,
 <otileshape 4,3,>,
 <padding >,
 <inpslidx 1,4,0,2,>, 
 <internal >, 
 <realidx 1,4,0,2,>, 
 <ndtsize >, 
  local_first False
 next_id -11)
, 140372191067248: maxpool2d140372191067248[0,-1] PI( <0,1,>,
 <otileshape 4,3,>,
 <padding >,
 <inpslidx 2,9,0,5,>, 
 <internal >, 
 <realidx 2,9,0,5,>, 
 <ndtsize 2,2,>, 
  local_first False
 next_id -99)
, 140372191067488: conv2d140372191067488[1,0] PI( <0,1,>,
 <otileshape 8,6,>,
 <padding 0,0,1,0,>,
 <inpslidx 1,10,0,6,>, 
 <internal 1,1,0,1,>, 
 <realidx 1,10,-1,6,>, 
 <ndtsize 4,4,>, 
  local_first True
 next_id 140372191067248)
, 140372191117952: maxpool2d140372191117952[2,-1] PI( <0,1,>,
 <otileshape 10,7,>,
 <padding >,
 <inpslidx 2,21,0,13,>, 
 <internal >, 
 <realidx 2,21,0,13,>, 
 <ndtsize 4,4,>, 
  local_first False
 next_id 140372191067488)
, 140372191117808: conv2d140372191117808[3,0] PI( <0,1,>,
 <otileshape 20,14,>,
 <padding 0,0,1,0,>,
 <inpslidx 1,22,0,14,>, 
 <internal 1,1,0,1,>, 
 <realidx 1,22,-1,14,>, 
 <ndtsize 8,8,>, 
  local_first True
 next_id 140372191117952)
}
------------------------------
b_info {140372191117808: bk-conv2d140372191117808[0,0] PI( <0,1,>,
 <otileshape 10,9,>,
 <padding 0,0,1,0,>,
 <inpslidx 7,16,0,8,>, 
 <internal 1,1,0,1,>, 
 <realidx 7,16,-1,8,>, 
 <ndtsize >, 
  local_first False
 next_id -99)
, 140372191117952: bk-maxpool2d140372191117952[1,-1] PI( <0,1,>,
 <otileshape 6,5,>,
 <padding >,
 <inpslidx 3,8,0,4,>, 
 <internal >, 
 <realidx 3,8,0,4,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191117808)
, 140372191067488: bk-conv2d140372191067488[2,0] PI( <0,1,>,
 <otileshape 8,6,>,
 <padding 0,0,1,0,>,
 <inpslidx 2,9,0,5,>, 
 <internal 1,1,0,1,>, 
 <realidx 2,9,-1,5,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191117952)
, 140372191067248: bk-maxpool2d140372191067248[3,-1] PI( <0,1,>,
 <otileshape 4,3,>,
 <padding >,
 <inpslidx 1,4,0,2,>, 
 <internal >, 
 <realidx 1,4,0,2,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191067488)
}
++++++++++++++++++++++++++++++++++++++++++++++++
***input tile torch.Size([1, 1, 15, 22])
id 140372191117808
== tiled conv2d forward
input shape torch.Size([1, 1, 15, 22])
shape input_tile_for_next
 torch.Size([1, 1, 14, 20])
1 out_temp torch.Size([1, 1, 14, 20])
max 1 torch.Size([1, 1, 7, 10])
id 140372191067488
== tiled conv2d forward
input shape torch.Size([1, 1, 7, 10])
shape input_tile_for_next
 torch.Size([1, 1, 6, 8])
2 out_temp torch.Size([1, 1, 6, 8])
max 2 torch.Size([1, 3, 4])
(4, 3) [4, 3] [1, 4, 0, 2]
coord [1, 4, 0, 2]
out_temp torch.Size([1, 1, 3, 4])
coord [0, 2]
bwd_out_shape  (4, 3)
fwd_out_shape  (2, 2)
Yes, fwd is smaller
------------------------------
f_info {-11: fake[-11,-11] PI( <0,2,>,
 <otileshape 4,3,>,
 <padding >,
 <inpslidx 3,6,0,2,>, 
 <internal >, 
 <realidx 3,6,0,2,>, 
 <ndtsize >, 
  local_first False
 next_id -11)
, 140372191067248: maxpool2d140372191067248[0,-1] PI( <0,2,>,
 <otileshape 4,3,>,
 <padding >,
 <inpslidx 6,13,0,5,>, 
 <internal >, 
 <realidx 6,13,0,5,>, 
 <ndtsize 2,2,>, 
  local_first False
 next_id -99)
, 140372191067488: conv2d140372191067488[1,0] PI( <0,2,>,
 <otileshape 8,6,>,
 <padding 0,0,1,0,>,
 <inpslidx 5,14,0,6,>, 
 <internal 1,1,0,1,>, 
 <realidx 5,14,-1,6,>, 
 <ndtsize 4,4,>, 
  local_first True
 next_id 140372191067248)
, 140372191117952: maxpool2d140372191117952[2,-1] PI( <0,2,>,
 <otileshape 10,7,>,
 <padding >,
 <inpslidx 10,29,0,13,>, 
 <internal >, 
 <realidx 10,29,0,13,>, 
 <ndtsize 4,4,>, 
  local_first False
 next_id 140372191067488)
, 140372191117808: conv2d140372191117808[3,0] PI( <0,2,>,
 <otileshape 20,14,>,
 <padding 0,0,1,0,>,
 <inpslidx 9,30,0,14,>, 
 <internal 1,1,0,1,>, 
 <realidx 9,30,-1,14,>, 
 <ndtsize 8,8,>, 
  local_first True
 next_id 140372191117952)
}
------------------------------
b_info {140372191117808: bk-conv2d140372191117808[0,0] PI( <0,2,>,
 <otileshape 10,9,>,
 <padding 0,0,1,0,>,
 <inpslidx 15,24,0,8,>, 
 <internal 1,1,0,1,>, 
 <realidx 15,24,-1,8,>, 
 <ndtsize >, 
  local_first False
 next_id -99)
, 140372191117952: bk-maxpool2d140372191117952[1,-1] PI( <0,2,>,
 <otileshape 6,5,>,
 <padding >,
 <inpslidx 7,12,0,4,>, 
 <internal >, 
 <realidx 7,12,0,4,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191117808)
, 140372191067488: bk-conv2d140372191067488[2,0] PI( <0,2,>,
 <otileshape 8,6,>,
 <padding 0,0,1,0,>,
 <inpslidx 6,13,0,5,>, 
 <internal 1,1,0,1,>, 
 <realidx 6,13,-1,5,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191117952)
, 140372191067248: bk-maxpool2d140372191067248[3,-1] PI( <0,2,>,
 <otileshape 4,3,>,
 <padding >,
 <inpslidx 3,6,0,2,>, 
 <internal >, 
 <realidx 3,6,0,2,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191067488)
}
++++++++++++++++++++++++++++++++++++++++++++++++
***input tile torch.Size([1, 1, 15, 22])
id 140372191117808
== tiled conv2d forward
input shape torch.Size([1, 1, 15, 22])
shape input_tile_for_next
 torch.Size([1, 1, 14, 20])
1 out_temp torch.Size([1, 1, 14, 20])
max 1 torch.Size([1, 1, 7, 10])
id 140372191067488
== tiled conv2d forward
input shape torch.Size([1, 1, 7, 10])
shape input_tile_for_next
 torch.Size([1, 1, 6, 8])
2 out_temp torch.Size([1, 1, 6, 8])
max 2 torch.Size([1, 3, 4])
(4, 3) [4, 3] [3, 6, 0, 2]
coord [3, 6, 0, 2]
out_temp torch.Size([1, 1, 3, 4])
coord [0, 3]
bwd_out_shape  (3, 3)
fwd_out_shape  (2, 2)
Yes, fwd is smaller
------------------------------
f_info {-11: fake[-11,-11] PI( <0,3,>,
 <otileshape 3,3,>,
 <padding >,
 <inpslidx 5,7,0,2,>, 
 <internal >, 
 <realidx 5,7,0,2,>, 
 <ndtsize >, 
  local_first False
 next_id -11)
, 140372191067248: maxpool2d140372191067248[0,-1] PI( <0,3,>,
 <otileshape 3,3,>,
 <padding >,
 <inpslidx 10,15,0,5,>, 
 <internal >, 
 <realidx 10,15,0,5,>, 
 <ndtsize 2,2,>, 
  local_first False
 next_id -99)
, 140372191067488: conv2d140372191067488[1,0] PI( <0,3,>,
 <otileshape 6,6,>,
 <padding 0,1,1,0,>,
 <inpslidx 9,15,0,6,>, 
 <internal 1,0,0,1,>, 
 <realidx 9,16,-1,6,>, 
 <ndtsize 4,4,>, 
  local_first True
 next_id 140372191067248)
, 140372191117952: maxpool2d140372191117952[2,-1] PI( <0,3,>,
 <otileshape 7,7,>,
 <padding >,
 <inpslidx 18,31,0,13,>, 
 <internal >, 
 <realidx 18,31,0,13,>, 
 <ndtsize 4,4,>, 
  local_first False
 next_id 140372191067488)
, 140372191117808: conv2d140372191117808[3,0] PI( <0,3,>,
 <otileshape 14,14,>,
 <padding 0,1,1,0,>,
 <inpslidx 17,31,0,14,>, 
 <internal 1,0,0,1,>, 
 <realidx 17,32,-1,14,>, 
 <ndtsize 8,8,>, 
  local_first True
 next_id 140372191117952)
}
------------------------------
b_info {140372191117808: bk-conv2d140372191117808[0,0] PI( <0,3,>,
 <otileshape 9,9,>,
 <padding 0,1,1,0,>,
 <inpslidx 23,31,0,8,>, 
 <internal 1,0,0,1,>, 
 <realidx 23,32,-1,8,>, 
 <ndtsize >, 
  local_first False
 next_id -99)
, 140372191117952: bk-maxpool2d140372191117952[1,-1] PI( <0,3,>,
 <otileshape 5,5,>,
 <padding >,
 <inpslidx 11,15,0,4,>, 
 <internal >, 
 <realidx 11,15,0,4,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191117808)
, 140372191067488: bk-conv2d140372191067488[2,0] PI( <0,3,>,
 <otileshape 6,6,>,
 <padding 0,1,1,0,>,
 <inpslidx 10,15,0,5,>, 
 <internal 1,0,0,1,>, 
 <realidx 10,16,-1,5,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191117952)
, 140372191067248: bk-maxpool2d140372191067248[3,-1] PI( <0,3,>,
 <otileshape 3,3,>,
 <padding >,
 <inpslidx 5,7,0,2,>, 
 <internal >, 
 <realidx 5,7,0,2,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191067488)
}
++++++++++++++++++++++++++++++++++++++++++++++++
***input tile torch.Size([1, 1, 15, 15])
id 140372191117808
== tiled conv2d forward
input shape torch.Size([1, 1, 15, 15])
shape input_tile_for_next
 torch.Size([1, 1, 14, 14])
1 out_temp torch.Size([1, 1, 14, 14])
max 1 torch.Size([1, 1, 7, 7])
id 140372191067488
== tiled conv2d forward
input shape torch.Size([1, 1, 7, 7])
shape input_tile_for_next
 torch.Size([1, 1, 6, 6])
2 out_temp torch.Size([1, 1, 6, 6])
max 2 torch.Size([1, 3, 3])
(3, 3) [3, 3] [5, 7, 0, 2]
coord [5, 7, 0, 2]
out_temp torch.Size([1, 1, 3, 3])
coord [1, 0]
bwd_out_shape  (3, 4)
fwd_out_shape  (2, 2)
Yes, fwd is smaller
------------------------------
f_info {-11: fake[-11,-11] PI( <1,0,>,
 <otileshape 3,4,>,
 <padding >,
 <inpslidx 0,2,1,4,>, 
 <internal >, 
 <realidx 0,2,1,4,>, 
 <ndtsize >, 
  local_first False
 next_id -11)
, 140372191067248: maxpool2d140372191067248[0,-1] PI( <1,0,>,
 <otileshape 3,4,>,
 <padding >,
 <inpslidx 0,5,2,9,>, 
 <internal >, 
 <realidx 0,5,2,9,>, 
 <ndtsize 2,2,>, 
  local_first False
 next_id -99)
, 140372191067488: conv2d140372191067488[1,0] PI( <1,0,>,
 <otileshape 6,8,>,
 <padding 1,0,0,0,>,
 <inpslidx 0,6,1,10,>, 
 <internal 0,1,1,1,>, 
 <realidx -1,6,1,10,>, 
 <ndtsize 4,4,>, 
  local_first True
 next_id 140372191067248)
, 140372191117952: maxpool2d140372191117952[2,-1] PI( <1,0,>,
 <otileshape 7,10,>,
 <padding >,
 <inpslidx 0,13,2,21,>, 
 <internal >, 
 <realidx 0,13,2,21,>, 
 <ndtsize 4,4,>, 
  local_first False
 next_id 140372191067488)
, 140372191117808: conv2d140372191117808[3,0] PI( <1,0,>,
 <otileshape 14,20,>,
 <padding 1,0,0,0,>,
 <inpslidx 0,14,1,22,>, 
 <internal 0,1,1,1,>, 
 <realidx -1,14,1,22,>, 
 <ndtsize 8,8,>, 
  local_first True
 next_id 140372191117952)
}
------------------------------
b_info {140372191117808: bk-conv2d140372191117808[0,0] PI( <1,0,>,
 <otileshape 9,10,>,
 <padding 1,0,0,0,>,
 <inpslidx 0,8,7,16,>, 
 <internal 0,1,1,1,>, 
 <realidx -1,8,7,16,>, 
 <ndtsize >, 
  local_first False
 next_id -99)
, 140372191117952: bk-maxpool2d140372191117952[1,-1] PI( <1,0,>,
 <otileshape 5,6,>,
 <padding >,
 <inpslidx 0,4,3,8,>, 
 <internal >, 
 <realidx 0,4,3,8,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191117808)
, 140372191067488: bk-conv2d140372191067488[2,0] PI( <1,0,>,
 <otileshape 6,8,>,
 <padding 1,0,0,0,>,
 <inpslidx 0,5,2,9,>, 
 <internal 0,1,1,1,>, 
 <realidx -1,5,2,9,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191117952)
, 140372191067248: bk-maxpool2d140372191067248[3,-1] PI( <1,0,>,
 <otileshape 3,4,>,
 <padding >,
 <inpslidx 0,2,1,4,>, 
 <internal >, 
 <realidx 0,2,1,4,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191067488)
}
++++++++++++++++++++++++++++++++++++++++++++++++
***input tile torch.Size([1, 1, 22, 15])
id 140372191117808
== tiled conv2d forward
input shape torch.Size([1, 1, 22, 15])
shape input_tile_for_next
 torch.Size([1, 1, 20, 14])
1 out_temp torch.Size([1, 1, 20, 14])
max 1 torch.Size([1, 1, 10, 7])
id 140372191067488
== tiled conv2d forward
input shape torch.Size([1, 1, 10, 7])
shape input_tile_for_next
 torch.Size([1, 1, 8, 6])
2 out_temp torch.Size([1, 1, 8, 6])
max 2 torch.Size([1, 4, 3])
(3, 4) [3, 4] [0, 2, 1, 4]
coord [0, 2, 1, 4]
out_temp torch.Size([1, 1, 4, 3])
coord [1, 1]
bwd_out_shape  (4, 4)
fwd_out_shape  (2, 2)
Yes, fwd is smaller
------------------------------
f_info {-11: fake[-11,-11] PI( <1,1,>,
 <otileshape 4,4,>,
 <padding >,
 <inpslidx 1,4,1,4,>, 
 <internal >, 
 <realidx 1,4,1,4,>, 
 <ndtsize >, 
  local_first False
 next_id -11)
, 140372191067248: maxpool2d140372191067248[0,-1] PI( <1,1,>,
 <otileshape 4,4,>,
 <padding >,
 <inpslidx 2,9,2,9,>, 
 <internal >, 
 <realidx 2,9,2,9,>, 
 <ndtsize 2,2,>, 
  local_first False
 next_id -99)
, 140372191067488: conv2d140372191067488[1,0] PI( <1,1,>,
 <otileshape 8,8,>,
 <padding 0,0,0,0,>,
 <inpslidx 1,10,1,10,>, 
 <internal 1,1,1,1,>, 
 <realidx 1,10,1,10,>, 
 <ndtsize 4,4,>, 
  local_first True
 next_id 140372191067248)
, 140372191117952: maxpool2d140372191117952[2,-1] PI( <1,1,>,
 <otileshape 10,10,>,
 <padding >,
 <inpslidx 2,21,2,21,>, 
 <internal >, 
 <realidx 2,21,2,21,>, 
 <ndtsize 4,4,>, 
  local_first False
 next_id 140372191067488)
, 140372191117808: conv2d140372191117808[3,0] PI( <1,1,>,
 <otileshape 20,20,>,
 <padding 0,0,0,0,>,
 <inpslidx 1,22,1,22,>, 
 <internal 1,1,1,1,>, 
 <realidx 1,22,1,22,>, 
 <ndtsize 8,8,>, 
  local_first True
 next_id 140372191117952)
}
------------------------------
b_info {140372191117808: bk-conv2d140372191117808[0,0] PI( <1,1,>,
 <otileshape 10,10,>,
 <padding 0,0,0,0,>,
 <inpslidx 7,16,7,16,>, 
 <internal 1,1,1,1,>, 
 <realidx 7,16,7,16,>, 
 <ndtsize >, 
  local_first False
 next_id -99)
, 140372191117952: bk-maxpool2d140372191117952[1,-1] PI( <1,1,>,
 <otileshape 6,6,>,
 <padding >,
 <inpslidx 3,8,3,8,>, 
 <internal >, 
 <realidx 3,8,3,8,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191117808)
, 140372191067488: bk-conv2d140372191067488[2,0] PI( <1,1,>,
 <otileshape 8,8,>,
 <padding 0,0,0,0,>,
 <inpslidx 2,9,2,9,>, 
 <internal 1,1,1,1,>, 
 <realidx 2,9,2,9,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191117952)
, 140372191067248: bk-maxpool2d140372191067248[3,-1] PI( <1,1,>,
 <otileshape 4,4,>,
 <padding >,
 <inpslidx 1,4,1,4,>, 
 <internal >, 
 <realidx 1,4,1,4,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191067488)
}
++++++++++++++++++++++++++++++++++++++++++++++++
***input tile torch.Size([1, 1, 22, 22])
id 140372191117808
== tiled conv2d forward
input shape torch.Size([1, 1, 22, 22])
shape input_tile_for_next
 torch.Size([1, 1, 20, 20])
1 out_temp torch.Size([1, 1, 20, 20])
max 1 torch.Size([1, 1, 10, 10])
id 140372191067488
== tiled conv2d forward
input shape torch.Size([1, 1, 10, 10])
shape input_tile_for_next
 torch.Size([1, 1, 8, 8])
2 out_temp torch.Size([1, 1, 8, 8])
max 2 torch.Size([1, 4, 4])
(4, 4) [4, 4] [1, 4, 1, 4]
coord [1, 4, 1, 4]
out_temp torch.Size([1, 1, 4, 4])
coord [1, 2]
bwd_out_shape  (4, 4)
fwd_out_shape  (2, 2)
Yes, fwd is smaller
------------------------------
f_info {-11: fake[-11,-11] PI( <1,2,>,
 <otileshape 4,4,>,
 <padding >,
 <inpslidx 3,6,1,4,>, 
 <internal >, 
 <realidx 3,6,1,4,>, 
 <ndtsize >, 
  local_first False
 next_id -11)
, 140372191067248: maxpool2d140372191067248[0,-1] PI( <1,2,>,
 <otileshape 4,4,>,
 <padding >,
 <inpslidx 6,13,2,9,>, 
 <internal >, 
 <realidx 6,13,2,9,>, 
 <ndtsize 2,2,>, 
  local_first False
 next_id -99)
, 140372191067488: conv2d140372191067488[1,0] PI( <1,2,>,
 <otileshape 8,8,>,
 <padding 0,0,0,0,>,
 <inpslidx 5,14,1,10,>, 
 <internal 1,1,1,1,>, 
 <realidx 5,14,1,10,>, 
 <ndtsize 4,4,>, 
  local_first True
 next_id 140372191067248)
, 140372191117952: maxpool2d140372191117952[2,-1] PI( <1,2,>,
 <otileshape 10,10,>,
 <padding >,
 <inpslidx 10,29,2,21,>, 
 <internal >, 
 <realidx 10,29,2,21,>, 
 <ndtsize 4,4,>, 
  local_first False
 next_id 140372191067488)
, 140372191117808: conv2d140372191117808[3,0] PI( <1,2,>,
 <otileshape 20,20,>,
 <padding 0,0,0,0,>,
 <inpslidx 9,30,1,22,>, 
 <internal 1,1,1,1,>, 
 <realidx 9,30,1,22,>, 
 <ndtsize 8,8,>, 
  local_first True
 next_id 140372191117952)
}
------------------------------
b_info {140372191117808: bk-conv2d140372191117808[0,0] PI( <1,2,>,
 <otileshape 10,10,>,
 <padding 0,0,0,0,>,
 <inpslidx 15,24,7,16,>, 
 <internal 1,1,1,1,>, 
 <realidx 15,24,7,16,>, 
 <ndtsize >, 
  local_first False
 next_id -99)
, 140372191117952: bk-maxpool2d140372191117952[1,-1] PI( <1,2,>,
 <otileshape 6,6,>,
 <padding >,
 <inpslidx 7,12,3,8,>, 
 <internal >, 
 <realidx 7,12,3,8,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191117808)
, 140372191067488: bk-conv2d140372191067488[2,0] PI( <1,2,>,
 <otileshape 8,8,>,
 <padding 0,0,0,0,>,
 <inpslidx 6,13,2,9,>, 
 <internal 1,1,1,1,>, 
 <realidx 6,13,2,9,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191117952)
, 140372191067248: bk-maxpool2d140372191067248[3,-1] PI( <1,2,>,
 <otileshape 4,4,>,
 <padding >,
 <inpslidx 3,6,1,4,>, 
 <internal >, 
 <realidx 3,6,1,4,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191067488)
}
++++++++++++++++++++++++++++++++++++++++++++++++
***input tile torch.Size([1, 1, 22, 22])
id 140372191117808
== tiled conv2d forward
input shape torch.Size([1, 1, 22, 22])
shape input_tile_for_next
 torch.Size([1, 1, 20, 20])
1 out_temp torch.Size([1, 1, 20, 20])
max 1 torch.Size([1, 1, 10, 10])
id 140372191067488
== tiled conv2d forward
input shape torch.Size([1, 1, 10, 10])
shape input_tile_for_next
 torch.Size([1, 1, 8, 8])
2 out_temp torch.Size([1, 1, 8, 8])
max 2 torch.Size([1, 4, 4])
(4, 4) [4, 4] [3, 6, 1, 4]
coord [3, 6, 1, 4]
out_temp torch.Size([1, 1, 4, 4])
coord [1, 3]
bwd_out_shape  (3, 4)
fwd_out_shape  (2, 2)
Yes, fwd is smaller
------------------------------
f_info {-11: fake[-11,-11] PI( <1,3,>,
 <otileshape 3,4,>,
 <padding >,
 <inpslidx 5,7,1,4,>, 
 <internal >, 
 <realidx 5,7,1,4,>, 
 <ndtsize >, 
  local_first False
 next_id -11)
, 140372191067248: maxpool2d140372191067248[0,-1] PI( <1,3,>,
 <otileshape 3,4,>,
 <padding >,
 <inpslidx 10,15,2,9,>, 
 <internal >, 
 <realidx 10,15,2,9,>, 
 <ndtsize 2,2,>, 
  local_first False
 next_id -99)
, 140372191067488: conv2d140372191067488[1,0] PI( <1,3,>,
 <otileshape 6,8,>,
 <padding 0,1,0,0,>,
 <inpslidx 9,15,1,10,>, 
 <internal 1,0,1,1,>, 
 <realidx 9,16,1,10,>, 
 <ndtsize 4,4,>, 
  local_first True
 next_id 140372191067248)
, 140372191117952: maxpool2d140372191117952[2,-1] PI( <1,3,>,
 <otileshape 7,10,>,
 <padding >,
 <inpslidx 18,31,2,21,>, 
 <internal >, 
 <realidx 18,31,2,21,>, 
 <ndtsize 4,4,>, 
  local_first False
 next_id 140372191067488)
, 140372191117808: conv2d140372191117808[3,0] PI( <1,3,>,
 <otileshape 14,20,>,
 <padding 0,1,0,0,>,
 <inpslidx 17,31,1,22,>, 
 <internal 1,0,1,1,>, 
 <realidx 17,32,1,22,>, 
 <ndtsize 8,8,>, 
  local_first True
 next_id 140372191117952)
}
------------------------------
b_info {140372191117808: bk-conv2d140372191117808[0,0] PI( <1,3,>,
 <otileshape 9,10,>,
 <padding 0,1,0,0,>,
 <inpslidx 23,31,7,16,>, 
 <internal 1,0,1,1,>, 
 <realidx 23,32,7,16,>, 
 <ndtsize >, 
  local_first False
 next_id -99)
, 140372191117952: bk-maxpool2d140372191117952[1,-1] PI( <1,3,>,
 <otileshape 5,6,>,
 <padding >,
 <inpslidx 11,15,3,8,>, 
 <internal >, 
 <realidx 11,15,3,8,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191117808)
, 140372191067488: bk-conv2d140372191067488[2,0] PI( <1,3,>,
 <otileshape 6,8,>,
 <padding 0,1,0,0,>,
 <inpslidx 10,15,2,9,>, 
 <internal 1,0,1,1,>, 
 <realidx 10,16,2,9,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191117952)
, 140372191067248: bk-maxpool2d140372191067248[3,-1] PI( <1,3,>,
 <otileshape 3,4,>,
 <padding >,
 <inpslidx 5,7,1,4,>, 
 <internal >, 
 <realidx 5,7,1,4,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191067488)
}
++++++++++++++++++++++++++++++++++++++++++++++++
***input tile torch.Size([1, 1, 22, 15])
id 140372191117808
== tiled conv2d forward
input shape torch.Size([1, 1, 22, 15])
shape input_tile_for_next
 torch.Size([1, 1, 20, 14])
1 out_temp torch.Size([1, 1, 20, 14])
max 1 torch.Size([1, 1, 10, 7])
id 140372191067488
== tiled conv2d forward
input shape torch.Size([1, 1, 10, 7])
shape input_tile_for_next
 torch.Size([1, 1, 8, 6])
2 out_temp torch.Size([1, 1, 8, 6])
max 2 torch.Size([1, 4, 3])
(3, 4) [3, 4] [5, 7, 1, 4]
coord [5, 7, 1, 4]
out_temp torch.Size([1, 1, 4, 3])
coord [2, 0]
bwd_out_shape  (3, 4)
fwd_out_shape  (2, 2)
Yes, fwd is smaller
------------------------------
f_info {-11: fake[-11,-11] PI( <2,0,>,
 <otileshape 3,4,>,
 <padding >,
 <inpslidx 0,2,3,6,>, 
 <internal >, 
 <realidx 0,2,3,6,>, 
 <ndtsize >, 
  local_first False
 next_id -11)
, 140372191067248: maxpool2d140372191067248[0,-1] PI( <2,0,>,
 <otileshape 3,4,>,
 <padding >,
 <inpslidx 0,5,6,13,>, 
 <internal >, 
 <realidx 0,5,6,13,>, 
 <ndtsize 2,2,>, 
  local_first False
 next_id -99)
, 140372191067488: conv2d140372191067488[1,0] PI( <2,0,>,
 <otileshape 6,8,>,
 <padding 1,0,0,0,>,
 <inpslidx 0,6,5,14,>, 
 <internal 0,1,1,1,>, 
 <realidx -1,6,5,14,>, 
 <ndtsize 4,4,>, 
  local_first True
 next_id 140372191067248)
, 140372191117952: maxpool2d140372191117952[2,-1] PI( <2,0,>,
 <otileshape 7,10,>,
 <padding >,
 <inpslidx 0,13,10,29,>, 
 <internal >, 
 <realidx 0,13,10,29,>, 
 <ndtsize 4,4,>, 
  local_first False
 next_id 140372191067488)
, 140372191117808: conv2d140372191117808[3,0] PI( <2,0,>,
 <otileshape 14,20,>,
 <padding 1,0,0,0,>,
 <inpslidx 0,14,9,30,>, 
 <internal 0,1,1,1,>, 
 <realidx -1,14,9,30,>, 
 <ndtsize 8,8,>, 
  local_first True
 next_id 140372191117952)
}
------------------------------
b_info {140372191117808: bk-conv2d140372191117808[0,0] PI( <2,0,>,
 <otileshape 9,10,>,
 <padding 1,0,0,0,>,
 <inpslidx 0,8,15,24,>, 
 <internal 0,1,1,1,>, 
 <realidx -1,8,15,24,>, 
 <ndtsize >, 
  local_first False
 next_id -99)
, 140372191117952: bk-maxpool2d140372191117952[1,-1] PI( <2,0,>,
 <otileshape 5,6,>,
 <padding >,
 <inpslidx 0,4,7,12,>, 
 <internal >, 
 <realidx 0,4,7,12,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191117808)
, 140372191067488: bk-conv2d140372191067488[2,0] PI( <2,0,>,
 <otileshape 6,8,>,
 <padding 1,0,0,0,>,
 <inpslidx 0,5,6,13,>, 
 <internal 0,1,1,1,>, 
 <realidx -1,5,6,13,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191117952)
, 140372191067248: bk-maxpool2d140372191067248[3,-1] PI( <2,0,>,
 <otileshape 3,4,>,
 <padding >,
 <inpslidx 0,2,3,6,>, 
 <internal >, 
 <realidx 0,2,3,6,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191067488)
}
++++++++++++++++++++++++++++++++++++++++++++++++
***input tile torch.Size([1, 1, 22, 15])
id 140372191117808
== tiled conv2d forward
input shape torch.Size([1, 1, 22, 15])
shape input_tile_for_next
 torch.Size([1, 1, 20, 14])
1 out_temp torch.Size([1, 1, 20, 14])
max 1 torch.Size([1, 1, 10, 7])
id 140372191067488
== tiled conv2d forward
input shape torch.Size([1, 1, 10, 7])
shape input_tile_for_next
 torch.Size([1, 1, 8, 6])
2 out_temp torch.Size([1, 1, 8, 6])
max 2 torch.Size([1, 4, 3])
(3, 4) [3, 4] [0, 2, 3, 6]
coord [0, 2, 3, 6]
out_temp torch.Size([1, 1, 4, 3])
coord [2, 1]
bwd_out_shape  (4, 4)
fwd_out_shape  (2, 2)
Yes, fwd is smaller
------------------------------
f_info {-11: fake[-11,-11] PI( <2,1,>,
 <otileshape 4,4,>,
 <padding >,
 <inpslidx 1,4,3,6,>, 
 <internal >, 
 <realidx 1,4,3,6,>, 
 <ndtsize >, 
  local_first False
 next_id -11)
, 140372191067248: maxpool2d140372191067248[0,-1] PI( <2,1,>,
 <otileshape 4,4,>,
 <padding >,
 <inpslidx 2,9,6,13,>, 
 <internal >, 
 <realidx 2,9,6,13,>, 
 <ndtsize 2,2,>, 
  local_first False
 next_id -99)
, 140372191067488: conv2d140372191067488[1,0] PI( <2,1,>,
 <otileshape 8,8,>,
 <padding 0,0,0,0,>,
 <inpslidx 1,10,5,14,>, 
 <internal 1,1,1,1,>, 
 <realidx 1,10,5,14,>, 
 <ndtsize 4,4,>, 
  local_first True
 next_id 140372191067248)
, 140372191117952: maxpool2d140372191117952[2,-1] PI( <2,1,>,
 <otileshape 10,10,>,
 <padding >,
 <inpslidx 2,21,10,29,>, 
 <internal >, 
 <realidx 2,21,10,29,>, 
 <ndtsize 4,4,>, 
  local_first False
 next_id 140372191067488)
, 140372191117808: conv2d140372191117808[3,0] PI( <2,1,>,
 <otileshape 20,20,>,
 <padding 0,0,0,0,>,
 <inpslidx 1,22,9,30,>, 
 <internal 1,1,1,1,>, 
 <realidx 1,22,9,30,>, 
 <ndtsize 8,8,>, 
  local_first True
 next_id 140372191117952)
}
------------------------------
b_info {140372191117808: bk-conv2d140372191117808[0,0] PI( <2,1,>,
 <otileshape 10,10,>,
 <padding 0,0,0,0,>,
 <inpslidx 7,16,15,24,>, 
 <internal 1,1,1,1,>, 
 <realidx 7,16,15,24,>, 
 <ndtsize >, 
  local_first False
 next_id -99)
, 140372191117952: bk-maxpool2d140372191117952[1,-1] PI( <2,1,>,
 <otileshape 6,6,>,
 <padding >,
 <inpslidx 3,8,7,12,>, 
 <internal >, 
 <realidx 3,8,7,12,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191117808)
, 140372191067488: bk-conv2d140372191067488[2,0] PI( <2,1,>,
 <otileshape 8,8,>,
 <padding 0,0,0,0,>,
 <inpslidx 2,9,6,13,>, 
 <internal 1,1,1,1,>, 
 <realidx 2,9,6,13,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191117952)
, 140372191067248: bk-maxpool2d140372191067248[3,-1] PI( <2,1,>,
 <otileshape 4,4,>,
 <padding >,
 <inpslidx 1,4,3,6,>, 
 <internal >, 
 <realidx 1,4,3,6,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191067488)
}
++++++++++++++++++++++++++++++++++++++++++++++++
***input tile torch.Size([1, 1, 22, 22])
id 140372191117808
== tiled conv2d forward
input shape torch.Size([1, 1, 22, 22])
shape input_tile_for_next
 torch.Size([1, 1, 20, 20])
1 out_temp torch.Size([1, 1, 20, 20])
max 1 torch.Size([1, 1, 10, 10])
id 140372191067488
== tiled conv2d forward
input shape torch.Size([1, 1, 10, 10])
shape input_tile_for_next
 torch.Size([1, 1, 8, 8])
2 out_temp torch.Size([1, 1, 8, 8])
max 2 torch.Size([1, 4, 4])
(4, 4) [4, 4] [1, 4, 3, 6]
coord [1, 4, 3, 6]
out_temp torch.Size([1, 1, 4, 4])
coord [2, 2]
bwd_out_shape  (4, 4)
fwd_out_shape  (2, 2)
Yes, fwd is smaller
------------------------------
f_info {-11: fake[-11,-11] PI( <2,2,>,
 <otileshape 4,4,>,
 <padding >,
 <inpslidx 3,6,3,6,>, 
 <internal >, 
 <realidx 3,6,3,6,>, 
 <ndtsize >, 
  local_first False
 next_id -11)
, 140372191067248: maxpool2d140372191067248[0,-1] PI( <2,2,>,
 <otileshape 4,4,>,
 <padding >,
 <inpslidx 6,13,6,13,>, 
 <internal >, 
 <realidx 6,13,6,13,>, 
 <ndtsize 2,2,>, 
  local_first False
 next_id -99)
, 140372191067488: conv2d140372191067488[1,0] PI( <2,2,>,
 <otileshape 8,8,>,
 <padding 0,0,0,0,>,
 <inpslidx 5,14,5,14,>, 
 <internal 1,1,1,1,>, 
 <realidx 5,14,5,14,>, 
 <ndtsize 4,4,>, 
  local_first True
 next_id 140372191067248)
, 140372191117952: maxpool2d140372191117952[2,-1] PI( <2,2,>,
 <otileshape 10,10,>,
 <padding >,
 <inpslidx 10,29,10,29,>, 
 <internal >, 
 <realidx 10,29,10,29,>, 
 <ndtsize 4,4,>, 
  local_first False
 next_id 140372191067488)
, 140372191117808: conv2d140372191117808[3,0] PI( <2,2,>,
 <otileshape 20,20,>,
 <padding 0,0,0,0,>,
 <inpslidx 9,30,9,30,>, 
 <internal 1,1,1,1,>, 
 <realidx 9,30,9,30,>, 
 <ndtsize 8,8,>, 
  local_first True
 next_id 140372191117952)
}
------------------------------
b_info {140372191117808: bk-conv2d140372191117808[0,0] PI( <2,2,>,
 <otileshape 10,10,>,
 <padding 0,0,0,0,>,
 <inpslidx 15,24,15,24,>, 
 <internal 1,1,1,1,>, 
 <realidx 15,24,15,24,>, 
 <ndtsize >, 
  local_first False
 next_id -99)
, 140372191117952: bk-maxpool2d140372191117952[1,-1] PI( <2,2,>,
 <otileshape 6,6,>,
 <padding >,
 <inpslidx 7,12,7,12,>, 
 <internal >, 
 <realidx 7,12,7,12,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191117808)
, 140372191067488: bk-conv2d140372191067488[2,0] PI( <2,2,>,
 <otileshape 8,8,>,
 <padding 0,0,0,0,>,
 <inpslidx 6,13,6,13,>, 
 <internal 1,1,1,1,>, 
 <realidx 6,13,6,13,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191117952)
, 140372191067248: bk-maxpool2d140372191067248[3,-1] PI( <2,2,>,
 <otileshape 4,4,>,
 <padding >,
 <inpslidx 3,6,3,6,>, 
 <internal >, 
 <realidx 3,6,3,6,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191067488)
}
++++++++++++++++++++++++++++++++++++++++++++++++
***input tile torch.Size([1, 1, 22, 22])
id 140372191117808
== tiled conv2d forward
input shape torch.Size([1, 1, 22, 22])
shape input_tile_for_next
 torch.Size([1, 1, 20, 20])
1 out_temp torch.Size([1, 1, 20, 20])
max 1 torch.Size([1, 1, 10, 10])
id 140372191067488
== tiled conv2d forward
input shape torch.Size([1, 1, 10, 10])
shape input_tile_for_next
 torch.Size([1, 1, 8, 8])
2 out_temp torch.Size([1, 1, 8, 8])
max 2 torch.Size([1, 4, 4])
(4, 4) [4, 4] [3, 6, 3, 6]
coord [3, 6, 3, 6]
out_temp torch.Size([1, 1, 4, 4])
coord [2, 3]
bwd_out_shape  (3, 4)
fwd_out_shape  (2, 2)
Yes, fwd is smaller
------------------------------
f_info {-11: fake[-11,-11] PI( <2,3,>,
 <otileshape 3,4,>,
 <padding >,
 <inpslidx 5,7,3,6,>, 
 <internal >, 
 <realidx 5,7,3,6,>, 
 <ndtsize >, 
  local_first False
 next_id -11)
, 140372191067248: maxpool2d140372191067248[0,-1] PI( <2,3,>,
 <otileshape 3,4,>,
 <padding >,
 <inpslidx 10,15,6,13,>, 
 <internal >, 
 <realidx 10,15,6,13,>, 
 <ndtsize 2,2,>, 
  local_first False
 next_id -99)
, 140372191067488: conv2d140372191067488[1,0] PI( <2,3,>,
 <otileshape 6,8,>,
 <padding 0,1,0,0,>,
 <inpslidx 9,15,5,14,>, 
 <internal 1,0,1,1,>, 
 <realidx 9,16,5,14,>, 
 <ndtsize 4,4,>, 
  local_first True
 next_id 140372191067248)
, 140372191117952: maxpool2d140372191117952[2,-1] PI( <2,3,>,
 <otileshape 7,10,>,
 <padding >,
 <inpslidx 18,31,10,29,>, 
 <internal >, 
 <realidx 18,31,10,29,>, 
 <ndtsize 4,4,>, 
  local_first False
 next_id 140372191067488)
, 140372191117808: conv2d140372191117808[3,0] PI( <2,3,>,
 <otileshape 14,20,>,
 <padding 0,1,0,0,>,
 <inpslidx 17,31,9,30,>, 
 <internal 1,0,1,1,>, 
 <realidx 17,32,9,30,>, 
 <ndtsize 8,8,>, 
  local_first True
 next_id 140372191117952)
}
------------------------------
b_info {140372191117808: bk-conv2d140372191117808[0,0] PI( <2,3,>,
 <otileshape 9,10,>,
 <padding 0,1,0,0,>,
 <inpslidx 23,31,15,24,>, 
 <internal 1,0,1,1,>, 
 <realidx 23,32,15,24,>, 
 <ndtsize >, 
  local_first False
 next_id -99)
, 140372191117952: bk-maxpool2d140372191117952[1,-1] PI( <2,3,>,
 <otileshape 5,6,>,
 <padding >,
 <inpslidx 11,15,7,12,>, 
 <internal >, 
 <realidx 11,15,7,12,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191117808)
, 140372191067488: bk-conv2d140372191067488[2,0] PI( <2,3,>,
 <otileshape 6,8,>,
 <padding 0,1,0,0,>,
 <inpslidx 10,15,6,13,>, 
 <internal 1,0,1,1,>, 
 <realidx 10,16,6,13,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191117952)
, 140372191067248: bk-maxpool2d140372191067248[3,-1] PI( <2,3,>,
 <otileshape 3,4,>,
 <padding >,
 <inpslidx 5,7,3,6,>, 
 <internal >, 
 <realidx 5,7,3,6,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191067488)
}
++++++++++++++++++++++++++++++++++++++++++++++++
***input tile torch.Size([1, 1, 22, 15])
id 140372191117808
== tiled conv2d forward
input shape torch.Size([1, 1, 22, 15])
shape input_tile_for_next
 torch.Size([1, 1, 20, 14])
1 out_temp torch.Size([1, 1, 20, 14])
max 1 torch.Size([1, 1, 10, 7])
id 140372191067488
== tiled conv2d forward
input shape torch.Size([1, 1, 10, 7])
shape input_tile_for_next
 torch.Size([1, 1, 8, 6])
2 out_temp torch.Size([1, 1, 8, 6])
max 2 torch.Size([1, 4, 3])
(3, 4) [3, 4] [5, 7, 3, 6]
coord [5, 7, 3, 6]
out_temp torch.Size([1, 1, 4, 3])
coord [3, 0]
bwd_out_shape  (3, 3)
fwd_out_shape  (2, 2)
Yes, fwd is smaller
------------------------------
f_info {-11: fake[-11,-11] PI( <3,0,>,
 <otileshape 3,3,>,
 <padding >,
 <inpslidx 0,2,5,7,>, 
 <internal >, 
 <realidx 0,2,5,7,>, 
 <ndtsize >, 
  local_first False
 next_id -11)
, 140372191067248: maxpool2d140372191067248[0,-1] PI( <3,0,>,
 <otileshape 3,3,>,
 <padding >,
 <inpslidx 0,5,10,15,>, 
 <internal >, 
 <realidx 0,5,10,15,>, 
 <ndtsize 2,2,>, 
  local_first False
 next_id -99)
, 140372191067488: conv2d140372191067488[1,0] PI( <3,0,>,
 <otileshape 6,6,>,
 <padding 1,0,0,1,>,
 <inpslidx 0,6,9,15,>, 
 <internal 0,1,1,0,>, 
 <realidx -1,6,9,16,>, 
 <ndtsize 4,4,>, 
  local_first True
 next_id 140372191067248)
, 140372191117952: maxpool2d140372191117952[2,-1] PI( <3,0,>,
 <otileshape 7,7,>,
 <padding >,
 <inpslidx 0,13,18,31,>, 
 <internal >, 
 <realidx 0,13,18,31,>, 
 <ndtsize 4,4,>, 
  local_first False
 next_id 140372191067488)
, 140372191117808: conv2d140372191117808[3,0] PI( <3,0,>,
 <otileshape 14,14,>,
 <padding 1,0,0,1,>,
 <inpslidx 0,14,17,31,>, 
 <internal 0,1,1,0,>, 
 <realidx -1,14,17,32,>, 
 <ndtsize 8,8,>, 
  local_first True
 next_id 140372191117952)
}
------------------------------
b_info {140372191117808: bk-conv2d140372191117808[0,0] PI( <3,0,>,
 <otileshape 9,9,>,
 <padding 1,0,0,1,>,
 <inpslidx 0,8,23,31,>, 
 <internal 0,1,1,0,>, 
 <realidx -1,8,23,32,>, 
 <ndtsize >, 
  local_first False
 next_id -99)
, 140372191117952: bk-maxpool2d140372191117952[1,-1] PI( <3,0,>,
 <otileshape 5,5,>,
 <padding >,
 <inpslidx 0,4,11,15,>, 
 <internal >, 
 <realidx 0,4,11,15,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191117808)
, 140372191067488: bk-conv2d140372191067488[2,0] PI( <3,0,>,
 <otileshape 6,6,>,
 <padding 1,0,0,1,>,
 <inpslidx 0,5,10,15,>, 
 <internal 0,1,1,0,>, 
 <realidx -1,5,10,16,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191117952)
, 140372191067248: bk-maxpool2d140372191067248[3,-1] PI( <3,0,>,
 <otileshape 3,3,>,
 <padding >,
 <inpslidx 0,2,5,7,>, 
 <internal >, 
 <realidx 0,2,5,7,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191067488)
}
++++++++++++++++++++++++++++++++++++++++++++++++
***input tile torch.Size([1, 1, 15, 15])
id 140372191117808
== tiled conv2d forward
input shape torch.Size([1, 1, 15, 15])
shape input_tile_for_next
 torch.Size([1, 1, 14, 14])
1 out_temp torch.Size([1, 1, 14, 14])
max 1 torch.Size([1, 1, 7, 7])
id 140372191067488
== tiled conv2d forward
input shape torch.Size([1, 1, 7, 7])
shape input_tile_for_next
 torch.Size([1, 1, 6, 6])
2 out_temp torch.Size([1, 1, 6, 6])
max 2 torch.Size([1, 3, 3])
(3, 3) [3, 3] [0, 2, 5, 7]
coord [0, 2, 5, 7]
out_temp torch.Size([1, 1, 3, 3])
coord [3, 1]
bwd_out_shape  (4, 3)
fwd_out_shape  (2, 2)
Yes, fwd is smaller
------------------------------
f_info {-11: fake[-11,-11] PI( <3,1,>,
 <otileshape 4,3,>,
 <padding >,
 <inpslidx 1,4,5,7,>, 
 <internal >, 
 <realidx 1,4,5,7,>, 
 <ndtsize >, 
  local_first False
 next_id -11)
, 140372191067248: maxpool2d140372191067248[0,-1] PI( <3,1,>,
 <otileshape 4,3,>,
 <padding >,
 <inpslidx 2,9,10,15,>, 
 <internal >, 
 <realidx 2,9,10,15,>, 
 <ndtsize 2,2,>, 
  local_first False
 next_id -99)
, 140372191067488: conv2d140372191067488[1,0] PI( <3,1,>,
 <otileshape 8,6,>,
 <padding 0,0,0,1,>,
 <inpslidx 1,10,9,15,>, 
 <internal 1,1,1,0,>, 
 <realidx 1,10,9,16,>, 
 <ndtsize 4,4,>, 
  local_first True
 next_id 140372191067248)
, 140372191117952: maxpool2d140372191117952[2,-1] PI( <3,1,>,
 <otileshape 10,7,>,
 <padding >,
 <inpslidx 2,21,18,31,>, 
 <internal >, 
 <realidx 2,21,18,31,>, 
 <ndtsize 4,4,>, 
  local_first False
 next_id 140372191067488)
, 140372191117808: conv2d140372191117808[3,0] PI( <3,1,>,
 <otileshape 20,14,>,
 <padding 0,0,0,1,>,
 <inpslidx 1,22,17,31,>, 
 <internal 1,1,1,0,>, 
 <realidx 1,22,17,32,>, 
 <ndtsize 8,8,>, 
  local_first True
 next_id 140372191117952)
}
------------------------------
b_info {140372191117808: bk-conv2d140372191117808[0,0] PI( <3,1,>,
 <otileshape 10,9,>,
 <padding 0,0,0,1,>,
 <inpslidx 7,16,23,31,>, 
 <internal 1,1,1,0,>, 
 <realidx 7,16,23,32,>, 
 <ndtsize >, 
  local_first False
 next_id -99)
, 140372191117952: bk-maxpool2d140372191117952[1,-1] PI( <3,1,>,
 <otileshape 6,5,>,
 <padding >,
 <inpslidx 3,8,11,15,>, 
 <internal >, 
 <realidx 3,8,11,15,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191117808)
, 140372191067488: bk-conv2d140372191067488[2,0] PI( <3,1,>,
 <otileshape 8,6,>,
 <padding 0,0,0,1,>,
 <inpslidx 2,9,10,15,>, 
 <internal 1,1,1,0,>, 
 <realidx 2,9,10,16,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191117952)
, 140372191067248: bk-maxpool2d140372191067248[3,-1] PI( <3,1,>,
 <otileshape 4,3,>,
 <padding >,
 <inpslidx 1,4,5,7,>, 
 <internal >, 
 <realidx 1,4,5,7,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191067488)
}
++++++++++++++++++++++++++++++++++++++++++++++++
***input tile torch.Size([1, 1, 15, 22])
id 140372191117808
== tiled conv2d forward
input shape torch.Size([1, 1, 15, 22])
shape input_tile_for_next
 torch.Size([1, 1, 14, 20])
1 out_temp torch.Size([1, 1, 14, 20])
max 1 torch.Size([1, 1, 7, 10])
id 140372191067488
== tiled conv2d forward
input shape torch.Size([1, 1, 7, 10])
shape input_tile_for_next
 torch.Size([1, 1, 6, 8])
2 out_temp torch.Size([1, 1, 6, 8])
max 2 torch.Size([1, 3, 4])
(4, 3) [4, 3] [1, 4, 5, 7]
coord [1, 4, 5, 7]
out_temp torch.Size([1, 1, 3, 4])
coord [3, 2]
bwd_out_shape  (4, 3)
fwd_out_shape  (2, 2)
Yes, fwd is smaller
------------------------------
f_info {-11: fake[-11,-11] PI( <3,2,>,
 <otileshape 4,3,>,
 <padding >,
 <inpslidx 3,6,5,7,>, 
 <internal >, 
 <realidx 3,6,5,7,>, 
 <ndtsize >, 
  local_first False
 next_id -11)
, 140372191067248: maxpool2d140372191067248[0,-1] PI( <3,2,>,
 <otileshape 4,3,>,
 <padding >,
 <inpslidx 6,13,10,15,>, 
 <internal >, 
 <realidx 6,13,10,15,>, 
 <ndtsize 2,2,>, 
  local_first False
 next_id -99)
, 140372191067488: conv2d140372191067488[1,0] PI( <3,2,>,
 <otileshape 8,6,>,
 <padding 0,0,0,1,>,
 <inpslidx 5,14,9,15,>, 
 <internal 1,1,1,0,>, 
 <realidx 5,14,9,16,>, 
 <ndtsize 4,4,>, 
  local_first True
 next_id 140372191067248)
, 140372191117952: maxpool2d140372191117952[2,-1] PI( <3,2,>,
 <otileshape 10,7,>,
 <padding >,
 <inpslidx 10,29,18,31,>, 
 <internal >, 
 <realidx 10,29,18,31,>, 
 <ndtsize 4,4,>, 
  local_first False
 next_id 140372191067488)
, 140372191117808: conv2d140372191117808[3,0] PI( <3,2,>,
 <otileshape 20,14,>,
 <padding 0,0,0,1,>,
 <inpslidx 9,30,17,31,>, 
 <internal 1,1,1,0,>, 
 <realidx 9,30,17,32,>, 
 <ndtsize 8,8,>, 
  local_first True
 next_id 140372191117952)
}
------------------------------
b_info {140372191117808: bk-conv2d140372191117808[0,0] PI( <3,2,>,
 <otileshape 10,9,>,
 <padding 0,0,0,1,>,
 <inpslidx 15,24,23,31,>, 
 <internal 1,1,1,0,>, 
 <realidx 15,24,23,32,>, 
 <ndtsize >, 
  local_first False
 next_id -99)
, 140372191117952: bk-maxpool2d140372191117952[1,-1] PI( <3,2,>,
 <otileshape 6,5,>,
 <padding >,
 <inpslidx 7,12,11,15,>, 
 <internal >, 
 <realidx 7,12,11,15,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191117808)
, 140372191067488: bk-conv2d140372191067488[2,0] PI( <3,2,>,
 <otileshape 8,6,>,
 <padding 0,0,0,1,>,
 <inpslidx 6,13,10,15,>, 
 <internal 1,1,1,0,>, 
 <realidx 6,13,10,16,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191117952)
, 140372191067248: bk-maxpool2d140372191067248[3,-1] PI( <3,2,>,
 <otileshape 4,3,>,
 <padding >,
 <inpslidx 3,6,5,7,>, 
 <internal >, 
 <realidx 3,6,5,7,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191067488)
}
++++++++++++++++++++++++++++++++++++++++++++++++
***input tile torch.Size([1, 1, 15, 22])
id 140372191117808
== tiled conv2d forward
input shape torch.Size([1, 1, 15, 22])
shape input_tile_for_next
 torch.Size([1, 1, 14, 20])
1 out_temp torch.Size([1, 1, 14, 20])
max 1 torch.Size([1, 1, 7, 10])
id 140372191067488
== tiled conv2d forward
input shape torch.Size([1, 1, 7, 10])
shape input_tile_for_next
 torch.Size([1, 1, 6, 8])
2 out_temp torch.Size([1, 1, 6, 8])
max 2 torch.Size([1, 3, 4])
(4, 3) [4, 3] [3, 6, 5, 7]
coord [3, 6, 5, 7]
out_temp torch.Size([1, 1, 3, 4])
coord [3, 3]
bwd_out_shape  (3, 3)
fwd_out_shape  (2, 2)
[torch/csrc/autograd/engine.cpp] call_function SumBackward0
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function MaxPool2DWithIndicesBackward
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function CudnnConvolutionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
Yes, fwd is smaller
------------------------------
f_info {-11: fake[-11,-11] PI( <3,3,>,
 <otileshape 3,3,>,
 <padding >,
 <inpslidx 5,7,5,7,>, 
 <internal >, 
 <realidx 5,7,5,7,>, 
 <ndtsize >, 
  local_first False
 next_id -11)
, 140372191067248: maxpool2d140372191067248[0,-1] PI( <3,3,>,
 <otileshape 3,3,>,
 <padding >,
 <inpslidx 10,15,10,15,>, 
 <internal >, 
 <realidx 10,15,10,15,>, 
 <ndtsize 2,2,>, 
  local_first False
 next_id -99)
, 140372191067488: conv2d140372191067488[1,0] PI( <3,3,>,
 <otileshape 6,6,>,
 <padding 0,1,0,1,>,
 <inpslidx 9,15,9,15,>, 
 <internal 1,0,1,0,>, 
 <realidx 9,16,9,16,>, 
 <ndtsize 4,4,>, 
  local_first True
 next_id 140372191067248)
, 140372191117952: maxpool2d140372191117952[2,-1] PI( <3,3,>,
 <otileshape 7,7,>,
 <padding >,
 <inpslidx 18,31,18,31,>, 
 <internal >, 
 <realidx 18,31,18,31,>, 
 <ndtsize 4,4,>, 
  local_first False
 next_id 140372191067488)
, 140372191117808: conv2d140372191117808[3,0] PI( <3,3,>,
 <otileshape 14,14,>,
 <padding 0,1,0,1,>,
 <inpslidx 17,31,17,31,>, 
 <internal 1,0,1,0,>, 
 <realidx 17,32,17,32,>, 
 <ndtsize 8,8,>, 
  local_first True
 next_id 140372191117952)
}
------------------------------
b_info {140372191117808: bk-conv2d140372191117808[0,0] PI( <3,3,>,
 <otileshape 9,9,>,
 <padding 0,1,0,1,>,
 <inpslidx 23,31,23,31,>, 
 <internal 1,0,1,0,>, 
 <realidx 23,32,23,32,>, 
 <ndtsize >, 
  local_first False
 next_id -99)
, 140372191117952: bk-maxpool2d140372191117952[1,-1] PI( <3,3,>,
 <otileshape 5,5,>,
 <padding >,
 <inpslidx 11,15,11,15,>, 
 <internal >, 
 <realidx 11,15,11,15,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191117808)
, 140372191067488: bk-conv2d140372191067488[2,0] PI( <3,3,>,
 <otileshape 6,6,>,
 <padding 0,1,0,1,>,
 <inpslidx 10,15,10,15,>, 
 <internal 1,0,1,0,>, 
 <realidx 10,16,10,16,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191117952)
, 140372191067248: bk-maxpool2d140372191067248[3,-1] PI( <3,3,>,
 <otileshape 3,3,>,
 <padding >,
 <inpslidx 5,7,5,7,>, 
 <internal >, 
 <realidx 5,7,5,7,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191067488)
}
++++++++++++++++++++++++++++++++++++++++++++++++
***input tile torch.Size([1, 1, 15, 15])
id 140372191117808
== tiled conv2d forward
input shape torch.Size([1, 1, 15, 15])
shape input_tile_for_next
 torch.Size([1, 1, 14, 14])
1 out_temp torch.Size([1, 1, 14, 14])
max 1 torch.Size([1, 1, 7, 7])
id 140372191067488
== tiled conv2d forward
input shape torch.Size([1, 1, 7, 7])
shape input_tile_for_next
 torch.Size([1, 1, 6, 6])
2 out_temp torch.Size([1, 1, 6, 6])
max 2 torch.Size([1, 3, 3])
(3, 3) [3, 3] [5, 7, 5, 7]
coord [5, 7, 5, 7]
out_temp torch.Size([1, 1, 3, 3])

&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&

done
Inside MaxPool2d backward
grad_output size :  torch.Size([1, 1, 8, 8])
ref grad_output  :
  tensor([[[[1., 1., 1., 1., 1., 1., 1., 1.],
          [1., 1., 1., 1., 1., 1., 1., 1.],
          [1., 1., 1., 1., 1., 1., 1., 1.],
          [1., 1., 1., 1., 1., 1., 1., 1.],
          [1., 1., 1., 1., 1., 1., 1., 1.],
          [1., 1., 1., 1., 1., 1., 1., 1.],
          [1., 1., 1., 1., 1., 1., 1., 1.],
          [1., 1., 1., 1., 1., 1., 1., 1.]]]], device='cuda:0')
grad_input size :  torch.Size([1, 1, 16, 16])
ref grad_input  : 
 tensor([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
          [0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0.],
          [0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0.],
          [0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.],
          [0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0.],
          [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.],
          [0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0.],
          [0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0.],
          [1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.],
          [0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0.],
          [0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],
          [0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0.],
          [0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]],
       device='cuda:0')
Inside Conv2d backward
grad_output size :  torch.Size([1, 1, 16, 16])
ref grad_output  :
  tensor([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
          [0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0.],
          [0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0.],
          [0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.],
          [0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0.],
          [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.],
          [0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0.],
          [0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0.],
          [1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.],
          [0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0.],
          [0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],
          [0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0.],
          [0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]],
       device='cuda:0')
grad_input size :  torch.Size([1, 1, 16, 16])
ref grad_input  : 
 [torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function MaxPool2DWithIndicesBackward
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
tensor([[[[-2.1738e-01,  6.6999e-02,  5.1049e-01,  8.7237e-03,  2.8438e-01,
            2.2611e-01, -2.1738e-01,  6.6999e-02,  5.1049e-01,  2.2611e-01,
           -2.1738e-01,  2.8438e-01,  8.7237e-03,  6.6999e-02,  5.1049e-01,
            2.2611e-01],
          [ 6.5786e-02,  5.2611e-02,  6.3984e-01,  4.4314e-01,  2.0421e-01,
            1.5125e-01,  6.5786e-02,  5.2611e-02,  6.3984e-01,  3.7736e-01,
            6.5786e-02, -1.3175e-02,  5.0142e-01,  2.7872e-01,  6.3984e-01,
            3.7736e-01],
          [ 1.1362e-01,  9.1585e-01,  5.8409e-01,  4.0136e-01,  1.7296e-02,
           -1.3903e-02,  8.4149e-01,  8.5757e-01,  3.5798e-01, -1.4703e-01,
            6.1539e-01,  5.2657e-01,  4.5431e-01,  7.8272e-01,  3.5798e-01,
            7.0351e-02],
          [-1.5160e-01,  8.1959e-01,  6.1204e-01, -8.0903e-02, -1.5160e-01,
            3.3699e-01,  8.6595e-01,  7.0836e-01,  1.7296e-02,  2.6926e-01,
            4.3031e-01,  2.6488e-01,  5.1906e-01,  4.7621e-01,  2.3468e-01,
           -8.0903e-02],
          [ 3.9679e-01,  4.3888e-01, -1.4703e-01,  2.8438e-01,  6.2290e-01,
            8.3567e-01,  5.0924e-01,  7.0351e-02,  6.5786e-02,  3.1783e-01,
            6.7031e-01,  2.1099e-01,  2.0421e-01,  1.5125e-01, -2.1738e-01,
            2.8438e-01],
          [ 3.3100e-01,  2.3468e-01, -1.5117e-02,  2.0421e-01,  4.8226e-01,
            5.6568e-01,  1.5378e-01, -8.0903e-02,  3.3100e-01,  3.0046e-01,
           -9.4077e-02,  7.6664e-01,  4.6078e-01, -2.9828e-01,  3.5017e-01,
            4.3031e-01],
          [-2.1738e-01,  6.6999e-02,  8.4149e-01,  4.6078e-01, -2.9828e-01,
            6.6999e-02,  5.1049e-01,  2.2611e-01, -2.1738e-01,  6.1539e-01,
            5.2657e-01, -9.4077e-02,  4.3563e-01,  2.9189e-01,  5.3521e-01,
            3.8593e-01],
          [ 6.5786e-02,  5.2611e-02,  6.3984e-01,  3.7736e-01, -1.5160e-01,
            5.5437e-01,  5.8157e-01,  1.5125e-01,  6.5786e-02, -1.3175e-02,
            7.6664e-01,  5.2657e-01, -9.4077e-02,  5.4926e-01,  7.4516e-01,
            1.4520e-01],
          [ 6.1539e-01,  8.5757e-01,  3.5798e-01,  7.0351e-02,  3.9679e-01,
            7.6989e-01,  8.7647e-02, -1.3903e-02,  8.4149e-01,  5.2657e-01,
            1.2330e-01,  4.8226e-01,  3.0046e-01,  1.8909e-01,  3.5546e-01,
            1.5125e-01],
          [ 2.0421e-01,  4.8226e-01,  2.3468e-01, -8.0903e-02,  3.3100e-01,
            1.7296e-02,  2.6926e-01,  4.9610e-01,  3.5546e-01,  4.8226e-01,
            1.7296e-02,  2.0348e-01,  5.5711e-01,  5.6568e-01,  1.5378e-01,
           -8.0903e-02],
          [ 1.7296e-02,  2.0348e-01,  8.7237e-03,  6.6999e-02,  5.1049e-01,
            2.9189e-01,  5.3521e-01,  4.9955e-01,  4.3816e-01,  1.4520e-01,
            6.5786e-02,  2.0421e-01, -6.6128e-02,  6.6999e-02,  5.1049e-01,
            2.2611e-01],
          [ 6.5786e-02,  2.0421e-01, -3.4238e-04,  3.3699e-01,  8.6595e-01,
            7.0836e-01,  2.3468e-01, -1.5117e-02, -1.3175e-02,  4.3563e-01,
            5.5711e-01,  2.3468e-01, -1.5117e-02,  2.6999e-01,  3.5546e-01,
            1.5125e-01],
          [ 1.1362e-01,  5.1906e-01,  5.4199e-01,  8.3567e-01,  5.0924e-01,
            7.0351e-02, -2.1738e-01,  6.1539e-01,  5.2657e-01, -9.4077e-02,
            4.3563e-01,  8.7237e-03,  6.1539e-01,  5.7441e-01,  4.3816e-01,
            1.4520e-01],
          [-1.5160e-01,  2.7121e-01,  9.9274e-01,  5.7441e-01,  4.3816e-01,
            1.4520e-01, -1.5160e-01,  2.7121e-01,  9.9274e-01,  5.2657e-01,
           -9.4077e-02,  5.0142e-01,  2.1293e-01,  2.8404e-01,  7.1469e-01,
            3.7736e-01],
          [ 3.9679e-01,  5.0467e-01,  2.7456e-01,  2.1704e-01,  2.0421e-01,
            1.5125e-01,  3.9679e-01,  5.0467e-01,  2.7456e-01,  4.8226e-01,
            3.0046e-01,  4.5431e-01,  4.5172e-01,  5.2009e-01,  5.9014e-01,
            7.0351e-02],
          [ 3.3100e-01,  5.6568e-01,  1.5378e-01,  2.5010e-01,  2.3468e-01,
           -8.0903e-02,  3.3100e-01,  5.6568e-01,  1.5378e-01, -8.0903e-02,
            3.3100e-01,  2.3468e-01,  2.5010e-01,  5.6568e-01,  1.5378e-01,
           -8.0903e-02]]]], device='cuda:0')
Inside MaxPool2d backward
grad_output size :  torch.Size([1, 1, 16, 16])
ref grad_output  :
  tensor([[[[-2.1738e-01,  6.6999e-02,  5.1049e-01,  8.7237e-03,  2.8438e-01,
            2.2611e-01, -2.1738e-01,  6.6999e-02,  5.1049e-01,  2.2611e-01,
           -2.1738e-01,  2.8438e-01,  8.7237e-03,  6.6999e-02,  5.1049e-01,
            2.2611e-01],
          [ 6.5786e-02,  5.2611e-02,  6.3984e-01,  4.4314e-01,  2.0421e-01,
            1.5125e-01,  6.5786e-02,  5.2611e-02,  6.3984e-01,  3.7736e-01,
            6.5786e-02, -1.3175e-02,  5.0142e-01,  2.7872e-01,  6.3984e-01,
            3.7736e-01],
          [ 1.1362e-01,  9.1585e-01,  5.8409e-01,  4.0136e-01,  1.7296e-02,
           -1.3903e-02,  8.4149e-01,  8.5757e-01,  3.5798e-01, -1.4703e-01,
            6.1539e-01,  5.2657e-01,  4.5431e-01,  7.8272e-01,  3.5798e-01,
            7.0351e-02],
          [-1.5160e-01,  8.1959e-01,  6.1204e-01, -8.0903e-02, -1.5160e-01,
            3.3699e-01,  8.6595e-01,  7.0836e-01,  1.7296e-02,  2.6926e-01,
            4.3031e-01,  2.6488e-01,  5.1906e-01,  4.7621e-01,  2.3468e-01,
           -8.0903e-02],
          [ 3.9679e-01,  4.3888e-01, -1.4703e-01,  2.8438e-01,  6.2290e-01,
            8.3567e-01,  5.0924e-01,  7.0351e-02,  6.5786e-02,  3.1783e-01,
            6.7031e-01,  2.1099e-01,  2.0421e-01,  1.5125e-01, -2.1738e-01,
            2.8438e-01],
          [ 3.3100e-01,  2.3468e-01, -1.5117e-02,  2.0421e-01,  4.8226e-01,
            5.6568e-01,  1.5378e-01, -8.0903e-02,  3.3100e-01,  3.0046e-01,
           -9.4077e-02,  7.6664e-01,  4.6078e-01, -2.9828e-01,  3.5017e-01,
            4.3031e-01],
          [-2.1738e-01,  6.6999e-02,  8.4149e-01,  4.6078e-01, -2.9828e-01,
            6.6999e-02,  5.1049e-01,  2.2611e-01, -2.1738e-01,  6.1539e-01,
            5.2657e-01, -9.4077e-02,  4.3563e-01,  2.9189e-01,  5.3521e-01,
            3.8593e-01],
          [ 6.5786e-02,  5.2611e-02,  6.3984e-01,  3.7736e-01, -1.5160e-01,
            5.5437e-01,  5.8157e-01,  1.5125e-01,  6.5786e-02, -1.3175e-02,
            7.6664e-01,  5.2657e-01, -9.4077e-02,  5.4926e-01,  7.4516e-01,
            1.4520e-01],
          [ 6.1539e-01,  8.5757e-01,  3.5798e-01,  7.0351e-02,  3.9679e-01,
            7.6989e-01,  8.7647e-02, -1.3903e-02,  8.4149e-01,  5.2657e-01,
            1.2330e-01,  4.8226e-01,  3.0046e-01,  1.8909e-01,  3.5546e-01,
            1.5125e-01],
          [ 2.0421e-01,  4.8226e-01,  2.3468e-01, -8.0903e-02,  3.3100e-01,
            1.7296e-02,  2.6926e-01,  4.9610e-01,  3.5546e-01,  4.8226e-01,
            1.7296e-02,  2.0348e-01,  5.5711e-01,  5.6568e-01,  1.5378e-01,
           -8.0903e-02],
          [ 1.7296e-02,  2.0348e-01,  8.7237e-03,  6.6999e-02,  5.1049e-01,
            2.9189e-01,  5.3521e-01,  4.9955e-01,  4.3816e-01,  1.4520e-01,
            6.5786e-02,  2.0421e-01, -6.6128e-02,  6.6999e-02,  5.1049e-01,
            2.2611e-01],
          [ 6.5786e-02,  2.0421e-01, -3.4238e-04,  3.3699e-01,  8.6595e-01,
            7.0836e-01,  2.3468e-01, -1.5117e-02, -1.3175e-02,  4.3563e-01,
            5.5711e-01,  2.3468e-01, -1.5117e-02,  2.6999e-01,  3.5546e-01,
            1.5125e-01],
          [ 1.1362e-01,  5.1906e-01,  5.4199e-01,  8.3567e-01,  5.0924e-01,
            7.0351e-02, -2.1738e-01,  6.1539e-01,  5.2657e-01, -9.4077e-02,
            4.3563e-01,  8.7237e-03,  6.1539e-01,  5.7441e-01,  4.3816e-01,
            1.4520e-01],
          [-1.5160e-01,  2.7121e-01,  9.9274e-01,  5.7441e-01,  4.3816e-01,
            1.4520e-01, -1.5160e-01,  2.7121e-01,  9.9274e-01,  5.2657e-01,
           -9.4077e-02,  5.0142e-01,  2.1293e-01,  2.8404e-01,  7.1469e-01,
            3.7736e-01],
          [ 3.9679e-01,  5.0467e-01,  2.7456e-01,  2.1704e-01,  2.0421e-01,
            1.5125e-01,  3.9679e-01,  5.0467e-01,  2.7456e-01,  4.8226e-01,
            3.0046e-01,  4.5431e-01,  4.5172e-01,  5.2009e-01,  5.9014e-01,
            7.0351e-02],
          [ 3.3100e-01,  5.6568e-01,  1.5378e-01,  2.5010e-01,  2.3468e-01,
           -8.0903e-02,  3.3100e-01,  5.6568e-01,  1.5378e-01, -8.0903e-02,
            3.3100e-01,  2.3468e-01,  2.5010e-01,  5.6568e-01,  1.5378e-01,
           -8.0903e-02]]]], device='cuda:0')
grad_input size :  torch.Size([1, 1, 32, 32])
ref grad_input  : 
 tensor([[[[ 0.0000e+00,  0.0000e+00,  6.6999e-02,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00],
          [-2.1738e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  5.1049e-01,
            0.0000e+00,  8.7237e-03,  0.0000e+00,  0.0000e+00,  2.8438e-01,
            2.2611e-01,  0.0000e+00, -2.1738e-01,  0.0000e+00,  6.6999e-02,
            0.0000e+00,  5.1049e-01,  0.0000e+00,  2.2611e-01,  0.0000e+00,
            0.0000e+00, -2.1738e-01,  2.8438e-01,  0.0000e+00,  8.7237e-03,
            0.0000e+00,  6.6999e-02,  0.0000e+00,  5.1049e-01,  0.0000e+00,
            0.0000e+00,  2.2611e-01],
          [ 0.0000e+00,  6.5786e-02,  0.0000e+00,  5.2611e-02,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  2.0421e-01,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            5.2611e-02,  6.3984e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            6.5786e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            5.0142e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            3.7736e-01,  0.0000e+00],
          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            6.3984e-01,  0.0000e+00,  4.4314e-01,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  1.5125e-01,  6.5786e-02,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  3.7736e-01,  0.0000e+00,
            0.0000e+00,  0.0000e+00, -1.3175e-02,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  2.7872e-01,  0.0000e+00,  0.0000e+00,  6.3984e-01,
            0.0000e+00,  0.0000e+00],
          [ 1.1362e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  5.8409e-01,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  1.7296e-02,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  8.5757e-01,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  7.8272e-01,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  7.0351e-02],
          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  9.1585e-01,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  4.0136e-01,  0.0000e+00,  0.0000e+00,
           -1.3903e-02,  0.0000e+00,  8.4149e-01,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  3.5798e-01,  0.0000e+00,  0.0000e+00, -1.4703e-01,
            0.0000e+00,  6.1539e-01,  5.2657e-01,  0.0000e+00,  4.5431e-01,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  3.5798e-01,  0.0000e+00,
            0.0000e+00,  0.0000e+00],
          [ 0.0000e+00,  0.0000e+00,  8.1959e-01,  0.0000e+00,  6.1204e-01,
            0.0000e+00, -8.0903e-02,  0.0000e+00, -1.5160e-01,  0.0000e+00,
            0.0000e+00,  3.3699e-01,  0.0000e+00,  0.0000e+00,  7.0836e-01,
            0.0000e+00,  1.7296e-02,  0.0000e+00,  2.6926e-01,  0.0000e+00,
            0.0000e+00,  4.3031e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  2.3468e-01,  0.0000e+00,
            0.0000e+00, -8.0903e-02],
          [ 0.0000e+00, -1.5160e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  8.6595e-01,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  2.6488e-01,  5.1906e-01,
            0.0000e+00,  4.7621e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00],
          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.4703e-01,
            0.0000e+00,  2.8438e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  8.3567e-01,  0.0000e+00,  0.0000e+00,  7.0351e-02,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  3.1783e-01,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  2.1099e-01,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  1.5125e-01,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  2.8438e-01],
          [ 0.0000e+00,  3.9679e-01,  4.3888e-01,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  6.2290e-01,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  5.0924e-01,  0.0000e+00,
            0.0000e+00,  6.5786e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  6.7031e-01,  0.0000e+00,  0.0000e+00,  2.0421e-01,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -2.1738e-01,
            0.0000e+00,  0.0000e+00],
          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  2.3468e-01, -1.5117e-02,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  4.8226e-01,
            0.0000e+00,  0.0000e+00,  1.5378e-01,  0.0000e+00,  0.0000e+00,
           -8.0903e-02,  0.0000e+00,  3.3100e-01,  0.0000e+00,  0.0000e+00,
           -9.4077e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00, -2.9828e-01,  0.0000e+00,  0.0000e+00,  3.5017e-01,
            4.3031e-01,  0.0000e+00],
          [ 3.3100e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  2.0421e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  5.6568e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  3.0046e-01,
            0.0000e+00,  0.0000e+00,  7.6664e-01,  0.0000e+00,  0.0000e+00,
            4.6078e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00],
          [-2.1738e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  8.4149e-01,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -2.9828e-01,
            0.0000e+00,  6.6999e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            2.2611e-01,  0.0000e+00,  0.0000e+00,  6.1539e-01,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            4.3563e-01,  2.9189e-01,  0.0000e+00,  5.3521e-01,  0.0000e+00,
            0.0000e+00,  0.0000e+00],
          [ 0.0000e+00,  0.0000e+00,  6.6999e-02,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  4.6078e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  5.1049e-01,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00, -2.1738e-01,  0.0000e+00,  0.0000e+00,
            5.2657e-01,  0.0000e+00,  0.0000e+00, -9.4077e-02,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            3.8593e-01,  0.0000e+00],
          [ 0.0000e+00,  0.0000e+00,  5.2611e-02,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.3175e-02,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  5.2657e-01, -9.4077e-02,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  1.4520e-01],
          [ 6.5786e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  6.3984e-01,
            0.0000e+00,  0.0000e+00,  3.7736e-01,  0.0000e+00, -1.5160e-01,
            5.5437e-01,  0.0000e+00,  0.0000e+00,  5.8157e-01,  0.0000e+00,
            1.5125e-01,  0.0000e+00,  6.5786e-02,  0.0000e+00,  0.0000e+00,
            7.6664e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  5.4926e-01,  7.4516e-01,  0.0000e+00,
            0.0000e+00,  0.0000e+00],
          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  8.5757e-01,  3.5798e-01,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  3.9679e-01,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  8.7647e-02,  0.0000e+00,  0.0000e+00,
           -1.3903e-02,  0.0000e+00,  0.0000e+00,  5.2657e-01,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  1.8909e-01,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00],
          [ 0.0000e+00,  6.1539e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  7.0351e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  7.6989e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  8.4149e-01,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  1.2330e-01,  4.8226e-01,  0.0000e+00,  0.0000e+00,
            3.0046e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  3.5546e-01,
            1.5125e-01,  0.0000e+00],
          [ 0.0000e+00,  2.0421e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00, -8.0903e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  3.5546e-01,  0.0000e+00,  4.8226e-01,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  2.0348e-01,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  1.5378e-01,  0.0000e+00,
           -8.0903e-02,  0.0000e+00],
          [ 0.0000e+00,  0.0000e+00,  4.8226e-01,  0.0000e+00,  0.0000e+00,
            2.3468e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  3.3100e-01,
            0.0000e+00,  1.7296e-02,  2.6926e-01,  0.0000e+00,  4.9610e-01,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            1.7296e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  5.5711e-01,
            0.0000e+00,  0.0000e+00,  5.6568e-01,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00],
          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            2.9189e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  4.9955e-01,
            0.0000e+00,  4.3816e-01,  0.0000e+00,  0.0000e+00,  1.4520e-01,
            0.0000e+00,  6.5786e-02,  2.0421e-01,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  5.1049e-01,
            2.2611e-01,  0.0000e+00],
          [ 1.7296e-02,  0.0000e+00,  0.0000e+00,  2.0348e-01,  0.0000e+00,
            8.7237e-03,  0.0000e+00,  6.6999e-02,  0.0000e+00,  5.1049e-01,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  5.3521e-01,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
           -6.6128e-02,  0.0000e+00,  6.6999e-02,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00],
          [ 6.5786e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  3.3699e-01,  0.0000e+00,  8.6595e-01,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.5117e-02,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  2.3468e-01,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  2.6999e-01,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  1.5125e-01],
          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  2.0421e-01,  0.0000e+00,
           -3.4238e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            7.0836e-01,  0.0000e+00,  0.0000e+00,  2.3468e-01,  0.0000e+00,
            0.0000e+00, -1.3175e-02,  0.0000e+00,  0.0000e+00,  4.3563e-01,
            0.0000e+00,  5.5711e-01,  0.0000e+00,  0.0000e+00, -1.5117e-02,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  3.5546e-01,
            0.0000e+00,  0.0000e+00],
          [ 0.0000e+00,  1.1362e-01,  5.1906e-01,  0.0000e+00,  5.4199e-01,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  7.0351e-02, -2.1738e-01,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00, -9.4077e-02,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  8.7237e-03,  0.0000e+00,
            6.1539e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            1.4520e-01,  0.0000e+00],
          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  8.3567e-01,  0.0000e+00,  5.0924e-01,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  6.1539e-01,
            0.0000e+00,  0.0000e+00,  5.2657e-01,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  4.3563e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  5.7441e-01,  0.0000e+00,  0.0000e+00,  4.3816e-01,
            0.0000e+00,  0.0000e+00],
          [ 0.0000e+00, -1.5160e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            9.9274e-01,  0.0000e+00,  0.0000e+00,  4.3816e-01,  0.0000e+00,
            0.0000e+00,  1.4520e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  9.9274e-01,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  5.0142e-01,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  7.1469e-01,  0.0000e+00,
            0.0000e+00,  0.0000e+00],
          [ 0.0000e+00,  0.0000e+00,  2.7121e-01,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  5.7441e-01,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00, -1.5160e-01,  0.0000e+00,  0.0000e+00,
            2.7121e-01,  0.0000e+00,  0.0000e+00,  5.2657e-01,  0.0000e+00,
            0.0000e+00, -9.4077e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            2.1293e-01,  0.0000e+00,  2.8404e-01,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  3.7736e-01],
          [ 0.0000e+00,  3.9679e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  2.1704e-01,  0.0000e+00,  0.0000e+00,  2.0421e-01,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  5.0467e-01,
            0.0000e+00,  0.0000e+00,  2.7456e-01,  4.8226e-01,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  4.5172e-01,
            0.0000e+00,  5.2009e-01,  0.0000e+00,  0.0000e+00,  5.9014e-01,
            7.0351e-02,  0.0000e+00],
          [ 0.0000e+00,  0.0000e+00,  5.0467e-01,  0.0000e+00,  2.7456e-01,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  1.5125e-01,  3.9679e-01,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            3.0046e-01,  0.0000e+00,  0.0000e+00,  4.5431e-01,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00],
          [ 3.3100e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.5378e-01,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            5.6568e-01,  1.5378e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  3.3100e-01,  2.3468e-01,  0.0000e+00,  0.0000e+00,
            2.5010e-01,  0.0000e+00,  0.0000e+00,  1.5378e-01,  0.0000e+00,
            0.0000e+00,  0.0000e+00],
          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  5.6568e-01,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  2.5010e-01,  2.3468e-01,  0.0000e+00,
           -8.0903e-02,  0.0000e+00,  0.0000e+00,  3.3100e-01,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -8.0903e-02,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  5.6568e-01,  0.0000e+00,  0.0000e+00,
           -8.0903e-02,  0.0000e+00]]]], device='cuda:0')[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function CudnnConvolutionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward

Inside Conv2d backward
grad_output size :  torch.Size([1, 1, 32, 32])
ref grad_output  :
  tensor([[[[ 0.0000e+00,  0.0000e+00,  6.6999e-02,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00],
          [-2.1738e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  5.1049e-01,
            0.0000e+00,  8.7237e-03,  0.0000e+00,  0.0000e+00,  2.8438e-01,
            2.2611e-01,  0.0000e+00, -2.1738e-01,  0.0000e+00,  6.6999e-02,
            0.0000e+00,  5.1049e-01,  0.0000e+00,  2.2611e-01,  0.0000e+00,
            0.0000e+00, -2.1738e-01,  2.8438e-01,  0.0000e+00,  8.7237e-03,
            0.0000e+00,  6.6999e-02,  0.0000e+00,  5.1049e-01,  0.0000e+00,
            0.0000e+00,  2.2611e-01],
          [ 0.0000e+00,  6.5786e-02,  0.0000e+00,  5.2611e-02,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  2.0421e-01,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            5.2611e-02,  6.3984e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            6.5786e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            5.0142e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            3.7736e-01,  0.0000e+00],
          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            6.3984e-01,  0.0000e+00,  4.4314e-01,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  1.5125e-01,  6.5786e-02,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  3.7736e-01,  0.0000e+00,
            0.0000e+00,  0.0000e+00, -1.3175e-02,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  2.7872e-01,  0.0000e+00,  0.0000e+00,  6.3984e-01,
            0.0000e+00,  0.0000e+00],
          [ 1.1362e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  5.8409e-01,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  1.7296e-02,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  8.5757e-01,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  7.8272e-01,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  7.0351e-02],
          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  9.1585e-01,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  4.0136e-01,  0.0000e+00,  0.0000e+00,
           -1.3903e-02,  0.0000e+00,  8.4149e-01,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  3.5798e-01,  0.0000e+00,  0.0000e+00, -1.4703e-01,
            0.0000e+00,  6.1539e-01,  5.2657e-01,  0.0000e+00,  4.5431e-01,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  3.5798e-01,  0.0000e+00,
            0.0000e+00,  0.0000e+00],
          [ 0.0000e+00,  0.0000e+00,  8.1959e-01,  0.0000e+00,  6.1204e-01,
            0.0000e+00, -8.0903e-02,  0.0000e+00, -1.5160e-01,  0.0000e+00,
            0.0000e+00,  3.3699e-01,  0.0000e+00,  0.0000e+00,  7.0836e-01,
            0.0000e+00,  1.7296e-02,  0.0000e+00,  2.6926e-01,  0.0000e+00,
            0.0000e+00,  4.3031e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  2.3468e-01,  0.0000e+00,
            0.0000e+00, -8.0903e-02],
          [ 0.0000e+00, -1.5160e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  8.6595e-01,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  2.6488e-01,  5.1906e-01,
            0.0000e+00,  4.7621e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00],
          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.4703e-01,
            0.0000e+00,  2.8438e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  8.3567e-01,  0.0000e+00,  0.0000e+00,  7.0351e-02,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  3.1783e-01,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  2.1099e-01,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  1.5125e-01,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  2.8438e-01],
          [ 0.0000e+00,  3.9679e-01,  4.3888e-01,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  6.2290e-01,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  5.0924e-01,  0.0000e+00,
            0.0000e+00,  6.5786e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  6.7031e-01,  0.0000e+00,  0.0000e+00,  2.0421e-01,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -2.1738e-01,
            0.0000e+00,  0.0000e+00],
          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  2.3468e-01, -1.5117e-02,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  4.8226e-01,
            0.0000e+00,  0.0000e+00,  1.5378e-01,  0.0000e+00,  0.0000e+00,
           -8.0903e-02,  0.0000e+00,  3.3100e-01,  0.0000e+00,  0.0000e+00,
           -9.4077e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00, -2.9828e-01,  0.0000e+00,  0.0000e+00,  3.5017e-01,
            4.3031e-01,  0.0000e+00],
          [ 3.3100e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  2.0421e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  5.6568e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  3.0046e-01,
            0.0000e+00,  0.0000e+00,  7.6664e-01,  0.0000e+00,  0.0000e+00,
            4.6078e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00],
          [-2.1738e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  8.4149e-01,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -2.9828e-01,
            0.0000e+00,  6.6999e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            2.2611e-01,  0.0000e+00,  0.0000e+00,  6.1539e-01,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            4.3563e-01,  2.9189e-01,  0.0000e+00,  5.3521e-01,  0.0000e+00,
            0.0000e+00,  0.0000e+00],
          [ 0.0000e+00,  0.0000e+00,  6.6999e-02,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  4.6078e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  5.1049e-01,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00, -2.1738e-01,  0.0000e+00,  0.0000e+00,
            5.2657e-01,  0.0000e+00,  0.0000e+00, -9.4077e-02,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            3.8593e-01,  0.0000e+00],
          [ 0.0000e+00,  0.0000e+00,  5.2611e-02,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.3175e-02,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  5.2657e-01, -9.4077e-02,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  1.4520e-01],
          [ 6.5786e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  6.3984e-01,
            0.0000e+00,  0.0000e+00,  3.7736e-01,  0.0000e+00, -1.5160e-01,
            5.5437e-01,  0.0000e+00,  0.0000e+00,  5.8157e-01,  0.0000e+00,
            1.5125e-01,  0.0000e+00,  6.5786e-02,  0.0000e+00,  0.0000e+00,
            7.6664e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  5.4926e-01,  7.4516e-01,  0.0000e+00,
            0.0000e+00,  0.0000e+00],
          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  8.5757e-01,  3.5798e-01,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  3.9679e-01,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  8.7647e-02,  0.0000e+00,  0.0000e+00,
           -1.3903e-02,  0.0000e+00,  0.0000e+00,  5.2657e-01,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  1.8909e-01,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00],
          [ 0.0000e+00,  6.1539e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  7.0351e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  7.6989e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  8.4149e-01,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  1.2330e-01,  4.8226e-01,  0.0000e+00,  0.0000e+00,
            3.0046e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  3.5546e-01,
            1.5125e-01,  0.0000e+00],
          [ 0.0000e+00,  2.0421e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00, -8.0903e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  3.5546e-01,  0.0000e+00,  4.8226e-01,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  2.0348e-01,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  1.5378e-01,  0.0000e+00,
           -8.0903e-02,  0.0000e+00],
          [ 0.0000e+00,  0.0000e+00,  4.8226e-01,  0.0000e+00,  0.0000e+00,
            2.3468e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  3.3100e-01,
            0.0000e+00,  1.7296e-02,  2.6926e-01,  0.0000e+00,  4.9610e-01,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            1.7296e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  5.5711e-01,
            0.0000e+00,  0.0000e+00,  5.6568e-01,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00],
          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            2.9189e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  4.9955e-01,
            0.0000e+00,  4.3816e-01,  0.0000e+00,  0.0000e+00,  1.4520e-01,
            0.0000e+00,  6.5786e-02,  2.0421e-01,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  5.1049e-01,
            2.2611e-01,  0.0000e+00],
          [ 1.7296e-02,  0.0000e+00,  0.0000e+00,  2.0348e-01,  0.0000e+00,
            8.7237e-03,  0.0000e+00,  6.6999e-02,  0.0000e+00,  5.1049e-01,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  5.3521e-01,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
           -6.6128e-02,  0.0000e+00,  6.6999e-02,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00],
          [ 6.5786e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  3.3699e-01,  0.0000e+00,  8.6595e-01,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.5117e-02,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  2.3468e-01,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  2.6999e-01,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  1.5125e-01],
          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  2.0421e-01,  0.0000e+00,
           -3.4238e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            7.0836e-01,  0.0000e+00,  0.0000e+00,  2.3468e-01,  0.0000e+00,
            0.0000e+00, -1.3175e-02,  0.0000e+00,  0.0000e+00,  4.3563e-01,
            0.0000e+00,  5.5711e-01,  0.0000e+00,  0.0000e+00, -1.5117e-02,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  3.5546e-01,
            0.0000e+00,  0.0000e+00],
          [ 0.0000e+00,  1.1362e-01,  5.1906e-01,  0.0000e+00,  5.4199e-01,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  7.0351e-02, -2.1738e-01,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00, -9.4077e-02,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  8.7237e-03,  0.0000e+00,
            6.1539e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            1.4520e-01,  0.0000e+00],
          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  8.3567e-01,  0.0000e+00,  5.0924e-01,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  6.1539e-01,
            0.0000e+00,  0.0000e+00,  5.2657e-01,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  4.3563e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  5.7441e-01,  0.0000e+00,  0.0000e+00,  4.3816e-01,
            0.0000e+00,  0.0000e+00],
          [ 0.0000e+00, -1.5160e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            9.9274e-01,  0.0000e+00,  0.0000e+00,  4.3816e-01,  0.0000e+00,
            0.0000e+00,  1.4520e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  9.9274e-01,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  5.0142e-01,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  7.1469e-01,  0.0000e+00,
            0.0000e+00,  0.0000e+00],
          [ 0.0000e+00,  0.0000e+00,  2.7121e-01,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  5.7441e-01,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00, -1.5160e-01,  0.0000e+00,  0.0000e+00,
            2.7121e-01,  0.0000e+00,  0.0000e+00,  5.2657e-01,  0.0000e+00,
            0.0000e+00, -9.4077e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            2.1293e-01,  0.0000e+00,  2.8404e-01,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  3.7736e-01],
          [ 0.0000e+00,  3.9679e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  2.1704e-01,  0.0000e+00,  0.0000e+00,  2.0421e-01,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  5.0467e-01,
            0.0000e+00,  0.0000e+00,  2.7456e-01,  4.8226e-01,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  4.5172e-01,
            0.0000e+00,  5.2009e-01,  0.0000e+00,  0.0000e+00,  5.9014e-01,
            7.0351e-02,  0.0000e+00],
          [ 0.0000e+00,  0.0000e+00,  5.0467e-01,  0.0000e+00,  2.7456e-01,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  1.5125e-01,  3.9679e-01,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            3.0046e-01,  0.0000e+00,  0.0000e+00,  4.5431e-01,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00],
          [ 3.3100e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.5378e-01,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            5.6568e-01,  1.5378e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  3.3100e-01,  2.3468e-01,  0.0000e+00,  0.0000e+00,
            2.5010e-01,  0.0000e+00,  0.0000e+00,  1.5378e-01,  0.0000e+00,
            0.0000e+00,  0.0000e+00],
          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  5.6568e-01,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  2.5010e-01,  2.3468e-01,  0.0000e+00,
           -8.0903e-02,  0.0000e+00,  0.0000e+00,  3.3100e-01,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -8.0903e-02,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  5.6568e-01,  0.0000e+00,  0.0000e+00,
           -8.0903e-02,  0.0000e+00]]]], device='cuda:0')
grad_input size :  torch.Size([1, 1, 32, 32])
ref grad_input  : 
 tensor([[[[-6.6920e-02,  3.4512e-02, -1.1018e-02,  1.5891e-01,  1.5715e-01,
           -9.8396e-02,  2.6856e-03, -1.7243e-03,  8.1630e-02,  1.5245e-01,
            1.3397e-02, -1.0709e-01, -6.6920e-02,  6.2198e-02,  2.0626e-02,
            1.3329e-01,  1.5715e-01, -3.5998e-02,  6.9606e-02, -4.4691e-02,
           -6.2398e-02,  1.4709e-02,  1.3051e-01, -5.3705e-02,  2.6856e-03,
            1.7507e-02,  2.0626e-02,  1.3329e-01,  1.5715e-01, -1.0090e-01,
            6.4902e-02,  6.9606e-02],
          [ 5.4631e-02, -3.6956e-02,  4.2666e-03, -4.7925e-02, -9.4347e-02,
            9.3239e-02, -1.4346e-03,  6.0228e-02,  2.6977e-02, -1.1566e-01,
            1.5372e-02,  6.9217e-02,  3.5748e-02, -4.8628e-02,  4.0836e-03,
            1.4782e-01,  1.0263e-01, -6.0661e-02, -3.7183e-02,  6.0668e-02,
            4.7684e-02, -1.3142e-02, -8.6939e-02,  5.1454e-02,  1.4249e-01,
            1.4752e-01, -1.1013e-01, -5.2039e-02, -8.3948e-02,  2.0266e-01,
            8.7636e-02, -1.1177e-01],
          [-1.5336e-02, -1.1788e-02,  5.5183e-03, -1.3844e-01,  2.0990e-01,
            1.9703e-01,  1.0157e-03,  1.1069e-01, -1.9348e-01, -1.0548e-02,
            5.2001e-02,  1.2172e-01, -1.6678e-02, -3.1007e-02, -4.4713e-03,
           -2.1889e-01, -7.8980e-02,  1.7135e-01,  1.2349e-01, -8.1880e-02,
            4.4451e-02, -7.0962e-02,  4.1763e-03,  1.6544e-03, -6.2994e-02,
           -1.9450e-02,  1.8063e-01, -1.8458e-01,  2.0018e-01,  1.5163e-01,
           -2.4601e-01,  7.7054e-02],
          [ 1.8253e-02, -2.0330e-02, -1.3083e-02,  1.6936e-01,  9.9300e-02,
           -2.2067e-01,  6.2323e-02, -1.1983e-01,  9.3827e-02, -2.5079e-03,
           -1.9087e-02, -3.3175e-02,  1.7134e-02,  2.5832e-01,  2.5063e-01,
           -3.3048e-01,  2.0938e-02, -4.4767e-02, -6.2056e-02,  5.3011e-02,
            2.1286e-03,  1.9560e-03,  2.1666e-03, -2.4348e-03, -1.2749e-01,
           -1.8948e-02,  1.8108e-01,  2.9247e-01, -2.3545e-01, -2.0116e-01,
            1.5065e-01,  2.3340e-02],
          [-1.8685e-02,  2.0998e-02,  2.6289e-01,  2.0823e-01, -4.3975e-01,
            1.2865e-01,  5.3901e-03,  1.3571e-01, -8.0198e-02, -7.9435e-04,
           -4.2736e-02,  2.3246e-01,  2.6185e-01, -2.7425e-01, -1.4103e-01,
            2.6124e-01,  1.1020e-01, -1.6670e-01, -2.9994e-02, -4.3580e-02,
            2.0570e-01,  3.4394e-01,  4.0042e-02,  2.6268e-02,  1.3986e-01,
           -1.6066e-01, -8.9756e-02, -2.4718e-02,  9.2174e-02, -5.0054e-02,
           -6.0242e-03, -1.1569e-02],
          [ 3.6765e-03,  2.3577e-01,  1.3674e-01, -2.8543e-01,  3.7657e-01,
           -1.4159e-01, -7.5554e-02, -9.7923e-02,  2.8063e-02,  3.1795e-02,
            9.9017e-02, -5.0183e-03, -2.0499e-01,  1.4080e-01,  2.4582e-01,
           -1.7640e-01, -5.3545e-02,  1.4003e-01,  1.0145e-01, -2.9043e-02,
            1.8688e-02, -3.5178e-02, -5.7921e-02,  3.9981e-02, -7.4710e-02,
            8.3958e-02, -1.9901e-01,  4.7514e-02,  1.6867e-02,  1.9771e-02,
           -4.1109e-02, -2.2629e-02],
          [-4.3515e-02, -1.5010e-01, -3.3767e-01,  1.0386e-01, -9.6563e-02,
            1.2332e-01, -8.8741e-02,  1.7166e-02,  2.6720e-02, -2.4481e-02,
           -4.2976e-02, -2.0866e-02,  3.5609e-01, -2.5680e-01, -1.1649e-01,
            3.7709e-02,  8.7389e-03, -2.9186e-02, -6.8972e-03,  4.5004e-02,
           -2.1142e-01, -1.8473e-01,  1.7534e-01,  1.1737e-01,  1.2214e-01,
            3.6124e-02,  1.4660e-01, -2.1476e-01, -2.7009e-02,  4.4966e-02,
            1.0209e-02,  1.3304e-02],
          [ 1.9131e-02, -1.8345e-01, -1.4961e-03, -1.9416e-01, -2.5459e-02,
            1.3399e-01,  8.4928e-02, -1.8027e-02, -4.9052e-03, -6.7612e-04,
            1.5419e-01,  1.5889e-01, -3.0608e-01,  1.2210e-04,  4.4578e-02,
           -1.5144e-02,  5.5965e-04, -6.8384e-02,  9.9943e-02,  9.9044e-02,
           -1.7223e-01,  1.3924e-02,  2.9056e-02, -4.4108e-02, -7.8111e-02,
            3.5830e-02, -3.4895e-02,  7.4901e-02, -2.2303e-02,  1.0467e-03,
            1.0220e-01,  8.4928e-02],
          [ 1.5244e-01,  2.4322e-01,  5.6006e-02, -6.8193e-02,  2.4179e-02,
           -6.3059e-02, -4.6766e-02,  2.3135e-01,  1.9176e-01, -1.2312e-01,
           -1.0546e-01, -3.5759e-01,  3.2863e-01,  1.5175e-01, -1.1222e-01,
            3.1884e-02,  2.0252e-02, -1.3003e-02, -4.0108e-02, -5.2267e-02,
            2.5114e-01,  2.0635e-01, -2.2646e-01, -9.9481e-02,  1.1983e-01,
           -1.5912e-01, -3.6787e-03, -2.2749e-02, -3.4445e-02, -6.6920e-02,
            7.0794e-03, -4.6766e-02],
          [-5.0073e-02, -1.2064e-01,  6.8518e-02,  1.8640e-01, -5.5797e-02,
           -6.9972e-02,  9.2017e-03, -7.7337e-02,  3.5995e-02,  2.6358e-01,
           -3.0779e-01,  7.1180e-02, -1.3196e-02, -1.3202e-01,  7.3163e-02,
           -3.2894e-02,  1.0019e-01,  1.1406e-01, -1.4623e-01, -1.6720e-02,
           -1.1213e-01, -9.1637e-02,  7.0233e-02, -1.8943e-02, -3.2640e-02,
           -4.7882e-02, -1.3028e-01,  6.3852e-02,  1.2862e-01,  2.6706e-01,
           -4.9219e-02, -7.5852e-02],
          [ 1.0147e-03, -1.6417e-01, -1.3644e-02, -3.4727e-02,  4.5855e-02,
            5.5822e-02,  6.2865e-02, -1.9873e-01, -4.0703e-02, -7.6528e-02,
            2.5150e-01,  1.5474e-01, -2.6657e-01,  4.4896e-02,  1.2481e-02,
           -3.4218e-03, -5.4593e-02, -5.4140e-02,  1.4742e-01,  1.0437e-01,
           -2.1434e-01,  2.2436e-01,  2.3900e-01, -2.0345e-01,  1.3887e-01,
            1.8040e-01, -4.2024e-02, -5.5124e-02,  1.1081e-02, -1.1892e-01,
           -7.0212e-03,  7.9523e-02],
          [-1.2135e-01,  1.0414e-01, -5.9667e-02,  2.5298e-01,  2.5961e-01,
           -1.9216e-01, -3.3581e-02,  3.7738e-02, -2.0824e-01, -7.6222e-02,
            8.9542e-03, -1.1150e-01,  9.6274e-02,  6.8584e-04,  8.5471e-02,
            6.6988e-02, -1.2921e-01,  1.8735e-01,  1.5300e-01, -1.4713e-01,
            5.2483e-02, -9.7165e-02, -1.2607e-01,  1.4168e-01,  6.6898e-02,
            2.1796e-01,  7.9256e-02,  9.4605e-02,  7.5734e-02, -2.0386e-01,
            1.5485e-02,  1.9192e-03],
          [ 4.6458e-02, -1.9465e-02,  2.0626e-02, -1.1943e-01, -1.3838e-01,
            2.3586e-01,  1.4846e-01, -9.0165e-02,  3.7642e-02,  4.9052e-02,
           -2.0740e-01,  1.5382e-01,  1.7206e-01, -1.0090e-01, -2.8533e-02,
           -3.7183e-02, -2.0613e-02, -1.4458e-01, -1.3463e-01,  2.7460e-01,
            1.6344e-01, -2.9900e-01, -2.1981e-03, -2.5542e-02, -1.5353e-01,
           -9.3565e-02,  3.4561e-02, -1.3598e-02, -8.8014e-02,  2.0969e-01,
            1.1881e-01, -7.6281e-02],
          [-7.0338e-03,  5.6771e-03,  5.1782e-03, -2.1197e-01,  2.7228e-02,
           -5.4395e-02, -7.5775e-02,  8.5155e-02,  7.5839e-02, -9.6516e-03,
           -1.8365e-02, -6.2252e-02, -8.3650e-02,  9.4340e-02, -5.7488e-02,
            7.3161e-03,  2.8441e-02, -1.2071e-01, -2.4043e-02, -6.7761e-02,
           -8.3989e-02,  9.7312e-02,  1.6302e-01,  1.5057e-01, -2.6119e-01,
           -4.1523e-02,  1.1388e-02, -1.3478e-01,  1.7318e-02, -4.6315e-02,
           -2.1786e-02,  1.1602e-01],
          [ 2.0252e-02, -3.6677e-02, -6.4838e-03,  1.9368e-01,  1.9697e-01,
           -2.4362e-01,  1.2323e-01,  1.1822e-01, -1.1810e-01,  1.1246e-01,
            2.0063e-01, -2.3937e-01,  1.8345e-01,  1.8131e-01, -7.1533e-02,
            4.6563e-02,  4.4257e-02,  1.3218e-02, -1.2310e-02,  8.8344e-02,
            2.5061e-01, -1.4918e-01, -4.2531e-02, -7.7765e-02,  1.1236e-01,
           -1.7386e-02,  1.5766e-01,  3.8298e-01,  1.2083e-01, -2.4541e-01,
           -5.8361e-03, -2.2157e-02],
          [-1.0818e-02, -1.2189e-03,  2.4786e-01,  2.8625e-01, -1.6452e-01,
            4.7488e-02, -4.7620e-02,  5.1840e-02,  2.1102e-01, -1.2346e-01,
           -1.1918e-01,  1.2761e-01, -4.6408e-02, -1.1296e-01,  8.4398e-02,
           -2.9153e-02,  2.2399e-02,  1.4033e-01,  1.7761e-01, -2.0125e-01,
           -1.2613e-01,  1.4168e-01, -1.3388e-01,  4.0957e-02, -6.9557e-04,
           -4.1958e-04, -1.5036e-02, -1.2615e-01, -5.8410e-02,  1.3771e-01,
           -3.6918e-02,  4.6983e-03],
          [ 1.7877e-01,  1.8974e-01, -2.2985e-01, -3.4888e-01,  1.2032e-01,
            8.9204e-02, -7.4287e-02, -5.1768e-02, -2.5025e-02, -7.2527e-02,
            2.3825e-01,  2.2842e-01, -3.1445e-01,  3.5015e-02, -3.4108e-02,
            7.1804e-03,  2.2292e-01,  1.9473e-01, -2.5262e-01, -9.7607e-02,
            6.0200e-02,  1.7981e-01,  1.2409e-01, -9.5321e-02,  8.6246e-02,
            9.2497e-02, -2.2290e-01, -2.0278e-01,  1.6354e-01,  1.5617e-01,
           -2.3696e-02, -2.9896e-02],
          [-1.9042e-02, -3.8334e-02, -1.4468e-01, -6.3269e-02,  1.5408e-02,
           -3.0504e-02, -3.6475e-02, -7.1892e-02,  1.2839e-02,  1.7697e-03,
           -9.7155e-02, -1.4889e-01,  1.4511e-01,  3.9091e-04,  3.5349e-03,
            1.0158e-01,  3.1745e-03, -2.0409e-01,  3.2101e-01, -9.2972e-02,
           -1.5560e-02, -8.1135e-02,  1.8877e-03,  1.5176e-01, -7.8135e-02,
           -4.9411e-02,  7.4507e-03,  5.0258e-02,  3.3257e-03, -1.3116e-01,
            1.5912e-02,  4.3943e-02],
          [-1.8223e-01,  1.2476e-01,  1.8895e-01, -9.5321e-02,  6.7363e-02,
            6.4568e-02, -3.0805e-02, -1.4637e-02,  9.5013e-02,  1.0190e-01,
           -2.5621e-01,  1.0753e-01,  8.2907e-02,  8.9180e-02,  1.5272e-01,
           -1.4291e-01, -2.7240e-01,  3.2061e-02, -7.5553e-02,  9.4088e-02,
           -2.6026e-02, -1.2204e-01, -9.5234e-03,  1.2860e-01,  1.3272e-01,
           -1.0039e-01,  1.6372e-01,  1.5474e-01, -2.2747e-01,  1.1673e-02,
            1.9784e-02, -1.4277e-02],
          [-5.1920e-02, -5.4251e-02, -7.8396e-02,  8.9123e-02, -2.9615e-02,
           -1.8023e-02,  4.0752e-02, -3.6082e-04, -4.1771e-02,  2.9352e-02,
            1.4885e-01, -9.4518e-02, -4.1083e-02,  1.3055e-01,  7.2205e-02,
            2.8335e-02,  1.4639e-01, -2.0763e-01,  5.7284e-02,  4.4668e-02,
           -1.2661e-02,  8.2064e-02, -1.8729e-03, -1.0408e-01, -9.0708e-02,
            1.0296e-01, -7.1386e-02, -1.3212e-01,  2.5605e-01,  2.4331e-01,
           -3.3912e-02, -4.5052e-02],
          [ 5.3246e-03, -1.2603e-01,  7.4011e-02,  6.4791e-02, -9.7382e-02,
            1.0279e-02,  1.8554e-02,  2.0626e-02,  4.9130e-02,  1.3103e-01,
           -1.5182e-01, -1.3958e-02,  1.6242e-01, -2.3210e-02, -1.7189e-01,
            3.9240e-02, -7.2054e-02,  8.0973e-02, -1.8324e-02, -2.8276e-02,
            1.9092e-02, -3.6511e-02, -2.1424e-02, -1.0391e-01, -9.5516e-04,
           -1.7873e-02, -1.1152e-01,  3.8929e-02, -7.5140e-02, -1.1248e-01,
            5.7158e-02,  4.1785e-02],
          [ 1.7408e-02, -9.8064e-03, -2.5678e-02, -3.3462e-02,  3.6503e-02,
            9.5296e-02,  9.6899e-02,  1.7094e-01,  2.1454e-01, -3.2932e-01,
            1.0378e-01,  1.3018e-03, -6.7541e-02, -2.1937e-01,  1.1042e-01,
           -1.0619e-01,  1.4177e-02,  1.9542e-03, -3.6918e-02,  4.6983e-03,
           -1.6078e-02,  1.7571e-02,  7.9146e-02, -4.5475e-02,  8.3449e-03,
            1.0875e-02,  5.6824e-02,  7.2099e-02, -1.7078e-01, -4.0970e-02,
            5.3009e-02,  4.7572e-02],
          [-1.0259e-02,  1.2235e-02,  6.8815e-03,  6.9449e-02, -4.1771e-02,
           -4.2349e-02, -7.2346e-02, -4.4832e-02, -2.7190e-01,  3.7988e-01,
            2.2035e-01, -1.4001e-01, -6.8716e-02,  9.1471e-02, -4.1512e-02,
           -6.5754e-03, -4.0558e-03,  2.6041e-03,  1.2505e-01,  1.3411e-01,
            7.3809e-02,  1.4189e-01, -1.4871e-01,  3.9030e-02,  1.2159e-02,
            8.4827e-04, -5.1401e-02, -4.2232e-02,  1.5223e-01,  1.0943e-01,
           -8.9346e-02, -2.4873e-02],
          [ 3.4743e-02,  1.8426e-01,  1.1156e-01,  1.9399e-02,  2.0463e-01,
           -1.9275e-01,  1.0841e-02, -2.1866e-01,  2.8019e-02, -8.5529e-02,
           -9.6295e-02,  9.0168e-02, -1.1044e-01,  8.2178e-03,  4.2880e-02,
            1.5952e-03,  2.1666e-03, -2.9439e-02, -8.3936e-02, -5.3044e-02,
            1.0203e-02, -1.5128e-01,  1.1305e-01,  5.6399e-03,  1.7740e-01,
            1.8665e-01, -1.9028e-01,  8.7361e-03, -4.3653e-02, -1.6775e-02,
            7.1935e-02, -2.3806e-02],
          [-1.4339e-02, -8.4187e-02, -1.1628e-01,  3.4136e-02, -8.8132e-02,
            3.4003e-01,  2.5726e-01, -1.9003e-02,  1.5677e-01, -2.8076e-01,
            1.4043e-02,  1.9022e-02, -1.0918e-02,  1.4406e-01,  1.9049e-01,
           -1.1828e-01,  1.5072e-01,  1.7392e-01, -1.9937e-01, -3.2901e-03,
           -1.4657e-02,  1.5214e-01, -8.4722e-02,  2.4089e-03, -7.6535e-02,
            6.3613e-02,  2.9056e-01, -1.1353e-01,  3.5393e-02,  1.2806e-01,
           -1.0890e-01,  2.6834e-02],
          [-7.2403e-02, -1.7496e-01,  4.7266e-02, -1.3549e-01,  3.0250e-01,
            2.0257e-01, -3.3365e-01,  2.1594e-01,  5.1143e-02,  7.5050e-03,
            2.3793e-02,  1.0225e-01, -3.5420e-02, -7.8628e-02, -1.0120e-01,
            1.1373e-01,  2.1851e-01,  2.4294e-01, -1.0195e-01, -4.1958e-04,
           -5.4974e-02, -7.1639e-02,  2.2222e-01,  1.5464e-01, -2.5553e-01,
           -5.2575e-02, -9.1715e-02,  3.1130e-01,  1.6472e-01, -2.5023e-01,
            8.5671e-02,  6.4760e-04],
          [ 1.9131e-02,  1.0278e-01,  5.5475e-02, -5.3605e-02, -1.2528e-01,
           -3.7573e-01,  3.7538e-01, -4.2095e-03, -1.6911e-01,  8.3244e-02,
           -1.8324e-02, -6.7393e-02, -1.9834e-02, -1.2650e-01,  9.7760e-02,
            8.6235e-02, -3.1276e-01,  4.9310e-03,  3.4791e-01, -1.0408e-01,
           -1.3776e-01, -1.4866e-02, -4.2738e-02, -8.2458e-02,  1.5378e-01,
           -8.0493e-02,  5.8030e-02, -1.8745e-04, -2.8507e-01,  1.4626e-01,
            1.1027e-01,  1.1617e-01],
          [ 1.5244e-01,  8.3021e-02, -1.2370e-01,  5.0120e-02, -2.5241e-01,
            9.4422e-02, -1.2440e-03, -2.4876e-01,  1.7895e-01,  6.4819e-02,
           -7.7280e-02,  2.3829e-02,  2.5577e-02,  1.1685e-01,  1.2114e-01,
           -1.4435e-01, -1.2348e-01,  1.8862e-01,  1.2029e-02,  1.9912e-03,
            1.1872e-02,  1.5471e-02, -1.4487e-01,  1.4589e-01,  1.1443e-01,
            2.4989e-02,  1.6362e-01, -3.3122e-01,  2.4501e-01,  2.0505e-01,
           -1.4261e-01, -7.5961e-02],
          [-5.0073e-02,  1.0656e-02,  2.3747e-01, -1.9731e-02,  8.4522e-02,
           -8.1657e-02, -1.8174e-01,  5.8696e-02, -2.3208e-02, -3.3581e-02,
            8.1155e-02,  1.9900e-01,  8.7350e-02, -1.4279e-01, -1.5195e-01,
            1.0204e-01, -3.3438e-02, -2.3989e-01, -1.1529e-02,  1.7772e-01,
            1.1642e-01, -6.2432e-02,  1.2999e-01,  8.2854e-02, -2.1822e-01,
            2.4736e-02, -1.5680e-01,  1.0531e-01, -7.3205e-02, -1.0592e-01,
            1.5471e-03,  2.5211e-02],
          [ 1.0147e-03, -1.1627e-01, -8.1222e-02,  1.0276e-01,  2.1889e-03,
           -3.4837e-02,  7.0227e-03,  9.6799e-04, -5.1920e-02,  6.6075e-03,
           -1.8177e-02, -7.4946e-02, -3.7299e-02, -5.4984e-02,  1.7870e-01,
            2.2054e-01, -1.3428e-01, -1.4413e-01,  1.6829e-02, -3.5766e-02,
            4.5602e-02,  2.2479e-01, -5.0511e-02, -2.3595e-01,  1.7036e-01,
           -5.3227e-02, -3.2605e-02,  4.6460e-02, -1.0270e-01, -2.9186e-02,
            4.9084e-03,  3.1376e-04],
          [-5.4433e-02, -6.7142e-02,  1.7870e-01,  8.7183e-02, -1.2821e-01,
            2.9643e-02,  7.1790e-02,  1.4436e-01,  2.2811e-02, -6.9608e-02,
           -6.3362e-02, -8.0000e-02,  1.0853e-01,  1.0367e-01, -1.3681e-01,
           -1.1243e-01,  7.9253e-02,  2.8418e-02, -2.3223e-02, -1.0130e-01,
           -1.6058e-02, -8.2708e-02, -9.2930e-02,  5.8070e-02, -2.9535e-02,
           -4.1129e-02,  2.0860e-01,  1.5474e-01, -1.3710e-01,  5.1958e-03,
           -2.4906e-02,  1.5991e-02],
          [ 1.0710e-02,  1.4763e-03, -7.1386e-02, -1.3212e-01,  1.0952e-01,
            6.8584e-04, -3.1561e-02, -7.0744e-02,  7.6277e-03,  5.3579e-02,
            1.3304e-02, -1.4951e-02, -4.1771e-02, -5.4433e-02, -8.2654e-02,
           -2.0794e-02,  7.4986e-03,  6.8584e-04,  1.0209e-02,  1.3304e-02,
           -9.9110e-02, -4.8957e-02,  9.0697e-03,  1.0467e-03, -6.3589e-02,
            8.0926e-03, -7.0270e-02, -1.3212e-01,  1.0952e-01,  1.0895e-02,
            1.3304e-02, -1.4951e-02]]]], device='cuda:0')[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad

tensor([[[[-6.6920e-02,  3.4512e-02, -1.1018e-02,  1.5891e-01,  1.5715e-01,
           -9.8396e-02,  2.6856e-03, -1.7243e-03,  8.1630e-02,  1.5245e-01,
            1.3397e-02, -1.0709e-01, -6.6920e-02,  6.2198e-02,  2.0626e-02,
            1.3329e-01,  1.5715e-01, -3.5998e-02,  6.9606e-02, -4.4691e-02,
           -6.2398e-02,  1.4709e-02,  1.3051e-01, -5.3705e-02,  2.6856e-03,
            1.7507e-02,  2.0626e-02,  1.3329e-01,  1.5715e-01, -1.0090e-01,
            6.4902e-02,  6.9606e-02],
          [ 5.4631e-02, -3.6956e-02,  4.2666e-03, -4.7925e-02, -9.4347e-02,
            9.3239e-02, -1.4346e-03,  6.0228e-02,  2.6977e-02, -1.1566e-01,
            1.5372e-02,  6.9217e-02,  3.5748e-02, -4.8628e-02,  4.0836e-03,
            1.4782e-01,  1.0263e-01, -6.0661e-02, -3.7183e-02,  6.0668e-02,
            4.7684e-02, -1.3142e-02, -8.6939e-02,  5.1454e-02,  1.4249e-01,
            1.4752e-01, -1.1013e-01, -5.2039e-02, -8.3948e-02,  2.0266e-01,
            8.7636e-02, -1.1177e-01],
          [-1.5336e-02, -1.1788e-02,  5.5183e-03, -1.3844e-01,  2.0990e-01,
            1.9703e-01,  1.0157e-03,  1.1069e-01, -1.9348e-01, -1.0548e-02,
            5.2001e-02,  1.2172e-01, -1.6678e-02, -3.1007e-02, -4.4713e-03,
           -2.1889e-01, -7.8980e-02,  1.7135e-01,  1.2349e-01, -8.1880e-02,
            4.4451e-02, -7.0962e-02,  4.1763e-03,  1.6544e-03, -6.2994e-02,
           -1.9450e-02,  1.8063e-01, -1.8458e-01,  2.0018e-01,  1.5163e-01,
           -2.4601e-01,  7.7054e-02],
          [ 1.8253e-02, -2.0330e-02, -1.3083e-02,  1.6936e-01,  9.9300e-02,
           -2.2067e-01,  6.2323e-02, -1.1983e-01,  9.3827e-02, -2.5079e-03,
           -1.9087e-02, -3.3175e-02,  1.7134e-02,  2.5832e-01,  2.5063e-01,
           -3.3048e-01,  2.0938e-02, -4.4767e-02, -6.2056e-02,  5.3011e-02,
            2.1286e-03,  1.9560e-03,  2.1666e-03, -2.4348e-03, -1.2749e-01,
           -1.8948e-02,  1.8108e-01,  2.9247e-01, -2.3545e-01, -2.0116e-01,
            1.5065e-01,  2.3340e-02],
          [-1.8685e-02,  2.0998e-02,  2.6289e-01,  2.0823e-01, -4.3975e-01,
            1.2865e-01,  5.3901e-03,  1.3571e-01, -8.0198e-02, -7.9435e-04,
           -4.2736e-02,  2.3246e-01,  2.6185e-01, -2.7425e-01, -1.4103e-01,
            2.6124e-01,  1.1020e-01, -1.6670e-01, -2.9994e-02, -4.3580e-02,
            2.0570e-01,  3.4394e-01,  4.0042e-02,  2.6268e-02,  1.3986e-01,
           -1.6066e-01, -8.9756e-02, -2.4718e-02,  9.2174e-02, -5.0054e-02,
           -6.0242e-03, -1.1569e-02],
          [ 3.6765e-03,  2.3577e-01,  1.3674e-01, -2.8543e-01,  3.7657e-01,
           -1.4159e-01, -7.5554e-02, -9.7923e-02,  2.8063e-02,  3.1795e-02,
            9.9017e-02, -5.0183e-03, -2.0499e-01,  1.4080e-01,  2.4582e-01,
           -1.7640e-01, -5.3545e-02,  1.4003e-01,  1.0145e-01, -2.9043e-02,
            1.8688e-02, -3.5178e-02, -5.7921e-02,  3.9981e-02, -7.4710e-02,
            8.3958e-02, -1.9901e-01,  4.7514e-02,  1.6867e-02,  1.9771e-02,
           -4.1109e-02, -2.2629e-02],
          [-4.3515e-02, -1.5010e-01, -3.3767e-01,  1.0386e-01, -9.6563e-02,
            1.2332e-01, -8.8741e-02,  1.7166e-02,  2.6720e-02, -2.4481e-02,
           -4.2976e-02, -2.0866e-02,  3.5609e-01, -2.5680e-01, -1.1649e-01,
            3.7709e-02,  8.7389e-03, -2.9186e-02, -6.8972e-03,  4.5004e-02,
           -2.1142e-01, -1.8473e-01,  1.7534e-01,  1.1737e-01,  1.2214e-01,
            3.6124e-02,  1.4660e-01, -2.1476e-01, -2.7009e-02,  4.4966e-02,
            1.0209e-02,  1.3304e-02],
          [ 1.9131e-02, -1.8345e-01, -1.4961e-03, -1.9416e-01, -2.5459e-02,
            1.3399e-01,  8.4928e-02, -1.8027e-02, -4.9052e-03, -6.7612e-04,
            1.5419e-01,  1.5889e-01, -3.0608e-01,  1.2210e-04,  4.4578e-02,
           -1.5144e-02,  5.5965e-04, -6.8384e-02,  9.9943e-02,  9.9044e-02,
           -1.7223e-01,  1.3924e-02,  2.9056e-02, -4.4108e-02, -7.8111e-02,
            3.5830e-02, -3.4895e-02,  7.4901e-02, -2.2303e-02,  1.0467e-03,
            1.0220e-01,  8.4928e-02],
          [ 1.5244e-01,  2.4322e-01,  5.6006e-02, -6.8193e-02,  2.4179e-02,
           -6.3059e-02, -4.6766e-02,  2.3135e-01,  1.9176e-01, -1.2312e-01,
           -1.0546e-01, -3.5759e-01,  3.2863e-01,  1.5175e-01, -1.1222e-01,
            3.1884e-02,  2.0252e-02, -1.3003e-02, -4.0108e-02, -5.2267e-02,
            2.5114e-01,  2.0635e-01, -2.2646e-01, -9.9481e-02,  1.1983e-01,
           -1.5912e-01, -3.6787e-03, -2.2749e-02, -3.4445e-02, -6.6920e-02,
            7.0794e-03, -4.6766e-02],
          [-5.0073e-02, -1.2064e-01,  6.8518e-02,  1.8640e-01, -5.5797e-02,
           -6.9972e-02,  9.2017e-03, -7.7337e-02,  3.5995e-02,  2.6358e-01,
           -3.0779e-01,  7.1180e-02, -1.3196e-02, -1.3202e-01,  7.3163e-02,
           -3.2894e-02,  1.0019e-01,  1.1406e-01, -1.4623e-01, -1.6720e-02,
           -1.1213e-01, -9.1637e-02,  7.0233e-02, -1.8943e-02, -3.2640e-02,
           -4.7882e-02, -1.3028e-01,  6.3852e-02,  1.2862e-01,  2.6706e-01,
           -4.9219e-02, -7.5852e-02],
          [ 1.0147e-03, -1.6417e-01, -1.3644e-02, -3.4727e-02,  4.5855e-02,
            5.5822e-02,  6.2865e-02, -1.9873e-01, -4.0703e-02, -7.6528e-02,
            2.5150e-01,  1.5474e-01, -2.6657e-01,  4.4896e-02,  1.2481e-02,
           -3.4218e-03, -5.4593e-02, -5.4140e-02,  1.4742e-01,  1.0437e-01,
           -2.1434e-01,  2.2436e-01,  2.3900e-01, -2.0345e-01,  1.3887e-01,
            1.8040e-01, -4.2024e-02, -5.5124e-02,  1.1081e-02, -1.1892e-01,
           -7.0212e-03,  7.9523e-02],
          [-1.2135e-01,  1.0414e-01, -5.9667e-02,  2.5298e-01,  2.5961e-01,
           -1.9216e-01, -3.3581e-02,  3.7738e-02, -2.0824e-01, -7.6222e-02,
            8.9542e-03, -1.1150e-01,  9.6274e-02,  6.8584e-04,  8.5471e-02,
            6.6988e-02, -1.2921e-01,  1.8735e-01,  1.5300e-01, -1.4713e-01,
            5.2483e-02, -9.7165e-02, -1.2607e-01,  1.4168e-01,  6.6898e-02,
            2.1796e-01,  7.9256e-02,  9.4605e-02,  7.5734e-02, -2.0386e-01,
            1.5485e-02,  1.9192e-03],
          [ 4.6458e-02, -1.9465e-02,  2.0626e-02, -1.1943e-01, -1.3838e-01,
            2.3586e-01,  1.4846e-01, -9.0165e-02,  3.7642e-02,  4.9052e-02,
           -2.0740e-01,  1.5382e-01,  1.7206e-01, -1.0090e-01, -2.8533e-02,
           -3.7183e-02, -2.0613e-02, -1.4458e-01, -1.3463e-01,  2.7460e-01,
            1.6344e-01, -2.9900e-01, -2.1981e-03, -2.5542e-02, -1.5353e-01,
           -9.3565e-02,  3.4561e-02, -1.3598e-02, -8.8014e-02,  2.0969e-01,
            1.1881e-01, -7.6281e-02],
          [-7.0338e-03,  5.6771e-03,  5.1782e-03, -2.1197e-01,  2.7228e-02,
           -5.4395e-02, -7.5775e-02,  8.5155e-02,  7.5839e-02, -9.6516e-03,
           -1.8365e-02, -6.2252e-02, -8.3650e-02,  9.4340e-02, -5.7488e-02,
            7.3161e-03,  2.8441e-02, -1.2071e-01, -2.4043e-02, -6.7761e-02,
           -8.3989e-02,  9.7312e-02,  1.6302e-01,  1.5057e-01, -2.6119e-01,
           -4.1523e-02,  1.1388e-02, -1.3478e-01,  1.7318e-02, -4.6315e-02,
           -2.1786e-02,  1.1602e-01],
          [ 2.0252e-02, -3.6677e-02, -6.4838e-03,  1.9368e-01,  1.9697e-01,
           -2.4362e-01,  1.2323e-01,  1.1822e-01, -1.1810e-01,  1.1246e-01,
            2.0063e-01, -2.3937e-01,  1.8345e-01,  1.8131e-01, -7.1533e-02,
            4.6563e-02,  4.4257e-02,  1.3218e-02, -1.2310e-02,  8.8344e-02,
            2.5061e-01, -1.4918e-01, -4.2531e-02, -7.7765e-02,  1.1236e-01,
           -1.7386e-02,  1.5766e-01,  3.8298e-01,  1.2083e-01, -2.4541e-01,
           -5.8361e-03, -2.2157e-02],
          [-1.0818e-02, -1.2189e-03,  2.4786e-01,  2.8625e-01, -1.6452e-01,
            4.7488e-02, -4.7620e-02,  5.1840e-02,  2.1102e-01, -1.2346e-01,
           -1.1918e-01,  1.2761e-01, -4.6408e-02, -1.1296e-01,  8.4398e-02,
           -2.9153e-02,  2.2399e-02,  1.4033e-01,  1.7761e-01, -2.0125e-01,
           -1.2613e-01,  1.4168e-01, -1.3388e-01,  4.0957e-02, -6.9557e-04,
           -4.1958e-04, -1.5036e-02, -1.2615e-01, -5.8410e-02,  1.3771e-01,
           -3.6918e-02,  4.6983e-03],
          [ 1.7877e-01,  1.8974e-01, -2.2985e-01, -3.4888e-01,  1.2032e-01,
            8.9204e-02, -7.4287e-02, -5.1768e-02, -2.5025e-02, -7.2527e-02,
            2.3825e-01,  2.2842e-01, -3.1445e-01,  3.5015e-02, -3.4108e-02,
            7.1804e-03,  2.2292e-01,  1.9473e-01, -2.5262e-01, -9.7607e-02,
            6.0200e-02,  1.7981e-01,  1.2409e-01, -9.5321e-02,  8.6246e-02,
            9.2497e-02, -2.2290e-01, -2.0278e-01,  1.6354e-01,  1.5617e-01,
           -2.3696e-02, -2.9896e-02],
          [-1.9042e-02, -3.8334e-02, -1.4468e-01, -6.3269e-02,  1.5408e-02,
           -3.0504e-02, -3.6475e-02, -7.1892e-02,  1.2839e-02,  1.7697e-03,
           -9.7155e-02, -1.4889e-01,  1.4511e-01,  3.9091e-04,  3.5349e-03,
            1.0158e-01,  3.1745e-03, -2.0409e-01,  3.2101e-01, -9.2972e-02,
           -1.5560e-02, -8.1135e-02,  1.8877e-03,  1.5176e-01, -7.8135e-02,
           -4.9411e-02,  7.4507e-03,  5.0258e-02,  3.3257e-03, -1.3116e-01,
            1.5912e-02,  4.3943e-02],
          [-1.8223e-01,  1.2476e-01,  1.8895e-01, -9.5321e-02,  6.7363e-02,
            6.4568e-02, -3.0805e-02, -1.4637e-02,  9.5013e-02,  1.0190e-01,
           -2.5621e-01,  1.0753e-01,  8.2907e-02,  8.9180e-02,  1.5272e-01,
           -1.4291e-01, -2.7240e-01,  3.2061e-02, -7.5553e-02,  9.4088e-02,
           -2.6026e-02, -1.2204e-01, -9.5234e-03,  1.2860e-01,  1.3272e-01,
           -1.0039e-01,  1.6372e-01,  1.5474e-01, -2.2747e-01,  1.1673e-02,
            1.9784e-02, -1.4277e-02],
          [-5.1920e-02, -5.4251e-02, -7.8396e-02,  8.9123e-02, -2.9615e-02,
           -1.8023e-02,  4.0752e-02, -3.6082e-04, -4.1771e-02,  2.9352e-02,
            1.4885e-01, -9.4518e-02, -4.1083e-02,  1.3055e-01,  7.2205e-02,
            2.8335e-02,  1.4639e-01, -2.0763e-01,  5.7284e-02,  4.4668e-02,
           -1.2661e-02,  8.2064e-02, -1.8729e-03, -1.0408e-01, -9.0708e-02,
            1.0296e-01, -7.1386e-02, -1.3212e-01,  2.5605e-01,  2.4331e-01,
           -3.3912e-02, -4.5052e-02],
          [ 5.3246e-03, -1.2603e-01,  7.4011e-02,  6.4791e-02, -9.7382e-02,
            1.0279e-02,  1.8554e-02,  2.0626e-02,  4.9130e-02,  1.3103e-01,
           -1.5182e-01, -1.3958e-02,  1.6242e-01, -2.3210e-02, -1.7189e-01,
            3.9240e-02, -7.2054e-02,  8.0973e-02, -1.8324e-02, -2.8276e-02,
            1.9092e-02, -3.6511e-02, -2.1424e-02, -1.0391e-01, -9.5516e-04,
           -1.7873e-02, -1.1152e-01,  3.8929e-02, -7.5140e-02, -1.1248e-01,
            5.7158e-02,  4.1785e-02],
          [ 1.7408e-02, -9.8064e-03, -2.5678e-02, -3.3462e-02,  3.6503e-02,
            9.5296e-02,  9.6899e-02,  1.7094e-01,  2.1454e-01, -3.2932e-01,
            1.0378e-01,  1.3018e-03, -6.7541e-02, -2.1937e-01,  1.1042e-01,
           -1.0619e-01,  1.4177e-02,  1.9542e-03, -3.6918e-02,  4.6983e-03,
           -1.6078e-02,  1.7571e-02,  7.9146e-02, -4.5475e-02,  8.3449e-03,
            1.0875e-02,  5.6824e-02,  7.2099e-02, -1.7078e-01, -4.0970e-02,
            5.3009e-02,  4.7572e-02],
          [-1.0259e-02,  1.2235e-02,  6.8815e-03,  6.9449e-02, -4.1771e-02,
           -4.2349e-02, -7.2346e-02, -4.4832e-02, -2.7190e-01,  3.7988e-01,
            2.2035e-01, -1.4001e-01, -6.8716e-02,  9.1471e-02, -4.1512e-02,
           -6.5754e-03, -4.0558e-03,  2.6041e-03,  1.2505e-01,  1.3411e-01,
            7.3809e-02,  1.4189e-01, -1.4871e-01,  3.9030e-02,  1.2159e-02,
            8.4827e-04, -5.1401e-02, -4.2232e-02,  1.5223e-01,  1.0943e-01,
           -8.9346e-02, -2.4873e-02],
          [ 3.4743e-02,  1.8426e-01,  1.1156e-01,  1.9399e-02,  2.0463e-01,
           -1.9275e-01,  1.0841e-02, -2.1866e-01,  2.8019e-02, -8.5529e-02,
           -9.6295e-02,  9.0168e-02, -1.1044e-01,  8.2178e-03,  4.2880e-02,
            1.5952e-03,  2.1666e-03, -2.9439e-02, -8.3936e-02, -5.3044e-02,
            1.0203e-02, -1.5128e-01,  1.1305e-01,  5.6399e-03,  1.7740e-01,
            1.8665e-01, -1.9028e-01,  8.7361e-03, -4.3653e-02, -1.6775e-02,
            7.1935e-02, -2.3806e-02],
          [-1.4339e-02, -8.4187e-02, -1.1628e-01,  3.4136e-02, -8.8132e-02,
            3.4003e-01,  2.5726e-01, -1.9003e-02,  1.5677e-01, -2.8076e-01,
            1.4043e-02,  1.9022e-02, -1.0918e-02,  1.4406e-01,  1.9049e-01,
           -1.1828e-01,  1.5072e-01,  1.7392e-01, -1.9937e-01, -3.2901e-03,
           -1.4657e-02,  1.5214e-01, -8.4722e-02,  2.4089e-03, -7.6535e-02,
            6.3613e-02,  2.9056e-01, -1.1353e-01,  3.5393e-02,  1.2806e-01,
           -1.0890e-01,  2.6834e-02],
          [-7.2403e-02, -1.7496e-01,  4.7266e-02, -1.3549e-01,  3.0250e-01,
            2.0257e-01, -3.3365e-01,  2.1594e-01,  5.1143e-02,  7.5050e-03,
            2.3793e-02,  1.0225e-01, -3.5420e-02, -7.8628e-02, -1.0120e-01,
            1.1373e-01,  2.1851e-01,  2.4294e-01, -1.0195e-01, -4.1958e-04,
           -5.4974e-02, -7.1639e-02,  2.2222e-01,  1.5464e-01, -2.5553e-01,
           -5.2575e-02, -9.1715e-02,  3.1130e-01,  1.6472e-01, -2.5023e-01,
            8.5671e-02,  6.4760e-04],
          [ 1.9131e-02,  1.0278e-01,  5.5475e-02, -5.3605e-02, -1.2528e-01,
           -3.7573e-01,  3.7538e-01, -4.2095e-03, -1.6911e-01,  8.3244e-02,
           -1.8324e-02, -6.7393e-02, -1.9834e-02, -1.2650e-01,  9.7760e-02,
            8.6235e-02, -3.1276e-01,  4.9310e-03,  3.4791e-01, -1.0408e-01,
           -1.3776e-01, -1.4866e-02, -4.2738e-02, -8.2458e-02,  1.5378e-01,
           -8.0493e-02,  5.8030e-02, -1.8745e-04, -2.8507e-01,  1.4626e-01,
            1.1027e-01,  1.1617e-01],
          [ 1.5244e-01,  8.3021e-02, -1.2370e-01,  5.0120e-02, -2.5241e-01,
            9.4422e-02, -1.2440e-03, -2.4876e-01,  1.7895e-01,  6.4819e-02,
           -7.7280e-02,  2.3829e-02,  2.5577e-02,  1.1685e-01,  1.2114e-01,
           -1.4435e-01, -1.2348e-01,  1.8862e-01,  1.2029e-02,  1.9912e-03,
            1.1872e-02,  1.5471e-02, -1.4487e-01,  1.4589e-01,  1.1443e-01,
            2.4989e-02,  1.6362e-01, -3.3122e-01,  2.4501e-01,  2.0505e-01,
           -1.4261e-01, -7.5961e-02],
          [-5.0073e-02,  1.0656e-02,  2.3747e-01, -1.9731e-02,  8.4522e-02,
           -8.1657e-02, -1.8174e-01,  5.8696e-02, -2.3208e-02, -3.3581e-02,
            8.1155e-02,  1.9900e-01,  8.7350e-02, -1.4279e-01, -1.5195e-01,
            1.0204e-01, -3.3438e-02, -2.3989e-01, -1.1529e-02,  1.7772e-01,
            1.1642e-01, -6.2432e-02,  1.2999e-01,  8.2854e-02, -2.1822e-01,
            2.4736e-02, -1.5680e-01,  1.0531e-01, -7.3205e-02, -1.0592e-01,
            1.5471e-03,  2.5211e-02],
          [ 1.0147e-03, -1.1627e-01, -8.1222e-02,  1.0276e-01,  2.1889e-03,
           -3.4837e-02,  7.0227e-03,  9.6799e-04, -5.1920e-02,  6.6075e-03,
           -1.8177e-02, -7.4946e-02, -3.7299e-02, -5.4984e-02,  1.7870e-01,
            2.2054e-01, -1.3428e-01, -1.4413e-01,  1.6829e-02, -3.5766e-02,
            4.5602e-02,  2.2479e-01, -5.0511e-02, -2.3595e-01,  1.7036e-01,
           -5.3227e-02, -3.2605e-02,  4.6460e-02, -1.0270e-01, -2.9186e-02,
            4.9084e-03,  3.1376e-04],
          [-5.4433e-02, -6.7142e-02,  1.7870e-01,  8.7183e-02, -1.2821e-01,
            2.9643e-02,  7.1790e-02,  1.4436e-01,  2.2811e-02, -6.9608e-02,
           -6.3362e-02, -8.0000e-02,  1.0853e-01,  1.0367e-01, -1.3681e-01,
           -1.1243e-01,  7.9253e-02,  2.8418e-02, -2.3223e-02, -1.0130e-01,
           -1.6058e-02, -8.2708e-02, -9.2930e-02,  5.8070e-02, -2.9535e-02,
           -4.1129e-02,  2.0860e-01,  1.5474e-01, -1.3710e-01,  5.1958e-03,
           -2.4906e-02,  1.5991e-02],
          [ 1.0710e-02,  1.4763e-03, -7.1386e-02, -1.3212e-01,  1.0952e-01,
            6.8584e-04, -3.1561e-02, -7.0744e-02,  7.6277e-03,  5.3579e-02,
            1.3304e-02, -1.4951e-02, -4.1771e-02, -5.4433e-02, -8.2654e-02,
           -2.0794e-02,  7.4986e-03,  6.8584e-04,  1.0209e-02,  1.3304e-02,
           -9.9110e-02, -4.8957e-02,  9.0697e-03,  1.0467e-03, -6.3589e-02,
            8.0926e-03, -7.0270e-02, -1.3212e-01,  1.0952e-01,  1.0895e-02,
            1.3304e-02, -1.4951e-02]]]], device='cuda:0')[torch/csrc/autograd/engine.cpp] call_function SumBackward0
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function [torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::CopyBackwards
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward


&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&


&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&


^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 1, 6, 6])
grad_out size torch.Size([1, 1, 8, 8])
arg size torch.Size([1, 1, 3, 3])
maxpool2d140372191067248[0,-1] PI( <3,3,>,
 <otileshape 3,3,>,
 <padding >,
 <inpslidx 10,15,10,15,>, 
 <internal >, 
 <realidx 10,15,10,15,>, 
 <ndtsize 2,2,>, 
  local_first False
 next_id -99)

bk-maxpool2d140372191067248[3,-1] PI( <3,3,>,
 <otileshape 3,3,>,
 <padding >,
 <inpslidx 5,7,5,7,>, 
 <internal >, 
 <realidx 5,7,5,7,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191067488)

new_grad_out torch.Size([1, 1, 3, 3])
##############grad_in in maxp torch.Size([1, 1, 6, 6])
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 1, 8, 8])
local last ++ input tensor([[[[ 0.2586,  0.1248,  0.1634,  0.3155,  0.2679, -0.0112,  0.1765,
            0.0000],
          [ 0.1913,  0.2709, -0.0339,  0.2026,  0.2329,  0.1941,  0.2226,
            0.0000],
          [ 0.3763,  0.3157,  0.2616,  0.1460,  0.3505,  0.2850,  0.0497,
            0.0000],
          [ 0.0804,  0.2784,  0.2649,  0.4995,  0.2634,  0.1926,  0.3223,
            0.0000],
          [ 0.2235,  0.0890,  0.2459,  0.1678,  0.1321,  0.4379,  0.1063,
            0.0000],
          [ 0.4876,  0.0402,  0.4138,  0.2646,  0.2885,  0.3276,  0.2683,
            0.0000],
          [ 0.2592,  0.2528,  0.1004,  0.4103,  0.4021,  0.0565,  0.2456,
            0.0000],
          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
            0.0000]]]], device='cuda:0')
weight shape torch.Size([1, 1, 3, 3])
grad_output shape torch.Size([1, 1, 6, 6])
grad_input torch.Size([1, 1, 8, 8])
padding info :: [0, 1, 0, 1]
new grad_input torch.Size([1, 1, 7, 7])

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 1, 14, 14])
grad_out size torch.Size([1, 1, 7, 7])
arg size torch.Size([1, 1, 7, 7])
maxpool2d140372191117952[2,-1] PI( <3,3,>,
 <otileshape 7,7,>,
 <padding >,
 <inpslidx 18,31,18,31,>, 
 <internal >, 
 <realidx 18,31,18,31,>, 
 <ndtsize 4,4,>, 
  local_first False
 next_id 140372191067488)

bk-maxpool2d140372191117952[1,-1] PI( <3,3,>,
 <otileshape 5,5,>,
 <padding >,
 <inpslidx 11,15,11,15,>, 
 <internal >, 
 <realidx 11,15,11,15,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191117808)

##############grad_in in maxp torch.Size([1, 1, 14, 14])
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 1, 16, 16])
weight shape torch.Size([1, 1, 3, 3])
grad_output shape torch.Size([1, 1, 14, 14])
final torch.Size([1, 1, 16, 16]) tensor([[[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            8.1630e-02,  8.7546e-02, -5.6209e-02,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00],
          [ 0.0000e+00,  0.0000e+00, -6.2398e-02, -6.6920e-02,  4.2967e-02,
           -3.5887e-02,  1.8136e-02,  1.2216e-01, -4.4691e-02,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00],
          [ 0.0000e+00,  0.0000e+00,  2.7432e-02,  5.4631e-02,  3.8695e-02,
           -2.2442e-02, -5.9694e-02, -3.5914e-02,  4.1785e-02,  0.0000e+00,
            0.0000e+00,  1.4653e-01,  2.2205e-01, -3.1294e-02, -4.4691e-02,
            0.0000e+00],
          [ 0.0000e+00,  0.0000e+00,  5.5269e-02, -1.5336e-02, -3.7557e-02,
           -2.1424e-02, -1.9749e-02, -1.1665e-02, -1.9349e-02,  3.2302e-02,
            2.0626e-02, -7.7663e-02, -1.1248e-01,  5.7158e-02,  4.1785e-02,
            0.0000e+00],
          [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -1.6726e-02,  1.7571e-02,
            7.9146e-02, -4.5475e-02,  8.3449e-03,  1.0875e-02,  5.6824e-02,
            7.2099e-02, -1.7078e-01, -4.0970e-02,  5.3009e-02,  4.7572e-02,
           -2.9896e-02],
          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  9.5013e-02,  7.2284e-02,
           -1.0402e-01,  3.9030e-02,  1.2159e-02,  8.4827e-04, -5.1401e-02,
           -4.2232e-02,  1.5223e-01,  1.0943e-01, -8.9346e-02, -2.4873e-02,
            2.7952e-02],
          [-6.2398e-02, -6.6920e-02,  4.2967e-02, -4.1771e-02, -1.1410e-01,
            7.1269e-02,  5.6399e-03,  1.7740e-01,  1.8665e-01, -1.9028e-01,
            8.7361e-03, -4.3653e-02, -1.6775e-02,  7.1935e-02, -2.3806e-02,
            6.7459e-04],
          [ 2.7432e-02,  3.5748e-02, -4.0173e-02, -2.5289e-03,  9.8256e-02,
           -5.5834e-02,  2.4089e-03, -7.6535e-02,  6.3613e-02,  2.9056e-01,
           -1.1353e-01,  3.5393e-02,  1.2806e-01, -1.0890e-01,  2.6834e-02,
            0.0000e+00],
          [ 5.5269e-02, -7.0338e-03, -9.6952e-04, -3.5887e-02, -4.6766e-02,
            1.9427e-01,  1.5464e-01, -2.5553e-01, -5.2575e-02, -9.1715e-02,
            3.1130e-01,  1.6472e-01, -2.5023e-01,  8.5671e-02,  6.4760e-04,
            0.0000e+00],
          [ 1.8883e-02,  2.0252e-02, -1.3003e-02, -7.6086e-02,  5.1459e-03,
           -5.9404e-02, -8.2458e-02,  1.5378e-01, -8.0493e-02,  5.8030e-02,
           -1.8745e-04, -2.8507e-01,  1.4626e-01,  1.1027e-01,  1.1617e-01,
           -7.4587e-02],
          [ 8.6711e-02,  9.1081e-02, -5.3267e-02,  1.6626e-03,  2.1666e-03,
           -1.2992e-01,  1.4589e-01,  1.1443e-01,  2.4989e-02,  1.6362e-01,
           -3.3122e-01,  2.4501e-01,  2.0505e-01, -1.4261e-01, -7.5961e-02,
            6.9738e-02],
          [-5.8497e-02, -5.2304e-02,  1.4771e-01,  9.5847e-02, -5.9814e-02,
            1.3035e-01,  8.2854e-02, -2.1822e-01,  2.4736e-02, -1.5680e-01,
            1.0531e-01, -7.3205e-02, -1.0592e-01,  1.5471e-03,  2.5211e-02,
            1.6830e-03],
          [-8.4158e-02,  1.0710e-02, -3.6440e-02,  4.5602e-02,  2.2479e-01,
           -5.0511e-02, -2.3595e-01,  1.7036e-01, -5.3227e-02, -3.2605e-02,
            4.6460e-02, -1.0270e-01, -2.9186e-02,  4.9084e-03,  3.1376e-04,
            0.0000e+00],
          [ 0.0000e+00,  0.0000e+00, -7.6393e-02, -3.2049e-02, -8.2708e-02,
           -9.2930e-02,  5.8070e-02, -2.9535e-02, -4.1129e-02,  2.0860e-01,
            1.5474e-01, -1.3710e-01,  5.1958e-03, -2.4906e-02,  1.5991e-02,
            0.0000e+00],
          [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -8.4158e-02, -4.8957e-02,
            9.0697e-03,  1.0467e-03, -6.3589e-02,  8.0926e-03, -7.0270e-02,
           -1.3212e-01,  1.0952e-01,  1.0895e-02,  1.3304e-02, -1.4951e-02,
            0.0000e+00],
          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.4383e-01,
            1.8304e-02,  2.5229e-03,  2.0570e-02, -2.6178e-03, -3.6082e-04,
            0.0000e+00]]]], device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 1, 6, 8])
grad_out size torch.Size([1, 1, 8, 8])
arg size torch.Size([1, 1, 3, 4])
maxpool2d140372191067248[0,-1] PI( <3,2,>,
 <otileshape 4,3,>,
 <padding >,
 <inpslidx 6,13,10,15,>, 
 <internal >, 
 <realidx 6,13,10,15,>, 
 <ndtsize 2,2,>, 
  local_first False
 next_id -99)

bk-maxpool2d140372191067248[3,-1] PI( <3,2,>,
 <otileshape 4,3,>,
 <padding >,
 <inpslidx 3,6,5,7,>, 
 <internal >, 
 <realidx 3,6,5,7,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191067488)

new_grad_out torch.Size([1, 1, 3, 4])
##############grad_in in maxp torch.Size([1, 1, 6, 8])
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 1, 8, 10])
local last ++ input [torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
tensor([[[[ 0.0020,  0.2103,  0.2201,  0.1518,  0.2586,  0.1248,  0.1634,
            0.3155,  0.2679, -0.0112],
          [ 0.2083,  0.3464,  0.3073,  0.3793,  0.1913,  0.2709, -0.0339,
            0.2026,  0.2329,  0.1941],
          [ 0.4450,  0.2202,  0.1920,  0.1985,  0.3763,  0.3157,  0.2616,
            0.1460,  0.3505,  0.2850],
          [-0.0315,  0.2760,  0.3580,  0.4210,  0.0804,  0.2784,  0.2649,
            0.4995,  0.2634,  0.1926],
          [ 0.2310,  0.1645,  0.0601,  0.2607,  0.2235,  0.0890,  0.2459,
            0.1678,  0.1321,  0.4379],
          [ 0.1631,  0.0086,  0.3127,  0.1987,  0.4876,  0.0402,  0.4138,
            0.2646,  0.2885,  0.3276],
          [ 0.2325,  0.3420,  0.5963,  0.1908,  0.2592,  0.2528,  0.1004,
            0.4103,  0.4021,  0.0565],
          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
            0.0000,  0.0000,  0.0000]]]], device='cuda:0')
weight shape torch.Size([1, 1, 3, 3])
grad_output shape torch.Size([1, 1, 6, 8])
grad_input torch.Size([1, 1, 8, 10])
padding info :: [0, 0, 0, 1]
new grad_input torch.Size([1, 1, 7, 10])

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 1, 14, 20])
grad_out size torch.Size([1, 1, 7, 10])
arg size torch.Size([1, 1, 7, 10])
maxpool2d140372191117952[2,-1] PI( <3,2,>,
 <otileshape 10,7,>,
 <padding >,
 <inpslidx 10,29,18,31,>, 
 <internal >, 
 <realidx 10,29,18,31,>, 
 <ndtsize 4,4,>, 
  local_first False
 next_id 140372191067488)

bk-maxpool2d140372191117952[1,-1] PI( <3,2,>,
 <otileshape 6,5,>,
 <padding >,
 <inpslidx 7,12,11,15,>, 
 <internal >, 
 <realidx 7,12,11,15,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191117808)

##############grad_in in maxp torch.Size([1, 1, 14, 20])
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 1, 16, 22])
weight shape torch.Size([1, 1, 3, 3])
grad_output shape torch.Size([1, 1, 14, 20])
final torch.Size([1, 1, 16, 22]) tensor([[[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  8.1630e-02,  8.7546e-02,
           -5.6209e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00],
          [ 0.0000e+00, -6.2398e-02,  1.4709e-02,  1.3051e-01,  8.6925e-03,
            6.9606e-02, -4.4691e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,
           -6.2398e-02, -6.6920e-02,  4.2967e-02, -3.5887e-02,  1.8136e-02,
            1.2216e-01, -4.4691e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00],
          [ 1.8883e-02,  4.7684e-02, -1.3142e-02, -8.6939e-02,  5.0402e-03,
           -5.7540e-02,  1.3649e-01,  8.7546e-02, -5.6209e-02,  6.4902e-02,
            9.7038e-02,  9.9403e-03,  3.8695e-02, -2.2442e-02, -5.9694e-02,
           -3.5914e-02,  4.1785e-02,  0.0000e+00,  0.0000e+00,  6.4902e-02,
            6.9606e-02, -4.4691e-02],
          [-8.3017e-03,  4.4451e-02, -6.7181e-02,  6.6848e-02,  1.4990e-02,
           -2.2172e-02, -4.7099e-02, -4.6766e-02,  5.2555e-02, -2.8533e-02,
            1.8087e-02,  2.6450e-02, -3.7557e-02, -2.1424e-02, -1.9749e-02,
           -1.1665e-02, -1.9349e-02,  9.4700e-02,  8.7546e-02, -8.4743e-02,
           -3.7183e-02,  4.1785e-02],
          [-1.6726e-02,  2.1286e-03,  2.9340e-04, -2.5770e-02, -2.1108e-02,
            3.0945e-02, -6.9611e-02,  9.2017e-03,  1.2683e-03, -5.7488e-02,
            7.3161e-03, -1.5718e-02,  1.7571e-02,  7.9146e-02, -4.5475e-02,
            8.3449e-03,  1.0875e-02,  1.0508e-02,  1.6099e-02, -4.5295e-02,
            7.3161e-03,  1.0084e-03],
          [ 9.5013e-02,  1.0190e-01, -6.5425e-02,  1.5443e-02,  8.0760e-02,
           -4.2989e-02, -6.5754e-03, -4.0558e-03,  2.6041e-03,  1.2505e-01,
            1.3411e-01,  7.3809e-02,  1.4189e-01, -1.4871e-01,  3.9030e-02,
            1.2159e-02,  8.4827e-04, -9.8369e-02, -2.4380e-02,  8.2423e-02,
            4.6563e-02, -2.9896e-02],
          [-4.1771e-02, -5.4433e-02, -1.2267e-03, -9.6535e-02,  8.2178e-03,
            4.2880e-02,  1.5952e-03,  2.1666e-03, -2.9439e-02, -8.3936e-02,
           -5.3044e-02,  1.0203e-02, -1.5128e-01,  1.1305e-01,  5.6399e-03,
            1.7740e-01,  1.8665e-01, -1.7355e-01,  6.6075e-03, -1.8177e-02,
           -2.4873e-02,  2.7952e-02],
          [-8.4158e-02,  1.0710e-02,  2.8909e-02, -2.3919e-02,  1.4406e-01,
            1.9049e-01, -1.1828e-01,  1.5072e-01,  1.7392e-01, -1.9937e-01,
           -3.2901e-03, -1.4657e-02,  1.5214e-01, -8.4722e-02,  2.4089e-03,
           -7.6535e-02,  3.0998e-02,  2.5558e-01, -9.1076e-02, -6.1679e-02,
           -2.0012e-02,  1.6665e-02],
          [ 0.0000e+00,  0.0000e+00,  5.5269e-02, -7.0338e-03, -7.8628e-02,
           -1.0120e-01,  1.1373e-01,  2.1851e-01,  2.4294e-01, -1.0195e-01,
           -4.1958e-04, -5.4974e-02, -7.1639e-02,  2.2222e-01,  1.5464e-01,
           -2.5553e-01, -3.8236e-02, -7.3030e-02,  1.5006e-01,  7.9815e-02,
           -3.1387e-02, -1.4951e-02],
          [ 0.0000e+00,  0.0000e+00, -4.3515e-02, -4.6668e-02, -1.2650e-01,
            9.7760e-02,  8.6235e-02, -3.1276e-01,  4.9310e-03,  3.4791e-01,
           -1.0408e-01, -1.3776e-01, -1.4866e-02, -4.2738e-02, -8.2458e-02,
            1.5378e-01, -5.1604e-02,  9.7869e-02,  1.0763e-01, -1.0272e-01,
            3.9167e-02, -3.6082e-04],
          [ 0.0000e+00,  0.0000e+00,  1.9131e-02,  2.4930e-02,  1.1685e-01,
            1.2114e-01, -1.4435e-01, -1.2348e-01,  1.8862e-01,  1.2029e-02,
            1.9912e-03,  1.1872e-02,  1.5471e-02, -1.4487e-01,  1.4589e-01,
            1.1443e-01, -8.8907e-02,  2.2335e-02, -1.5350e-01,  1.3124e-01,
            4.7572e-02, -2.9896e-02],
          [ 0.0000e+00,  0.0000e+00,  1.5244e-01,  1.1725e-01, -1.4279e-01,
           -1.5195e-01,  1.0204e-01, -3.3438e-02, -2.3989e-01, -1.1529e-02,
            1.7772e-01,  1.1642e-01, -6.2432e-02,  1.2999e-01,  8.2854e-02,
           -2.1822e-01,  7.4809e-02, -1.3009e-01,  3.6883e-02, -1.7144e-02,
           -2.4873e-02,  2.7952e-02],
          [ 0.0000e+00,  0.0000e+00, -5.0073e-02, -6.5251e-02, -5.4984e-02,
            1.7870e-01,  2.2054e-01, -1.3428e-01, -1.4413e-01,  1.6829e-02,
           -3.5766e-02,  4.5602e-02,  2.2479e-01, -5.0511e-02, -2.3595e-01,
            1.7036e-01,  4.7658e-02, -4.5444e-02, -2.2673e-02, -6.3362e-02,
            2.0885e-02,  6.7459e-04],
          [ 0.0000e+00,  0.0000e+00, -1.0088e-01,  1.0785e-01,  1.0367e-01,
           -1.3681e-01, -1.1243e-01,  7.9253e-02,  2.8418e-02, -2.3223e-02,
           -1.0130e-01, -1.6058e-02, -8.2708e-02, -9.2930e-02,  5.8070e-02,
           -2.9535e-02, -4.1129e-02,  1.1358e-01,  8.2454e-02, -3.3081e-02,
           -1.4951e-02,  0.0000e+00],
          [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -4.1771e-02, -5.4433e-02,
           -8.2654e-02, -2.0794e-02,  7.4986e-03,  6.8584e-04,  1.0209e-02,
            1.3304e-02, -9.9110e-02, -4.8957e-02,  9.0697e-03,  1.0467e-03,
           -6.3589e-02,  8.0926e-03, -2.8499e-02, -1.8023e-02,  4.0752e-02,
           -3.6082e-04,  0.0000e+00],
          [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -8.4158e-02,  1.0710e-02,
            1.4763e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,  2.0570e-02,
           -2.6178e-03, -3.6082e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00, -5.9667e-02,  7.5935e-03,  1.0467e-03,
            0.0000e+00,  0.0000e+00]]]], device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 1, 6, 8])
grad_out size torch.Size([1, 1, 8, 8])
arg size torch.Size([1, 1, 3, 4])
maxpool2d140372191067248[0,-1] PI( <3,1,>,
 <otileshape 4,3,>,
 <padding >,
 <inpslidx 2,9,10,15,>, 
 <internal >, 
 <realidx 2,9,10,15,>, 
 <ndtsize 2,2,>, 
  local_first False
 next_id -99)

bk-maxpool2d140372191067248[3,-1] PI( <3,1,>,
 <otileshape 4,3,>,
 <padding >,
 <inpslidx 1,4,5,7,>, 
 <internal >, 
 <realidx 1,4,5,7,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191067488)

new_grad_out torch.Size([1, 1, 3, 4])
##############grad_in in maxp torch.Size([1, 1, 6, 8])
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 1, 8, 10])
local last ++ input [torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
tensor([[[[ 0.2186,  0.2440,  0.0266,  0.4202,  0.0020,  0.2103,  0.2201,
            0.1518,  0.2586,  0.1248],
          [ 0.1671,  0.0660,  0.1479,  0.1243,  0.2083,  0.3464,  0.3073,
            0.3793,  0.1913,  0.2709],
          [ 0.2721,  0.1425,  0.1788,  0.2640,  0.4450,  0.2202,  0.1920,
            0.1985,  0.3763,  0.3157],
          [ 0.2738,  0.1315,  0.3119,  0.3298, -0.0315,  0.2760,  0.3580,
            0.4210,  0.0804,  0.2784],
          [ 0.2194,  0.5167,  0.2021,  0.3370,  0.2310,  0.1645,  0.0601,
            0.2607,  0.2235,  0.0890],
          [ 0.3174,  0.0663,  0.1230,  0.2333,  0.1631,  0.0086,  0.3127,
            0.1987,  0.4876,  0.0402],
          [ 0.1794,  0.2947,  0.4402,  0.2929,  0.2325,  0.3420,  0.5963,
            0.1908,  0.2592,  0.2528],
          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
            0.0000,  0.0000,  0.0000]]]], device='cuda:0')
weight shape torch.Size([1, 1, 3, 3])
grad_output shape torch.Size([1, 1, 6, 8])
grad_input torch.Size([1, 1, 8, 10])
padding info :: [0, 0, 0, 1]
new grad_input torch.Size([1, 1, 7, 10])

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 1, 14, 20])
grad_out size torch.Size([1, 1, 7, 10])
arg size torch.Size([1, 1, 7, 10])
maxpool2d140372191117952[2,-1] PI( <3,1,>,
 <otileshape 10,7,>,
 <padding >,
 <inpslidx 2,21,18,31,>, 
 <internal >, 
 <realidx 2,21,18,31,>, 
 <ndtsize 4,4,>, 
  local_first False
 next_id 140372191067488)

bk-maxpool2d140372191117952[1,-1] PI( <3,1,>,
 <otileshape 6,5,>,
 <padding >,
 <inpslidx 3,8,11,15,>, 
 <internal >, 
 <realidx 3,8,11,15,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191117808)

##############grad_in in maxp torch.Size([1, 1, 14, 20])
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 1, 16, 22])
weight shape torch.Size([1, 1, 3, 3])
grad_output shape torch.Size([1, 1, 14, 20])
final torch.Size([1, 1, 16, 22]) tensor([[[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00],
          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -6.2398e-02,
            1.4709e-02,  1.3051e-01,  8.6925e-03,  6.9606e-02, -4.4691e-02,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00],
          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  8.3785e-02,  1.1729e-01,
           -5.7833e-02, -8.6939e-02,  5.0402e-03, -5.7540e-02,  1.3649e-01,
            8.7546e-02, -5.6209e-02,  6.4902e-02,  6.9606e-02, -4.4691e-02,
            0.0000e+00,  0.0000e+00],
          [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -6.2398e-02, -6.6920e-02,
            6.2198e-02,  2.0626e-02,  1.3329e-01,  1.2032e-01, -9.3632e-02,
           -2.5395e-02,  6.6848e-02,  1.4990e-02, -2.2172e-02, -4.7099e-02,
           -4.6766e-02,  5.2555e-02, -2.8533e-02, -3.7183e-02,  4.1785e-02,
            0.0000e+00,  0.0000e+00],
          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  2.7432e-02,  1.3248e-01,
            5.5114e-02,  1.7094e-01,  2.1454e-01, -3.2932e-01,  1.0378e-01,
            1.3018e-03, -2.5770e-02, -2.1108e-02,  3.0945e-02, -6.9611e-02,
            9.2017e-03,  1.2683e-03, -5.7488e-02,  7.3161e-03,  1.0084e-03,
            0.0000e+00,  0.0000e+00],
          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.1755e-02, -9.6229e-02,
           -4.3458e-02, -4.4832e-02, -2.7190e-01,  3.7988e-01,  2.2035e-01,
           -1.4001e-01,  1.5443e-02,  8.0760e-02, -4.2989e-02, -6.5754e-03,
           -4.0558e-03,  2.6041e-03,  1.2505e-01,  1.3411e-01, -2.1204e-02,
            6.9606e-02, -4.4691e-02],
          [ 0.0000e+00,  0.0000e+00,  1.1390e-01,  1.4128e-01, -1.3918e-01,
           -1.7112e-02, -2.1866e-01,  2.8019e-02, -8.5529e-02, -9.6295e-02,
            9.0168e-02, -1.1044e-01,  8.2178e-03,  4.2880e-02,  1.5952e-03,
            2.1666e-03,  3.2959e-02, -1.7016e-02, -9.6011e-02,  5.1974e-02,
           -3.7183e-02,  4.1785e-02],
          [ 0.0000e+00,  0.0000e+00, -5.0073e-02, -2.6708e-02,  3.0830e-01,
            2.5658e-01, -1.9003e-02,  1.5677e-01, -2.8076e-01,  1.4043e-02,
            1.9022e-02, -1.0918e-02,  1.4406e-01,  1.9049e-01, -1.1828e-01,
            1.5072e-01,  1.4648e-01, -2.3512e-01,  3.6883e-02, -1.2128e-02,
            5.3879e-02, -2.8888e-02],
          [ 0.0000e+00,  0.0000e+00, -1.0088e-01,  1.8948e-01,  8.5758e-02,
           -2.5906e-01,  2.1594e-01,  5.1143e-02,  7.5050e-03,  2.3793e-02,
            1.0225e-01, -3.5420e-02, -7.8628e-02, -1.0120e-01,  1.1373e-01,
            2.1851e-01,  1.8767e-01, -9.4919e-02,  5.4993e-04, -1.9087e-02,
           -2.4873e-02,  2.7952e-02],
          [-6.2398e-02, -6.6920e-02,  4.2967e-02, -7.7658e-02, -3.1367e-01,
            3.0565e-01, -4.2095e-03, -1.6911e-01,  8.3244e-02, -1.8324e-02,
           -6.7393e-02, -1.9834e-02, -1.2650e-01,  9.7760e-02,  8.6235e-02,
           -3.1276e-01, -1.3952e-02,  3.2766e-01, -9.1076e-02, -6.1679e-02,
           -2.0012e-02,  1.6665e-02],
          [ 2.7432e-02,  3.5748e-02, -4.0173e-02, -1.5646e-01,  8.2212e-02,
           -2.9270e-03, -2.4876e-01,  1.7895e-01,  6.4819e-02, -7.7280e-02,
            2.3829e-02,  2.5577e-02,  1.1685e-01,  1.2114e-01, -1.4435e-01,
           -1.2348e-01,  1.0191e-01, -7.9052e-02,  5.5259e-02,  1.0209e-02,
            1.3304e-02, -1.4951e-02],
          [ 7.4153e-02,  1.3218e-02,  4.4644e-02,  6.2865e-02, -6.7752e-02,
           -1.8174e-01,  5.8696e-02, -2.3208e-02, -3.3581e-02,  8.1155e-02,
            1.9900e-01,  8.7350e-02, -1.4279e-01, -1.5195e-01,  1.0204e-01,
           -3.3438e-02, -1.8139e-01,  4.0776e-02,  3.0007e-02,  2.0570e-02,
           -2.6178e-03, -3.6082e-04],
          [-8.3017e-03, -1.0818e-02,  5.3750e-02,  3.8664e-02, -6.3829e-02,
            7.0227e-03,  9.6799e-04, -5.1920e-02,  6.6075e-03, -1.8177e-02,
           -7.4946e-02, -3.7299e-02, -5.4984e-02,  1.7870e-01,  2.2054e-01,
           -1.3428e-01, -5.9967e-02,  6.1186e-03,  6.7459e-04,  0.0000e+00,
            0.0000e+00,  0.0000e+00],
          [-1.6726e-02,  9.7141e-02,  2.0658e-02, -9.7410e-02,  4.4280e-02,
            7.1790e-02,  1.4436e-01,  2.2811e-02, -6.9608e-02, -6.3362e-02,
           -8.0000e-02,  1.0853e-01,  1.0367e-01, -1.3681e-01, -1.1243e-01,
            7.9253e-02,  2.8418e-02, -2.3223e-02, -2.4906e-02,  1.5991e-02,
            0.0000e+00,  0.0000e+00],
          [ 0.0000e+00, -4.1771e-02, -1.1410e-01,  6.8765e-02,  1.0467e-03,
           -3.1561e-02, -7.0744e-02,  7.6277e-03,  5.3579e-02,  1.3304e-02,
           -1.4951e-02, -4.1771e-02, -5.4433e-02, -8.2654e-02, -2.0794e-02,
            7.4986e-03,  6.8584e-04,  1.0209e-02,  1.3304e-02, -1.4951e-02,
            0.0000e+00,  0.0000e+00],
          [ 0.0000e+00, -8.4158e-02,  1.0710e-02,  1.4763e-03,  0.0000e+00,
           -6.3589e-02, -5.1575e-02,  8.7089e-03,  2.1616e-02, -2.6178e-03,
           -3.6082e-04, -8.4158e-02,  1.0710e-02,  1.4763e-03,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  2.0570e-02, -2.6178e-03, -3.6082e-04,
            0.0000e+00,  0.0000e+00]]]], device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 1, 6, 6])
grad_out size torch.Size([1, 1, 8, 8])
arg size torch.Size([1, 1, 3, 3])
maxpool2d140372191067248[0,-1] PI( <3,0,>,
 <otileshape 3,3,>,
 <padding >,
 <inpslidx 0,5,10,15,>, 
 <internal >, 
 <realidx 0,5,10,15,>, 
 <ndtsize 2,2,>, 
  local_first False
 next_id -99)

bk-maxpool2d140372191067248[3,-1] PI( <3,0,>,
 <otileshape 3,3,>,
 <padding >,
 <inpslidx 0,2,5,7,>, 
 <internal >, 
 <realidx 0,2,5,7,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191067488)

new_grad_out torch.Size([1, 1, 3, 3])
##############grad_in in maxp torch.Size([1, 1, 6, 6])
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 1, 8, 8])
local last ++ input [torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
tensor([[[[ 0.0000,  0.1871,  0.2186,  0.2440,  0.0266,  0.4202,  0.0020,
            0.2103],
          [ 0.0000,  0.2296,  0.1671,  0.0660,  0.1479,  0.1243,  0.2083,
            0.3464],
          [ 0.0000,  0.1295,  0.2721,  0.1425,  0.1788,  0.2640,  0.4450,
            0.2202],
          [ 0.0000,  0.2465,  0.2738,  0.1315,  0.3119,  0.3298, -0.0315,
            0.2760],
          [ 0.0000,  0.2966,  0.2194,  0.5167,  0.2021,  0.3370,  0.2310,
            0.1645],
          [ 0.0000,  0.1245,  0.3174,  0.0663,  0.1230,  0.2333,  0.1631,
            0.0086],
          [ 0.0000,  0.2921,  0.1794,  0.2947,  0.4402,  0.2929,  0.2325,
            0.3420],
          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
            0.0000]]]], device='cuda:0')
weight shape torch.Size([1, 1, 3, 3])
grad_output shape torch.Size([1, 1, 6, 6])
grad_input torch.Size([1, 1, 8, 8])
padding info :: [1, 0, 0, 1]
new grad_input torch.Size([1, 1, 7, 7])

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 1, 14, 14])
grad_out size torch.Size([1, 1, 7, 7])
arg size torch.Size([1, 1, 7, 7])
maxpool2d140372191117952[2,-1] PI( <3,0,>,
 <otileshape 7,7,>,
 <padding >,
 <inpslidx 0,13,18,31,>, 
 <internal >, 
 <realidx 0,13,18,31,>, 
 <ndtsize 4,4,>, 
  local_first False
 next_id 140372191067488)

bk-maxpool2d140372191117952[1,-1] PI( <3,0,>,
 <otileshape 5,5,>,
 <padding >,
 <inpslidx 0,4,11,15,>, 
 <internal >, 
 <realidx 0,4,11,15,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191117808)

##############grad_in in maxp torch.Size([1, 1, 14, 14])
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 1, 16, 16])
weight shape torch.Size([1, 1, 3, 3])
grad_output shape torch.Size([1, 1, 14, 14])
final torch.Size([1, 1, 16, 16]) tensor([[[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00],
          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00],
          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            6.4902e-02,  6.9606e-02, -4.4691e-02,  0.0000e+00,  0.0000e+00,
            0.0000e+00],
          [-6.2398e-02, -6.6920e-02,  4.2967e-02,  8.1630e-02,  8.7546e-02,
           -5.3705e-02,  2.6856e-03,  1.7507e-02,  2.0626e-02,  1.3329e-01,
            1.2862e-01, -1.3808e-01,  4.1785e-02,  0.0000e+00,  0.0000e+00,
            0.0000e+00],
          [ 4.6316e-02,  5.6000e-02, -5.3176e-02, -3.5887e-02, -4.6766e-02,
            5.1454e-02,  9.5296e-02,  9.6899e-02,  1.7094e-01,  2.1454e-01,
           -3.1259e-01,  1.0166e-01,  1.0084e-03,  0.0000e+00,  0.0000e+00,
            0.0000e+00],
          [ 4.6968e-02, -1.7852e-02,  1.1188e-02, -1.3688e-02,  7.2066e-02,
           -4.1410e-02, -4.2349e-02, -7.2346e-02, -4.4832e-02, -2.7190e-01,
            2.8487e-01,  1.1845e-01, -7.4587e-02,  0.0000e+00,  0.0000e+00,
            0.0000e+00],
          [-1.6726e-02,  3.4743e-02,  1.8426e-01,  1.1156e-01,  1.9399e-02,
            2.0463e-01, -1.9275e-01,  1.0841e-02, -2.1866e-01,  2.8019e-02,
           -4.3758e-02, -4.1862e-02,  9.1395e-02, -1.3905e-02,  0.0000e+00,
            0.0000e+00],
          [ 0.0000e+00, -1.4339e-02, -8.4187e-02, -1.1628e-01,  3.4136e-02,
           -8.8132e-02,  3.4003e-01,  2.5726e-01, -1.9003e-02,  1.5677e-01,
           -1.9660e-01,  3.3323e-03, -9.8861e-03,  1.3001e-02,  0.0000e+00,
            0.0000e+00],
          [ 0.0000e+00, -7.2403e-02, -1.7496e-01,  4.7266e-02, -1.3549e-01,
            3.0250e-01,  2.0257e-01, -3.3365e-01,  2.1594e-01,  5.1143e-02,
            7.5050e-03,  2.3793e-02,  4.6977e-02, -2.8386e-02,  0.0000e+00,
            0.0000e+00],
          [ 0.0000e+00,  1.9131e-02,  1.0278e-01,  5.5475e-02, -5.3605e-02,
           -1.2528e-01, -3.7573e-01,  3.7538e-01, -4.2095e-03, -1.6911e-01,
            8.3244e-02, -1.8324e-02, -2.3878e-02,  2.6834e-02,  0.0000e+00,
            0.0000e+00],
          [ 0.0000e+00,  1.5244e-01,  8.3021e-02, -1.2370e-01,  5.0120e-02,
           -2.5241e-01,  9.4422e-02, -1.2440e-03, -2.4876e-01,  1.7895e-01,
            6.4819e-02, -7.7280e-02,  4.6983e-03,  6.4760e-04,  0.0000e+00,
            0.0000e+00],
          [ 0.0000e+00, -5.0073e-02,  1.0656e-02,  2.3747e-01, -1.9731e-02,
            8.4522e-02, -8.1657e-02, -1.8174e-01,  5.8696e-02, -2.3208e-02,
           -3.3581e-02,  8.1155e-02,  4.6563e-02, -2.9896e-02,  0.0000e+00,
            0.0000e+00],
          [ 9.5013e-02,  1.0147e-03, -1.1627e-01, -8.1222e-02,  1.0276e-01,
            2.1889e-03, -3.4837e-02,  7.0227e-03,  9.6799e-04, -5.1920e-02,
            6.6075e-03, -1.8177e-02, -2.4873e-02,  2.7952e-02,  0.0000e+00,
            0.0000e+00],
          [-4.1771e-02, -5.4433e-02, -6.7142e-02,  1.7870e-01,  8.7183e-02,
           -1.2821e-01,  2.9643e-02,  7.1790e-02,  1.4436e-01,  2.2811e-02,
           -6.9608e-02, -6.3362e-02,  2.0885e-02,  6.7459e-04,  0.0000e+00,
            0.0000e+00],
          [-8.4158e-02,  1.0710e-02,  1.4763e-03, -7.1386e-02, -1.3212e-01,
            1.0952e-01,  6.8584e-04, -3.1561e-02, -7.0744e-02,  7.6277e-03,
            5.3579e-02,  1.3304e-02, -1.4951e-02,  0.0000e+00,  0.0000e+00,
            0.0000e+00],
          [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -1.4383e-01,  1.8304e-02,
            2.5229e-03,  0.0000e+00, -6.3589e-02, -5.1575e-02,  8.7089e-03,
            2.1616e-02, -2.6178e-03, -3.6082e-04,  0.0000e+00,  0.0000e+00,
            0.0000e+00]]]], device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 1, 8, 6])
grad_out size torch.Size([1, 1, 8, 8])
arg size torch.Size([1, 1, 4, 3])
maxpool2d140372191067248[0,-1] PI( <2,3,>,
 <otileshape 3,4,>,
 <padding >,
 <inpslidx 10,15,6,13,>, 
 <internal >, 
 <realidx 10,15,6,13,>, 
 <ndtsize 2,2,>, 
  local_first False
 next_id -99)

bk-maxpool2d140372191067248[3,-1] PI( <2,3,>,
 <otileshape 3,4,>,
 <padding >,
 <inpslidx 5,7,3,6,>, 
 <internal >, 
 <realidx 5,7,3,6,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191067488)

new_grad_out torch.Size([1, 1, 4, 3])
##############grad_in in maxp torch.Size([1, 1, 8, 6])
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 1, 10, 8])
local last ++ input tensor([[[[ 0.2884,  0.0402,  0.2938,  0.2875, -0.0126,  0.2264,  0.2046,
            0.0000],
          [ 0.4037,  0.4304, -0.0569,  0.2592,  0.2352,  0.0946,  0.3255,
            0.0000],
          [ 0.1541,  0.3395,  0.3022,  0.3429,  0.1588,  0.3002,  0.1257,
            0.0000],
          [ 0.2301,  0.2176,  0.3750,  0.2856,  0.2908,  0.3157,  0.3333,
            0.0000],
          [ 0.2586,  0.1248,  0.1634,  0.3155,  0.2679, -0.0112,  0.1765,
            0.0000],
          [ 0.1913,  0.2709, -0.0339,  0.2026,  0.2329,  0.1941,  0.2226,
            0.0000],
          [ 0.3763,  0.3157,  0.2616,  0.1460,  0.3505,  0.2850,  0.0497,
            0.0000],
          [ 0.0804,  0.2784,  0.2649,  0.4995,  0.2634,  0.1926,  0.3223,
            0.0000],
          [ 0.2235,  0.0890,  0.2459,  0.1678,  0.1321,  0.4379,  0.1063,
            0.0000],
          [ 0.4876,  0.0402,  0.4138,  0.2646,  0.2885,  0.3276,  0.2683,
            0.0000]]]], device='cuda:0')
weight shape torch.Size([1, 1, 3, 3])
grad_output shape torch.Size([1, 1, 8, 6])
grad_input torch.Size([1, 1, 10, 8])
padding info :: [0, 1, 0, 0]
new grad_input torch.Size([1, 1, 10, 7])

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 1, 20, 14])
grad_out size torch.Size([1, 1, 10, 7])
arg size torch.Size([1, 1, 10, 7])
maxpool2d140372191117952[2,-1] PI( <2,3,>,
 <otileshape 7,10,>,
 <padding >,
 <inpslidx 18,31,10,29,>, 
 <internal >, 
 <realidx 18,31,10,29,>, 
 <ndtsize 4,4,>, 
  local_first False
 next_id 140372191067488)

[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
bk-maxpool2d140372191117952[1,-1] PI( <2,3,>,
 <otileshape 5,6,>,
 <padding >,
 <inpslidx 11,15,7,12,>, 
 <internal >, 
 <realidx 11,15,7,12,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191117808)

##############grad_in in maxp torch.Size([1, 1, 20, 14])
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 1, 22, 16])
weight shape torch.Size([1, 1, 3, 3])
grad_output shape torch.Size([1, 1, 20, 14])
final torch.Size([1, 1, 22, 16]) tensor([[[[ 0.0000e+00,  0.0000e+00, -6.2398e-02, -6.6920e-02,  4.2967e-02,
            0.0000e+00,  0.0000e+00,  0.0000e+00, -6.2398e-02, -6.6920e-02,
            4.2967e-02,  8.1630e-02,  1.5245e-01,  1.3397e-02, -4.4691e-02,
            0.0000e+00],
          [ 0.0000e+00,  0.0000e+00,  2.7432e-02,  3.5748e-02,  4.1457e-02,
            8.7546e-02, -5.6209e-02,  6.4902e-02,  9.7038e-02, -8.9429e-03,
           -4.0173e-02, -3.5887e-02, -7.5299e-02,  1.5372e-02,  4.1785e-02,
            0.0000e+00],
          [ 0.0000e+00,  0.0000e+00,  5.5269e-02, -7.0338e-03, -3.6857e-02,
           -4.6766e-02,  5.2555e-02,  9.6513e-02,  2.3598e-01,  3.8504e-02,
           -4.7195e-05, -9.4396e-03, -8.8648e-02,  8.5844e-03,  1.0084e-03,
            0.0000e+00],
          [ 0.0000e+00,  0.0000e+00,  1.8883e-02,  2.0252e-02, -8.5307e-02,
            5.4200e-03, -2.7875e-03, -1.0986e-01, -1.0116e-01,  3.3515e-02,
            2.8173e-02, -3.3581e-02,  8.1155e-02,  4.6563e-02, -2.9896e-02,
            0.0000e+00],
          [ 0.0000e+00, -6.2398e-02, -7.5222e-02,  3.2148e-02,  1.2157e-02,
            1.5281e-01,  1.3727e-01, -2.4624e-01, -4.1523e-02,  1.1388e-02,
           -5.0618e-02,  6.6075e-03, -1.8177e-02,  1.6806e-02,  7.2653e-02,
           -2.8700e-02],
          [ 0.0000e+00,  2.7432e-02,  1.9566e-01,  1.5140e-01, -1.2134e-01,
           -6.3100e-02, -7.5147e-02,  1.1272e-01, -1.7386e-02,  1.5766e-01,
            3.8298e-01,  1.2083e-01, -1.8574e-01, -1.3430e-02, -2.3204e-02,
            2.6834e-02],
          [ 1.8883e-02,  7.5521e-02, -9.7695e-02, -1.0217e-01,  1.1373e-01,
           -1.3388e-01,  4.0957e-02, -6.9557e-04, -4.1958e-04, -1.5036e-02,
           -1.2615e-01, -5.8410e-02,  1.3771e-01, -3.6918e-02,  4.6983e-03,
            6.4760e-04],
          [-8.3017e-03, -1.0818e-02, -1.4431e-01,  7.8528e-02,  2.0404e-01,
            1.0810e-01, -9.5321e-02,  8.6246e-02,  9.2497e-02, -2.2290e-01,
           -2.0278e-01,  1.6354e-01,  1.5617e-01, -2.3696e-02, -2.9896e-02,
            0.0000e+00],
          [ 7.8287e-02,  1.0403e-01, -6.5131e-02, -2.5770e-02, -9.4440e-02,
            1.6839e-02,  1.5176e-01, -7.8135e-02, -4.9411e-02,  7.4507e-03,
            5.0258e-02,  3.3257e-03, -1.3116e-01,  1.5912e-02,  4.3943e-02,
            0.0000e+00],
          [-4.1771e-02, -5.4433e-02,  6.6136e-02, -4.6595e-02, -1.1943e-01,
           -9.1626e-03,  1.2860e-01,  1.3272e-01, -1.0039e-01,  1.6372e-01,
            1.5474e-01, -2.2747e-01,  1.1673e-02,  1.9784e-02, -1.4277e-02,
            0.0000e+00],
          [-8.4158e-02,  1.0710e-02, -7.0641e-04,  1.6039e-02,  8.2064e-02,
           -1.8729e-03, -1.0408e-01, -9.0708e-02,  1.0296e-01, -7.1386e-02,
           -1.3212e-01,  2.5605e-01,  2.4331e-01, -3.3912e-02, -4.5052e-02,
            0.0000e+00],
          [ 0.0000e+00,  0.0000e+00, -4.3976e-03, -7.7421e-03, -3.6511e-02,
           -2.1424e-02, -1.0391e-01, -9.5516e-04, -1.7873e-02, -1.1152e-01,
            3.8929e-02, -7.5140e-02, -1.1248e-01,  5.7158e-02,  4.1785e-02,
            0.0000e+00],
          [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -1.6726e-02,  1.7571e-02,
            7.9146e-02, -4.5475e-02,  8.3449e-03,  1.0875e-02,  5.6824e-02,
            7.2099e-02, -1.7078e-01, -4.0970e-02,  5.3009e-02,  4.7572e-02,
           -2.9896e-02],
          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  9.5013e-02,  7.2284e-02,
           -1.0402e-01,  3.9030e-02,  1.2159e-02,  8.4827e-04, -5.1401e-02,
           -4.2232e-02,  1.5223e-01,  1.0943e-01, -8.9346e-02, -2.4873e-02,
            2.7952e-02],
          [-6.2398e-02, -6.6920e-02,  4.2967e-02, -4.1771e-02, -1.1410e-01,
            7.1269e-02,  5.6399e-03,  1.7740e-01,  1.8665e-01, -1.9028e-01,
            8.7361e-03, -4.3653e-02, -1.6775e-02,  7.1935e-02, -2.3806e-02,
            6.7459e-04],
          [ 2.7432e-02,  3.5748e-02, -4.0173e-02, -2.5289e-03,  9.8256e-02,
           -5.5834e-02,  2.4089e-03, -7.6535e-02,  6.3613e-02,  2.9056e-01,
           -1.1353e-01,  3.5393e-02,  1.2806e-01, -1.0890e-01,  2.6834e-02,
            0.0000e+00],
          [ 5.5269e-02, -7.0338e-03, -9.6952e-04, -3.5887e-02, -4.6766e-02,
            1.1264e-01,  6.7097e-02, -1.9932e-01, -5.2575e-02, -9.1715e-02,
            1.6477e-01,  7.5719e-03, -1.4933e-01,  8.5671e-02,  6.4760e-04,
            0.0000e+00],
          [ 1.8883e-02,  2.0252e-02, -1.3003e-02, -1.3688e-02,  7.2066e-02,
           -6.6483e-02, -3.5692e-02,  9.8726e-02, -8.3179e-02,  4.0523e-02,
            4.3607e-02, -1.8788e-01,  5.1916e-02,  4.5370e-02,  4.6563e-02,
           -2.9896e-02],
          [ 8.6711e-02,  9.1081e-02, -5.3267e-02, -2.5770e-02, -3.3581e-02,
           -1.7444e-02,  7.4385e-02,  4.7443e-02, -8.1765e-03,  8.7343e-02,
           -1.3705e-01,  1.1408e-01,  4.9933e-02, -9.0378e-02, -8.8825e-03,
            2.7952e-02],
          [-5.8497e-02, -5.2304e-02,  1.2883e-01,  2.0325e-02, -3.9778e-02,
            7.2701e-02,  4.7379e-02, -1.3995e-01,  1.8416e-02, -9.5401e-02,
            5.3243e-02, -2.8647e-02, -2.8383e-02,  1.8217e-02, -1.0057e-02,
            6.7459e-04],
          [-8.4158e-02,  1.0710e-02, -2.8139e-02, -3.8592e-02,  4.3369e-02,
           -3.1561e-02, -1.0080e-01,  5.3813e-02, -6.2542e-02,  8.0926e-03,
            1.1155e-03, -5.9667e-02,  2.8163e-02, -1.5711e-03, -3.6082e-04,
            0.0000e+00],
          [ 0.0000e+00,  0.0000e+00, -5.9667e-02,  7.5935e-03,  1.0467e-03,
           -6.3589e-02,  8.0926e-03,  1.1155e-03,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00]]]], device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 1, 8, 8])
grad_out size torch.Size([1, 1, 8, 8])
arg size torch.Size([1, 1, 4, 4])
maxpool2d140372191067248[0,-1] PI( <2,2,>,
 <otileshape 4,4,>,
 <padding >,
 <inpslidx 6,13,6,13,>, 
 <internal >, 
 <realidx 6,13,6,13,>, 
 <ndtsize 2,2,>, 
  local_first False
 next_id -99)

bk-maxpool2d140372191067248[3,-1] PI( <2,2,>,
 <otileshape 4,4,>,
 <padding >,
 <inpslidx 3,6,3,6,>, 
 <internal >, 
 <realidx 3,6,3,6,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191067488)

new_grad_out torch.Size([1, 1, 4, 4])
##############grad_in in maxp torch.Size([1, 1, 8, 8])
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 1, 10, 10])
local last ++ input tensor([[[[ 0.3443,  0.4470,  0.2105,  0.2988,  0.2884,  0.0402,  0.2938,
            0.2875, -0.0126,  0.2264],
          [ 0.1104,  0.2068,  0.2279,  0.2150,  0.4037,  0.4304, -0.0569,
            0.2592,  0.2352,  0.0946],
          [ 0.2999,  0.3111,  0.2239,  0.0695,  0.1541,  0.3395,  0.3022,
            0.3429,  0.1588,  0.3002],
          [ 0.4782,  0.1304,  0.1500,  0.1994,  0.2301,  0.2176,  0.3750,
            0.2856,  0.2908,  0.3157],
          [ 0.0020,  0.2103,  0.2201,  0.1518,  0.2586,  0.1248,  0.1634,
            0.3155,  0.2679, -0.0112],
          [ 0.2083,  0.3464,  0.3073,  0.3793,  0.1913,  0.2709, -0.0339,
            0.2026,  0.2329,  0.1941],
          [ 0.4450,  0.2202,  0.1920,  0.1985,  0.3763,  0.3157,  0.2616,
            0.1460,  0.3505,  0.2850],
          [-0.0315,  0.2760,  0.3580,  0.4210,  0.0804,  0.2784,  0.2649,
            0.4995,  0.2634,  0.1926],
          [ 0.2310,  0.1645,  0.0601,  0.2607,  0.2235,  0.0890,  0.2459,
            0.1678,  0.1321,  0.4379],
          [ 0.1631,  0.0086,  0.3127,  0.1987,  0.4876,  0.0402,  0.4138,
            0.2646,  0.2885,  0.3276]]]], device='cuda:0')
weight shape torch.Size([1, 1, 3, 3])
grad_output shape torch.Size([1, 1, 8, 8])
grad_input torch.Size([1, 1, 10, 10])
padding info :: [0, 0, 0, 0]
new grad_input torch.Size([1, 1, 10, 10])

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 1, 20, 20])
grad_out size [torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
torch.Size([1, 1, 10, 10])
arg size torch.Size([1, 1, 10, 10])
maxpool2d140372191117952[2,-1] PI( <2,2,>,
 <otileshape 10,10,>,
 <padding >,
 <inpslidx 10,29,10,29,>, 
 <internal >, 
 <realidx 10,29,10,29,>, 
 <ndtsize 4,4,>, 
  local_first False
 next_id 140372191067488)

bk-maxpool2d140372191117952[1,-1] PI( <2,2,>,
 <otileshape 6,6,>,
 <padding >,
 <inpslidx 7,12,7,12,>, 
 <internal >, 
 <realidx 7,12,7,12,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191117808)

##############grad_in in maxp torch.Size([1, 1, 20, 20])
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 1, 22, 22])
weight shape torch.Size([1, 1, 3, 3])
grad_output shape torch.Size([1, 1, 20, 20])
final torch.Size([1, 1, 22, 22]) tensor([[[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
           -6.2398e-02, -6.6920e-02,  4.2967e-02,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00],
          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            2.7432e-02,  3.5748e-02,  4.1457e-02,  8.7546e-02, -5.6209e-02,
            6.4902e-02,  6.9606e-02, -4.4691e-02,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00],
          [ 0.0000e+00, -6.2398e-02, -6.6920e-02,  4.2967e-02,  0.0000e+00,
            6.4902e-02,  6.9606e-02, -4.4691e-02,  8.1630e-02,  8.7546e-02,
           -9.3992e-04, -7.0338e-03, -3.6857e-02, -4.6766e-02,  5.2555e-02,
            9.6513e-02,  1.6183e-01,  2.5286e-02, -4.4691e-02,  0.0000e+00,
            0.0000e+00,  0.0000e+00],
          [ 0.0000e+00,  2.7432e-02,  1.1738e-01,  4.7373e-02, -5.6209e-02,
           -2.8533e-02, -3.7183e-02, -2.0613e-02, -1.0281e-01, -3.7993e-03,
            1.3634e-01,  8.9858e-02, -1.3000e-01,  5.4200e-03, -2.7875e-03,
           -1.0986e-01, -9.2856e-02,  4.4333e-02,  4.1785e-02,  0.0000e+00,
            0.0000e+00,  0.0000e+00],
          [ 0.0000e+00,  5.5269e-02, -4.2921e-02, -4.7735e-02,  5.2555e-02,
           -5.7488e-02,  7.3161e-03,  2.8441e-02, -3.6556e-02, -3.4753e-02,
           -3.9622e-02, -4.5397e-02,  5.3943e-02,  1.5281e-01,  1.3727e-01,
           -2.4624e-01, -2.4797e-02,  9.2590e-03,  1.0084e-03,  0.0000e+00,
            0.0000e+00,  0.0000e+00],
          [ 1.8883e-02,  2.0252e-02, -8.5307e-02,  6.7818e-02,  6.4133e-02,
            3.0537e-03,  4.6563e-02,  4.4257e-02,  1.3218e-02, -1.2310e-02,
            1.4801e-01,  2.4302e-01, -1.5023e-01, -6.3100e-02, -7.5147e-02,
            1.1272e-01, -1.7386e-02,  1.2505e-01,  1.9901e-01, -1.6499e-02,
           -4.4691e-02,  0.0000e+00],
          [-8.3017e-03, -1.0818e-02,  1.7122e-02, -2.0445e-02, -3.7000e-02,
            1.4660e-02, -2.9153e-02,  2.2399e-02,  1.4033e-01,  1.7761e-01,
           -2.0125e-01, -1.2613e-01,  1.4168e-01, -1.3388e-01,  4.0957e-02,
           -6.9557e-04, -4.1958e-04, -1.9581e-02, -6.2213e-02,  1.8953e-02,
            4.1785e-02,  0.0000e+00],
          [-1.6726e-02,  9.7141e-02,  1.0001e-01, -1.2019e-01,  9.8039e-03,
           -3.5791e-02,  7.1804e-03,  2.2292e-01,  1.9473e-01, -2.5262e-01,
           -9.7607e-02,  6.0200e-02,  1.7981e-01,  1.2409e-01, -9.5321e-02,
            8.6246e-02,  9.2497e-02, -1.8571e-01, -6.3669e-02,  7.5462e-02,
            4.7572e-02, -2.9896e-02],
          [ 0.0000e+00, -4.1771e-02, -5.8831e-02,  6.1731e-02,  7.7141e-05,
            3.5349e-03,  1.0158e-01,  3.1745e-03, -2.0409e-01,  3.2101e-01,
           -9.2972e-02, -1.5560e-02, -8.1135e-02,  1.8877e-03,  1.5176e-01,
           -7.8135e-02, -4.9411e-02,  2.4177e-02, -1.9233e-02, -4.3443e-02,
           -8.8825e-03,  2.7952e-02],
          [ 0.0000e+00, -1.4656e-01,  4.4303e-02,  1.5224e-01,  7.3189e-02,
            1.5272e-01, -1.4291e-01, -2.7240e-01,  3.2061e-02, -7.5553e-02,
            9.4088e-02, -2.6026e-02, -1.2204e-01, -9.5234e-03,  1.2860e-01,
            1.3272e-01, -1.0039e-01,  6.8703e-02,  8.2454e-02, -7.1537e-02,
           -1.0057e-02,  6.7459e-04],
          [ 1.8883e-02,  4.7684e-02, -2.1444e-02, -9.7757e-02,  1.4550e-01,
            7.2205e-02,  2.8335e-02,  1.4639e-01, -2.0763e-01,  5.7284e-02,
            4.4668e-02, -1.2661e-02,  8.2064e-02, -1.8729e-03, -1.0408e-01,
           -9.0708e-02,  1.0296e-01, -2.9615e-02, -1.8023e-02,  1.0565e-01,
            6.9245e-02, -4.4691e-02],
          [-8.3017e-03,  4.4451e-02, -8.3907e-02,  1.6399e-01, -2.2849e-02,
           -1.7189e-01,  3.9240e-02, -7.2054e-02,  8.0973e-02, -1.8324e-02,
           -2.8276e-02,  1.9092e-02, -3.6511e-02, -2.1424e-02, -1.0391e-01,
           -9.5516e-04, -1.7873e-02,  3.5033e-02,  9.5140e-02, -8.3696e-02,
           -3.7183e-02,  4.1785e-02],
          [-1.6726e-02,  2.1286e-03,  2.9340e-04, -6.7541e-02, -2.1937e-01,
            1.1042e-01, -1.0619e-01,  1.4177e-02,  1.9542e-03, -3.6918e-02,
            4.6983e-03, -1.6078e-02,  1.7571e-02,  7.9146e-02, -4.5475e-02,
            8.3449e-03,  1.0875e-02,  1.0508e-02,  1.6099e-02, -4.5295e-02,
            7.3161e-03,  1.0084e-03],
          [ 9.5013e-02,  1.0190e-01, -6.5425e-02, -6.8716e-02,  9.1471e-02,
           -4.1512e-02, -6.5754e-03, -4.0558e-03,  2.6041e-03,  1.2505e-01,
            1.3411e-01,  7.3809e-02,  1.4189e-01, -1.4871e-01,  3.9030e-02,
            1.2159e-02,  8.4827e-04, -9.8369e-02, -2.4380e-02,  8.2423e-02,
            4.6563e-02, -2.9896e-02],
          [-4.1771e-02, -5.4433e-02, -1.2267e-03, -9.6535e-02,  8.2178e-03,
            4.2880e-02,  1.5952e-03,  2.1666e-03, -2.9439e-02, -8.3936e-02,
           -5.3044e-02,  1.0203e-02, -1.5128e-01,  1.1305e-01,  5.6399e-03,
            1.7740e-01,  1.8665e-01, -1.7355e-01,  6.6075e-03, -1.8177e-02,
           -2.4873e-02,  2.7952e-02],
          [-8.4158e-02,  1.0710e-02,  2.8909e-02, -2.3919e-02,  1.4406e-01,
            1.9049e-01, -1.1828e-01,  1.5072e-01,  1.7392e-01, -1.9937e-01,
           -3.2901e-03, -1.4657e-02,  1.5214e-01, -8.4722e-02,  2.4089e-03,
           -7.6535e-02,  3.0998e-02,  2.5558e-01, -9.1076e-02, -6.1679e-02,
           -2.0012e-02,  1.6665e-02],
          [ 0.0000e+00,  0.0000e+00,  5.5269e-02, -7.0338e-03, -7.8628e-02,
           -1.0120e-01,  1.1373e-01,  7.1979e-02,  8.5788e-02, -1.0529e-03,
           -4.1958e-04, -5.4974e-02, -7.1639e-02,  1.4059e-01,  6.7097e-02,
           -1.9932e-01, -3.8236e-02, -7.3030e-02,  8.5155e-02,  1.0209e-02,
            1.3304e-02, -1.4951e-02],
          [ 0.0000e+00,  0.0000e+00,  1.8883e-02,  2.0252e-02, -1.6947e-01,
            7.8528e-02,  6.5609e-02, -2.3510e-01,  2.3978e-02,  1.8397e-01,
           -5.9388e-02, -7.5367e-02,  5.2055e-02, -4.9818e-02, -3.5692e-02,
            9.8726e-02, -5.4290e-02,  1.7963e-02,  4.8618e-02, -9.3265e-03,
           -2.6178e-03, -3.6082e-04],
          [ 0.0000e+00,  0.0000e+00, -8.3017e-03, -1.0818e-02,  7.9520e-02,
            4.6475e-02, -7.9967e-02, -1.0810e-01,  4.7795e-02,  7.0630e-02,
           -9.8980e-03, -1.5560e-02, -2.0277e-02, -3.2395e-02,  7.4385e-02,
            4.7443e-02, -1.0319e-01, -6.2547e-03, -8.8825e-03,  2.7952e-02,
            0.0000e+00,  0.0000e+00],
          [ 0.0000e+00,  0.0000e+00,  7.8287e-02,  1.0403e-01, -9.4746e-02,
           -9.0512e-02,  4.9977e-02,  1.1120e-02, -1.0486e-01, -5.9662e-02,
            1.2987e-01,  4.0895e-02, -4.2396e-02,  7.2340e-02,  4.7379e-02,
           -1.3995e-01,  6.0186e-02, -2.4241e-02, -1.0057e-02,  6.7459e-04,
            0.0000e+00,  0.0000e+00],
          [ 0.0000e+00,  0.0000e+00, -4.1771e-02, -5.4433e-02,  1.5041e-03,
            7.5935e-03,  1.0467e-03,  2.0570e-02, -8.6776e-02,  1.0349e-02,
           -2.8139e-02, -3.8592e-02,  4.3369e-02, -3.1561e-02, -1.0080e-01,
            5.3813e-02,  2.1616e-02, -2.6178e-03, -3.6082e-04,  0.0000e+00,
            0.0000e+00,  0.0000e+00],
          [ 0.0000e+00,  0.0000e+00, -8.4158e-02,  1.0710e-02,  1.4763e-03,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
           -5.9667e-02,  7.5935e-03,  1.0467e-03, -6.3589e-02,  8.0926e-03,
            1.1155e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00]]]], device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 1, 8, 8])
grad_out size torch.Size([1, 1, 8, 8])
arg size torch.Size([1, 1, 4, 4])
maxpool2d140372191067248[0,-1] PI( <2,1,>,
 <otileshape 4,4,>,
 <padding >,
 <inpslidx 2,9,6,13,>, 
 <internal >, 
 <realidx 2,9,6,13,>, 
 <ndtsize 2,2,>, 
  local_first False
 next_id -99)

bk-maxpool2d140372191067248[3,-1] PI( <2,1,>,
 <otileshape 4,4,>,
 <padding >,
 <inpslidx 1,4,3,6,>, 
 <internal >, 
 <realidx 1,4,3,6,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191067488)

new_grad_out torch.Size([1, 1, 4, 4])
##############grad_in in maxp torch.Size([1, 1, 8, 8])
@@@ using cudnn bkw
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
local last ++ input shape torch.Size([1, 1, 10, 10])
local last ++ input tensor([[[[ 0.1378,  0.1910,  0.1472,  0.3417,  0.3443,  0.4470,  0.2105,
            0.2988,  0.2884,  0.0402],
          [ 0.2403,  0.2121,  0.1864,  0.0631,  0.1104,  0.2068,  0.2279,
            0.2150,  0.4037,  0.4304],
          [ 0.1511,  0.2220,  0.2021,  0.3134,  0.2999,  0.3111,  0.2239,
            0.0695,  0.1541,  0.3395],
          [ 0.2223,  0.2330,  0.2266,  0.1150,  0.4782,  0.1304,  0.1500,
            0.1994,  0.2301,  0.2176],
          [ 0.2186,  0.2440,  0.0266,  0.4202,  0.0020,  0.2103,  0.2201,
            0.1518,  0.2586,  0.1248],
          [ 0.1671,  0.0660,  0.1479,  0.1243,  0.2083,  0.3464,  0.3073,
            0.3793,  0.1913,  0.2709],
          [ 0.2721,  0.1425,  0.1788,  0.2640,  0.4450,  0.2202,  0.1920,
            0.1985,  0.3763,  0.3157],
          [ 0.2738,  0.1315,  0.3119,  0.3298, -0.0315,  0.2760,  0.3580,
            0.4210,  0.0804,  0.2784],
          [ 0.2194,  0.5167,  0.2021,  0.3370,  0.2310,  0.1645,  0.0601,
            0.2607,  0.2235,  0.0890],
          [ 0.3174,  0.0663,  0.1230,  0.2333,  0.1631,  0.0086,  0.3127,
            0.1987,  0.4876,  0.0402]]]], device='cuda:0')
weight shape torch.Size([1, 1, 3, 3])
grad_output shape torch.Size([1, 1, 8, 8])
grad_input torch.Size([1, 1, 10, 10])
padding info :: [0, 0, 0, 0]
new grad_input torch.Size([1, 1, 10, 10])

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 1, 20, 20])
grad_out size torch.Size([1, 1, 10, 10])
arg size torch.Size([1, 1, 10, 10])
maxpool2d140372191117952[2,-1] PI( <2,1,>,
 <otileshape 10,10,>,
 <padding >,
 <inpslidx 2,21,10,29,>, 
 <internal >, 
 <realidx 2,21,10,29,>, 
 <ndtsize 4,4,>, 
  local_first False
 next_id 140372191067488)

bk-maxpool2d140372191117952[1,-1] PI( <2,1,>,
 <otileshape 6,6,>,
 <padding >,
 <inpslidx 3,8,7,12,>, 
 <internal >, 
 <realidx 3,8,7,12,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191117808)

##############grad_in in maxp torch.Size([1, 1, 20, 20])
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 1, 22, 22])
weight shape torch.Size([1, 1, 3, 3])
grad_output shape torch.Size([1, 1, 20, 20])
final torch.Size([1, 1, 22, 22]) tensor([[[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00],
          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00],
          [ 0.0000e+00,  0.0000e+00,  8.1630e-02,  8.7546e-02, -5.6209e-02,
            0.0000e+00,  0.0000e+00, -6.2398e-02, -6.6920e-02,  6.2198e-02,
            2.0626e-02, -1.3243e-02,  0.0000e+00,  6.4902e-02,  6.9606e-02,
           -4.4691e-02,  8.1630e-02,  8.7546e-02, -5.6209e-02,  0.0000e+00,
            0.0000e+00,  0.0000e+00],
          [-6.2398e-02, -6.6920e-02,  7.0794e-03, -4.6766e-02,  1.1746e-01,
            6.9606e-02, -4.4691e-02,  2.7432e-02,  3.5748e-02, -4.8628e-02,
            1.3551e-01,  1.6953e-01, -1.0090e-01, -2.8533e-02, -3.7183e-02,
           -2.0613e-02, -1.0281e-01, -3.7993e-03,  1.1746e-01,  6.9606e-02,
           -4.4691e-02,  0.0000e+00],
          [-1.6082e-02, -1.0921e-02, -8.2513e-02,  9.2017e-03, -2.7265e-02,
           -3.7183e-02,  4.1785e-02,  5.5269e-02, -7.0338e-03, -1.8004e-02,
           -6.2252e-02, -8.3650e-02,  9.4340e-02, -5.7488e-02,  7.3161e-03,
            2.8441e-02, -3.6556e-02,  2.7645e-02,  3.5600e-02, -7.7545e-02,
            4.1785e-02,  0.0000e+00],
          [ 7.4400e-02,  1.7896e-02,  1.1126e-01,  1.5041e-01, -1.5406e-01,
            1.1563e-01,  1.1718e-01, -1.1810e-01,  1.1246e-01,  2.0063e-01,
           -2.3937e-01,  1.8345e-01,  1.8131e-01, -7.1533e-02,  4.6563e-02,
            4.4257e-02,  1.3218e-02, -3.9742e-02, -4.7653e-02,  9.1617e-02,
           -2.8888e-02,  0.0000e+00],
          [ 3.8543e-02,  1.0899e-01,  1.8580e-01, -2.3665e-02,  3.5453e-03,
           -4.7620e-02,  5.1840e-02,  2.1102e-01, -1.2346e-01, -1.1918e-01,
            1.2761e-01, -4.6408e-02, -1.1296e-01,  8.4398e-02, -2.9153e-02,
            2.2399e-02,  1.2145e-01,  1.0209e-01, -1.0356e-01, -2.3963e-02,
            2.7952e-02,  0.0000e+00],
          [ 0.0000e+00, -5.0073e-02, -2.4486e-01,  1.6964e-02,  1.0348e-01,
           -7.4287e-02, -5.1768e-02, -2.5025e-02, -7.2527e-02,  2.3825e-01,
            2.2842e-01, -3.1445e-01,  3.5015e-02, -3.4108e-02,  7.1804e-03,
            2.2292e-01,  2.0303e-01, -2.4181e-01,  4.6698e-02, -1.8328e-02,
           -2.4231e-02,  1.5991e-02],
          [ 0.0000e+00, -1.0088e-01, -9.8748e-02,  1.5971e-02, -3.0143e-02,
           -3.6475e-02, -7.1892e-02,  1.2839e-02,  1.7697e-03, -9.7155e-02,
           -1.4889e-01,  1.4511e-01,  3.9091e-04,  3.5349e-03,  1.0158e-01,
            3.1745e-03, -2.8238e-01,  2.1698e-01, -2.7841e-02,  1.0209e-02,
            1.3304e-02, -1.4951e-02],
          [ 9.5013e-02,  1.0190e-01, -6.5425e-02,  6.7363e-02,  6.4568e-02,
           -3.0805e-02, -1.4637e-02,  9.5013e-02,  1.0190e-01, -2.5621e-01,
            1.0753e-01,  8.2907e-02,  8.9180e-02,  1.5272e-01, -1.4291e-01,
           -2.7240e-01,  7.3831e-02, -2.1120e-02,  2.7952e-02,  2.0570e-02,
           -2.6178e-03, -3.6082e-04],
          [-4.1771e-02, -5.4433e-02,  6.1171e-02, -2.9615e-02, -1.8023e-02,
            4.0752e-02, -3.6082e-04, -4.1771e-02,  2.9352e-02,  1.4885e-01,
           -9.4518e-02, -4.1083e-02,  1.3055e-01,  7.2205e-02,  2.8335e-02,
            1.4639e-01, -1.2347e-01,  4.6573e-02,  4.5375e-02, -2.8700e-02,
            0.0000e+00,  0.0000e+00],
          [-8.4158e-02,  1.0710e-02,  1.4763e-03, -1.2206e-01, -5.9327e-02,
            6.3245e-02,  2.0626e-02,  4.9130e-02,  1.3103e-01, -1.5182e-01,
           -1.3958e-02,  1.6242e-01, -2.3210e-02, -1.7189e-01,  3.9240e-02,
           -7.2054e-02,  8.0973e-02, -1.8324e-02, -2.3878e-02,  2.6834e-02,
            0.0000e+00,  0.0000e+00],
          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  2.7432e-02,  1.3248e-01,
            5.5114e-02,  1.7094e-01,  2.1454e-01, -3.2932e-01,  1.0378e-01,
            1.3018e-03, -6.7541e-02, -2.1937e-01,  1.1042e-01, -1.0619e-01,
            1.4177e-02,  1.9542e-03, -3.6918e-02,  4.6983e-03,  6.4760e-04,
            0.0000e+00,  0.0000e+00],
          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.1755e-02, -9.6229e-02,
           -4.3458e-02, -4.4832e-02, -2.7190e-01,  3.7988e-01,  2.2035e-01,
           -1.4001e-01, -6.8716e-02,  9.1471e-02, -4.1512e-02, -6.5754e-03,
           -4.0558e-03,  2.6041e-03,  1.2505e-01,  1.3411e-01, -2.1204e-02,
            6.9606e-02, -4.4691e-02],
          [ 0.0000e+00,  0.0000e+00,  1.1390e-01,  1.4128e-01, -1.3918e-01,
           -1.7112e-02, -2.1866e-01,  2.8019e-02, -8.5529e-02, -9.6295e-02,
            9.0168e-02, -1.1044e-01,  8.2178e-03,  4.2880e-02,  1.5952e-03,
            2.1666e-03,  3.2959e-02, -1.7016e-02, -9.6011e-02,  5.1974e-02,
           -3.7183e-02,  4.1785e-02],
          [ 0.0000e+00,  0.0000e+00, -5.0073e-02, -2.6708e-02,  3.0830e-01,
            2.5658e-01, -1.9003e-02,  1.5677e-01, -2.8076e-01,  1.4043e-02,
            1.9022e-02, -1.0918e-02,  1.4406e-01,  1.9049e-01, -1.1828e-01,
            1.5072e-01,  1.4648e-01, -2.3512e-01,  3.6883e-02, -1.2128e-02,
            5.3879e-02, -2.8888e-02],
          [ 0.0000e+00,  0.0000e+00, -1.0088e-01,  1.0785e-01, -1.7883e-03,
           -2.0285e-01,  1.3431e-01, -3.6403e-02,  6.3714e-02, -4.1109e-02,
            3.2640e-02,  9.2708e-03, -7.8628e-02, -1.0120e-01,  1.1373e-01,
            7.1979e-02,  3.0519e-02,  5.9809e-03,  5.4993e-04, -1.9087e-02,
           -2.4873e-02,  2.7952e-02],
          [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -4.1771e-02, -2.6690e-01,
            2.5059e-01,  2.8992e-02, -1.2062e-01,  3.0689e-02,  1.0209e-02,
            3.2188e-02,  5.3008e-03, -1.6947e-01,  7.8528e-02,  6.5609e-02,
           -2.3510e-01,  5.0943e-03,  1.6372e-01, -4.6385e-02, -6.1679e-02,
           -2.0012e-02,  1.6665e-02],
          [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -8.4158e-02,  1.0710e-02,
           -6.9909e-02, -1.3212e-01,  1.0952e-01,  6.8584e-04,  2.0570e-02,
           -1.0919e-02, -1.1179e-02,  7.9520e-02,  4.6475e-02, -7.9967e-02,
           -1.0810e-01, -3.8916e-02, -2.0451e-02,  4.3369e-02,  1.0209e-02,
            1.3304e-02, -1.4951e-02],
          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
           -1.4383e-01,  1.8304e-02,  2.5229e-03,  0.0000e+00,  0.0000e+00,
            7.8287e-02,  1.0403e-01, -9.4746e-02, -9.0512e-02,  4.9977e-02,
            1.1120e-02, -4.6363e-02, -7.3577e-03,  1.0467e-03,  2.0570e-02,
           -2.6178e-03, -3.6082e-04],
          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
           -4.1771e-02, -5.4433e-02,  1.5041e-03,  7.5935e-03,  1.0467e-03,
            2.0570e-02, -2.6178e-03, -3.6082e-04,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00],
          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
           -8.4158e-02,  1.0710e-02,  1.4763e-03,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00]]]], device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 1, 8, 6])
grad_out size torch.Size([1, 1, 8, 8])
arg size torch.Size([1, 1, 4, 3])
maxpool2d140372191067248[0,-1] PI( <2,0,>,
 <otileshape 3,4,>,
 <padding >,
 <inpslidx 0,5,6,13,>, 
 <internal >, 
 <realidx 0,5,6,13,>, 
 <ndtsize 2,2,>, 
  local_first False
 next_id -99)

bk-maxpool2d140372191067248[3,-1] PI( <2,0,>,
 <otileshape 3,4,>,
 <padding >,
 <inpslidx 0,2,3,6,>, 
 <internal >, 
 <realidx 0,2,3,6,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191067488)

new_grad_out torch.Size([1, 1, 4, 3])
##############grad_in in maxp torch.Size([1, 1, 8, 6])
@@@ using cudnn bkw
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
local last ++ input shape torch.Size([1, 1, 10, 8])
local last ++ input tensor([[[[ 0.0000,  0.1687,  0.1378,  0.1910,  0.1472,  0.3417,  0.3443,
            0.4470],
          [ 0.0000,  0.1897,  0.2403,  0.2121,  0.1864,  0.0631,  0.1104,
            0.2068],
          [ 0.0000,  0.2225,  0.1511,  0.2220,  0.2021,  0.3134,  0.2999,
            0.3111],
          [ 0.0000,  0.3772,  0.2223,  0.2330,  0.2266,  0.1150,  0.4782,
            0.1304],
          [ 0.0000,  0.1871,  0.2186,  0.2440,  0.0266,  0.4202,  0.0020,
            0.2103],
          [ 0.0000,  0.2296,  0.1671,  0.0660,  0.1479,  0.1243,  0.2083,
            0.3464],
          [ 0.0000,  0.1295,  0.2721,  0.1425,  0.1788,  0.2640,  0.4450,
            0.2202],
          [ 0.0000,  0.2465,  0.2738,  0.1315,  0.3119,  0.3298, -0.0315,
            0.2760],
          [ 0.0000,  0.2966,  0.2194,  0.5167,  0.2021,  0.3370,  0.2310,
            0.1645],
          [ 0.0000,  0.1245,  0.3174,  0.0663,  0.1230,  0.2333,  0.1631,
            0.0086]]]], device='cuda:0')
weight shape torch.Size([1, 1, 3, 3])
grad_output shape torch.Size([1, 1, 8, 6])
grad_input torch.Size([1, 1, 10, 8])
padding info :: [1, 0, 0, 0]
new grad_input torch.Size([1, 1, 10, 7])

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 1, 20, 14])
grad_out size torch.Size([1, 1, 10, 7])
arg size torch.Size([1, 1, 10, 7])
maxpool2d140372191117952[2,-1] PI( <2,0,>,
 <otileshape 7,10,>,
 <padding >,
 <inpslidx 0,13,10,29,>, 
 <internal >, 
 <realidx 0,13,10,29,>, 
 <ndtsize 4,4,>, 
  local_first False
 next_id 140372191067488)

bk-maxpool2d140372191117952[1,-1] PI( <2,0,>,
 <otileshape 5,6,>,
 <padding >,
 <inpslidx 0,4,7,12,>, 
 <internal >, 
 <realidx 0,4,7,12,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191117808)

##############grad_in in maxp torch.Size([1, 1, 20, 14])
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 1, 22, 16])
weight shape torch.Size([1, 1, 3, 3])
grad_output shape torch.Size([1, 1, 20, 14])
final torch.Size([1, 1, 22, 16]) tensor([[[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00],
          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00],
          [-6.2398e-02, -6.6920e-02,  4.2967e-02,  0.0000e+00,  1.4653e-01,
            1.5715e-01, -1.0090e-01,  0.0000e+00,  0.0000e+00, -6.2398e-02,
           -6.6920e-02,  1.2460e-01,  8.7546e-02, -5.6209e-02,  0.0000e+00,
            0.0000e+00],
          [ 2.7432e-02,  3.5748e-02, -2.0941e-02,  2.0626e-02, -7.7663e-02,
           -8.3948e-02,  1.5924e-01,  6.9606e-02, -4.4691e-02,  2.7432e-02,
            3.5748e-02, -7.6060e-02,  1.8136e-02,  1.2216e-01, -4.4691e-02,
            0.0000e+00],
          [ 5.5269e-02, -7.0338e-03,  5.6771e-03,  5.1782e-03, -1.2781e-01,
            1.6518e-02, -2.6256e-02, -3.7183e-02,  4.1785e-02,  5.5269e-02,
           -7.0338e-03, -7.3274e-02, -1.9331e-02, -3.5914e-02,  4.1785e-02,
            0.0000e+00],
          [ 1.8883e-02,  2.0252e-02, -3.6677e-02, -6.4838e-03,  1.9368e-01,
            1.9697e-01, -1.8396e-01,  1.1563e-01,  1.1718e-01, -1.1810e-01,
            9.3577e-02,  1.8037e-01, -1.5406e-01,  1.1563e-01,  1.1718e-01,
           -7.4587e-02],
          [-8.3017e-03, -1.0818e-02, -1.2189e-03,  2.4786e-01,  2.8625e-01,
           -1.6452e-01,  4.7488e-02, -4.7620e-02,  5.1840e-02,  2.1102e-01,
           -1.1515e-01, -1.0836e-01,  1.1049e-01, -2.5963e-02, -7.5961e-02,
            6.9738e-02],
          [-1.6726e-02,  1.7877e-01,  1.8974e-01, -2.2985e-01, -3.4888e-01,
            1.2032e-01,  8.9204e-02, -7.4287e-02, -5.1768e-02, -2.5025e-02,
           -5.5801e-02,  1.4111e-01,  1.2841e-01, -1.9426e-01,  2.5211e-02,
            1.6830e-03],
          [ 0.0000e+00, -1.9042e-02, -3.8334e-02, -1.4468e-01, -6.3269e-02,
            1.5408e-02, -3.0504e-02, -3.6475e-02, -7.1892e-02,  1.2839e-02,
            1.7697e-03, -5.5385e-02, -9.0060e-02,  8.3384e-02,  3.1376e-04,
            0.0000e+00],
          [ 0.0000e+00, -1.8223e-01,  1.2476e-01,  1.8895e-01, -9.5321e-02,
            6.7363e-02,  6.4568e-02, -3.0805e-02, -1.4637e-02,  9.5013e-02,
            1.0190e-01, -1.0965e-01,  6.3223e-02, -6.9334e-02,  1.5991e-02,
            0.0000e+00],
          [ 0.0000e+00, -5.1920e-02, -5.4251e-02, -7.8396e-02,  8.9123e-02,
           -2.9615e-02, -1.8023e-02,  4.0752e-02, -3.6082e-04, -4.1771e-02,
            1.0469e-02,  1.0116e-01, -7.3074e-02,  5.6674e-02, -1.4951e-02,
            0.0000e+00],
          [ 4.9648e-03,  5.3246e-03, -1.2603e-01,  7.4011e-02,  6.4791e-02,
           -9.7382e-02,  1.0279e-02,  1.8554e-02,  2.0626e-02,  4.9130e-02,
            1.3933e-01, -1.9627e-01,  6.9948e-02, -1.5711e-03, -3.6082e-04,
            0.0000e+00],
          [ 1.6701e-02,  1.7408e-02, -9.8064e-03, -2.5678e-02, -3.3462e-02,
            3.6503e-02,  9.5296e-02,  9.6899e-02,  1.7094e-01,  2.1454e-01,
           -3.1259e-01,  1.0166e-01,  1.0084e-03,  0.0000e+00,  0.0000e+00,
            0.0000e+00],
          [-1.2699e-02, -1.0259e-02,  1.2235e-02,  6.8815e-03,  6.9449e-02,
           -4.1771e-02, -4.2349e-02, -7.2346e-02, -4.4832e-02, -2.7190e-01,
            2.8487e-01,  1.1845e-01, -7.4587e-02,  0.0000e+00,  0.0000e+00,
            0.0000e+00],
          [-1.6726e-02,  3.4743e-02,  1.8426e-01,  1.1156e-01,  1.9399e-02,
            2.0463e-01, -1.9275e-01,  1.0841e-02, -2.1866e-01,  2.8019e-02,
           -4.3758e-02, -4.1862e-02,  9.1395e-02, -1.3905e-02,  0.0000e+00,
            0.0000e+00],
          [ 0.0000e+00, -1.4339e-02, -8.4187e-02, -1.1628e-01,  3.4136e-02,
           -8.8132e-02,  3.4003e-01,  2.5726e-01, -1.9003e-02,  1.5677e-01,
           -1.9660e-01,  3.3323e-03, -9.8861e-03,  1.3001e-02,  0.0000e+00,
            0.0000e+00],
          [ 0.0000e+00, -1.0006e-02, -1.0804e-01,  4.2991e-03, -1.3549e-01,
            1.5597e-01,  4.5422e-02, -2.3275e-01,  1.3431e-01, -3.6403e-02,
            6.3714e-02, -4.1109e-02, -2.2629e-02,  1.6305e-02,  0.0000e+00,
            0.0000e+00],
          [ 0.0000e+00, -8.3017e-03,  4.7798e-02,  7.5022e-02, -4.0363e-02,
           -6.0858e-02, -2.9178e-01,  2.7854e-01,  2.8992e-02, -1.2062e-01,
            3.0689e-02,  1.0209e-02,  1.3304e-02, -1.4951e-02,  0.0000e+00,
            0.0000e+00],
          [ 0.0000e+00,  7.8287e-02,  7.8258e-02, -9.8713e-02,  3.7738e-02,
           -1.2261e-01,  1.5604e-02, -6.9235e-02, -1.3212e-01,  1.0952e-01,
            6.8584e-04,  2.0570e-02, -2.6178e-03, -3.6082e-04,  0.0000e+00,
            0.0000e+00],
          [ 0.0000e+00, -4.1771e-02, -3.8990e-02,  1.4002e-01, -6.8697e-02,
           -2.4906e-02,  1.5991e-02, -1.4383e-01,  1.8304e-02,  2.5229e-03,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00],
          [ 0.0000e+00, -8.4158e-02, -1.8905e-02, -3.7116e-02,  5.3579e-02,
            1.3304e-02, -1.4951e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00],
          [ 0.0000e+00,  0.0000e+00, -5.9667e-02,  7.5935e-03,  2.1616e-02,
           -2.6178e-03, -3.6082e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00]]]], device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 1, 8, 6])
grad_out size torch.Size([1, 1, 8, 8])
arg size torch.Size([1, 1, 4, 3])
maxpool2d140372191067248[0,-1] PI( <1,3,>,
 <otileshape 3,4,>,
 <padding >,
 <inpslidx 10,15,2,9,>, 
 <internal >, 
 <realidx 10,15,2,9,>, 
 <ndtsize 2,2,>, 
  local_first False
 next_id -99)

bk-maxpool2d140372191067248[3,-1] PI( <1,3,>,
 <otileshape 3,4,>,
 <padding >,
 <inpslidx 5,7,1,4,>, 
 <internal >, 
 <realidx 5,7,1,4,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191067488)

new_grad_out torch.Size([1, 1, 4, 3])
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
##############grad_in in maxp torch.Size([1, 1, 8, 6])
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 1, 10, 8])
local last ++ input tensor([[[[ 0.3350,  0.1998,  0.1490,  0.2962,  0.2056,  0.4027,  0.0769,
            0.0000],
          [ 0.0855,  0.3101,  0.1455,  0.1912,  0.3859,  0.0513,  0.4074,
            0.0000],
          [ 0.2489,  0.1839,  0.4211,  0.2067,  0.3607,  0.3721,  0.1305,
            0.0000],
          [ 0.1491,  0.2589,  0.1332,  0.2109,  0.2276,  0.0301,  0.1843,
            0.0000],
          [ 0.2884,  0.0402,  0.2938,  0.2875, -0.0126,  0.2264,  0.2046,
            0.0000],
          [ 0.4037,  0.4304, -0.0569,  0.2592,  0.2352,  0.0946,  0.3255,
            0.0000],
          [ 0.1541,  0.3395,  0.3022,  0.3429,  0.1588,  0.3002,  0.1257,
            0.0000],
          [ 0.2301,  0.2176,  0.3750,  0.2856,  0.2908,  0.3157,  0.3333,
            0.0000],
          [ 0.2586,  0.1248,  0.1634,  0.3155,  0.2679, -0.0112,  0.1765,
            0.0000],
          [ 0.1913,  0.2709, -0.0339,  0.2026,  0.2329,  0.1941,  0.2226,
            0.0000]]]], device='cuda:0')
weight shape torch.Size([1, 1, 3, 3])
grad_output shape torch.Size([1, 1, 8, 6])
grad_input torch.Size([1, 1, 10, 8])
padding info :: [0, 1, 0, 0]
new grad_input torch.Size([1, 1, 10, 7])

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 1, 20, 14])
grad_out size torch.Size([1, 1, 10, 7])
arg size torch.Size([1, 1, 10, 7])
maxpool2d140372191117952[2,-1] PI( <1,3,>,
 <otileshape 7,10,>,
 <padding >,
 <inpslidx 18,31,2,21,>, 
 <internal >, 
 <realidx 18,31,2,21,>, 
 <ndtsize 4,4,>, 
  local_first False
 next_id 140372191067488)

bk-maxpool2d140372191117952[1,-1] PI( <1,3,>,
 <otileshape 5,6,>,
 <padding >,
 <inpslidx 11,15,3,8,>, 
 <internal >, 
 <realidx 11,15,3,8,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191117808)

##############grad_in in maxp torch.Size([1, 1, 20, 14])
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 1, 22, 16])
weight shape torch.Size([1, 1, 3, 3])
grad_output shape torch.Size([1, 1, 20, 14])
final torch.Size([1, 1, 22, 16]) tensor([[[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  8.1630e-02,  8.7546e-02, -5.6209e-02,
            0.0000e+00,  0.0000e+00,  6.4902e-02,  6.9606e-02, -4.4691e-02,
            0.0000e+00],
          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -6.2398e-02,
           -6.6920e-02,  4.2967e-02, -3.5887e-02, -4.4262e-02,  5.5240e-02,
           -1.7243e-03,  8.1630e-02,  5.9013e-02, -9.3392e-02,  4.1785e-02,
            0.0000e+00],
          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  2.7432e-02,
            3.5748e-02, -4.0173e-02, -7.2304e-02,  8.1008e-03,  6.2133e-02,
            6.8427e-02, -7.8786e-02, -1.0425e-01,  1.0329e-01,  4.7572e-02,
           -2.9896e-02],
          [ 0.0000e+00, -6.2398e-02, -6.6920e-02,  1.2460e-01,  2.2660e-01,
            2.6615e-02, -4.7195e-05,  6.2865e-02, -4.2581e-02, -2.7107e-02,
            2.2963e-02,  3.0670e-02, -3.1161e-02, -1.7819e-02, -2.4873e-02,
            2.7952e-02],
          [ 1.8883e-02,  4.7684e-02,  2.2745e-02, -1.7444e-02, -2.0736e-02,
           -3.5809e-02,  2.8173e-02, -3.3581e-02,  3.7738e-02, -5.5182e-02,
            4.8616e-02,  3.9632e-02, -8.6470e-03, -6.1679e-02, -2.0012e-02,
            1.6665e-02],
          [-8.3017e-03,  4.4451e-02,  5.1236e-03, -9.9043e-02, -9.8593e-02,
            1.2448e-01,  1.7992e-01,  1.1404e-01,  3.5008e-02,  1.4660e-01,
           -1.7566e-01, -3.1985e-02,  4.4280e-02,  1.0209e-02,  1.3304e-02,
           -1.4951e-02],
          [-1.6726e-02,  3.4743e-02,  3.5272e-02, -7.4378e-02,  6.6075e-03,
            2.8048e-02, -4.4108e-02, -7.8111e-02,  3.5830e-02, -3.4895e-02,
            7.4901e-02, -2.2303e-02,  1.0467e-03,  1.0220e-01,  8.4928e-02,
           -5.6570e-02],
          [ 0.0000e+00, -1.4339e-02, -1.8685e-02,  1.6999e-01,  1.5979e-01,
           -1.9657e-01, -9.9481e-02,  1.1983e-01, -1.5912e-01, -3.6787e-03,
           -2.2749e-02, -3.4445e-02, -6.6920e-02,  7.0794e-03, -4.6766e-02,
            5.2555e-02],
          [ 0.0000e+00, -2.8889e-02, -1.0522e-04, -6.9051e-02, -8.2754e-02,
            4.2280e-02, -1.8943e-02, -3.2640e-02, -4.7882e-02, -1.3028e-01,
            6.3852e-02,  1.2862e-01,  2.6706e-01, -4.9219e-02, -7.5852e-02,
            1.2683e-03],
          [ 0.0000e+00,  1.8883e-02,  2.1915e-02, -1.4281e-01,  2.3442e-01,
            2.3832e-01, -2.0345e-01,  1.3887e-01,  1.8040e-01, -4.2024e-02,
           -5.5124e-02,  1.1081e-02, -1.1892e-01, -7.0212e-03,  7.9523e-02,
            0.0000e+00],
          [ 9.5013e-02,  9.3597e-02, -7.2893e-02,  1.1731e-02, -9.6804e-02,
           -1.2607e-01,  1.4168e-01,  6.6898e-02,  2.1796e-01,  7.9256e-02,
            9.4605e-02,  7.5734e-02, -2.0386e-01,  1.5485e-02,  1.9192e-03,
            0.0000e+00],
          [-4.1771e-02, -7.1159e-02,  1.4955e-01,  9.2790e-02, -2.5431e-01,
           -2.1981e-03, -2.5542e-02, -1.5353e-01, -9.3565e-02,  3.4561e-02,
           -1.3598e-02, -8.8014e-02,  2.0969e-01,  1.1881e-01, -7.6281e-02,
            0.0000e+00],
          [-8.4158e-02, -5.1687e-02, -1.0336e-01, -6.4440e-03,  5.5527e-02,
            1.6302e-01,  1.5057e-01, -2.6119e-01, -4.1523e-02,  1.1388e-02,
           -1.3478e-01,  1.7318e-02, -4.6315e-02, -2.1786e-02,  1.1602e-01,
           -2.8700e-02],
          [ 0.0000e+00,  2.7432e-02,  1.3600e-01,  1.5899e-01, -1.2029e-01,
           -4.2531e-02, -7.7765e-02,  1.1236e-01, -1.7386e-02,  1.5766e-01,
            3.8298e-01,  1.2083e-01, -2.4541e-01, -5.8361e-03, -2.2157e-02,
            2.6834e-02],
          [ 1.8883e-02,  7.5521e-02, -9.7695e-02, -1.0217e-01,  1.1373e-01,
           -1.3388e-01,  4.0957e-02, -6.9557e-04, -4.1958e-04, -1.5036e-02,
           -1.2615e-01, -5.8410e-02,  1.3771e-01, -3.6918e-02,  4.6983e-03,
            6.4760e-04],
          [-8.3017e-03, -1.0818e-02, -1.4431e-01,  7.8528e-02,  2.0404e-01,
            1.0810e-01, -9.5321e-02,  8.6246e-02,  9.2497e-02, -2.2290e-01,
           -2.0278e-01,  1.6354e-01,  1.5617e-01, -2.3696e-02, -2.9896e-02,
            0.0000e+00],
          [ 7.8287e-02,  1.0403e-01, -6.5131e-02, -2.5770e-02, -9.4440e-02,
           -6.4791e-02,  6.4218e-02, -2.1926e-02, -4.9411e-02,  7.4507e-03,
            5.0258e-02,  3.3257e-03, -1.3116e-01,  1.5912e-02,  4.3943e-02,
            0.0000e+00],
          [-4.1771e-02, -5.4433e-02,  1.2853e-01,  2.0325e-02, -1.6239e-01,
            2.6725e-02,  1.1047e-01,  1.0555e-02, -5.5703e-02,  1.6372e-01,
            1.5474e-01, -2.2747e-01,  1.1673e-02,  1.9784e-02, -1.4277e-02,
            0.0000e+00],
          [-8.4158e-02,  1.0710e-02, -2.8139e-02, -3.8592e-02,  4.3369e-02,
            2.0570e-02, -4.4389e-02, -5.4794e-02,  6.1171e-02, -7.1386e-02,
           -1.3212e-01,  1.0952e-01,  2.1255e-02, -2.6178e-03, -3.6082e-04,
            0.0000e+00],
          [ 0.0000e+00,  0.0000e+00, -5.9667e-02,  7.5935e-03,  1.0467e-03,
            0.0000e+00, -8.4158e-02,  1.0710e-02,  1.4763e-03, -1.4383e-01,
            1.8304e-02,  2.5229e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00],
          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00],
          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00]]]], device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 1, 8, 8])
grad_out size torch.Size([1, 1, 8, 8])
arg size torch.Size([1, 1, 4, 4])
maxpool2d140372191067248[0,-1] PI( <1,2,>,
 <otileshape 4,4,>,
 <padding >,
 <inpslidx 6,13,2,9,>, 
 <internal >, 
 <realidx 6,13,2,9,>, 
 <ndtsize 2,2,>, 
  local_first False
 next_id -99)

[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
bk-maxpool2d140372191067248[3,-1] PI( <1,2,>,
 <otileshape 4,4,>,
 <padding >,
 <inpslidx 3,6,1,4,>, 
 <internal >, 
 <realidx 3,6,1,4,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191067488)

new_grad_out torch.Size([1, 1, 4, 4])
##############grad_in in maxp torch.Size([1, 1, 8, 8])
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 1, 10, 10])
local last ++ input tensor([[[[ 0.2603,  0.1670,  0.0507,  0.1273,  0.3350,  0.1998,  0.1490,
            0.2962,  0.2056,  0.4027],
          [ 0.0426,  0.2653,  0.4077,  0.3255,  0.0855,  0.3101,  0.1455,
            0.1912,  0.3859,  0.0513],
          [ 0.3053,  0.3084,  0.3288,  0.1344,  0.2489,  0.1839,  0.4211,
            0.2067,  0.3607,  0.3721],
          [ 0.2874,  0.3051,  0.3212,  0.3645,  0.1491,  0.2589,  0.1332,
            0.2109,  0.2276,  0.0301],
          [ 0.3443,  0.4470,  0.2105,  0.2988,  0.2884,  0.0402,  0.2938,
            0.2875, -0.0126,  0.2264],
          [ 0.1104,  0.2068,  0.2279,  0.2150,  0.4037,  0.4304, -0.0569,
            0.2592,  0.2352,  0.0946],
          [ 0.2999,  0.3111,  0.2239,  0.0695,  0.1541,  0.3395,  0.3022,
            0.3429,  0.1588,  0.3002],
          [ 0.4782,  0.1304,  0.1500,  0.1994,  0.2301,  0.2176,  0.3750,
            0.2856,  0.2908,  0.3157],
          [ 0.0020,  0.2103,  0.2201,  0.1518,  0.2586,  0.1248,  0.1634,
            0.3155,  0.2679, -0.0112],
          [ 0.2083,  0.3464,  0.3073,  0.3793,  0.1913,  0.2709, -0.0339,
            0.2026,  0.2329,  0.1941]]]], device='cuda:0')
weight shape torch.Size([1, 1, 3, 3])
grad_output shape torch.Size([1, 1, 8, 8])
grad_input torch.Size([1, 1, 10, 10])
padding info :: [0, 0, 0, 0]
new grad_input torch.Size([1, 1, 10, 10])

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 1, 20, 20])
grad_out size torch.Size([1, 1, 10, 10])
arg size torch.Size([1, 1, 10, 10])
maxpool2d140372191117952[2,-1] PI( <1,2,>,
 <otileshape 10,10,>,
 <padding >,
 <inpslidx 10,29,2,21,>, 
 <internal >, 
 <realidx 10,29,2,21,>, 
 <ndtsize 4,4,>, 
  local_first False
 next_id 140372191067488)

bk-maxpool2d140372191117952[1,-1] PI( <1,2,>,
 <otileshape 6,6,>,
 <padding >,
 <inpslidx 7,12,3,8,>, 
 <internal >, 
 <realidx 7,12,3,8,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191117808)

##############grad_in in maxp torch.Size([1, 1, 20, 20])
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 1, 22, 22])
weight shape torch.Size([1, 1, 3, 3])
grad_output shape torch.Size([1, 1, 20, 20])
final torch.Size([1, 1, 22, 22]) tensor([[[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
           -6.2398e-02,  1.4709e-02,  1.3051e-01, -5.6209e-02,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            8.1630e-02,  8.7546e-02, -5.6209e-02,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00],
          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            2.7432e-02, -1.3923e-04, -8.6939e-02,  1.1746e-01,  6.9606e-02,
           -4.4691e-02,  0.0000e+00, -6.2398e-02, -6.6920e-02,  4.2967e-02,
           -3.5887e-02,  1.8136e-02,  1.2216e-01, -4.4691e-02,  0.0000e+00,
            0.0000e+00,  0.0000e+00],
          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  8.3785e-02,
            1.4513e-01, -1.3703e-01,  8.2322e-03, -2.7265e-02, -3.7183e-02,
            4.1785e-02,  0.0000e+00,  2.7432e-02,  3.5748e-02, -4.0173e-02,
           -7.2304e-02, -1.9331e-02,  7.5021e-03,  8.8348e-02, -2.9896e-02,
            0.0000e+00,  0.0000e+00],
          [-6.2398e-02, -6.6920e-02,  1.2460e-01,  8.7546e-02, -9.3044e-02,
           -4.8001e-02,  1.1256e-01,  6.2865e-02, -9.7850e-02, -1.1665e-02,
           -1.9349e-02,  9.4700e-02,  2.2660e-01,  2.6615e-02, -4.7195e-05,
            6.2865e-02, -9.7850e-02, -1.1771e-02, -2.3865e-02,  2.7952e-02,
            0.0000e+00,  0.0000e+00],
          [ 2.7432e-02, -7.7666e-03, -1.2273e-01, -1.6802e-02,  1.8167e-01,
            2.2751e-01, -1.5951e-01, -2.8257e-02,  1.1161e-01,  9.1237e-02,
           -4.2347e-02,  7.5410e-02,  4.8870e-02, -8.0499e-02,  2.8173e-02,
           -3.3581e-02,  3.7738e-02, -3.8456e-02,  4.8941e-03,  6.7459e-04,
            0.0000e+00,  0.0000e+00],
          [ 5.5269e-02,  1.2097e-02,  9.1902e-02,  1.3160e-01, -1.8469e-01,
           -1.1649e-01,  7.6806e-02,  3.7632e-03, -2.9872e-02, -2.7467e-02,
            4.7621e-02, -1.2690e-01, -1.3578e-01,  1.6627e-01,  1.7992e-01,
            1.1404e-01, -6.0005e-02,  4.4700e-02, -2.8700e-02,  0.0000e+00,
            0.0000e+00,  0.0000e+00],
          [ 0.0000e+00,  1.5244e-01,  5.5589e-02, -1.5945e-01, -6.9615e-02,
            4.4578e-02, -1.5144e-02,  5.5965e-04, -6.8384e-02,  9.9943e-02,
            9.9044e-02, -1.7223e-01,  1.3924e-02,  2.9056e-02, -4.4108e-02,
           -7.8111e-02,  7.7601e-02,  1.9538e-02,  7.3397e-02, -2.9896e-02,
            0.0000e+00,  0.0000e+00],
          [ 0.0000e+00, -5.0073e-02, -1.8948e-01,  2.1512e-01,  1.2841e-01,
           -9.8317e-02,  3.1884e-02,  2.0252e-02, -1.3003e-02, -4.0108e-02,
           -5.2267e-02,  2.5114e-01,  2.0635e-01, -2.2646e-01, -9.9481e-02,
            1.1983e-01, -7.4965e-02, -1.4389e-02, -2.4226e-02,  2.7952e-02,
            0.0000e+00,  0.0000e+00],
          [ 0.0000e+00, -1.0088e-01,  8.0202e-02,  1.8630e-02, -1.3645e-01,
            6.0162e-02, -3.2894e-02,  1.0019e-01,  1.1406e-01, -1.4623e-01,
           -1.6720e-02, -1.1213e-01, -9.1637e-02,  7.0233e-02, -1.8943e-02,
           -3.2640e-02,  1.4516e-02, -6.3362e-02,  2.0885e-02,  6.7459e-04,
            0.0000e+00,  0.0000e+00],
          [ 0.0000e+00,  9.5013e-02,  7.2284e-02, -2.1560e-01,  5.7570e-02,
            1.2167e-02, -3.4218e-03, -5.4593e-02, -5.4140e-02,  1.4742e-01,
            1.0437e-01, -2.1434e-01,  2.2436e-01,  2.3900e-01, -2.0345e-01,
            1.3887e-01,  1.5297e-01, -7.7772e-02, -1.4951e-02,  0.0000e+00,
            0.0000e+00,  0.0000e+00],
          [ 0.0000e+00, -1.0417e-01, -1.8102e-01,  1.1173e-01,  1.0467e-03,
            8.5471e-02,  6.6988e-02, -1.2921e-01,  1.8735e-01,  1.5300e-01,
           -1.4713e-01,  5.2483e-02, -9.7165e-02, -1.2607e-01,  1.4168e-01,
            6.6898e-02,  1.4381e-01,  6.6037e-02, -4.5052e-02,  0.0000e+00,
            0.0000e+00,  0.0000e+00],
          [ 0.0000e+00, -5.6726e-02,  1.2809e-01,  4.8849e-02, -5.6209e-02,
           -2.8533e-02, -3.7183e-02, -2.0613e-02, -1.4458e-01, -1.3463e-01,
            2.7460e-01,  1.6344e-01, -2.9900e-01, -2.1981e-03, -2.5542e-02,
           -1.5353e-01, -8.5263e-02,  4.5380e-02,  4.1785e-02,  0.0000e+00,
            0.0000e+00,  0.0000e+00],
          [ 0.0000e+00,  5.5269e-02, -4.2921e-02, -4.7735e-02,  5.2555e-02,
           -5.7488e-02,  7.3161e-03,  2.8441e-02, -1.2071e-01, -2.4043e-02,
           -6.7761e-02, -8.3989e-02,  9.7312e-02,  1.6302e-01,  1.5057e-01,
           -2.6119e-01, -2.4797e-02,  9.2590e-03,  1.0084e-03,  0.0000e+00,
            0.0000e+00,  0.0000e+00],
          [ 1.8883e-02,  2.0252e-02, -8.5307e-02,  6.7818e-02,  6.4133e-02,
            3.0537e-03,  4.6563e-02,  4.4257e-02,  1.3218e-02, -1.2310e-02,
            8.8344e-02,  2.5061e-01, -1.4918e-01, -4.2531e-02, -7.7765e-02,
            1.1236e-01, -1.7386e-02,  1.2505e-01,  1.9901e-01, -1.6499e-02,
           -4.4691e-02,  0.0000e+00],
          [-8.3017e-03, -1.0818e-02,  1.7122e-02, -2.0445e-02, -3.7000e-02,
            1.4660e-02, -2.9153e-02,  2.2399e-02,  1.4033e-01,  1.7761e-01,
           -2.0125e-01, -1.2613e-01,  1.4168e-01, -1.3388e-01,  4.0957e-02,
           -6.9557e-04, -4.1958e-04, -1.9581e-02, -6.2213e-02,  1.8953e-02,
            4.1785e-02,  0.0000e+00],
          [-1.6726e-02,  9.7141e-02,  1.0001e-01, -1.2019e-01,  9.8039e-03,
           -3.5791e-02,  7.1804e-03,  2.2292e-01,  1.9473e-01, -2.5262e-01,
           -9.7607e-02,  6.0200e-02,  1.7981e-01,  1.2409e-01, -9.5321e-02,
            8.6246e-02,  9.2497e-02, -1.8571e-01, -6.3669e-02,  7.5462e-02,
            4.7572e-02, -2.9896e-02],
          [ 0.0000e+00, -4.1771e-02, -5.8831e-02,  6.1731e-02,  7.7141e-05,
            3.5349e-03,  1.0158e-01,  3.1745e-03, -2.0409e-01,  3.2101e-01,
           -9.2972e-02, -1.5560e-02, -8.1135e-02, -7.9742e-02,  6.4218e-02,
           -2.1926e-02, -4.9411e-02,  2.4177e-02, -1.9233e-02, -4.3443e-02,
           -8.8825e-03,  2.7952e-02],
          [ 0.0000e+00, -8.4158e-02,  2.9594e-02,  2.1728e-02,  6.4496e-02,
            8.3117e-02, -9.8222e-02, -2.7240e-01,  3.2061e-02, -7.5553e-02,
            1.5649e-01,  4.0895e-02, -1.6501e-01,  2.6364e-02,  1.1047e-01,
            1.0555e-02, -5.5703e-02,  6.8703e-02,  8.2454e-02, -7.1537e-02,
           -1.0057e-02,  6.7459e-04],
          [ 0.0000e+00,  0.0000e+00, -8.3017e-03, -1.0818e-02,  1.4046e-01,
            1.2974e-01, -1.0815e-01,  5.8841e-02, -1.5142e-01, -7.6181e-03,
           -5.2370e-02, -2.2601e-02,  4.3369e-02,  2.0570e-02, -4.4389e-02,
           -5.4794e-02,  6.1171e-02, -2.9615e-02, -1.8023e-02,  4.0752e-02,
           -3.6082e-04,  0.0000e+00],
          [ 0.0000e+00,  0.0000e+00, -1.6726e-02,  9.7141e-02, -3.7839e-02,
           -1.4971e-01,  8.6339e-02, -2.5288e-02,  2.8418e-02,  1.0209e-02,
           -4.6363e-02, -7.3577e-03,  1.0467e-03,  0.0000e+00, -8.4158e-02,
            1.0710e-02,  1.4763e-03, -5.9667e-02,  7.5935e-03,  1.0467e-03,
            0.0000e+00,  0.0000e+00],
          [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -4.1771e-02, -1.9826e-01,
            7.9475e-02, -3.6575e-02,  4.9757e-03,  6.8584e-04,  2.0570e-02,
           -2.6178e-03, -3.6082e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00],
          [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -8.4158e-02,  1.0710e-02,
            1.4763e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00]]]], device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 1, 8, 8])
grad_out size torch.Size([1, 1, 8, 8])
arg size torch.Size([1, 1, 4, 4])
maxpool2d140372191067248[0,-1] PI( <1,1,>,
 <otileshape 4,4,>,
 <padding >,
 <inpslidx 2,9,2,9,>, 
 <internal >, 
 <realidx 2,9,2,9,>, 
 <ndtsize 2,2,>, 
  local_first False
 next_id -99)

bk-maxpool2d140372191067248[3,-1] PI( <1,1,>,
 <otileshape 4,4,>,
 <padding >,
 <inpslidx 1,4,1,4,>, 
 <internal >, 
 <realidx 1,4,1,4,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191067488)

new_grad_out torch.Size([1, 1, 4, 4])
##############grad_in in maxp torch.Size([1, 1, 8, 8])
@@@ using cudnn bkw
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
local last ++ input shape torch.Size([1, 1, 10, 10])
local last ++ input tensor([[[[ 0.2056,  0.3195,  0.1760, -0.0424,  0.2603,  0.1670,  0.0507,
            0.1273,  0.3350,  0.1998],
          [ 0.4188,  0.1703,  0.2766,  0.4127,  0.0426,  0.2653,  0.4077,
            0.3255,  0.0855,  0.3101],
          [ 0.2722,  0.2960,  0.2889,  0.0363,  0.3053,  0.3084,  0.3288,
            0.1344,  0.2489,  0.1839],
          [ 0.3017, -0.0242,  0.0373,  0.4412,  0.2874,  0.3051,  0.3212,
            0.3645,  0.1491,  0.2589],
          [ 0.1378,  0.1910,  0.1472,  0.3417,  0.3443,  0.4470,  0.2105,
            0.2988,  0.2884,  0.0402],
          [ 0.2403,  0.2121,  0.1864,  0.0631,  0.1104,  0.2068,  0.2279,
            0.2150,  0.4037,  0.4304],
          [ 0.1511,  0.2220,  0.2021,  0.3134,  0.2999,  0.3111,  0.2239,
            0.0695,  0.1541,  0.3395],
          [ 0.2223,  0.2330,  0.2266,  0.1150,  0.4782,  0.1304,  0.1500,
            0.1994,  0.2301,  0.2176],
          [ 0.2186,  0.2440,  0.0266,  0.4202,  0.0020,  0.2103,  0.2201,
            0.1518,  0.2586,  0.1248],
          [ 0.1671,  0.0660,  0.1479,  0.1243,  0.2083,  0.3464,  0.3073,
            0.3793,  0.1913,  0.2709]]]], device='cuda:0')
weight shape torch.Size([1, 1, 3, 3])
grad_output shape torch.Size([1, 1, 8, 8])
grad_input torch.Size([1, 1, 10, 10])
padding info :: [0, 0, 0, 0]
new grad_input torch.Size([1, 1, 10, 10])

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 1, 20, 20])
grad_out size torch.Size([1, 1, 10, 10])
arg size torch.Size([1, 1, 10, 10])
maxpool2d140372191117952[2,-1] PI( <1,1,>,
 <otileshape 10,10,>,
 <padding >,
 <inpslidx 2,21,2,21,>, 
 <internal >, 
 <realidx 2,21,2,21,>, 
 <ndtsize 4,4,>, 
  local_first False
 next_id 140372191067488)

bk-maxpool2d140372191117952[1,-1] PI( <1,1,>,
 <otileshape 6,6,>,
 <padding >,
 <inpslidx 3,8,3,8,>, 
 <internal >, 
 <realidx 3,8,3,8,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191117808)

##############grad_in in maxp torch.Size([1, 1, 20, 20])
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 1, 22, 22])
weight shape torch.Size([1, 1, 3, 3])
grad_output shape torch.Size([1, 1, 20, 20])
final torch.Size([1, 1, 22, 22]) tensor([[[[ 0.0000e+00, -6.2398e-02, -6.6920e-02,  4.2967e-02,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00, -6.2398e-02,  1.4709e-02,
            1.3051e-01, -5.6209e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00],
          [ 0.0000e+00,  2.7432e-02,  3.5748e-02,  4.1457e-02,  8.7546e-02,
            8.6925e-03,  6.9606e-02, -4.4691e-02,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  2.7432e-02, -1.3923e-04,
           -8.6939e-02,  1.1746e-01,  6.9606e-02, -4.4691e-02,  0.0000e+00,
            0.0000e+00,  0.0000e+00],
          [ 0.0000e+00,  5.5269e-02,  5.1582e-02,  2.6008e-02, -8.7128e-02,
            2.4022e-02, -9.9580e-02, -2.5135e-02,  4.2967e-02,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  8.3785e-02,  1.4513e-01, -1.3703e-01,
            8.2322e-03, -2.7265e-02, -3.7183e-02,  4.1785e-02,  0.0000e+00,
            0.0000e+00,  0.0000e+00],
          [ 0.0000e+00,  1.8883e-02, -5.5177e-03, -1.1889e-01,  4.6940e-02,
           -1.2803e-02,  8.1311e-02,  6.8603e-03, -2.0941e-02,  2.0626e-02,
            1.3329e-01,  1.5715e-01, -1.3774e-01, -4.8001e-02,  1.1256e-01,
            6.2865e-02, -9.7850e-02,  5.0732e-02,  4.7572e-02, -2.9896e-02,
            0.0000e+00,  0.0000e+00],
          [ 9.5013e-02,  9.3597e-02, -6.0800e-02,  9.1010e-02, -6.8697e-02,
           -4.3993e-02,  2.8724e-03, -2.5750e-02,  2.0539e-02,  8.5713e-02,
            5.1704e-02, -1.5056e-01,  2.2346e-01,  2.2751e-01, -1.5951e-01,
           -2.8257e-02,  9.2727e-02,  4.3553e-02, -6.5092e-02,  9.2854e-02,
            6.9606e-02, -4.4691e-02],
          [-4.1771e-02, -7.1159e-02,  3.3685e-02, -3.8299e-02,  5.3579e-02,
           -2.5152e-02,  9.0735e-03,  2.5604e-02, -4.5050e-02, -4.0358e-02,
            6.3653e-02,  3.4538e-01, -2.5827e-01, -1.1649e-01,  7.6806e-02,
            3.7632e-03, -2.1571e-02, -7.1918e-02,  4.2498e-02, -2.7859e-02,
           -3.7183e-02,  4.1785e-02],
          [-8.4158e-02,  1.0710e-02, -1.2059e-01, -5.9327e-02,  1.4621e-01,
            8.4928e-02, -1.8027e-02, -4.9052e-03, -6.7612e-04,  1.5419e-01,
            1.5889e-01, -3.0608e-01,  1.2210e-04,  4.4578e-02, -1.5144e-02,
            5.5965e-04, -5.1657e-02,  6.5200e-02,  6.3772e-02, -9.7850e-02,
            7.3161e-03,  1.0084e-03],
          [ 0.0000e+00,  0.0000e+00,  2.7432e-02,  3.5748e-02, -7.6060e-02,
           -4.6766e-02,  2.3135e-01,  1.9176e-01, -1.2312e-01, -1.0546e-01,
           -3.5759e-01,  3.2863e-01,  1.5175e-01, -1.1222e-01,  3.1884e-02,
            2.0252e-02, -1.3003e-02, -2.5770e-02, -3.3581e-02,  8.1155e-02,
            4.6563e-02, -2.9896e-02],
          [ 0.0000e+00,  0.0000e+00,  7.4153e-02,  1.3218e-02, -8.6277e-02,
            9.2017e-03, -7.7337e-02,  3.5995e-02,  2.6358e-01, -3.0779e-01,
            7.1180e-02, -1.3196e-02, -1.3202e-01,  7.3163e-02, -3.2894e-02,
            1.0019e-01,  1.1406e-01, -1.1734e-01, -1.6615e-02, -4.3082e-02,
           -8.8825e-03,  2.7952e-02],
          [ 0.0000e+00,  0.0000e+00, -8.3017e-03, -1.0818e-02,  7.0774e-02,
            6.2865e-02, -1.9873e-01, -4.0703e-02, -7.6528e-02,  2.5150e-01,
            1.5474e-01, -2.6657e-01,  4.4896e-02,  1.2481e-02, -3.4218e-03,
           -5.4593e-02, -5.4140e-02,  1.2853e-01,  8.2454e-02, -7.1537e-02,
           -1.0057e-02,  6.7459e-04],
          [ 0.0000e+00,  0.0000e+00,  1.5992e-01,  1.9157e-01, -1.4711e-01,
           -3.3581e-02,  3.7738e-02, -2.0824e-01, -7.6222e-02,  8.9542e-03,
           -1.1150e-01,  9.6274e-02,  6.8584e-04,  8.5471e-02,  6.6988e-02,
           -1.2921e-01,  9.2340e-02,  5.9407e-02, -7.4232e-02,  4.0752e-02,
           -3.6082e-04,  0.0000e+00],
          [-6.2398e-02, -6.6920e-02, -3.4691e-02, -1.0120e-01,  1.9407e-01,
            1.4846e-01, -9.0165e-02,  3.7642e-02,  4.9052e-02, -2.0740e-01,
            1.5382e-01,  1.7206e-01, -1.0090e-01, -2.8533e-02, -3.7183e-02,
           -2.0613e-02, -1.0281e-01, -6.3466e-02,  1.2505e-01,  7.0653e-02,
           -4.4691e-02,  0.0000e+00],
          [-1.6082e-02, -1.0921e-02, -1.6667e-01,  1.9912e-02, -5.5403e-02,
           -7.5775e-02,  8.5155e-02,  7.5839e-02, -9.6516e-03, -1.8365e-02,
           -6.2252e-02, -8.3650e-02,  9.4340e-02, -5.7488e-02,  7.3161e-03,
            2.8441e-02, -3.6556e-02,  2.7645e-02,  3.5600e-02, -7.7545e-02,
            4.1785e-02,  0.0000e+00],
          [ 7.4400e-02,  1.7896e-02,  1.1126e-01,  1.5041e-01, -2.1373e-01,
            1.2323e-01,  1.1822e-01, -1.1810e-01,  1.1246e-01,  2.0063e-01,
           -2.3937e-01,  1.8345e-01,  1.8131e-01, -7.1533e-02,  4.6563e-02,
            4.4257e-02,  1.3218e-02, -3.9742e-02, -4.7653e-02,  9.1617e-02,
           -2.8888e-02,  0.0000e+00],
          [ 3.8543e-02,  1.0899e-01,  1.8580e-01, -2.3665e-02,  3.5453e-03,
           -4.7620e-02,  5.1840e-02,  2.1102e-01, -1.2346e-01, -1.1918e-01,
            1.2761e-01, -4.6408e-02, -1.1296e-01,  8.4398e-02, -2.9153e-02,
            2.2399e-02,  1.2145e-01,  1.0209e-01, -1.0356e-01, -2.3963e-02,
            2.7952e-02,  0.0000e+00],
          [ 0.0000e+00, -5.0073e-02, -2.4486e-01,  1.6964e-02,  1.0348e-01,
           -7.4287e-02, -5.1768e-02, -2.5025e-02, -7.2527e-02,  2.3825e-01,
            2.2842e-01, -3.1445e-01,  3.5015e-02, -3.4108e-02,  7.1804e-03,
            2.2292e-01,  2.0303e-01, -2.4181e-01,  4.6698e-02, -1.8328e-02,
           -2.4231e-02,  1.5991e-02],
          [ 0.0000e+00, -1.0088e-01, -9.8748e-02,  1.5971e-02, -3.0143e-02,
           -3.6475e-02, -7.1892e-02,  1.2839e-02,  1.7697e-03, -9.7155e-02,
           -1.4889e-01,  1.4511e-01,  3.9091e-04,  3.5349e-03,  1.0158e-01,
            3.1745e-03, -2.8238e-01,  2.1698e-01, -2.7841e-02,  1.0209e-02,
            1.3304e-02, -1.4951e-02],
          [ 9.5013e-02,  1.0190e-01, -6.5425e-02,  6.7363e-02,  6.4568e-02,
           -3.0805e-02, -1.4637e-02,  9.5013e-02,  1.0190e-01, -1.9381e-01,
            9.2817e-02, -4.7605e-02,  8.0487e-02,  8.3117e-02, -9.8222e-02,
           -2.7240e-01,  7.3831e-02, -2.1120e-02,  2.7952e-02,  2.0570e-02,
           -2.6178e-03, -3.6082e-04],
          [-4.1771e-02, -5.4433e-02,  6.1171e-02, -2.9615e-02, -1.8023e-02,
            4.0752e-02, -3.6082e-04, -4.1771e-02, -5.4433e-02,  3.1556e-02,
           -3.6685e-02,  4.5855e-02,  1.2551e-01,  1.2974e-01, -1.0815e-01,
            5.8841e-02, -6.7266e-02, -1.8328e-02, -2.4231e-02,  1.5991e-02,
            0.0000e+00,  0.0000e+00],
          [-8.4158e-02,  1.0710e-02,  1.4763e-03, -5.9667e-02,  7.5935e-03,
            1.0467e-03,  0.0000e+00, -8.4158e-02,  1.0710e-02, -5.8191e-02,
            1.1437e-02,  9.5570e-02, -3.8200e-02, -1.4971e-01,  8.6339e-02,
           -2.5288e-02,  2.8418e-02,  1.0209e-02,  1.3304e-02, -1.4951e-02,
            0.0000e+00,  0.0000e+00],
          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00, -4.1771e-02, -1.9826e-01,  7.9475e-02, -3.6575e-02,
            4.9757e-03,  6.8584e-04,  2.0570e-02, -2.6178e-03, -3.6082e-04,
            0.0000e+00,  0.0000e+00],
          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00, -8.4158e-02,  1.0710e-02,  1.4763e-03,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00]]]], device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 1, 8, 6])
grad_out size torch.Size([1, 1, 8, 8])
arg size torch.Size([1, 1, 4, 3])
maxpool2d140372191067248[0,-1] PI( <1,0,>,
 <otileshape 3,4,>,
 <padding >,
 <inpslidx 0,5,2,9,>, 
 <internal >, 
 <realidx 0,5,2,9,>, 
 <ndtsize 2,2,>, 
  local_first False
 next_id -99)

bk-maxpool2d140372191067248[3,-1] PI( <1,0,>,
 <otileshape 3,4,>,
 <padding >,
 <inpslidx 0,2,1,4,>, 
 <internal >, 
 <realidx 0,2,1,4,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191067488)

new_grad_out torch.Size([1, 1, 4, 3])
##############grad_in in maxp torch.Size([1, 1, 8, 6])
@@@ using cudnn bkw
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
local last ++ input shape torch.Size([1, 1, 10, 8])
local last ++ input tensor([[[[ 0.0000,  0.1782,  0.2056,  0.3195,  0.1760, -0.0424,  0.2603,
            0.1670],
          [ 0.0000,  0.2554,  0.4188,  0.1703,  0.2766,  0.4127,  0.0426,
            0.2653],
          [ 0.0000,  0.2028,  0.2722,  0.2960,  0.2889,  0.0363,  0.3053,
            0.3084],
          [ 0.0000,  0.1777,  0.3017, -0.0242,  0.0373,  0.4412,  0.2874,
            0.3051],
          [ 0.0000,  0.1687,  0.1378,  0.1910,  0.1472,  0.3417,  0.3443,
            0.4470],
          [ 0.0000,  0.1897,  0.2403,  0.2121,  0.1864,  0.0631,  0.1104,
            0.2068],
          [ 0.0000,  0.2225,  0.1511,  0.2220,  0.2021,  0.3134,  0.2999,
            0.3111],
          [ 0.0000,  0.3772,  0.2223,  0.2330,  0.2266,  0.1150,  0.4782,
            0.1304],
          [ 0.0000,  0.1871,  0.2186,  0.2440,  0.0266,  0.4202,  0.0020,
            0.2103],
          [ 0.0000,  0.2296,  0.1671,  0.0660,  0.1479,  0.1243,  0.2083,
            0.3464]]]], device='cuda:0')
weight shape torch.Size([1, 1, 3, 3])
grad_output shape torch.Size([1, 1, 8, 6])
grad_input torch.Size([1, 1, 10, 8])
padding info :: [1, 0, 0, 0]
new grad_input torch.Size([1, 1, 10, 7])

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 1, 20, 14])
grad_out size torch.Size([1, 1, 10, 7])
arg size torch.Size([1, 1, 10, 7])
maxpool2d140372191117952[2,-1] PI( <1,0,>,
 <otileshape 7,10,>,
 <padding >,
 <inpslidx 0,13,2,21,>, 
 <internal >, 
 <realidx 0,13,2,21,>, 
 <ndtsize 4,4,>, 
  local_first False
 next_id 140372191067488)

bk-maxpool2d140372191117952[1,-1] PI( <1,0,>,
 <otileshape 5,6,>,
 <padding >,
 <inpslidx 0,4,3,8,>, 
 <internal >, 
 <realidx 0,4,3,8,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191117808)

##############grad_in in maxp torch.Size([1, 1, 20, 14])
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 1, 22, 16])
weight shape torch.Size([1, 1, 3, 3])
grad_output shape torch.Size([1, 1, 20, 14])
final torch.Size([1, 1, 22, 16]) tensor([[[[ 0.0000e+00,  0.0000e+00,  0.0000e+00, -6.2398e-02, -6.6920e-02,
            4.2967e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00],
          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  2.7432e-02,  3.5748e-02,
            4.1457e-02,  8.7546e-02,  8.6925e-03,  6.9606e-02, -4.4691e-02,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00],
          [-6.2398e-02, -6.6920e-02,  4.2967e-02,  5.5269e-02,  1.1648e-01,
            9.5614e-02, -1.3182e-01,  2.4022e-02, -9.9580e-02, -2.5135e-02,
            4.2967e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00],
          [ 2.7432e-02,  3.5748e-02, -4.0173e-02,  1.0051e-01,  5.3495e-02,
           -2.1228e-01,  8.8725e-02, -1.2803e-02,  8.1311e-02,  6.8603e-03,
            4.1457e-02,  8.7546e-02,  8.6925e-03,  6.9606e-02, -4.4691e-02,
            0.0000e+00],
          [ 5.5269e-02, -7.0338e-03,  2.3429e-01,  2.0812e-01, -1.5331e-01,
            2.6705e-01, -1.4228e-01, -4.3993e-02,  2.8724e-03, -2.5750e-02,
           -6.8930e-03,  9.3480e-02,  1.7443e-01, -1.3375e-01,  4.1785e-02,
            0.0000e+00],
          [ 0.0000e+00, -4.3515e-02, -1.5010e-01, -1.9385e-01,  8.5559e-02,
           -9.9086e-02,  1.2332e-01, -2.5152e-02,  9.0735e-03,  2.5604e-02,
           -1.0032e-01, -5.2455e-02, -2.8248e-02,  2.1378e-01, -7.3579e-02,
            0.0000e+00],
          [ 0.0000e+00,  1.9131e-02, -1.8345e-01, -1.4961e-03, -1.9416e-01,
           -2.5459e-02,  1.3399e-01,  8.4928e-02, -1.8027e-02, -4.9052e-03,
           -6.7612e-04,  1.7547e-03,  1.0330e-01, -1.4662e-01,  6.9738e-02,
            0.0000e+00],
          [ 0.0000e+00,  1.5244e-01,  2.4322e-01,  5.6006e-02, -6.8193e-02,
            2.4179e-02, -6.3059e-02, -4.6766e-02,  2.3135e-01,  1.9176e-01,
           -1.2312e-01, -5.5385e-02, -1.6812e-01,  1.1351e-01,  2.3340e-02,
           -1.3905e-02],
          [ 0.0000e+00, -5.0073e-02, -1.2064e-01,  6.8518e-02,  1.8640e-01,
           -5.5797e-02, -6.9972e-02,  9.2017e-03, -7.7337e-02,  3.5995e-02,
            2.6358e-01, -2.0691e-01, -9.0215e-03, -3.1826e-02,  4.4217e-03,
            1.3001e-02],
          [ 9.5013e-02,  1.0147e-03, -1.6417e-01, -1.3644e-02, -3.4727e-02,
            4.5855e-02,  5.5822e-02,  6.2865e-02, -1.9873e-01, -4.0703e-02,
           -7.6528e-02,  1.5649e-01,  8.2454e-02, -5.0968e-02, -1.2675e-02,
            3.1376e-04],
          [-1.0417e-01, -1.2135e-01,  1.0414e-01, -5.9667e-02,  2.5298e-01,
            2.5961e-01, -1.9216e-01, -3.3581e-02,  3.7738e-02, -2.0824e-01,
           -7.6222e-02,  1.1312e-01,  6.9523e-02, -1.5458e-02, -3.6082e-04,
            0.0000e+00],
          [-5.6726e-02,  4.6458e-02, -1.9465e-02,  2.0626e-02, -1.1943e-01,
           -1.3838e-01,  2.3586e-01,  1.4846e-01, -9.0165e-02,  3.7642e-02,
            4.9052e-02, -1.5068e-01,  2.5729e-02,  1.2321e-01, -4.4691e-02,
            0.0000e+00],
          [ 5.5269e-02, -7.0338e-03,  5.6771e-03,  5.1782e-03, -2.1197e-01,
            2.7228e-02, -5.4395e-02, -7.5775e-02,  8.5155e-02,  7.5839e-02,
           -9.6516e-03, -7.3635e-02, -1.9331e-02, -3.5914e-02,  4.1785e-02,
            0.0000e+00],
          [ 1.8883e-02,  2.0252e-02, -3.6677e-02, -6.4838e-03,  1.9368e-01,
            1.9697e-01, -2.4362e-01,  1.2323e-01,  1.1822e-01, -1.1810e-01,
            9.3577e-02,  1.8037e-01, -1.5406e-01,  1.1563e-01,  1.1718e-01,
           -7.4587e-02],
          [-8.3017e-03, -1.0818e-02, -1.2189e-03,  2.4786e-01,  2.8625e-01,
           -1.6452e-01,  4.7488e-02, -4.7620e-02,  5.1840e-02,  2.1102e-01,
           -1.1515e-01, -1.0836e-01,  1.1049e-01, -2.5963e-02, -7.5961e-02,
            6.9738e-02],
          [-1.6726e-02,  1.7877e-01,  1.8974e-01, -2.2985e-01, -3.4888e-01,
            1.2032e-01,  8.9204e-02, -7.4287e-02, -5.1768e-02, -2.5025e-02,
           -5.5801e-02,  1.4111e-01,  1.2841e-01, -1.9426e-01,  2.5211e-02,
            1.6830e-03],
          [ 0.0000e+00, -1.9042e-02, -3.8334e-02, -1.4468e-01, -6.3269e-02,
            1.5408e-02, -3.0504e-02, -3.6475e-02, -7.1892e-02,  1.2839e-02,
            1.7697e-03, -5.5385e-02, -9.0060e-02,  8.3384e-02,  3.1376e-04,
            0.0000e+00],
          [ 0.0000e+00, -1.8223e-01,  1.2476e-01,  1.8895e-01, -9.5321e-02,
            6.7363e-02,  6.4568e-02, -3.0805e-02, -1.4637e-02,  9.5013e-02,
            1.0190e-01, -1.0965e-01,  6.3223e-02, -6.9334e-02,  1.5991e-02,
            0.0000e+00],
          [ 0.0000e+00, -5.1920e-02, -5.4251e-02, -7.8396e-02,  8.9123e-02,
           -2.9615e-02, -1.8023e-02,  4.0752e-02, -3.6082e-04, -4.1771e-02,
           -5.4433e-02,  3.1556e-02, -2.8383e-02,  5.6674e-02, -1.4951e-02,
            0.0000e+00],
          [ 6.7363e-02,  7.2245e-02, -1.6900e-01, -7.6181e-03, -2.2755e-02,
           -4.3676e-02,  7.5935e-03,  1.0467e-03,  0.0000e+00, -8.4158e-02,
            1.0710e-02, -5.8191e-02,  2.8163e-02, -1.5711e-03, -3.6082e-04,
            0.0000e+00],
          [-2.9615e-02, -3.8592e-02,  4.3369e-02,  1.0209e-02,  1.3304e-02,
           -1.4951e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00],
          [-5.9667e-02,  7.5935e-03,  1.0467e-03,  2.0570e-02, -2.6178e-03,
           -3.6082e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00]]]], device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 1, 6, 6])
grad_out size torch.Size([1, 1, 8, 8])
arg size torch.Size([1, 1, 3, 3])
maxpool2d140372191067248[0,-1] PI( <0,3,>,
 <otileshape 3,3,>,
 <padding >,
 <inpslidx 10,15,0,5,>, 
 <internal >, 
 <realidx 10,15,0,5,>, 
 <ndtsize 2,2,>, 
  local_first False
 next_id -99)

bk-maxpool2d140372191067248[3,-1] PI( <0,3,>,
 <otileshape 3,3,>,
 <padding >,
 <inpslidx 5,7,0,2,>, 
 <internal >, 
 <realidx 5,7,0,2,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191067488)

new_grad_out torch.Size([1, 1, 3, 3])
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
##############grad_in in maxp torch.Size([1, 1, 6, 6])
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 1, 8, 8])
local last ++ input tensor([[[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00],
          [-1.1412e-01,  1.3745e-01,  9.5295e-03,  2.8224e-01,  1.5147e-01,
            1.6005e-01, -1.6046e-04,  0.0000e+00],
          [ 3.3501e-01,  1.9982e-01,  1.4902e-01,  2.9622e-01,  2.0558e-01,
            4.0271e-01,  7.6900e-02,  0.0000e+00],
          [ 8.5486e-02,  3.1011e-01,  1.4548e-01,  1.9115e-01,  3.8594e-01,
            5.1286e-02,  4.0738e-01,  0.0000e+00],
          [ 2.4891e-01,  1.8386e-01,  4.2106e-01,  2.0667e-01,  3.6069e-01,
            3.7211e-01,  1.3048e-01,  0.0000e+00],
          [ 1.4913e-01,  2.5892e-01,  1.3322e-01,  2.1091e-01,  2.2755e-01,
            3.0140e-02,  1.8432e-01,  0.0000e+00],
          [ 2.8843e-01,  4.0194e-02,  2.9376e-01,  2.8749e-01, -1.2597e-02,
            2.2644e-01,  2.0458e-01,  0.0000e+00],
          [ 4.0373e-01,  4.3044e-01, -5.6921e-02,  2.5922e-01,  2.3516e-01,
            9.4626e-02,  3.2549e-01,  0.0000e+00]]]], device='cuda:0')
weight shape torch.Size([1, 1, 3, 3])
grad_output shape torch.Size([1, 1, 6, 6])
grad_input torch.Size([1, 1, 8, 8])
padding info :: [0, 1, 1, 0]
new grad_input torch.Size([1, 1, 7, 7])

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 1, 14, 14])
grad_out size torch.Size([1, 1, 7, 7])
arg size torch.Size([1, 1, 7, 7])
maxpool2d140372191117952[2,-1] PI( <0,3,>,
 <otileshape 7,7,>,
 <padding >,
 <inpslidx 18,31,0,13,>, 
 <internal >, 
 <realidx 18,31,0,13,>, 
 <ndtsize 4,4,>, 
  local_first False
 next_id 140372191067488)

bk-maxpool2d140372191117952[1,-1] PI( <0,3,>,
 <otileshape 5,5,>,
 <padding >,
 <inpslidx 11,15,0,4,>, 
 <internal >, 
 <realidx 11,15,0,4,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191117808)

##############grad_in in maxp torch.Size([1, 1, 14, 14])
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 1, 16, 16])
weight shape torch.Size([1, 1, 3, 3])
grad_output shape torch.Size([1, 1, 14, 14])
final torch.Size([1, 1, 16, 16]) tensor([[[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
            0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
            0.0000,  0.0000],
          [ 0.0000,  0.0000,  0.0000, -0.0624,  0.0147,  0.1305, -0.0537,
            0.0027,  0.0175,  0.0206,  0.1333,  0.1572, -0.1009,  0.0649,
            0.0696, -0.0447],
          [ 0.0000,  0.0000,  0.0189,  0.0477, -0.0131, -0.0869,  0.0515,
            0.1425,  0.1475, -0.1101, -0.0520, -0.0839,  0.2027,  0.0876,
           -0.1118,  0.0418],
          [ 0.0000,  0.0000, -0.0083,  0.0445, -0.0710,  0.0042,  0.0017,
           -0.0630, -0.0194,  0.1806, -0.1846,  0.2002,  0.1516, -0.2460,
            0.0771,  0.0010],
          [ 0.0000,  0.0000, -0.0167,  0.0021,  0.0020,  0.0022, -0.0024,
           -0.1275, -0.0189,  0.1811,  0.2925, -0.2355, -0.2012,  0.1506,
            0.0233, -0.0139],
          [ 0.0000, -0.0624, -0.0669,  0.2196,  0.3439,  0.0400,  0.0263,
            0.1399, -0.1607, -0.0898, -0.0247,  0.0922, -0.0501, -0.0060,
           -0.0116,  0.0130],
          [ 0.0189,  0.0477,  0.0227, -0.0592, -0.1048, -0.0132,  0.0400,
           -0.0747,  0.0840, -0.1990,  0.0475,  0.0169,  0.0198, -0.0411,
           -0.0226,  0.0163],
          [-0.0083,  0.0445,  0.0051, -0.1832, -0.1475,  0.1336,  0.1174,
            0.1221,  0.0361,  0.1466, -0.2148, -0.0270,  0.0450,  0.0102,
            0.0133, -0.0150],
          [-0.0167,  0.0347,  0.0353, -0.0744,  0.0066,  0.0280, -0.0441,
           -0.0781,  0.0358, -0.0349,  0.0749, -0.0223,  0.0010,  0.1022,
            0.0849, -0.0566],
          [ 0.0000, -0.0143, -0.0187,  0.1700,  0.1598, -0.1966, -0.0995,
            0.1198, -0.1591, -0.0037, -0.0227, -0.0344, -0.0669,  0.0071,
           -0.0468,  0.0526],
          [ 0.0000, -0.0289,  0.0623, -0.0021, -0.1257,  0.0423, -0.0189,
           -0.0326,  0.0145, -0.0634,  0.0209,  0.0470,  0.1146, -0.0626,
           -0.0312,  0.0013],
          [ 0.0000,  0.0189, -0.0055, -0.1786,  0.1930,  0.1508, -0.1472,
            0.0740,  0.0834, -0.0331, -0.0150,  0.0470, -0.0436, -0.0224,
            0.0377,  0.0000],
          [ 0.0950,  0.0936, -0.1282,  0.0188, -0.0599, -0.0793,  0.0891,
           -0.0296, -0.0180,  0.0408,  0.0947,  0.0852, -0.1152,  0.0069,
            0.0009,  0.0000],
          [-0.0418, -0.0712,  0.1307,  0.0725, -0.1690, -0.0076, -0.0228,
           -0.0437,  0.0076,  0.0010, -0.0418, -0.0544,  0.1285,  0.0722,
           -0.0464,  0.0000],
          [-0.0842,  0.0107, -0.0281, -0.0386,  0.0434,  0.0102,  0.0133,
           -0.0150,  0.0000,  0.0000, -0.0842,  0.0107, -0.0281, -0.0386,
            0.0434,  0.0000],
          [ 0.0000,  0.0000, -0.0597,  0.0076,  0.0010,  0.0206, -0.0026,
           -0.0004,  0.0000,  0.0000,  0.0000,  0.0000, -0.0597,  0.0076,
            0.0010,  0.0000]]]], device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 1, 6, 8])
grad_out size torch.Size([1, 1, 8, 8])
arg size torch.Size([1, 1, 3, 4])
maxpool2d140372191067248[0,-1] PI( <0,2,>,
 <otileshape 4,3,>,
 <padding >,
 <inpslidx 6,13,0,5,>, 
 <internal >, 
 <realidx 6,13,0,5,>, 
 <ndtsize 2,2,>, 
  local_first False
 next_id -99)

bk-maxpool2d140372191067248[3,-1] PI( <0,2,>,
 <otileshape 4,3,>,
 <padding >,
 <inpslidx 3,6,0,2,>, 
 <internal >, 
 <realidx 3,6,0,2,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191067488)

new_grad_out torch.Size([1, 1, 3, 4])
##############grad_in in maxp torch.Size([1, 1, 6, 8])
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 1, 8, 10])
local last ++ input tensor([[[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
            0.0000,  0.0000,  0.0000],
          [ 0.1524,  0.1359,  0.2622,  0.3323, -0.1141,  0.1375,  0.0095,
            0.2822,  0.1515,  0.1601],
          [ 0.2603,  0.1670,  0.0507,  0.1273,  0.3350,  0.1998,  0.1490,
            0.2962,  0.2056,  0.4027],
          [ 0.0426,  0.2653,  0.4077,  0.3255,  0.0855,  0.3101,  0.1455,
            0.1912,  0.3859,  0.0513],
          [ 0.3053,  0.3084,  0.3288,  0.1344,  0.2489,  0.1839,  0.4211,
            0.2067,  0.3607,  0.3721],
          [ 0.2874,  0.3051,  0.3212,  0.3645,  0.1491,  0.2589,  0.1332,
            0.2109,  0.2276,  0.0301],
          [ 0.3443,  0.4470,  0.2105,  0.2988,  0.2884,  0.0402,  0.2938,
            0.2875, -0.0126,  0.2264],
          [ 0.1104,  0.2068,  0.2279,  0.2150,  0.4037,  0.4304, -0.0569,
            0.2592,  0.2352,  0.0946]]]], device='cuda:0')
weight shape torch.Size([1, 1, 3, 3])
grad_output shape torch.Size([1, 1, 6, 8])
grad_input torch.Size([1, 1, 8, 10])
padding info :: [0, 0, 1, 0]
new grad_input torch.Size([1, 1, 7, 10])

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 1, 14, 20])
grad_out size torch.Size([1, 1, 7, 10])
arg size torch.Size([1, 1, 7, 10])
maxpool2d140372191117952[2,-1] PI( <0,2,>,
 <otileshape 10,7,>,
 <padding >,
 <inpslidx 10,29,0,13,>, 
 <internal >, 
 <realidx 10,29,0,13,>, 
 <ndtsize 4,4,>, 
  local_first False
 next_id 140372191067488)

bk-maxpool2d140372191117952[1,-1] PI( <0,2,>,
 <otileshape 6,5,>,
 <padding >,
 <inpslidx 7,12,0,4,>, 
 <internal >, 
 <realidx 7,12,0,4,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191117808)

##############grad_in in maxp torch.Size([1, 1, 14, 20])
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 1, 16, 22])
weight shape torch.Size([1, 1, 3, 3])
grad_output shape torch.Size([1, 1, 14, 20])
final torch.Size([1, 1, 16, 22]) [torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
tensor([[[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
            0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
            0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
            0.0000],
          [ 0.0000,  0.0000, -0.0624, -0.0669,  0.0622,  0.0206,  0.1333,
            0.1572, -0.0360,  0.0696, -0.0447, -0.0624,  0.0147,  0.1305,
           -0.0537,  0.0027,  0.0799,  0.0875,  0.0087,  0.0696, -0.0447,
            0.0000],
          [ 0.0000,  0.0000,  0.0274,  0.0357, -0.0486,  0.0041,  0.1478,
            0.1026, -0.0607, -0.0372,  0.0607,  0.0477, -0.0131, -0.0869,
            0.0515,  0.1425,  0.1201, -0.1459,  0.0240, -0.0372,  0.0418,
            0.0000],
          [ 0.0000,  0.0000,  0.0742,  0.0132, -0.0310, -0.0045, -0.2189,
           -0.0790,  0.1714,  0.1235, -0.0819,  0.0445, -0.0710,  0.0042,
            0.0017, -0.0630, -0.0312,  0.2343, -0.1413,  0.0507,  0.0476,
           -0.0299],
          [ 0.0000,  0.0000, -0.0083, -0.0108,  0.2583,  0.2506, -0.3305,
            0.0209, -0.0448, -0.0621,  0.0530,  0.0021,  0.0020,  0.0022,
           -0.0024, -0.1275, -0.0381,  0.0423,  0.1983, -0.0954, -0.0249,
            0.0280],
          [-0.0624, -0.0669,  0.2029,  0.1916, -0.2296, -0.1410,  0.2612,
            0.1102, -0.1667, -0.0300, -0.0436,  0.2057,  0.3439,  0.0400,
            0.0263,  0.1399, -0.1992, -0.0348, -0.0848,  0.0080,  0.0209,
            0.0007],
          [ 0.0274, -0.0078, -0.1645, -0.0712,  0.0990,  0.2458, -0.1764,
           -0.0535,  0.1400,  0.1014, -0.0290,  0.0187, -0.0352, -0.0579,
            0.0400, -0.0747,  0.0840, -0.0981,  0.0227,  0.0150, -0.0150,
            0.0000],
          [ 0.0553,  0.0121,  0.0077,  0.1423, -0.1832, -0.1165,  0.0377,
            0.0087, -0.0292, -0.0069,  0.0450, -0.2114, -0.1847,  0.1753,
            0.1174,  0.1221, -0.0589,  0.0447, -0.0081, -0.0026, -0.0004,
            0.0000],
          [ 0.0000,  0.1524,  0.0556, -0.1595, -0.0696,  0.0446, -0.0151,
            0.0006, -0.0684,  0.0999,  0.0990, -0.1722,  0.0139,  0.0291,
           -0.0441, -0.0781,  0.0776,  0.0195,  0.0734, -0.0299,  0.0000,
            0.0000],
          [ 0.0000, -0.0501, -0.1895,  0.2151,  0.1284, -0.0983,  0.0319,
            0.0203, -0.0130, -0.0401, -0.0523,  0.2511,  0.2064, -0.2265,
           -0.0995,  0.1198, -0.0750, -0.0144, -0.0242,  0.0280,  0.0000,
            0.0000],
          [ 0.0000, -0.1009,  0.0802,  0.0186, -0.1364,  0.0602, -0.0329,
            0.1002,  0.1141, -0.1462,  0.0457, -0.0452, -0.1346,  0.0702,
           -0.0189, -0.0326,  0.0145, -0.0634,  0.0209,  0.0007,  0.0000,
            0.0000],
          [ 0.0000,  0.0950,  0.0723, -0.2156,  0.0576,  0.0122, -0.0034,
           -0.0546, -0.0541,  0.1474,  0.0769, -0.2501,  0.1829,  0.1515,
           -0.1472,  0.0740,  0.0834, -0.0331, -0.0150,  0.0000,  0.0000,
            0.0000],
          [ 0.0000, -0.0418, -0.1141,  0.0688,  0.0010,  0.0206, -0.0026,
           -0.0845,  0.1057,  0.0655, -0.1462,  0.0595, -0.0603, -0.0793,
            0.0891, -0.0296, -0.0180,  0.0408, -0.0004,  0.0000,  0.0000,
            0.0000],
          [ 0.0000, -0.0842,  0.0107,  0.0015,  0.0000,  0.0000,  0.0000,
            0.0000, -0.0418, -0.1308,  0.1383,  0.0736, -0.1690, -0.0076,
           -0.0228, -0.0437,  0.0076,  0.0010,  0.0000,  0.0000,  0.0000,
            0.0000],
          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
            0.0000, -0.0842,  0.0107, -0.0281, -0.0386,  0.0434,  0.0102,
            0.0133, -0.0150,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
            0.0000],
          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
            0.0000,  0.0000,  0.0000, -0.0597,  0.0076,  0.0010,  0.0206,
           -0.0026, -0.0004,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
            0.0000]]]], device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 1, 6, 8])
grad_out size torch.Size([1, 1, 8, 8])
arg size torch.Size([1, 1, 3, 4])
maxpool2d140372191067248[0,-1] PI( <0,1,>,
 <otileshape 4,3,>,
 <padding >,
 <inpslidx 2,9,0,5,>, 
 <internal >, 
 <realidx 2,9,0,5,>, 
 <ndtsize 2,2,>, 
  local_first False
 next_id -99)

bk-maxpool2d140372191067248[3,-1] PI( <0,1,>,
 <otileshape 4,3,>,
 <padding >,
 <inpslidx 1,4,0,2,>, 
 <internal >, 
 <realidx 1,4,0,2,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191067488)

new_grad_out torch.Size([1, 1, 3, 4])
##############grad_in in maxp torch.Size([1, 1, 6, 8])
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 1, 8, 10])
local last ++ input tensor([[[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
            0.0000,  0.0000,  0.0000],
          [-0.0472,  0.2002,  0.1810,  0.1768,  0.1524,  0.1359,  0.2622,
            0.3323, -0.1141,  0.1375],
          [ 0.2056,  0.3195,  0.1760, -0.0424,  0.2603,  0.1670,  0.0507,
            0.1273,  0.3350,  0.1998],
          [ 0.4188,  0.1703,  0.2766,  0.4127,  0.0426,  0.2653,  0.4077,
            0.3255,  0.0855,  0.3101],
          [ 0.2722,  0.2960,  0.2889,  0.0363,  0.3053,  0.3084,  0.3288,
            0.1344,  0.2489,  0.1839],
          [ 0.3017, -0.0242,  0.0373,  0.4412,  0.2874,  0.3051,  0.3212,
            0.3645,  0.1491,  0.2589],
          [ 0.1378,  0.1910,  0.1472,  0.3417,  0.3443,  0.4470,  0.2105,
            0.2988,  0.2884,  0.0402],
          [ 0.2403,  0.2121,  0.1864,  0.0631,  0.1104,  0.2068,  0.2279,
            0.2150,  0.4037,  0.4304]]]], device='cuda:0')
weight shape torch.Size([1, 1, 3, 3])
grad_output shape torch.Size([1, 1, 6, 8])
grad_input torch.Size([1, 1, 8, 10])
padding info :: [0, 0, 1, 0]
new grad_input torch.Size([1, 1, 7, 10])

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 1, 14, 20])
grad_out size torch.Size([1, 1, 7, 10])
arg size torch.Size([1, 1, 7, 10])
maxpool2d140372191117952[2,-1] PI( <0,1,>,
 <otileshape 10,7,>,
 <padding >,
 <inpslidx 2,21,0,13,>, 
 <internal >, 
 <realidx 2,21,0,13,>, 
 <ndtsize 4,4,>, 
  local_first False
 next_id 140372191067488)

bk-maxpool2d140372191117952[1,-1] PI( <0,1,>,
 <otileshape 6,5,>,
 <padding >,
 <inpslidx 3,8,0,4,>, 
 <internal >, 
 <realidx 3,8,0,4,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191117808)

##############grad_in in maxp torch.Size([1, 1, 14, 20])
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 1, 16, 22])
weight shape torch.Size([1, 1, 3, 3])
grad_output shape torch.Size([1, 1, 14, 20])
final torch.Size([1, 1, 16, 22]) [torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
tensor([[[[-6.2398e-02, -6.6920e-02,  4.2967e-02,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00],
          [ 2.7432e-02,  3.5748e-02,  4.1457e-02,  8.7546e-02, -5.3705e-02,
            2.6856e-03, -1.7243e-03,  8.1630e-02,  1.5245e-01,  1.3397e-02,
           -1.0709e-01, -6.6920e-02,  6.2198e-02,  2.0626e-02,  1.3329e-01,
            1.5715e-01, -3.5998e-02,  6.9606e-02, -4.4691e-02,  0.0000e+00,
            0.0000e+00,  0.0000e+00],
          [ 5.5269e-02, -5.0548e-02, -8.3525e-02, -1.6802e-02,  5.1454e-02,
           -1.4346e-03,  6.0228e-02,  2.6977e-02, -1.1566e-01,  1.5372e-02,
            6.9217e-02,  3.5748e-02, -4.8628e-02,  4.0836e-03,  1.4782e-01,
            1.0263e-01, -6.0661e-02, -3.7183e-02,  4.1785e-02,  0.0000e+00,
            0.0000e+00,  0.0000e+00],
          [ 0.0000e+00,  1.9131e-02, -4.7375e-02,  1.2143e-01,  1.4946e-01,
            3.0912e-02,  1.1069e-01, -1.9348e-01, -1.0548e-02,  5.2001e-02,
            1.2172e-01, -1.6678e-02, -3.1007e-02, -4.4713e-03, -2.1889e-01,
           -7.8980e-02,  1.7135e-01,  1.2349e-01, -7.3579e-02,  0.0000e+00,
            0.0000e+00,  0.0000e+00],
          [ 0.0000e+00,  3.8543e-02,  1.2107e-01,  7.2777e-02, -1.6710e-01,
            3.4371e-02, -1.1983e-01,  9.3827e-02, -2.5079e-03, -1.9087e-02,
           -3.3175e-02,  1.7134e-02,  2.5832e-01,  2.5063e-01, -3.3048e-01,
            2.0938e-02, -4.4767e-02, -6.2056e-02,  6.9738e-02,  0.0000e+00,
            0.0000e+00,  0.0000e+00],
          [ 0.0000e+00,  1.1390e-01,  6.6767e-02, -2.7483e-01,  9.6917e-02,
            4.7155e-03,  1.3571e-01, -8.0198e-02, -7.9435e-04, -4.2736e-02,
            2.3246e-01,  2.6185e-01, -2.7425e-01, -1.4103e-01,  2.6124e-01,
            1.1020e-01, -1.6670e-01,  3.2404e-02,  2.3340e-02, -1.3905e-02,
            0.0000e+00,  0.0000e+00],
          [ 9.5013e-02,  5.1827e-02, -1.7490e-01,  1.5977e-01, -6.7650e-02,
           -7.5554e-02, -9.7923e-02,  2.8063e-02,  3.1795e-02,  9.9017e-02,
           -5.0183e-03, -2.0499e-01,  1.4080e-01,  2.4582e-01, -1.7640e-01,
           -5.3545e-02,  1.2114e-01,  5.3763e-02, -5.1788e-02,  7.7903e-02,
            6.9606e-02, -4.4691e-02],
          [-4.1771e-02, -1.5532e-01,  4.4395e-02, -3.6823e-02,  5.3579e-02,
           -8.8741e-02,  1.7166e-02,  2.6720e-02, -2.4481e-02, -4.2976e-02,
           -2.0866e-02,  3.5609e-01, -2.5680e-01, -1.1649e-01,  3.7709e-02,
            8.7389e-03, -2.0885e-02, -5.1348e-02,  3.9880e-02, -2.8219e-02,
           -3.7183e-02,  4.1785e-02],
          [-8.4158e-02,  1.0710e-02, -1.2059e-01, -5.9327e-02,  1.4621e-01,
            8.4928e-02, -1.8027e-02, -4.9052e-03, -6.7612e-04,  1.5419e-01,
            1.5889e-01, -3.0608e-01,  1.2210e-04,  4.4578e-02, -1.5144e-02,
            5.5965e-04, -5.1657e-02,  6.5200e-02,  6.3772e-02, -9.7850e-02,
            7.3161e-03,  1.0084e-03],
          [ 0.0000e+00,  0.0000e+00,  2.7432e-02,  3.5748e-02, -7.6060e-02,
           -4.6766e-02,  2.3135e-01,  1.9176e-01, -1.2312e-01, -1.0546e-01,
           -3.5759e-01,  3.2863e-01,  1.5175e-01, -1.1222e-01,  3.1884e-02,
            2.0252e-02, -1.3003e-02, -2.5770e-02, -3.3581e-02,  8.1155e-02,
            4.6563e-02, -2.9896e-02],
          [ 0.0000e+00,  0.0000e+00,  7.4153e-02,  1.3218e-02, -8.6277e-02,
            9.2017e-03, -7.7337e-02,  3.5995e-02,  2.6358e-01, -3.0779e-01,
            7.1180e-02, -1.3196e-02, -1.3202e-01,  7.3163e-02, -3.2894e-02,
            1.0019e-01,  1.1406e-01, -1.1734e-01, -1.6615e-02, -4.3082e-02,
           -8.8825e-03,  2.7952e-02],
          [ 0.0000e+00,  0.0000e+00, -8.3017e-03, -1.0818e-02,  7.0774e-02,
            6.2865e-02, -1.9873e-01, -4.0703e-02, -7.6528e-02,  2.5150e-01,
            1.5474e-01, -2.6657e-01,  4.4896e-02,  1.2481e-02, -3.4218e-03,
           -5.4593e-02, -5.4140e-02,  1.2853e-01,  8.2454e-02, -7.1537e-02,
           -1.0057e-02,  6.7459e-04],
          [ 0.0000e+00,  0.0000e+00,  7.8287e-02,  1.0403e-01, -9.0901e-02,
           -3.3581e-02,  3.7738e-02, -1.4584e-01, -9.3013e-03, -5.3244e-02,
           -1.3212e-01,  1.0952e-01,  6.8584e-04,  2.0570e-02, -2.6178e-03,
           -8.4519e-02,  1.0710e-02, -2.8139e-02, -1.8023e-02,  4.0752e-02,
           -3.6082e-04,  0.0000e+00],
          [ 0.0000e+00,  0.0000e+00, -4.1771e-02, -5.4433e-02,  7.6614e-02,
            7.8853e-02, -4.5475e-02,  1.0209e-02,  1.3304e-02, -1.5878e-01,
            1.8304e-02,  2.5229e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00, -5.9667e-02,  7.5935e-03,  1.0467e-03,
            0.0000e+00,  0.0000e+00],
          [ 0.0000e+00,  0.0000e+00, -8.4158e-02,  1.0710e-02, -2.8139e-02,
           -3.8592e-02,  4.3369e-02,  2.0570e-02, -2.6178e-03, -3.6082e-04,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00],
          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -5.9667e-02,
            7.5935e-03,  1.0467e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00]]]], device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 1, 6, 6])
grad_out size torch.Size([1, 1, 8, 8])
arg size torch.Size([1, 1, 3, 3])
maxpool2d140372191067248[0,-1] PI( <0,0,>,
 <otileshape 3,3,>,
 <padding >,
 <inpslidx 0,5,0,5,>, 
 <internal >, 
 <realidx 0,5,0,5,>, 
 <ndtsize 2,2,>, 
  local_first False
 next_id -99)

bk-maxpool2d140372191067248[3,-1] PI( <0,0,>,
 <otileshape 3,3,>,
 <padding >,
 <inpslidx 0,2,0,2,>, 
 <internal >, 
 <realidx 0,2,0,2,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191067488)

new_grad_out torch.Size([1, 1, 3, 3])
##############grad_in in maxp torch.Size([1, 1, 6, 6])
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 1, 8, 8])
local last ++ input tensor([[[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
            0.0000],
          [ 0.0000,  0.0680, -0.0472,  0.2002,  0.1810,  0.1768,  0.1524,
            0.1359],
          [ 0.0000,  0.1782,  0.2056,  0.3195,  0.1760, -0.0424,  0.2603,
            0.1670],
          [ 0.0000,  0.2554,  0.4188,  0.1703,  0.2766,  0.4127,  0.0426,
            0.2653],
          [ 0.0000,  0.2028,  0.2722,  0.2960,  0.2889,  0.0363,  0.3053,
            0.3084],
          [ 0.0000,  0.1777,  0.3017, -0.0242,  0.0373,  0.4412,  0.2874,
            0.3051],
          [ 0.0000,  0.1687,  0.1378,  0.1910,  0.1472,  0.3417,  0.3443,
            0.4470],
          [ 0.0000,  0.1897,  0.2403,  0.2121,  0.1864,  0.0631,  0.1104,
            0.2068]]]], device='cuda:0')
weight shape torch.Size([1, 1, 3, 3])
grad_output shape torch.Size([1, 1, 6, 6])
grad_input torch.Size([1, 1, 8, 8])
padding info :: [1, 0, 1, 0]
new grad_input torch.Size([1, 1, 7, 7])

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 1, 14, 14])
grad_out size torch.Size([1, 1, 7, 7])
arg size torch.Size([1, 1, 7, 7])
maxpool2d140372191117952[2,-1] PI( <0,0,>,
 <otileshape 7,7,>,
 <padding >,
 <inpslidx 0,13,0,13,>, 
 <internal >, 
 <realidx 0,13,0,13,>, 
 <ndtsize 4,4,>, 
  local_first False
 next_id 140372191067488)

bk-maxpool2d140372191117952[1,-1] PI( <0,0,>,
 <otileshape 5,5,>,
 <padding >,
 <inpslidx 0,4,0,4,>, 
 <internal >, 
 <realidx 0,4,0,4,>, 
 <ndtsize >, 
  local_first False
 next_id 140372191117808)

##############grad_in in maxp torch.Size([1, 1, 14, 14])
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 1, 16, 16])
weight shape torch.Size([1, 1, 3, 3])
grad_output shape torch.Size([1, 1, 14, 14])
final torch.Size([1, 1, 16, 16]) tensor([[[[ 0.0000e+00,  0.0000e+00,  1.9232e-02,  2.0626e-02, -1.3243e-02,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00],
          [-6.2398e-02, -6.6920e-02,  3.4512e-02, -1.1018e-02,  1.5891e-01,
            1.5715e-01, -9.8396e-02,  2.6856e-03, -1.7243e-03,  8.1630e-02,
            1.5245e-01,  1.3397e-02, -4.4691e-02,  0.0000e+00,  0.0000e+00,
            0.0000e+00],
          [ 2.7432e-02,  5.4631e-02, -3.6956e-02,  4.2666e-03, -4.7925e-02,
           -9.4347e-02,  9.3239e-02, -1.4346e-03,  6.0228e-02,  2.6977e-02,
           -1.1566e-01,  1.5372e-02,  4.1785e-02,  0.0000e+00,  0.0000e+00,
            0.0000e+00],
          [ 5.5269e-02, -1.5336e-02, -1.1788e-02,  5.5183e-03, -1.3844e-01,
            2.0990e-01,  1.9703e-01,  1.0157e-03,  1.1069e-01, -1.9348e-01,
           -1.0548e-02,  5.2001e-02,  4.7572e-02, -2.9896e-02,  0.0000e+00,
            0.0000e+00],
          [ 3.2615e-02,  1.8253e-02, -2.0330e-02, -1.3083e-02,  1.6936e-01,
            9.9300e-02, -2.2067e-01,  6.2323e-02, -1.1983e-01,  9.3827e-02,
           -2.5079e-03, -1.9087e-02, -2.4873e-02,  2.7952e-02,  0.0000e+00,
            0.0000e+00],
          [-1.4339e-02, -1.8685e-02,  2.0998e-02,  2.6289e-01,  2.0823e-01,
           -4.3975e-01,  1.2865e-01,  5.3901e-03,  1.3571e-01, -8.0198e-02,
            6.1603e-02,  2.4184e-02,  2.9577e-02,  7.0281e-02, -4.4691e-02,
            0.0000e+00],
          [-2.8889e-02,  3.6765e-03,  2.3577e-01,  1.3674e-01, -2.8543e-01,
            3.7657e-01, -1.4159e-01, -7.5554e-02, -9.7923e-02,  2.8063e-02,
            4.3631e-03,  1.0678e-01,  1.5948e-01, -1.3375e-01,  4.1785e-02,
            0.0000e+00],
          [ 0.0000e+00, -4.3515e-02, -1.5010e-01, -3.3767e-01,  1.0386e-01,
           -9.6563e-02,  1.2332e-01, -8.8741e-02,  1.7166e-02,  2.6720e-02,
           -7.9750e-02, -5.5073e-02, -2.8609e-02,  2.1378e-01, -7.3579e-02,
            0.0000e+00],
          [ 0.0000e+00,  1.9131e-02, -1.8345e-01, -1.4961e-03, -1.9416e-01,
           -2.5459e-02,  1.3399e-01,  8.4928e-02, -1.8027e-02, -4.9052e-03,
           -6.7612e-04,  1.7547e-03,  1.0330e-01, -1.4662e-01,  6.9738e-02,
            0.0000e+00],
          [ 0.0000e+00,  1.5244e-01,  2.4322e-01,  5.6006e-02, -6.8193e-02,
            2.4179e-02, -6.3059e-02, -4.6766e-02,  2.3135e-01,  1.9176e-01,
           -1.2312e-01, -5.5385e-02, -1.6812e-01,  1.1351e-01,  2.3340e-02,
           -1.3905e-02],
          [ 0.0000e+00, -5.0073e-02, -1.2064e-01,  6.8518e-02,  1.8640e-01,
           -5.5797e-02, -6.9972e-02,  9.2017e-03, -7.7337e-02,  3.5995e-02,
            2.6358e-01, -2.0691e-01, -9.0215e-03, -3.1826e-02,  4.4217e-03,
            1.3001e-02],
          [ 9.5013e-02,  1.0147e-03, -1.6417e-01, -1.3644e-02, -3.4727e-02,
            4.5855e-02,  5.5822e-02,  6.2865e-02, -1.9873e-01, -4.0703e-02,
           -7.6528e-02,  1.5649e-01,  8.2454e-02, -5.0968e-02, -1.2675e-02,
            3.1376e-04],
          [-4.1771e-02, -5.4433e-02,  6.1171e-02, -5.9667e-02,  1.0645e-01,
            1.0246e-01, -9.1262e-02, -3.3581e-02,  3.7738e-02, -1.4584e-01,
           -9.3013e-03, -1.1473e-02, -1.8023e-02,  4.0752e-02, -3.6082e-04,
            0.0000e+00],
          [-8.4158e-02,  1.0710e-02,  1.4763e-03,  0.0000e+00, -4.1771e-02,
           -5.4433e-02,  7.6614e-02,  7.8853e-02, -4.5475e-02,  1.0209e-02,
            1.3304e-02, -7.4618e-02,  7.5935e-03,  1.0467e-03,  0.0000e+00,
            0.0000e+00],
          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -8.4158e-02,
            1.0710e-02, -2.8139e-02, -3.8592e-02,  4.3369e-02,  2.0570e-02,
           -2.6178e-03, -3.6082e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00],
          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00, -5.9667e-02,  7.5935e-03,  1.0467e-03,  0.0000e+00,
            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
            0.0000e+00]]]], device='cuda:0')
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
