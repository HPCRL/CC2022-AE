[torch/csrc/autograd/engine.cpp] call_function SumBackward0
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function MaxPool2DWithIndicesBackward
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function CudnnConvolutionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function CudnnConvolutionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function CudnnConvolutionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function MaxPool2DWithIndicesBackward
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function CudnnConvolutionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function CudnnConvolutionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function CudnnConvolutionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function MaxPool2DWithIndicesBackward
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function CudnnConvolutionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function CudnnConvolutionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function CudnnConvolutionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function MaxPool2DWithIndicesBackward
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function CudnnConvolutionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autogr
&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&

Inside Conv2d forward
input size :  torch.Size([2, 3, 320, 320])
output size :  torch.Size([3, 320, 320])
Inside Conv2d forward
input size :  torch.Size([2, 3, 320, 320])
output size :  torch.Size([3, 320, 320])
Inside MaxPool2d forward
input size :  torch.Size([2, 3, 320, 320])
output size :  torch.Size([3, 160, 160])
Inside Conv2d forward
input size :  torch.Size([2, 3, 160, 160])
output size :  torch.Size([3, 160, 160])
Inside Conv2d forward
input size :  torch.Size([2, 3, 160, 160])
output size :  torch.Size([3, 160, 160])
Inside MaxPool2d forward
input size :  torch.Size([2, 3, 160, 160])
output size :  torch.Size([3, 80, 80])
Inside Conv2d forward
input size :  torch.Size([2, 3, 80, 80])
output size :  torch.Size([3, 80, 80])
Inside Conv2d forward
input size :  torch.Size([2, 3, 80, 80])
output size :  torch.Size([3, 80, 80])
Inside Conv2d forward
input size :  torch.Size([2, 3, 80, 80])
output size :  torch.Size([3, 80, 80])
Inside MaxPool2d forward
input size :  torch.Size([2, 3, 80, 80])
output size :  torch.Size([3, 40, 40])
Inside Conv2d forward
input size :  torch.Size([2, 3, 40, 40])
output size :  torch.Size([3, 40, 40])
Inside Conv2d forward
input size :  torch.Size([2, 3, 40, 40])
output size :  torch.Size([3, 40, 40])
Inside Conv2d forward
input size :  torch.Size([2, 3, 40, 40])
output size :  torch.Size([3, 40, 40])
Inside MaxPool2d forward
input size :  torch.Size([2, 3, 40, 40])
output size :  torch.Size([3, 20, 20])
Inside Conv2d forward
input size :  torch.Size([2, 3, 20, 20])
output size :  torch.Size([3, 20, 20])
Inside Conv2d forward
input size :  torch.Size([2, 3, 20, 20])
output size :  torch.Size([3, 20, 20])
Inside Conv2d forward
input size :  torch.Size([2, 3, 20, 20])
output size :  torch.Size([3, 20, 20])
Inside MaxPool2d forward
input size :  torch.Size([2, 3, 20, 20])
output size :  torch.Size([3, 10, 10])
done ref
Inside MaxPool2d backward
grad_output size :  torch.Size([2, 3, 10, 10])
grad_input size :  torch.Size([2, 3, 20, 20])
Inside Conv2d backward
grad_output size :  torch.Size([2, 3, 20, 20])
grad_input size :  torch.Size([2, 3, 20, 20])
Inside Conv2d backward
grad_output size :  torch.Size([2, 3, 20, 20])
grad_input size :  torch.Size([2, 3, 20, 20])
Inside Conv2d backward
grad_output size :  torch.Size([2, 3, 20, 20])
grad_input size :  torch.Size([2, 3, 20, 20])
Inside MaxPool2d backward
grad_output size :  torch.Size([2, 3, 20, 20])
grad_input size :  torch.Size([2, 3, 40, 40])
Inside Conv2d backward
grad_output size :  torch.Size([2, 3, 40, 40])
grad_input size :  torch.Size([2, 3, 40, 40])
Inside Conv2d backward
grad_output size :  torch.Size([2, 3, 40, 40])
grad_input size :  torch.Size([2, 3, 40, 40])
Inside Conv2d backward
grad_output size :  torch.Size([2, 3, 40, 40])
grad_input size :  torch.Size([2, 3, 40, 40])
Inside MaxPool2d backward
grad_output size :  torch.Size([2, 3, 40, 40])
grad_input size :  torch.Size([2, 3, 80, 80])
Inside Conv2d backward
grad_output size :  torch.Size([2, 3, 80, 80])
grad_input size :  torch.Size([2, 3, 80, 80])
Inside Conv2d backward
grad_output size :  torch.Size([2, 3, 80, 80])
grad_input size :  torch.Size([2, 3, 80, 80])
Inside Conv2d backward
grad_output size :  torch.Size([2, 3, 80, 80])
grad_input size :  torch.Size([2, 3, 80, 80])
Inside MaxPool2d backward
grad_output size :  torch.Size([2, 3, 80, 80])
grad_input size :  torch.Size([2, 3, 160, 160])
Inside Conv2d backward
grad_output size :  torch.Size([2, 3, 160, 160])
grad_input size :  torch.Size([2, 3, 160, 160])
Inside Conv2d backward
grad_output size :  torch.Size([2, 3, 160, 160])
grad_input size :  torch.Size([2, 3, 160, 160])
Inside MaxPool2d backward
grad_output size :  torch.Size([2, 3, 160, 160])
grad_input size :  torch.Size([2, 3, 320, 320])
Inside Conv2d backward
grad_output size :  torch.Size([2, 3, 320, 320])
grad_input size :  torch.Size([2, 3, 320, 320])
Inside Conv2d backward
grad_output size :  torch.Size([2, 3, 320, 320])
grad_input size :  torch.Size([2, 3, 320, 320])
done ref bkw

&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&

Input 2x3x320x320
L-->R current op 139824594865504
after conv2d 2x3x320x320
L-->R current op 139824594865648
after conv2d 2x3x320x320
L-->R current op 139822795187344
after maxpool2d 2x3x160x160
L-->R current op 139822786460256
after conv2d 2x3x160x160
L-->R current op 139822785814384
after conv2d 2x3x160x160
L-->R current op 139824594782192
after maxpool2d 2x3x80x80
L-->R current op 139824594781952
after conv2d 2x3x80x80
L-->R current op 139824594781520
after conv2d 2x3x80x80
L-->R current op 139824594781616
after conv2d 2x3x80x80
L-->R current op 139824594781280
after maxpool2d 2x3x40x40
L-->R current op 139822785897520
after conv2d 2x3x40x40
L-->R current op 139822785897184
after conv2d 2x3x40x40
L-->R current op 139822785897808
after conv2d 2x3x40x40
L-->R current op 139822785897856
after maxpool2d 2x3x20x20
L-->R current op 139822785897904
after conv2d 2x3x20x20
L-->R current op 139822785897952
after conv2d 2x3x20x20
L-->R current op 139822785898000
after conv2d 2x3x20x20
L-->R current op 139822785898048
after maxpool2d 2x3x10x10
!!!!!!! 10 10
torch.Size([2, 3, 320, 320])
torch.Size([2, 3, 320, 320])
torch.Size([2, 3, 320, 320])
torch.Size([2, 3, 160, 160])
torch.Size([2, 3, 160, 160])
torch.Size([2, 3, 160, 160])
torch.Size([2, 3, 80, 80])
torch.Size([2, 3, 80, 80])
torch.Size([2, 3, 80, 80])
torch.Size([2, 3, 80, 80])
torch.Size([2, 3, 40, 40])
torch.Size([2, 3, 40, 40])
torch.Size([2, 3, 40, 40])
torch.Size([2, 3, 40, 40])
torch.Size([2, 3, 20, 20])
torch.Size([2, 3, 20, 20])
torch.Size([2, 3, 20, 20])
torch.Size([2, 3, 20, 20])
coord [0, 0]
bwd_out_shape  (6, 6)
fwd_out_shape  (2, 2)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
***input tile torch.Size([2, 3, 282, 282])
in customized Sequential 2
id 139824594865504
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 282, 282])
in customized Sequential 3
id 139824594865648
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 280, 280])
in customized Sequential 3
in customized Sequential 3
id 139822786460256
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 140, 140])
in customized Sequential 3
id 139822785814384
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 138, 138])
in customized Sequential 3
in customized Sequential 3
id 139824594781952
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 70, 70])
in customized Sequential 3
id 139824594781520
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 68, 68])
in customized Sequential 3
id 139824594781616
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 66, 66])
in customized Sequential 3
in customized Sequential 3
id 139822785897520
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 34, 34])
in customized Sequential 3
id 139822785897184
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 32, 32])
in customized Sequential 3
id 139822785897808
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 30, 30])
in customized Sequential 3
in customized Sequential 3
id 139822785897904
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 16, 16])
in customized Sequential 3
id 139822785897952
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 14, 14])
in customized Sequential 3
id 139822785898000
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 12, 12])
in customized Sequential 3
(6, 6) [6, 6] [0, 5, 0, 5]
coord [0, 5, 0, 5]
out_temp torch.Size([2, 3, 6, 6])
coord [0, 1]
bwd_out_shape  (8, 6)
fwd_out_shape  (2, 2)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
***input tile torch.Size([2, 3, 282, 320])
in customized Sequential 2
id 139824594865504
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 282, 322])
in customized Sequential 3
id 139824594865648
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 280, 320])
in customized Sequential 3
in customized Sequential 3
id 139822786460256
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 140, 162])
in customized Sequential 3
id 139822785814384
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 138, 160])
in customized Sequential 3
in customized Sequential 3
id 139824594781952
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 70, 84])
in customized Sequential 3
id 139824594781520
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 68, 82])
in customized Sequential 3
id 139824594781616
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 66, 80])
in customized Sequential 3
in customized Sequential 3
id 139822785897520
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 34, 42])
in customized Sequential 3
id 139822785897184
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 32, 40])
in customized Sequential 3
id 139822785897808
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 30, 38])
in customized Sequential 3
in customized Sequential 3
id 139822785897904
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 16, 20])
in customized Sequential 3
id 139822785897952
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 14, 18])
in customized Sequential 3
id 139822785898000
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 12, 16])
in customized Sequential 3
(8, 6) [8, 6] [0, 7, 0, 5]
coord [0, 7, 0, 5]
out_temp torch.Size([2, 3, 6, 8])
coord [0, 2]
bwd_out_shape  (8, 6)
fwd_out_shape  (2, 2)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
***input tile torch.Size([2, 3, 282, 320])
in customized Sequential 2
id 139824594865504
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 282, 322])
in customized Sequential 3
id 139824594865648
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 280, 320])
in customized Sequential 3
in customized Sequential 3
id 139822786460256
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 140, 162])
in customized Sequential 3
id 139822785814384
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 138, 160])
in customized Sequential 3
in customized Sequential 3
id 139824594781952
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 70, 84])
in customized Sequential 3
id 139824594781520
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 68, 82])
in customized Sequential 3
id 139824594781616
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 66, 80])
in customized Sequential 3
in customized Sequential 3
id 139822785897520
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 34, 42])
in customized Sequential 3
id 139822785897184
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 32, 40])
in customized Sequential 3
id 139822785897808
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 30, 38])
in customized Sequential 3
in customized Sequential 3
id 139822785897904
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 16, 20])
in customized Sequential 3
id 139822785897952
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 14, 18])
in customized Sequential 3
id 139822785898000
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 12, 16])
in customized Sequential 3
(8, 6) [8, 6] [2, 9, 0, 5]
coord [2, 9, 0, 5]
out_temp torch.Size([2, 3, 6, 8])
coord [0, 3]
bwd_out_shape  (6, 6)
fwd_out_shape  (2, 2)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
***input tile torch.Size([2, 3, 282, 282])
in customized Sequential 2
id 139824594865504
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 282, 282])
in customized Sequential 3
id 139824594865648
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 280, 280])
in customized Sequential 3
in customized Sequential 3
id 139822786460256
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 140, 140])
in customized Sequential 3
id 139822785814384
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 138, 138])
in customized Sequential 3
in customized Sequential 3
id 139824594781952
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 70, 70])
in customized Sequential 3
id 139824594781520
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 68, 68])
in customized Sequential 3
id 139824594781616
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 66, 66])
in customized Sequential 3
in customized Sequential 3
id 139822785897520
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 34, 34])
in customized Sequential 3
id 139822785897184
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 32, 32])
in customized Sequential 3
id 139822785897808
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 30, 30])
in customized Sequential 3
in customized Sequential 3
id 139822785897904
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 16, 16])
in customized Sequential 3
id 139822785897952
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 14, 14])
in customized Sequential 3
id 139822785898000
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 12, 12])
in customized Sequential 3
(6, 6) [6, 6] [4, 9, 0, 5]
coord [4, 9, 0, 5]
out_temp torch.Size([2, 3, 6, 6])
coord [1, 0]
bwd_out_shape  (6, 8)
fwd_out_shape  (2, 2)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
***input tile torch.Size([2, 3, 320, 282])
in customized Sequential 2
id 139824594865504
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 322, 282])
in customized Sequential 3
id 139824594865648
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 320, 280])
in customized Sequential 3
in customized Sequential 3
id 139822786460256
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 162, 140])
in customized Sequential 3
id 139822785814384
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 160, 138])
in customized Sequential 3
in customized Sequential 3
id 139824594781952
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 84, 70])
in customized Sequential 3
id 139824594781520
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 82, 68])
in customized Sequential 3
id 139824594781616
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 80, 66])
in customized Sequential 3
in customized Sequential 3
id 139822785897520
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 42, 34])
in customized Sequential 3
id 139822785897184
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 40, 32])
in customized Sequential 3
id 139822785897808
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 38, 30])
in customized Sequential 3
in customized Sequential 3
id 139822785897904
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 20, 16])
in customized Sequential 3
id 139822785897952
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 18, 14])
in customized Sequential 3
id 139822785898000
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 16, 12])
in customized Sequential 3
(6, 8) [6, 8] [0, 5, 0, 7]
coord [0, 5, 0, 7]
out_temp torch.Size([2, 3, 8, 6])
coord [1, 1]
bwd_out_shape  (8, 8)
fwd_out_shape  (2, 2)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
***input tile torch.Size([2, 3, 320, 320])
in customized Sequential 2
id 139824594865504
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 322, 322])
in customized Sequential 3
id 139824594865648
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 320, 320])
in customized Sequential 3
in customized Sequential 3
id 139822786460256
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 162, 162])
in customized Sequential 3
id 139822785814384
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 160, 160])
in customized Sequential 3
in customized Sequential 3
id 139824594781952
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 84, 84])
in customized Sequential 3
id 139824594781520
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 82, 82])
in customized Sequential 3
id 139824594781616
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 80, 80])
in customized Sequential 3
in customized Sequential 3
id 139822785897520
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 42, 42])
in customized Sequential 3
id 139822785897184
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 40, 40])
in customized Sequential 3
id 139822785897808
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 38, 38])
in customized Sequential 3
in customized Sequential 3
id 139822785897904
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 20, 20])
in customized Sequential 3
id 139822785897952
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 18, 18])
in customized Sequential 3
id 139822785898000
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 16, 16])
in customized Sequential 3
(8, 8) [8, 8] [0, 7, 0, 7]
coord [0, 7, 0, 7]
out_temp torch.Size([2, 3, 8, 8])
coord [1, 2]
bwd_out_shape  (8, 8)
fwd_out_shape  (2, 2)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
***input tile torch.Size([2, 3, 320, 320])
in customized Sequential 2
id 139824594865504
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 322, 322])
in customized Sequential 3
id 139824594865648
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 320, 320])
in customized Sequential 3
in customized Sequential 3
id 139822786460256
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 162, 162])
in customized Sequential 3
id 139822785814384
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 160, 160])
in customized Sequential 3
in customized Sequential 3
id 139824594781952
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 84, 84])
in customized Sequential 3
id 139824594781520
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 82, 82])
in customized Sequential 3
id 139824594781616
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 80, 80])
in customized Sequential 3
in customized Sequential 3
id 139822785897520
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 42, 42])
in customized Sequential 3
id 139822785897184
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 40, 40])
in customized Sequential 3
id 139822785897808
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 38, 38])
in customized Sequential 3
in customized Sequential 3
id 139822785897904
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 20, 20])
in customized Sequential 3
id 139822785897952
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 18, 18])
in customized Sequential 3
id 139822785898000
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 16, 16])
in customized Sequential 3
(8, 8) [8, 8] [2, 9, 0, 7]
coord [2, 9, 0, 7]
out_temp torch.Size([2, 3, 8, 8])
coord [1, 3]
bwd_out_shape  (6, 8)
fwd_out_shape  (2, 2)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
***input tile torch.Size([2, 3, 320, 282])
in customized Sequential 2
id 139824594865504
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 322, 282])
in customized Sequential 3
id 139824594865648
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 320, 280])
in customized Sequential 3
in customized Sequential 3
id 139822786460256
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 162, 140])
in customized Sequential 3
id 139822785814384
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 160, 138])
in customized Sequential 3
in customized Sequential 3
id 139824594781952
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 84, 70])
in customized Sequential 3
id 139824594781520
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 82, 68])
in customized Sequential 3
id 139824594781616
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 80, 66])
in customized Sequential 3
in customized Sequential 3
id 139822785897520
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 42, 34])
in customized Sequential 3
id 139822785897184
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 40, 32])
in customized Sequential 3
id 139822785897808
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 38, 30])
in customized Sequential 3
in customized Sequential 3
id 139822785897904
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 20, 16])
in customized Sequential 3
id 139822785897952
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 18, 14])
in customized Sequential 3
id 139822785898000
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 16, 12])
in customized Sequential 3
(6, 8) [6, 8] [4, 9, 0, 7]
coord [4, 9, 0, 7]
out_temp torch.Size([2, 3, 8, 6])
coord [2, 0]
bwd_out_shape  (6, 8)
fwd_out_shape  (2, 2)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
***input tile torch.Size([2, 3, 320, 282])
in customized Sequential 2
id 139824594865504
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 322, 282])
in customized Sequential 3
id 139824594865648
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 320, 280])
in customized Sequential 3
in customized Sequential 3
id 139822786460256
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 162, 140])
in customized Sequential 3
id 139822785814384
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 160, 138])
in customized Sequential 3
in customized Sequential 3
id 139824594781952
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 84, 70])
in customized Sequential 3
id 139824594781520
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 82, 68])
in customized Sequential 3
id 139824594781616
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 80, 66])
in customized Sequential 3
in customized Sequential 3
id 139822785897520
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 42, 34])
in customized Sequential 3
id 139822785897184
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 40, 32])
in customized Sequential 3
id 139822785897808
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 38, 30])
in customized Sequential 3
in customized Sequential 3
id 139822785897904
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 20, 16])
in customized Sequential 3
id 139822785897952
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 18, 14])
in customized Sequential 3
id 139822785898000
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 16, 12])
in customized Sequential 3
(6, 8) [6, 8] [0, 5, 2, 9]
coord [0, 5, 2, 9]
out_temp torch.Size([2, 3, 8, 6])
coord [2, 1]
bwd_out_shape  (8, 8)
fwd_out_shape  (2, 2)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
***input tile torch.Size([2, 3, 320, 320])
in customized Sequential 2
id 139824594865504
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 322, 322])
in customized Sequential 3
id 139824594865648
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 320, 320])
in customized Sequential 3
in customized Sequential 3
id 139822786460256
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 162, 162])
in customized Sequential 3
id 139822785814384
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 160, 160])
in customized Sequential 3
in customized Sequential 3
id 139824594781952
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 84, 84])
in customized Sequential 3
id 139824594781520
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 82, 82])
in customized Sequential 3
id 139824594781616
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 80, 80])
in customized Sequential 3
in customized Sequential 3
id 139822785897520
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 42, 42])
in customized Sequential 3
id 139822785897184
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 40, 40])
in customized Sequential 3
id 139822785897808
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 38, 38])
in customized Sequential 3
in customized Sequential 3
id 139822785897904
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 20, 20])
in customized Sequential 3
id 139822785897952
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 18, 18])
in customized Sequential 3
id 139822785898000
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 16, 16])
in customized Sequential 3
(8, 8) [8, 8] [0, 7, 2, 9]
coord [0, 7, 2, 9]
out_temp torch.Size([2, 3, 8, 8])
coord [2, 2]
bwd_out_shape  (8, 8)
fwd_out_shape  (2, 2)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
***input tile torch.Size([2, 3, 320, 320])
in customized Sequential 2
id 139824594865504
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 322, 322])
in customized Sequential 3
id 139824594865648
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 320, 320])
in customized Sequential 3
in customized Sequential 3
id 139822786460256
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 162, 162])
in customized Sequential 3
id 139822785814384
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 160, 160])
in customized Sequential 3
in customized Sequential 3
id 139824594781952
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 84, 84])
in customized Sequential 3
id 139824594781520
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 82, 82])
in customized Sequential 3
id 139824594781616
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 80, 80])
in customized Sequential 3
in customized Sequential 3
id 139822785897520
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 42, 42])
in customized Sequential 3
id 139822785897184
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 40, 40])
in customized Sequential 3
id 139822785897808
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 38, 38])
in customized Sequential 3
in customized Sequential 3
id 139822785897904
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 20, 20])
in customized Sequential 3
id 139822785897952
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 18, 18])
in customized Sequential 3
id 139822785898000
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 16, 16])
in customized Sequential 3
(8, 8) [8, 8] [2, 9, 2, 9]
coord [2, 9, 2, 9]
out_temp torch.Size([2, 3, 8, 8])
coord [2, 3]
bwd_out_shape  (6, 8)
fwd_out_shape  (2, 2)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
***input tile torch.Size([2, 3, 320, 282])
in customized Sequential 2
id 139824594865504
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 322, 282])
in customized Sequential 3
id 139824594865648
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 320, 280])
in customized Sequential 3
in customized Sequential 3
id 139822786460256
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 162, 140])
in customized Sequential 3
id 139822785814384
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 160, 138])
in customized Sequential 3
in customized Sequential 3
id 139824594781952
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 84, 70])
in customized Sequential 3
id 139824594781520
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 82, 68])
in customized Sequential 3
id 139824594781616
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 80, 66])
in customized Sequential 3
in customized Sequential 3
id 139822785897520
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 42, 34])
in customized Sequential 3
id 139822785897184
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 40, 32])
in customized Sequential 3
id 139822785897808
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 38, 30])
in customized Sequential 3
in customized Sequential 3
id 139822785897904
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 20, 16])
in customized Sequential 3
id 139822785897952
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 18, 14])
in customized Sequential 3
id 139822785898000
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 16, 12])
in customized Sequential 3
(6, 8) [6, 8] [4, 9, 2, 9]
coord [4, 9, 2, 9]
out_temp torch.Size([2, 3, 8, 6])
coord [3, 0]
bwd_out_shape  (6, 6)
fwd_out_shape  (2, 2)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
***input tile torch.Size([2, 3, 282, 282])
in customized Sequential 2
id 139824594865504
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 282, 282])
in customized Sequential 3
id 139824594865648
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 280, 280])
in customized Sequential 3
in customized Sequential 3
id 139822786460256
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 140, 140])
in customized Sequential 3
id 139822785814384
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 138, 138])
in customized Sequential 3
in customized Sequential 3
id 139824594781952
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 70, 70])
in customized Sequential 3
id 139824594781520
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 68, 68])
in customized Sequential 3
id 139824594781616
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 66, 66])
in customized Sequential 3
in customized Sequential 3
id 139822785897520
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 34, 34])
in customized Sequential 3
id 139822785897184
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 32, 32])
in customized Sequential 3
id 139822785897808
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 30, 30])
in customized Sequential 3
in customized Sequential 3
id 139822785897904
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 16, 16])
in customized Sequential 3
id 139822785897952
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 14, 14])
in customized Sequential 3
id 139822785898000
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 12, 12])
in customized Sequential 3
(6, 6) [6, 6] [0, 5, 4, 9]
coord [0, 5, 4, 9]
out_temp torch.Size([2, 3, 6, 6])
coord [3, 1]
bwd_out_shape  (8, 6)
fwd_out_shape  (2, 2)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
***input tile torch.Size([2, 3, 282, 320])
in customized Sequential 2
id 139824594865504
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 282, 322])
in customized Sequential 3
id 139824594865648
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 280, 320])
in customized Sequential 3
in customized Sequential 3
id 139822786460256
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 140, 162])
in customized Sequential 3
id 139822785814384
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 138, 160])
in customized Sequential 3
in customized Sequential 3
id 139824594781952
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 70, 84])
in customized Sequential 3
id 139824594781520
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 68, 82])
in customized Sequential 3
id 139824594781616
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 66, 80])
in customized Sequential 3
in customized Sequential 3
id 139822785897520
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 34, 42])
in customized Sequential 3
id 139822785897184
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 32, 40])
in customized Sequential 3
id 139822785897808
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 30, 38])
in customized Sequential 3
in customized Sequential 3
id 139822785897904
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 16, 20])
in customized Sequential 3
id 139822785897952
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 14, 18])
in customized Sequential 3
id 139822785898000
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 12, 16])
in customized Sequential 3
(8, 6) [8, 6] [0, 7, 4, 9]
coord [0, 7, 4, 9]
out_temp torch.Size([2, 3, 6, 8])
coord [3, 2]
bwd_out_shape  (8, 6)
fwd_out_shape  (2, 2)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
***input tile torch.Size([2, 3, 282, 320])
in customized Sequential 2
id 139824594865504
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 282, 322])
in customized Sequential 3
id 139824594865648
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 280, 320])
in customized Sequential 3
in customized Sequential 3
id 139822786460256
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 140, 162])
in customized Sequential 3
id 139822785814384
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 138, 160])
in customized Sequential 3
in customized Sequential 3
id 139824594781952
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 70, 84])
in customized Sequential 3
id 139824594781520
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 68, 82])
in customized Sequential 3
id 139824594781616
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 66, 80])
in customized Sequential 3
in customized Sequential 3
id 139822785897520
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 34, 42])
in customized Sequential 3
id 139822785897184
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 32, 40])
in customized Sequential 3
id 139822785897808
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 30, 38])
in customized Sequential 3
in customized Sequential 3
id 139822785897904
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 16, 20])
in customized Sequential 3
id 139822785897952
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 14, 18])
in customized Sequential 3
id 139822785898000
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 12, 16])
in customized Sequential 3
(8, 6) [8, 6] [2, 9, 4, 9]
coord [2, 9, 4, 9]
out_temp torch.Size([2, 3, 6, 8])
coord [3, 3]
bwd_out_shape  (6, 6)
fwd_out_shape  (2, 2)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
***input tile torch.Size([2, 3, 282, 282])
in customized Sequential 2
id 139824594865504
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 282, 282])
in customized Sequential 3
id 139824594865648
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 280, 280])
in customized Sequential 3
in customized Sequential 3
id 139822786460256
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 140, 140])
in customized Sequential 3
id 139822785814384
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 138, 138])
in customized Sequential 3
in customized Sequential 3
id 139824594781952
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 70, 70])
in customized Sequential 3
id 139824594781520
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 68, 68])
in customized Sequential 3
id 139824594781616
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 66, 66])
in customized Sequential 3
in customized Sequential 3
id 139822785897520
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 34, 34])
in customized Sequential 3
id 139822785897184
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 32, 32])
in customized Sequential 3
id 139822785897808
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 30, 30])
in customized Sequential 3
in customized Sequential 3
id 139822785897904
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 16, 16])
in customized Sequential 3
id 139822785897952
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 14, 14])
in customized Sequential 3
id 139822785898000
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([2, 3, 12, 12])
in customized Sequential 3
(6, 6) [6, 6] [4, 9, 4, 9]
coord [4, 9, 4, 9]
out_temp torch.Size([2, 3, 6, 6])

&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&


&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 12, 12])
grad_out size torch.Size([2, 3, 10, 10])
arg size torch.Size([2, 3, 6, 6])
new_grad_out torch.Size([2, 3, 6, 6])
##############grad_in in maxp torch.Size([2, 3, 12, 12])
I am [3, 3]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 14, 14])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 12, 12])
grad_input torch.Size([2, 3, 14, 14])
padding info :: [0, 1, 0, 1]
grad_input old torch.Size([2, 3, 14, 14])
grad_input new torch.Size([2, 3, 14, 14])
new grad_input torch.Size([2, 3, 14, 14])
grad_out size torch.Size([2, 3, 12, 12])
crop [7, 0, 7, 0] [8, 19, 8, 19] [15, 19, 15, 19]
## 7 12 7 12
A 15 20 15 20
B 15 22 15 22
csaved input torch.Size([2, 3, 7, 7])
cgrad_output torch.Size([2, 3, 5, 5])
real nontiled_activation torch.Size([2, 3, 7, 7])
real nontiled_grad_out torch.Size([2, 3, 5, 5])
-----------------------------------------

-----------------------------------------

I am [3, 3]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([2, 3, 16, 16])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 14, 14])
padding info :: [0, 2, 0, 2]
grad_input old torch.Size([2, 3, 16, 16])
grad_input new torch.Size([2, 3, 16, 16])
grad_input torch.Size([2, 3, 16, 16])
grad_out size torch.Size([2, 3, 13, 13])
crop [8, 0, 8, 0] [7, 19, 7, 19] [15, 19, 15, 19]
## 8 13 8 13
A 15 20 15 20
B 15 22 15 22
csaved input torch.Size([2, 3, 7, 7])
cgrad_output torch.Size([2, 3, 5, 5])
real nontiled_activation torch.Size([2, 3, 7, 7])
real nontiled_grad_out torch.Size([2, 3, 5, 5])
-----------------------------------------

-----------------------------------------

I am [3, 3]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([2, 3, 18, 18])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 16, 16])
padding info :: [0, 3, 0, 3]
grad_input torch.Size([2, 3, 15, 15])
grad_out size torch.Size([2, 3, 14, 14])
crop [9, 0, 9, 0] [6, 19, 6, 19] [15, 19, 15, 19]
## 9 14 9 14
A 15 20 15 20
B 15 22 15 22
csaved input torch.Size([2, 3, 7, 7])
cgrad_output torch.Size([2, 3, 5, 5])
real nontiled_activation torch.Size([2, 3, 7, 7])
real nontiled_grad_out torch.Size([2, 3, 5, 5])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 30, 30])
grad_out size torch.Size([2, 3, 15, 15])
arg size torch.Size([2, 3, 15, 15])
##############grad_in in maxp torch.Size([2, 3, 30, 30])
I am [3, 3]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 32, 32])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 30, 30])
grad_input torch.Size([2, 3, 32, 32])
padding info :: [0, 1, 0, 1]
grad_input old torch.Size([2, 3, 32, 32])
grad_input new torch.Size([2, 3, 32, 32])
new grad_input torch.Size([2, 3, 32, 32])
grad_out size torch.Size([2, 3, 30, 30])
crop [20, 0, 20, 0] [10, 39, 10, 39] [30, 39, 30, 39]
## 20 30 20 30
A 30 40 30 40
B 30 42 30 42
csaved input torch.Size([2, 3, 12, 12])
cgrad_output torch.Size([2, 3, 10, 10])
real nontiled_activation torch.Size([2, 3, 12, 12])
real nontiled_grad_out torch.Size([2, 3, 10, 10])
-----------------------------------------

-----------------------------------------

I am [3, 3]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([2, 3, 34, 34])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 32, 32])
padding info :: [0, 2, 0, 2]
grad_input old torch.Size([2, 3, 34, 34])
grad_input new torch.Size([2, 3, 34, 34])
grad_input torch.Size([2, 3, 34, 34])
grad_out size torch.Size([2, 3, 31, 31])
crop [21, 0, 21, 0] [9, 39, 9, 39] [30, 39, 30, 39]
## 21 31 21 31
A 30 40 30 40
B 30 42 30 42
csaved input torch.Size([2, 3, 12, 12])
cgrad_output torch.Size([2, 3, 10, 10])
real nontiled_activation torch.Size([2, 3, 12, 12])
real nontiled_grad_out torch.Size([2, 3, 10, 10])
-----------------------------------------

-----------------------------------------

I am [3, 3]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([2, 3, 36, 36])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 34, 34])
padding info :: [0, 3, 0, 3]
grad_input torch.Size([2, 3, 33, 33])
grad_out size torch.Size([2, 3, 32, 32])
crop [22, 0, 22, 0] [8, 39, 8, 39] [30, 39, 30, 39]
## 22 32 22 32
A 30 40 30 40
B 30 42 30 42
csaved input torch.Size([2, 3, 12, 12])
cgrad_output torch.Size([2, 3, 10, 10])
real nontiled_activation torch.Size([2, 3, 12, 12])
real nontiled_grad_out torch.Size([2, 3, 10, 10])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 66, 66])
grad_out size torch.Size([2, 3, 33, 33])
arg size torch.Size([2, 3, 33, 33])
##############grad_in in maxp torch.Size([2, 3, 66, 66])
I am [3, 3]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 68, 68])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 66, 66])
grad_input torch.Size([2, 3, 68, 68])
padding info :: [0, 1, 0, 1]
grad_input old torch.Size([2, 3, 68, 68])
grad_input new torch.Size([2, 3, 68, 68])
new grad_input torch.Size([2, 3, 68, 68])
grad_out size torch.Size([2, 3, 66, 66])
crop [46, 0, 46, 0] [14, 79, 14, 79] [60, 79, 60, 79]
## 46 66 46 66
A 60 80 60 80
B 60 82 60 82
csaved input torch.Size([2, 3, 22, 22])
cgrad_output torch.Size([2, 3, 20, 20])
real nontiled_activation torch.Size([2, 3, 22, 22])
real nontiled_grad_out torch.Size([2, 3, 20, 20])
-----------------------------------------

-----------------------------------------

I am [3, 3]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([2, 3, 70, 70])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 68, 68])
padding info :: [0, 2, 0, 2]
grad_input old torch.Size([2, 3, 70, 70])
grad_input new torch.Size([2, 3, 70, 70])
grad_input torch.Size([2, 3, 70, 70])
grad_out size torch.Size([2, 3, 67, 67])
crop [47, 0, 47, 0] [13, 79, 13, 79] [60, 79, 60, 79]
## 47 67 47 67
A 60 80 60 80
B 60 82 60 82
csaved input torch.Size([2, 3, 22, 22])
cgrad_output torch.Size([2, 3, 20, 20])
real nontiled_activation torch.Size([2, 3, 22, 22])
real nontiled_grad_out torch.Size([2, 3, 20, 20])
-----------------------------------------

-----------------------------------------

I am [3, 3]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([2, 3, 72, 72])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 70, 70])
padding info :: [0, 3, 0, 3]
grad_input torch.Size([2, 3, 69, 69])
grad_out size torch.Size([2, 3, 68, 68])
crop [48, 0, 48, 0] [12, 79, 12, 79] [60, 79, 60, 79]
## 48 68 48 68
A 60 80 60 80
B 60 82 60 82
csaved input torch.Size([2, 3, 22, 22])
cgrad_output torch.Size([2, 3, 20, 20])
real nontiled_activation torch.Size([2, 3, 22, 22])
real nontiled_grad_out torch.Size([2, 3, 20, 20])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 138, 138])
grad_out size torch.Size([2, 3, 69, 69])
arg size torch.Size([2, 3, 69, 69])
##############grad_in in maxp torch.Size([2, 3, 138, 138])
I am [3, 3]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 140, 140])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 138, 138])
grad_input torch.Size([2, 3, 140, 140])
padding info :: [0, 1, 0, 1]
grad_input old torch.Size([2, 3, 140, 140])
grad_input new torch.Size([2, 3, 140, 140])
new grad_input torch.Size([2, 3, 140, 140])
grad_out size torch.Size([2, 3, 138, 138])
crop [98, 0, 98, 0] [22, 159, 22, 159] [120, 159, 120, 159]
## 98 138 98 138
A 120 160 120 160
B 120 162 120 162
csaved input torch.Size([2, 3, 42, 42])
cgrad_output torch.Size([2, 3, 40, 40])
real nontiled_activation torch.Size([2, 3, 42, 42])
real nontiled_grad_out torch.Size([2, 3, 40, 40])
-----------------------------------------

-----------------------------------------

I am [3, 3]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([2, 3, 142, 142])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 140, 140])
padding info :: [0, 2, 0, 2]
grad_input torch.Size([2, 3, 140, 140])
grad_out size torch.Size([2, 3, 139, 139])
crop [99, 0, 99, 0] [21, 159, 21, 159] [120, 159, 120, 159]
## 99 139 99 139
A 120 160 120 160
B 120 162 120 162
csaved input torch.Size([2, 3, 42, 42])
cgrad_output torch.Size([2, 3, 40, 40])
real nontiled_activation torch.Size([2, 3, 42, 42])
real nontiled_grad_out torch.Size([2, 3, 40, 40])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 280, 280])
grad_out size torch.Size([2, 3, 140, 140])
arg size torch.Size([2, 3, 140, 140])
##############grad_in in maxp torch.Size([2, 3, 280, 280])
I am [3, 3]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 282, 282])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 280, 280])
grad_input torch.Size([2, 3, 282, 282])
padding info :: [0, 1, 0, 1]
grad_input old torch.Size([2, 3, 282, 282])
grad_input new torch.Size([2, 3, 282, 282])
new grad_input torch.Size([2, 3, 282, 282])
grad_out size torch.Size([2, 3, 280, 280])
crop [200, 0, 200, 0] [40, 319, 40, 319] [240, 319, 240, 319]
## 200 280 200 280
A 240 320 240 320
B 240 322 240 322
csaved input torch.Size([2, 3, 82, 82])
cgrad_output torch.Size([2, 3, 80, 80])
real nontiled_activation torch.Size([2, 3, 82, 82])
real nontiled_grad_out torch.Size([2, 3, 80, 80])
-----------------------------------------

-----------------------------------------

I am [3, 3]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([2, 3, 284, 284])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 282, 282])
final torch.Size([2, 3, 284, 284])
padding info :: [0, 2, 0, 2]
after crop g_in torch.Size([2, 3, 80, 80])
grad_out size torch.Size([2, 3, 281, 281])
crop [201, 0, 201, 0] [39, 319, 39, 319] [240, 319, 240, 319]
## 201 281 201 281
A 240 320 240 320
B 240 322 240 322
csaved input torch.Size([2, 3, 82, 82])
cgrad_output torch.Size([2, 3, 80, 80])
real nontiled_activation torch.Size([2, 3, 82, 82])
real nontiled_grad_out torch.Size([2, 3, 80, 80])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 12, 16])
grad_out size torch.Size([2, 3, 10, 10])
arg size torch.Size([2, 3, 6, 8])
new_grad_out torch.Size([2, 3, 6, 8])
##############grad_in in maxp torch.Size([2, 3, 12, 16])
I am [3, 2]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 14, 18])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 12, 16])
grad_input torch.Size([2, 3, 14, 18])
padding info :: [0, 1, 0, 1]
grad_input old torch.Size([2, 3, 14, 18])
grad_input new torch.Size([2, 3, 14, 18])
new grad_input torch.Size([2, 3, 14, 18])
grad_out size torch.Size([2, 3, 12, 16])
crop [6, 5, 7, 0] [4, 19, 8, 19] [10, 14, 15, 19]
## 7 12 6 11
A 15 20 10 15
B 15 22 10 17
csaved input torch.Size([2, 3, 7, 7])
cgrad_output torch.Size([2, 3, 5, 5])
real nontiled_activation torch.Size([2, 3, 7, 7])
ad/engine.cpp] call_function CudnnConvolutionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function MaxPool2DWithIndicesBackward
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function CudnnConvolutionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function CudnnConvolutionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function SumBackward0
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function [torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::CopyBackwards
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/real nontiled_grad_out torch.Size([2, 3, 5, 5])
-----------------------------------------

-----------------------------------------

I am [3, 2]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([2, 3, 16, 20])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 14, 18])
padding info :: [0, 2, 0, 2]
grad_input old torch.Size([2, 3, 16, 20])
grad_input new torch.Size([2, 3, 16, 20])
grad_input torch.Size([2, 3, 16, 20])
grad_out size torch.Size([2, 3, 13, 17])
crop [7, 5, 8, 0] [3, 19, 7, 19] [10, 14, 15, 19]
## 8 13 7 12
A 15 20 10 15
B 15 22 10 17
csaved input torch.Size([2, 3, 7, 7])
cgrad_output torch.Size([2, 3, 5, 5])
real nontiled_activation torch.Size([2, 3, 7, 7])
real nontiled_grad_out torch.Size([2, 3, 5, 5])
-----------------------------------------

-----------------------------------------

I am [3, 2]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([2, 3, 18, 22])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 16, 20])
padding info :: [0, 3, 0, 3]
grad_input torch.Size([2, 3, 15, 19])
grad_out size torch.Size([2, 3, 14, 18])
crop [8, 5, 9, 0] [2, 19, 6, 19] [10, 14, 15, 19]
## 9 14 8 13
A 15 20 10 15
B 15 22 10 17
csaved input torch.Size([2, 3, 7, 7])
cgrad_output torch.Size([2, 3, 5, 5])
real nontiled_activation torch.Size([2, 3, 7, 7])
real nontiled_grad_out torch.Size([2, 3, 5, 5])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 30, 38])
grad_out size torch.Size([2, 3, 15, 19])
arg size torch.Size([2, 3, 15, 19])
##############grad_in in maxp torch.Size([2, 3, 30, 38])
I am [3, 2]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 32, 40])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 30, 38])
grad_input torch.Size([2, 3, 32, 40])
padding info :: [0, 1, 0, 1]
grad_input old torch.Size([2, 3, 32, 40])
grad_input new torch.Size([2, 3, 32, 40])
new grad_input torch.Size([2, 3, 32, 40])
grad_out size torch.Size([2, 3, 30, 38])
crop [18, 10, 20, 0] [2, 39, 10, 39] [20, 29, 30, 39]
## 20 30 18 28
A 30 40 20 30
B 30 42 20 32
csaved input torch.Size([2, 3, 12, 12])
cgrad_output torch.Size([2, 3, 10, 10])
real nontiled_activation torch.Size([2, 3, 12, 12])
real nontiled_grad_out torch.Size([2, 3, 10, 10])
-----------------------------------------

-----------------------------------------

I am [3, 2]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([2, 3, 34, 42])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 32, 40])
padding info :: [0, 2, 0, 2]
grad_input old torch.Size([2, 3, 34, 42])
grad_input new torch.Size([2, 3, 34, 42])
grad_input torch.Size([2, 3, 34, 42])
grad_out size torch.Size([2, 3, 31, 39])
crop [19, 10, 21, 0] [1, 39, 9, 39] [20, 29, 30, 39]
## 21 31 19 29
A 30 40 20 30
B 30 42 20 32
csaved input torch.Size([2, 3, 12, 12])
cgrad_output torch.Size([2, 3, 10, 10])
real nontiled_activation torch.Size([2, 3, 12, 12])
real nontiled_grad_out torch.Size([2, 3, 10, 10])
-----------------------------------------

-----------------------------------------

I am [3, 2]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([2, 3, 36, 44])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 34, 42])
padding info :: [1, 3, 0, 3]
grad_input torch.Size([2, 3, 33, 40])
grad_out size torch.Size([2, 3, 32, 40])
crop [20, 10, 22, 0] [0, 39, 8, 39] [20, 29, 30, 39]
## 22 32 20 30
A 30 40 20 30
B 30 42 20 32
csaved input torch.Size([2, 3, 12, 12])
cgrad_output torch.Size([2, 3, 10, 10])
real nontiled_activation torch.Size([2, 3, 12, 12])
real nontiled_grad_out torch.Size([2, 3, 10, 10])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 66, 80])
grad_out size torch.Size([2, 3, 33, 40])
arg size torch.Size([2, 3, 33, 40])
##############grad_in in maxp torch.Size([2, 3, 66, 80])
I am [3, 2]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 68, 82])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 66, 80])
grad_input torch.Size([2, 3, 68, 82])
padding info :: [1, 1, 0, 1]
grad_input old torch.Size([2, 3, 68, 82])
grad_input new torch.Size([2, 3, 68, 82])
new grad_input torch.Size([2, 3, 68, 82])
grad_out size torch.Size([2, 3, 66, 80])
crop [40, 20, 46, 0] [0, 79, 14, 79] [40, 59, 60, 79]
## 46 66 40 60
A 60 80 40 60
B 60 82 40 62
csaved input torch.Size([2, 3, 22, 22])
cgrad_output torch.Size([2, 3, 20, 20])
real nontiled_activation torch.Size([2, 3, 22, 22])
real nontiled_grad_out torch.Size([2, 3, 20, 20])
-----------------------------------------

-----------------------------------------

I am [3, 2]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([2, 3, 70, 84])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 68, 82])
padding info :: [2, 2, 0, 2]
grad_input old torch.Size([2, 3, 70, 84])
grad_input new torch.Size([2, 3, 70, 84])
grad_input torch.Size([2, 3, 70, 84])
grad_out size torch.Size([2, 3, 67, 80])
crop [40, 20, 47, 0] [0, 79, 13, 79] [40, 59, 60, 79]
## 47 67 40 60
A 60 80 40 60
B 60 82 40 62
csaved input torch.Size([2, 3, 22, 22])
cgrad_output torch.Size([2, 3, 20, 20])
real nontiled_activation torch.Size([2, 3, 22, 22])
real nontiled_grad_out torch.Size([2, 3, 20, 20])
-----------------------------------------

-----------------------------------------

I am [3, 2]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([2, 3, 72, 86])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 70, 84])
padding info :: [3, 3, 0, 3]
grad_input torch.Size([2, 3, 69, 80])
grad_out size torch.Size([2, 3, 68, 80])
crop [40, 20, 48, 0] [0, 79, 12, 79] [40, 59, 60, 79]
## 48 68 40 60
A 60 80 40 60
B 60 82 40 62
csaved input torch.Size([2, 3, 22, 22])
cgrad_output torch.Size([2, 3, 20, 20])
real nontiled_activation torch.Size([2, 3, 22, 22])
real nontiled_grad_out torch.Size([2, 3, 20, 20])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 138, 160])
grad_out size torch.Size([2, 3, 69, 80])
arg size torch.Size([2, 3, 69, 80])
##############grad_in in maxp torch.Size([2, 3, 138, 160])
I am [3, 2]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 140, 162])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 138, 160])
grad_input torch.Size([2, 3, 140, 162])
padding info :: [1, 1, 0, 1]
grad_input old torch.Size([2, 3, 140, 162])
grad_input new torch.Size([2, 3, 140, 162])
new grad_input torch.Size([2, 3, 140, 162])
grad_out size torch.Size([2, 3, 138, 160])
crop [80, 40, 98, 0] [0, 159, 22, 159] [80, 119, 120, 159]
## 98 138 80 120
A 120 160 80 120
B 120 162 80 122
csaved input torch.Size([2, 3, 42, 42])
cgrad_output torch.Size([2, 3, 40, 40])
real nontiled_activation torch.Size([2, 3, 42, 42])
real nontiled_grad_out torch.Size([2, 3, 40, 40])
-----------------------------------------

-----------------------------------------

I am [3, 2]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([2, 3, 142, 164])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 140, 162])
padding info :: [2, 2, 0, 2]
grad_input torch.Size([2, 3, 140, 160])
grad_out size torch.Size([2, 3, 139, 160])
crop [80, 40, 99, 0] [0, 159, 21, 159] [80, 119, 120, 159]
## 99 139 80 120
A 120 160 80 120
B 120 162 80 122
csaved input torch.Size([2, 3, 42, 42])
cgrad_output torch.Size([2, 3, 40, 40])
real nontiled_activation torch.Size([2, 3, 42, 42])
real nontiled_grad_out torch.Size([2, 3, 40, 40])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 280, 320])
grad_out size torch.Size([2, 3, 140, 160])
arg size torch.Size([2, 3, 140, 160])
##############grad_in in maxp torch.Size([2, 3, 280, 320])
I am [3, 2]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 282, 322])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 280, 320])
grad_input torch.Size([2, 3, 282, 322])
padding info :: [1, 1, 0, 1]
grad_input old torch.Size([2, 3, 282, 322])
grad_input new torch.Size([2, 3, 282, 322])
new grad_input torch.Size([2, 3, 282, 322])
grad_out size torch.Size([2, 3, 280, 320])
crop [160, 80, 200, 0] [0, 319, 40, 319] [160, 239, 240, 319]
## 200 280 160 240
A 240 320 160 240
B 240 322 160 242
csaved input torch.Size([2, 3, 82, 82])
cgrad_output torch.Size([2, 3, 80, 80])
real nontiled_activation torch.Size([2, 3, 82, 82])
real nontiled_grad_out torch.Size([2, 3, 80, 80])
-----------------------------------------

-----------------------------------------

I am [3, 2]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([2, 3, 284, 324])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 282, 322])
final torch.Size([2, 3, 284, 324])
padding info :: [2, 2, 0, 2]
after crop g_in torch.Size([2, 3, 80, 80])
grad_out size torch.Size([2, 3, 281, 320])
crop [160, 80, 201, 0] [0, 319, 39, 319] [160, 239, 240, 319]
## 201 281 160 240
A 240 320 160 240
B 240 322 160 242
csaved input torch.Size([2, 3, 82, 82])
cgrad_output torch.Size([2, 3, 80, 80])
real nontiled_activation torch.Size([2, 3, 82, 82])
real nontiled_grad_out torch.Size([2, 3, 80, 80])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 12, 16])
grad_out size torch.Size([2, 3, 10, 10])
arg size torch.Size([2, 3, 6, 8])
new_grad_out torch.Size([2, 3, 6, 8])
##############grad_in in maxp torch.Size([2, 3, 12, 16])
I am [3, 1]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 14, 18])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 12, 16])
grad_input torch.Size([2, 3, 14, 18])
padding info :: [1, 0, 0, 1]
grad_input old torch.Size([2, 3, 14, 18])
grad_input new torch.Size([2, 3, 14, 18])
new grad_input torch.Size([2, 3, 14, 18])
grad_out size torch.Size([2, 3, 12, 16])
crop [5, 6, 7, 0] [0, 15, 8, 19] [5, 9, 15, 19]
## 7 12 5 10
A 15 20 5 10
B 15 22 5 12
csaved input torch.Size([2, 3, 7, 7])
cgrad_output torch.Size([2, 3, 5, 5])
real nontiled_activation torch.Size([2, 3, 7, 7])
real nontiled_grad_out torch.Size([2, 3, 5, 5])
-----------------------------------------

-----------------------------------------

I am [3, 1]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([2, 3, 16, 20])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 14, 18])
padding info :: [2, 0, 0, 2]
grad_input old torch.Size([2, 3, 16, 20])
grad_input new torch.Size([2, 3, 16, 20])
grad_input torch.Size([2, 3, 16, 20])
grad_out size torch.Size([2, 3, 13, 17])
crop [5, 7, 8, 0] [0, 16, 7, 19] [5, 9, 15, 19]
## 8 13 5 10
A 15 20 5 10
B 15 22 5 12
csaved input torch.Size([2, 3, 7, 7])
cgrad_output torch.Size([2, 3, 5, 5])
real nontiled_activation torch.Size([2, 3, 7, 7])
real nontiled_grad_out torch.Size([2, 3, 5, 5])
-----------------------------------------

-----------------------------------------

I am [3, 1]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([2, 3, 18, 22])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 16, 20])
padding info :: [3, 0, 0, 3]
grad_input torch.Size([2, 3, 15, 19])
grad_out size torch.Size([2, 3, 14, 18])
crop [5, 8, 9, 0] [0, 17, 6, 19] [5, 9, 15, 19]
## 9 14 5 10
A 15 20 5 10
B 15 22 5 12
csaved input torch.Size([2, 3, 7, 7])
cgrad_output torch.Size([2, 3, 5, 5])
real nontiled_activation torch.Size([2, 3, 7, 7])
real nontiled_grad_out torch.Size([2, 3, 5, 5])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 30, 38])
grad_out size torch.Size([2, 3, 15, 19])
arg size torch.Size([2, 3, 15, 19])
##############grad_in in maxp torch.Size([2, 3, 30, 38])
I am [3, 1]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 32, 40])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 30, 38])
grad_input torch.Size([2, 3, 32, 40])
padding info :: [1, 0, 0, 1]
grad_input old torch.Size([2, 3, 32, 40])
grad_input new torch.Size([2, 3, 32, 40])
new grad_input torch.Size([2, 3, 32, 40])
grad_out size torch.Size([2, 3, 30, 38])
crop [10, 18, 20, 0] [0, 37, 10, 39] [10, 19, 30, 39]
## 20 30 10 20
A 30 40 10 20
B 30 42 10 22
csaved input torch.Size([2, 3, 12, 12])
cgrad_output torch.Size([2, 3, 10, 10])
real nontiled_activation torch.Size([2, 3, 12, 12])
real nontiled_grad_out torch.Size([2, 3, 10, 10])
-----------------------------------------

-----------------------------------------

I am [3, 1]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([2, 3, 34, 42])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 32, 40])
padding info :: [2, 0, 0, 2]
grad_input old torch.Size([2, 3, 34, 42])
grad_input new torch.Size([2, 3, 34, 42])
grad_input torch.Size([2, 3, 34, 42])
grad_out size torch.Size([2, 3, 31, 39])
crop [10, 19, 21, 0] [0, 38, 9, 39] [10, 19, 30, 39]
## 21 31 10 20
A 30 40 10 20
B 30 42 10 22
csaved input torch.Size([2, 3, 12, 12])
cgrad_output torch.Size([2, 3, 10, 10])
real nontiled_activation torch.Size([2, 3, 12, 12])
real nontiled_grad_out torch.Size([2, 3, 10, 10])
-----------------------------------------

-----------------------------------------

I am [3, 1]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([2, 3, 36, 44])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 34, 42])
padding info :: [3, 1, 0, 3]
grad_input torch.Size([2, 3, 33, 40])
grad_out size torch.Size([2, 3, 32, 40])
crop [10, 20, 22, 0] [0, 39, 8, 39] [10, 19, 30, 39]
## 22 32 10 20
A 30 40 10 20
B 30 42 10 22
csaved input torch.Size([2, 3, 12, 12])
cgrad_output torch.Size([2, 3, 10, 10])
real nontiled_activation torch.Size([2, 3, 12, 12])
real nontiled_grad_out torch.Size([2, 3, 10, 10])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 66, 80])
grad_out size torch.Size([2, 3, 33, 40])
arg size torch.Size([2, 3, 33, 40])
##############grad_in in maxp torch.Size([2, 3, 66, 80])
I am [3, 1]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 68, 82])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 66, 80])
grad_input torch.Size([2, 3, 68, 82])
padding info :: [1, 1, 0, 1]
grad_input old torch.Size([2, 3, 68, 82])
grad_input new torch.Size([2, 3, 68, 82])
new grad_input torch.Size([2, 3, 68, 82])
grad_out size torch.Size([2, 3, 66, 80])
crop [20, 40, 46, 0] [0, 79, 14, 79] [20, 39, 60, 79]
## 46 66 20 40
A 60 80 20 40
B 60 82 20 42
csaved input torch.Size([2, 3, 22, 22])
cgrad_output torch.Size([2, 3, 20, 20])
real nontiled_activation torch.Size([2, 3, 22, 22])
real nontiled_grad_out torch.Size([2, 3, 20, 20])
-----------------------------------------

-----------------------------------------

I am [3, 1]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([2, 3, 70, 84])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 68, 82])
padding info :: [2, 2, 0, 2]
grad_input old torch.Size([2, 3, 70, 84])
grad_input new torch.Size([2, 3, 70, 84])
grad_input torch.Size([2, 3, 70, 84])
grad_out size torch.Size([2, 3, 67, 80])
crop [20, 40, 47, 0] [0, 79, 13, 79] [20, 39, 60, 79]
## 47 67 20 40
A 60 80 20 40
B 60 82 20 42
csaved input torch.Size([2, 3, 22, 22])
cgrad_output torch.Size([2, 3, 20, 20])
real nontiled_activation torch.Size([2, 3, 22, 22])
real nontiled_grad_out torch.Size([2, 3, 20, 20])
-----------------------------------------

-----------------------------------------

I am [3, 1]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([2, 3, 72, 86])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 70, 84])
padding info :: [3, 3, 0, 3]
grad_input torch.Size([2, 3, 69, 80])
grad_out size torch.Size([2, 3, 68, 80])
crop [20, 40, 48, 0] [0, 79, 12, 79] [20, 39, 60, 79]
## 48 68 20 40
A 60 80 20 40
B 60 82 20 42
csaved input torch.Size([2, 3, 22, 22])
cgrad_output torch.Size([2, 3, 20, 20])
real nontiled_activation torch.Size([2, 3, 22, 22])
real nontiled_grad_out torch.Size([2, 3, 20, 20])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 138, 160])
grad_out size torch.Size([2, 3, 69, 80])
arg size torch.Size([2, 3, 69, 80])
##############grad_in in maxp torch.Size([2, 3, 138, 160])
I am [3, 1]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 140, 162])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 138, 160])
grad_input torch.Size([2, 3, 140, 162])
padding info :: [1, 1, 0, 1]
grad_input old torch.Size([2, 3, 140, 162])
grad_input new torch.Size([2, 3, 140, 162])
new grad_input torch.Size([2, 3, 140, 162])
grad_out size torch.Size([2, 3, 138, 160])
crop [40, 80, 98, 0] [0, 159, 22, 159] [40, 79, 120, 159]
## 98 138 40 80
A 120 160 40 80
B 120 162 40 82
csaved input torch.Size([2, 3, 42, 42])
cgrad_output torch.Size([2, 3, 40, 40])
real nontiled_activation torch.Size([2, 3, 42, 42])
real nontiled_grad_out torch.Size([2, 3, 40, 40])
-----------------------------------------

-----------------------------------------

I am [3, 1]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([2, 3, 142, 164])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 140, 162])
padding info :: [2, 2, 0, 2]
grad_input torch.Size([2, 3, 140, 160])
grad_out size torch.Size([2, 3, 139, 160])
crop [40, 80, 99, 0] [0, 159, 21, 159] [40, 79, 120, 159]
## 99 139 40 80
A 120 160 40 80
B 120 162 40 82
csaved input torch.Size([2, 3, 42, 42])
cgrad_output torch.Size([2, 3, 40, 40])
real nontiled_activation torch.Size([2, 3, 42, 42])
real nontiled_grad_out torch.Size([2, 3, 40, 40])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 280, 320])
grad_out size torch.Size([2, 3, 140, 160])
arg size torch.Size([2, 3, 140, 160])
##############grad_in in maxp torch.Size([2, 3, 280, 320])
I am [3, 1]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 282, 322])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 280, 320])
grad_input torch.Size([2, 3, 282, 322])
padding info :: [1, 1, 0, 1]
grad_input old torch.Size([2, 3, 282, 322])
grad_input new torch.Size([2, 3, 282, 322])
new grad_input torch.Size([2, 3, 282, 322])
grad_out size torch.Size([2, 3, 280, 320])
crop [80, 160, 200, 0] [0, 319, 40, 319] [80, 159, 240, 319]
## 200 280 80 160
A 240 320 80 160
B 240 322 80 162
csaved input torch.Size([2, 3, 82, 82])
cgrad_output torch.Size([2, 3, 80, 80])
real nontiled_activation torch.Size([2, 3, 82, 82])
real nontiled_grad_out torch.Size([2, 3, 80, 80])
-----------------------------------------

-----------------------------------------

I am [3, 1]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([2, 3, 284, 324])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 282, 322])
final torch.Size([2, 3, 284, 324])
padding info :: [2, 2, 0, 2]
after crop g_in torch.Size([2, 3, 80, 80])
grad_out size torch.Size([2, 3, 281, 320])
crop [80, 160, 201, 0] [0, 319, 39, 319] [80, 159, 240, 319]
## 201 281 80 160
A 240 320 80 160
B 240 322 80 162
csaved input torch.Size([2, 3, 82, 82])
cgrad_output torch.Size([2, 3, 80, 80])
real nontiled_activation torch.Size([2, 3, 82, 82])
real nontiled_grad_out torch.Size([2, 3, 80, 80])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 12, 12])
grad_out size torch.Size([2, 3, 10, 10])
arg size torch.Size([2, 3, 6, 6])
new_grad_out torch.Size([2, 3, 6, 6])
##############grad_in in maxp torch.Size([2, 3, 12, 12])
I am [3, 0]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 14, 14])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 12, 12])
grad_input torch.Size([2, 3, 14, 14])
padding info :: [1, 0, 0, 1]
grad_input old torch.Size([2, 3, 14, 14])
grad_input new torch.Size([2, 3, 14, 14])
new grad_input torch.Size([2, 3, 14, 14])
grad_out size torch.Size([2, 3, 12, 12])
crop [0, 7, 7, 0] [0, 11, 8, 19] [0, 4, 15, 19]
## 7 12 0 5
A 15 20 0 5
B 15 22 0 7
csaved input torch.Size([2, 3, 7, 7])
cgrad_output torch.Size([2, 3, 5, 5])
real nontiled_activation torch.Size([2, 3, 7, 7])
real nontiled_grad_out torch.Size([2, 3, 5, 5])
-----------------------------------------

-----------------------------------------

I am [3, 0]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([2, 3, 16, 16])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 14, 14])
padding info :: [2, 0, 0, 2]
grad_input old torch.Size([2, 3, 16, 16])
grad_input new torch.Size([2, 3, 16, 16])
grad_input torch.Size([2, 3, 16, 16])
grad_out size torch.Size([2, 3, 13, 13])
crop [0, 8, 8, 0] [0, 12, 7, 19] [0, 4, 15, 19]
## 8 13 0 5
A 15 20 0 5
B 15 22 0 7
csaved input torch.Size([2, 3, 7, 7])
cgrad_output torch.Size([2, 3, 5, 5])
real nontiled_activation torch.Size([2, 3, 7, 7])
real nontiled_grad_out torch.Size([2, 3, 5, 5])
-----------------------------------------

-----------------------------------------

I am [3, 0]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([2, 3, 18, 18])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 16, 16])
padding info :: [3, 0, 0, 3]
grad_input torch.Size([2, 3, 15, 15])
grad_out size torch.Size([2, 3, 14, 14])
crop [0, 9, 9, 0] [0, 13, 6, 19] [0, 4, 15, 19]
## 9 14 0 5
A 15 20 0 5
B 15 22 0 7
csaved input torch.Size([2, 3, 7, 7])
cgrad_output torch.Size([2, 3, 5, 5])
real nontiled_activation torch.Size([2, 3, 7, 7])
real nontiled_grad_out torch.Size([2, 3, 5, 5])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 30, 30])
grad_out size torch.Size([2, 3, 15, 15])
arg size torch.Size([2, 3, 15, 15])
##############grad_in in maxp torch.Size([2, 3, 30, 30])
I am [3, 0]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 32, 32])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 30, 30])
grad_input torch.Size([2, 3, 32, 32])
padding info :: [1, 0, 0, 1]
grad_input old torch.Size([2, 3, 32, 32])
grad_input new torch.Size([2, 3, 32, 32])
new grad_input torch.Size([2, 3, 32, 32])
grad_out size torch.Size([2, 3, 30, 30])
crop [0, 20, 20, 0] [0, 29, 10, 39] [0, 9, 30, 39]
## 20 30 0 10
A 30 40 0 10
B 30 42 0 12
csaved input torch.Size([2, 3, 12, 12])
cgrad_output torch.Size([2, 3, 10, 10])
real nontiled_activation torch.Size([2, 3, 12, 12])
real nontiled_grad_out torch.Size([2, 3, 10, 10])
-----------------------------------------

-----------------------------------------

I am [3, 0]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([2, 3, 34, 34])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 32, 32])
padding info :: [2, 0, 0, 2]
grad_input old torch.Size([2, 3, 34, 34])
grad_input new torch.Size([2, 3, 34, 34])
grad_input torch.Size([2, 3, 34, 34])
grad_out size torch.Size([2, 3, 31, 31])
crop [0, 21, 21, 0] [0, 30, 9, 39] [0, 9, 30, 39]
## 21 31 0 10
A 30 40 0 10
B 30 42 0 12
csaved input torch.Size([2, 3, 12, 12])
cgrad_output torch.Size([2, 3, 10, 10])
real nontiled_activation torch.Size([2, 3, 12, 12])
real nontiled_grad_out torch.Size([2, 3, 10, 10])
-----------------------------------------

-----------------------------------------

I am [3, 0]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([2, 3, 36, 36])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 34, 34])
padding info :: [3, 0, 0, 3]
grad_input torch.Size([2, 3, 33, 33])
grad_out size torch.Size([2, 3, 32, 32])
crop [0, 22, 22, 0] [0, 31, 8, 39] [0, 9, 30, 39]
## 22 32 0 10
A 30 40 0 10
B 30 42 0 12
csaved input torch.Size([2, 3, 12, 12])
cgrad_output torch.Size([2, 3, 10, 10])
real nontiled_activation torch.Size([2, 3, 12, 12])
real nontiled_grad_out torch.Size([2, 3, 10, 10])
-----------------------------------------

engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/cs-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 66, 66])
grad_out size torch.Size([2, 3, 33, 33])
arg size torch.Size([2, 3, 33, 33])
##############grad_in in maxp torch.Size([2, 3, 66, 66])
I am [3, 0]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 68, 68])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 66, 66])
grad_input torch.Size([2, 3, 68, 68])
padding info :: [1, 0, 0, 1]
grad_input old torch.Size([2, 3, 68, 68])
grad_input new torch.Size([2, 3, 68, 68])
new grad_input torch.Size([2, 3, 68, 68])
grad_out size torch.Size([2, 3, 66, 66])
crop [0, 46, 46, 0] [0, 65, 14, 79] [0, 19, 60, 79]
## 46 66 0 20
A 60 80 0 20
B 60 82 0 22
csaved input torch.Size([2, 3, 22, 22])
cgrad_output torch.Size([2, 3, 20, 20])
real nontiled_activation torch.Size([2, 3, 22, 22])
real nontiled_grad_out torch.Size([2, 3, 20, 20])
-----------------------------------------

-----------------------------------------

I am [3, 0]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([2, 3, 70, 70])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 68, 68])
padding info :: [2, 0, 0, 2]
grad_input old torch.Size([2, 3, 70, 70])
grad_input new torch.Size([2, 3, 70, 70])
grad_input torch.Size([2, 3, 70, 70])
grad_out size torch.Size([2, 3, 67, 67])
crop [0, 47, 47, 0] [0, 66, 13, 79] [0, 19, 60, 79]
## 47 67 0 20
A 60 80 0 20
B 60 82 0 22
csaved input torch.Size([2, 3, 22, 22])
cgrad_output torch.Size([2, 3, 20, 20])
real nontiled_activation torch.Size([2, 3, 22, 22])
real nontiled_grad_out torch.Size([2, 3, 20, 20])
-----------------------------------------

-----------------------------------------

I am [3, 0]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([2, 3, 72, 72])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 70, 70])
padding info :: [3, 0, 0, 3]
grad_input torch.Size([2, 3, 69, 69])
grad_out size torch.Size([2, 3, 68, 68])
crop [0, 48, 48, 0] [0, 67, 12, 79] [0, 19, 60, 79]
## 48 68 0 20
A 60 80 0 20
B 60 82 0 22
csaved input torch.Size([2, 3, 22, 22])
cgrad_output torch.Size([2, 3, 20, 20])
real nontiled_activation torch.Size([2, 3, 22, 22])
real nontiled_grad_out torch.Size([2, 3, 20, 20])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 138, 138])
grad_out size torch.Size([2, 3, 69, 69])
arg size torch.Size([2, 3, 69, 69])
##############grad_in in maxp torch.Size([2, 3, 138, 138])
I am [3, 0]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 140, 140])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 138, 138])
grad_input torch.Size([2, 3, 140, 140])
padding info :: [1, 0, 0, 1]
grad_input old torch.Size([2, 3, 140, 140])
grad_input new torch.Size([2, 3, 140, 140])
new grad_input torch.Size([2, 3, 140, 140])
grad_out size torch.Size([2, 3, 138, 138])
crop [0, 98, 98, 0] [0, 137, 22, 159] [0, 39, 120, 159]
## 98 138 0 40
A 120 160 0 40
B 120 162 0 42
csaved input torch.Size([2, 3, 42, 42])
cgrad_output torch.Size([2, 3, 40, 40])
real nontiled_activation torch.Size([2, 3, 42, 42])
real nontiled_grad_out torch.Size([2, 3, 40, 40])
-----------------------------------------

-----------------------------------------

I am [3, 0]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([2, 3, 142, 142])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 140, 140])
padding info :: [2, 0, 0, 2]
grad_input torch.Size([2, 3, 140, 140])
grad_out size torch.Size([2, 3, 139, 139])
crop [0, 99, 99, 0] [0, 138, 21, 159] [0, 39, 120, 159]
## 99 139 0 40
A 120 160 0 40
B 120 162 0 42
csaved input torch.Size([2, 3, 42, 42])
cgrad_output torch.Size([2, 3, 40, 40])
real nontiled_activation torch.Size([2, 3, 42, 42])
real nontiled_grad_out torch.Size([2, 3, 40, 40])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 280, 280])
grad_out size torch.Size([2, 3, 140, 140])
arg size torch.Size([2, 3, 140, 140])
##############grad_in in maxp torch.Size([2, 3, 280, 280])
I am [3, 0]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 282, 282])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 280, 280])
grad_input torch.Size([2, 3, 282, 282])
padding info :: [1, 0, 0, 1]
grad_input old torch.Size([2, 3, 282, 282])
grad_input new torch.Size([2, 3, 282, 282])
new grad_input torch.Size([2, 3, 282, 282])
grad_out size torch.Size([2, 3, 280, 280])
crop [0, 200, 200, 0] [0, 279, 40, 319] [0, 79, 240, 319]
## 200 280 0 80
A 240 320 0 80
B 240 322 0 82
csaved input torch.Size([2, 3, 82, 82])
cgrad_output torch.Size([2, 3, 80, 80])
real nontiled_activation torch.Size([2, 3, 82, 82])
real nontiled_grad_out torch.Size([2, 3, 80, 80])
-----------------------------------------

-----------------------------------------

I am [3, 0]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([2, 3, 284, 284])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 282, 282])
final torch.Size([2, 3, 284, 284])
padding info :: [2, 0, 0, 2]
after crop g_in torch.Size([2, 3, 80, 80])
grad_out size torch.Size([2, 3, 281, 281])
crop [0, 201, 201, 0] [0, 280, 39, 319] [0, 79, 240, 319]
## 201 281 0 80
A 240 320 0 80
B 240 322 0 82
csaved input torch.Size([2, 3, 82, 82])
cgrad_output torch.Size([2, 3, 80, 80])
real nontiled_activation torch.Size([2, 3, 82, 82])
real nontiled_grad_out torch.Size([2, 3, 80, 80])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 16, 12])
grad_out size torch.Size([2, 3, 10, 10])
arg size torch.Size([2, 3, 8, 6])
new_grad_out torch.Size([2, 3, 8, 6])
##############grad_in in maxp torch.Size([2, 3, 16, 12])
I am [2, 3]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 18, 14])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 16, 12])
grad_input torch.Size([2, 3, 18, 14])
padding info :: [0, 1, 0, 1]
grad_input old torch.Size([2, 3, 18, 14])
grad_input new torch.Size([2, 3, 18, 14])
new grad_input torch.Size([2, 3, 18, 14])
grad_out size torch.Size([2, 3, 16, 12])
crop [7, 0, 6, 5] [8, 19, 4, 19] [15, 19, 10, 14]
## 6 11 7 12
A 10 15 15 20
B 10 17 15 22
csaved input torch.Size([2, 3, 7, 7])
cgrad_output torch.Size([2, 3, 5, 5])
real nontiled_activation torch.Size([2, 3, 7, 7])
real nontiled_grad_out torch.Size([2, 3, 5, 5])
-----------------------------------------

-----------------------------------------

I am [2, 3]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([2, 3, 20, 16])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 18, 14])
padding info :: [0, 2, 0, 2]
grad_input old torch.Size([2, 3, 20, 16])
grad_input new torch.Size([2, 3, 20, 16])
grad_input torch.Size([2, 3, 20, 16])
grad_out size torch.Size([2, 3, 17, 13])
crop [8, 0, 7, 5] [7, 19, 3, 19] [15, 19, 10, 14]
## 7 12 8 13
A 10 15 15 20
B 10 17 15 22
csaved input torch.Size([2, 3, 7, 7])
cgrad_output torch.Size([2, 3, 5, 5])
real nontiled_activation torch.Size([2, 3, 7, 7])
real nontiled_grad_out torch.Size([2, 3, 5, 5])
-----------------------------------------

-----------------------------------------

I am [2, 3]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([2, 3, 22, 18])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 20, 16])
padding info :: [0, 3, 0, 3]
grad_input torch.Size([2, 3, 19, 15])
grad_out size torch.Size([2, 3, 18, 14])
crop [9, 0, 8, 5] [6, 19, 2, 19] [15, 19, 10, 14]
## 8 13 9 14
A 10 15 15 20
B 10 17 15 22
csaved input torch.Size([2, 3, 7, 7])
cgrad_output torch.Size([2, 3, 5, 5])
real nontiled_activation torch.Size([2, 3, 7, 7])
real nontiled_grad_out torch.Size([2, 3, 5, 5])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 38, 30])
grad_out size torch.Size([2, 3, 19, 15])
arg size torch.Size([2, 3, 19, 15])
##############grad_in in maxp torch.Size([2, 3, 38, 30])
I am [2, 3]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 40, 32])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 38, 30])
grad_input torch.Size([2, 3, 40, 32])
padding info :: [0, 1, 0, 1]
grad_input old torch.Size([2, 3, 40, 32])
grad_input new torch.Size([2, 3, 40, 32])
new grad_input torch.Size([2, 3, 40, 32])
grad_out size torch.Size([2, 3, 38, 30])
crop [20, 0, 18, 10] [10, 39, 2, 39] [30, 39, 20, 29]
## 18 28 20 30
A 20 30 30 40
B 20 32 30 42
csaved input torch.Size([2, 3, 12, 12])
cgrad_output torch.Size([2, 3, 10, 10])
real nontiled_activation torch.Size([2, 3, 12, 12])
real nontiled_grad_out torch.Size([2, 3, 10, 10])
-----------------------------------------

-----------------------------------------

I am [2, 3]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([2, 3, 42, 34])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 40, 32])
padding info :: [0, 2, 0, 2]
grad_input old torch.Size([2, 3, 42, 34])
grad_input new torch.Size([2, 3, 42, 34])
grad_input torch.Size([2, 3, 42, 34])
grad_out size torch.Size([2, 3, 39, 31])
crop [21, 0, 19, 10] [9, 39, 1, 39] [30, 39, 20, 29]
## 19 29 21 31
A 20 30 30 40
B 20 32 30 42
csaved input torch.Size([2, 3, 12, 12])
cgrad_output torch.Size([2, 3, 10, 10])
real nontiled_activation torch.Size([2, 3, 12, 12])
real nontiled_grad_out torch.Size([2, 3, 10, 10])
-----------------------------------------

-----------------------------------------

I am [2, 3]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([2, 3, 44, 36])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 42, 34])
padding info :: [0, 3, 1, 3]
grad_input torch.Size([2, 3, 40, 33])
grad_out size torch.Size([2, 3, 40, 32])
crop [22, 0, 20, 10] [8, 39, 0, 39] [30, 39, 20, 29]
## 20 30 22 32
A 20 30 30 40
B 20 32 30 42
csaved input torch.Size([2, 3, 12, 12])
cgrad_output torch.Size([2, 3, 10, 10])
real nontiled_activation torch.Size([2, 3, 12, 12])
real nontiled_grad_out torch.Size([2, 3, 10, 10])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 80, 66])
grad_out size torch.Size([2, 3, 40, 33])
arg size torch.Size([2, 3, 40, 33])
##############grad_in in maxp torch.Size([2, 3, 80, 66])
I am [2, 3]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 82, 68])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 80, 66])
grad_input torch.Size([2, 3, 82, 68])
padding info :: [0, 1, 1, 1]
grad_input old torch.Size([2, 3, 82, 68])
grad_input new torch.Size([2, 3, 82, 68])
new grad_input torch.Size([2, 3, 82, 68])
grad_out size torch.Size([2, 3, 80, 66])
crop [46, 0, 40, 20] [14, 79, 0, 79] [60, 79, 40, 59]
## 40 60 46 66
A 40 60 60 80
B 40 62 60 82
csaved input torch.Size([2, 3, 22, 22])
cgrad_output torch.Size([2, 3, 20, 20])
real nontiled_activation torch.Size([2, 3, 22, 22])
real nontiled_grad_out torch.Size([2, 3, 20, 20])
-----------------------------------------

-----------------------------------------

I am [2, 3]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([2, 3, 84, 70])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 82, 68])
padding info :: [0, 2, 2, 2]
grad_input old torch.Size([2, 3, 84, 70])
grad_input new torch.Size([2, 3, 84, 70])
grad_input torch.Size([2, 3, 84, 70])
grad_out size torch.Size([2, 3, 80, 67])
crop [47, 0, 40, 20] [13, 79, 0, 79] [60, 79, 40, 59]
## 40 60 47 67
A 40 60 60 80
B 40 62 60 82
csaved input torch.Size([2, 3, 22, 22])
cgrad_output torch.Size([2, 3, 20, 20])
real nontiled_activation torch.Size([2, 3, 22, 22])
real nontiled_grad_out torch.Size([2, 3, 20, 20])
-----------------------------------------

-----------------------------------------

I am [2, 3]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([2, 3, 86, 72])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 84, 70])
padding info :: [0, 3, 3, 3]
grad_input torch.Size([2, 3, 80, 69])
grad_out size torch.Size([2, 3, 80, 68])
crop [48, 0, 40, 20] [12, 79, 0, 79] [60, 79, 40, 59]
## 40 60 48 68
A 40 60 60 80
B 40 62 60 82
csaved input torch.Size([2, 3, 22, 22])
cgrad_output torch.Size([2, 3, 20, 20])
real nontiled_activation torch.Size([2, 3, 22, 22])
real nontiled_grad_out torch.Size([2, 3, 20, 20])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 160, 138])
grad_out size torch.Size([2, 3, 80, 69])
arg size torch.Size([2, 3, 80, 69])
##############grad_in in maxp torch.Size([2, 3, 160, 138])
I am [2, 3]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 162, 140])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 160, 138])
grad_input torch.Size([2, 3, 162, 140])
padding info :: [0, 1, 1, 1]
grad_input old torch.Size([2, 3, 162, 140])
grad_input new torch.Size([2, 3, 162, 140])
new grad_input torch.Size([2, 3, 162, 140])
grad_out size torch.Size([2, 3, 160, 138])
crop [98, 0, 80, 40] [22, 159, 0, 159] [120, 159, 80, 119]
## 80 120 98 138
A 80 120 120 160
B 80 122 120 162
csaved input torch.Size([2, 3, 42, 42])
cgrad_output torch.Size([2, 3, 40, 40])
real nontiled_activation torch.Size([2, 3, 42, 42])
real nontiled_grad_out torch.Size([2, 3, 40, 40])
-----------------------------------------

-----------------------------------------

I am [2, 3]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([2, 3, 164, 142])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 162, 140])
padding info :: [0, 2, 2, 2]
grad_input torch.Size([2, 3, 160, 140])
grad_out size torch.Size([2, 3, 160, 139])
crop [99, 0, 80, 40] [21, 159, 0, 159] [120, 159, 80, 119]
## 80 120 99 139
A 80 120 120 160
B 80 122 120 162
csaved input torch.Size([2, 3, 42, 42])
cgrad_output torch.Size([2, 3, 40, 40])
real nontiled_activation torch.Size([2, 3, 42, 42])
real nontiled_grad_out torch.Size([2, 3, 40, 40])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 320, 280])
grad_out size torch.Size([2, 3, 160, 140])
arg size torch.Size([2, 3, 160, 140])
##############grad_in in maxp torch.Size([2, 3, 320, 280])
I am [2, 3]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 322, 282])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 320, 280])
grad_input torch.Size([2, 3, 322, 282])
padding info :: [0, 1, 1, 1]
grad_input old torch.Size([2, 3, 322, 282])
grad_input new torch.Size([2, 3, 322, 282])
new grad_input torch.Size([2, 3, 322, 282])
grad_out size torch.Size([2, 3, 320, 280])
crop [200, 0, 160, 80] [40, 319, 0, 319] [240, 319, 160, 239]
## 160 240 200 280
A 160 240 240 320
B 160 242 240 322
csaved input torch.Size([2, 3, 82, 82])
cgrad_output torch.Size([2, 3, 80, 80])
real nontiled_activation torch.Size([2, 3, 82, 82])
real nontiled_grad_out torch.Size([2, 3, 80, 80])
-----------------------------------------

-----------------------------------------

I am [2, 3]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([2, 3, 324, 284])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 322, 282])
final torch.Size([2, 3, 324, 284])
padding info :: [0, 2, 2, 2]
after crop g_in torch.Size([2, 3, 80, 80])
grad_out size torch.Size([2, 3, 320, 281])
crop [201, 0, 160, 80] [39, 319, 0, 319] [240, 319, 160, 239]
## 160 240 201 281
A 160 240 240 320
B 160 242 240 322
csaved input torch.Size([2, 3, 82, 82])
cgrad_output torch.Size([2, 3, 80, 80])
real nontiled_activation torch.Size([2, 3, 82, 82])
real nontiled_grad_out torch.Size([2, 3, 80, 80])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 16, 16])
grad_out size torch.Size([2, 3, 10, 10])
arg size torch.Size([2, 3, 8, 8])
new_grad_out torch.Size([2, 3, 8, 8])
##############grad_in in maxp torch.Size([2, 3, 16, 16])
I am [2, 2]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 18, 18])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 16, 16])
grad_input torch.Size([2, 3, 18, 18])
padding info :: [0, 1, 0, 1]
grad_input old torch.Size([2, 3, 18, 18])
grad_input new torch.Size([2, 3, 18, 18])
new grad_input torch.Size([2, 3, 18, 18])
grad_out size torch.Size([2, 3, 16, 16])
crop [6, 5, 6, 5] [4, 19, 4, 19] [10, 14, 10, 14]
## 6 11 6 11
A 10 15 10 15
B 10 17 10 17
csaved input torch.Size([2, 3, 7, 7])
cgrad_output torch.Size([2, 3, 5, 5])
real nontiled_activation torch.Size([2, 3, 7, 7])
real nontiled_grad_out torch.Size([2, 3, 5, 5])
-----------------------------------------

-----------------------------------------

I am [2, 2]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([2, 3, 20, 20])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 18, 18])
padding info :: [0, 2, 0, 2]
grad_input old torch.Size([2, 3, 20, 20])
grad_input new torch.Size([2, 3, 20, 20])
grad_input torch.Size([2, 3, 20, 20])
grad_out size torch.Size([2, 3, 17, 17])
crop [7, 5, 7, 5] [3, 19, 3, 19] [10, 14, 10, 14]
## 7 12 7 12
A 10 15 10 15
B 10 17 10 17
csaved input torch.Size([2, 3, 7, 7])
cgrad_output torch.Size([2, 3, 5, 5])
real nontiled_activation torch.Size([2, 3, 7, 7])
real nontiled_grad_out torch.Size([2, 3, 5, 5])
-----------------------------------------

-----------------------------------------

I am [2, 2]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([2, 3, 22, 22])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 20, 20])
padding info :: [0, 3, 0, 3]
grad_input torch.Size([2, 3, 19, 19])
grad_out size torch.Size([2, 3, 18, 18])
crop [8, 5, 8, 5] [2, 19, 2, 19] [10, 14, 10, 14]
## 8 13 8 13
A 10 15 10 15
B 10 17 10 17
csaved input torch.Size([2, 3, 7, 7])
cgrad_output torch.Size([2, 3, 5, 5])
real nontiled_activation torch.Size([2, 3, 7, 7])
real nontiled_grad_out torch.Size([2, 3, 5, 5])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 38, 38])
grad_out size torch.Size([2, 3, 19, 19])
arg size torch.Size([2, 3, 19, 19])
##############grad_in in maxp torch.Size([2, 3, 38, 38])
I am [2, 2]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 40, 40])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 38, 38])
grad_input torch.Size([2, 3, 40, 40])
padding info :: [0, 1, 0, 1]
grad_input old torch.Size([2, 3, 40, 40])
grad_input new torch.Size([2, 3, 40, 40])
new grad_input torch.Size([2, 3, 40, 40])
grad_out size torch.Size([2, 3, 38, 38])
crop [18, 10, 18, 10] [2, 39, 2, 39] [20, 29, 20, 29]
## 18 28 18 28
A 20 30 20 30
B 20 32 20 32
csaved input torch.Size([2, 3, 12, 12])
cgrad_output torch.Size([2, 3, 10, 10])
real nontiled_activation torch.Size([2, 3, 12, 12])
real nontiled_grad_out torch.Size([2, 3, 10, 10])
-----------------------------------------

-----------------------------------------

I am [2, 2]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([2, 3, 42, 42])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 40, 40])
padding info :: [0, 2, 0, 2]
grad_input old torch.Size([2, 3, 42, 42])
grad_input new torch.Size([2, 3, 42, 42])
grad_input torch.Size([2, 3, 42, 42])
grad_out size torch.Size([2, 3, 39, 39])
crop [19, 10, 19, 10] [1, 39, 1, 39] [20, 29, 20, 29]
## 19 29 19 29
A 20 30 20 30
B 20 32 20 32
csaved input torch.Size([2, 3, 12, 12])
cgrad_output torch.Size([2, 3, 10, 10])
real nontiled_activation torch.Size([2, 3, 12, 12])
real nontiled_grad_out torch.Size([2, 3, 10, 10])
-----------------------------------------

-----------------------------------------

I am [2, 2]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([2, 3, 44, 44])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 42, 42])
padding info :: [1, 3, 1, 3]
grad_input torch.Size([2, 3, 40, 40])
grad_out size torch.Size([2, 3, 40, 40])
crop [20, 10, 20, 10] [0, 39, 0, 39] [20, 29, 20, 29]
## 20 30 20 30
A 20 30 20 30
B 20 32 20 32
csaved input torch.Size([2, 3, 12, 12])
cgrad_output torch.Size([2, 3, 10, 10])
real nontiled_activation torch.Size([2, 3, 12, 12])
real nontiled_grad_out torch.Size([2, 3, 10, 10])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 80, 80])
grad_out size torch.Size([2, 3, 40, 40])
arg size torch.Size([2, 3, 40, 40])
##############grad_in in maxp torch.Size([2, 3, 80, 80])
I am [2, 2]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 82, 82])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 80, 80])
grad_input torch.Size([2, 3, 82, 82])
padding info :: [1, 1, 1, 1]
grad_input old torch.Size([2, 3, 82, 82])
grad_input new torch.Size([2, 3, 82, 82])
new grad_input torch.Size([2, 3, 82, 82])
grad_out size torch.Size([2, 3, 80, 80])
crop [40, 20, 40, 20] [0, 79, 0, 79] [40, 59, 40, 59]
## 40 60 40 60
A 40 60 40 60
B 40 62 40 62
csaved input torch.Size([2, 3, 22, 22])
cgrad_output torch.Size([2, 3, 20, 20])
real nontiled_activation torch.Size([2, 3, 22, 22])
real nontiled_grad_out torch.Size([2, 3, 20, 20])
-----------------------------------------

-----------------------------------------

I am [2, 2]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([2, 3, 84, 84])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 82, 82])
padding info :: [2, 2, 2, 2]
grad_input old torch.Size([2, 3, 84, 84])
grad_input new torch.Size([2, 3, 84, 84])
grad_input torch.Size([2, 3, 84, 84])
grad_out size torch.Size([2, 3, 80, 80])
crop [40, 20, 40, 20] [0, 79, 0, 79] [40, 59, 40, 59]
## 40 60 40 60
A 40 60 40 60
B 40 62 40 62
csaved input torch.Size([2, 3, 22, 22])
cgrad_output torch.Size([2, 3, 20, 20])
real nontiled_activation torch.Size([2, 3, 22, 22])
real nontiled_grad_out torch.Size([2, 3, 20, 20])
-----------------------------------------

-----------------------------------------

I am [2, 2]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([2, 3, 86, 86])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 84, 84])
padding info :: [3, 3, 3, 3]
grad_input torch.Size([2, 3, 80, 80])
grad_out size torch.Size([2, 3, 80, 80])
crop [40, 20, 40, 20] [0, 79, 0, 79] [40, 59, 40, 59]
## 40 60 40 60
A 40 60 40 60
B 40 62 40 62
csaved input torch.Size([2, 3, 22, 22])
cgrad_output torch.Size([2, 3, 20, 20])
real nontiled_activation torch.Size([2, 3, 22, 22])
real nontiled_grad_out torch.Size([2, 3, 20, 20])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 160, 160])
grad_out size torch.Size([2, 3, 80, 80])
arg size torch.Size([2, 3, 80, 80])
##############grad_in in maxp torch.Size([2, 3, 160, 160])
I am [2, 2]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 162, 162])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 160, 160])
grad_input torch.Size([2, 3, 162, 162])
padding info :: [1, 1, 1, 1]
grad_input old torch.Size([2, 3, 162, 162])
grad_input new torch.Size([2, 3, 162, 162])
new grad_input torch.Size([2, 3, 162, 162])
grad_out size torch.Size([2, 3, 160, 160])
crop [80, 40, 80, 40] [0, 159, 0, 159] [80, 119, 80, 119]
## 80 120 80 120
A 80 120 80 120
B 80 122 80 122
csaved input torch.Size([2, 3, 42, 42])
cgrad_output torch.Size([2, 3, 40, 40])
real nontiled_activation torch.Size([2, 3, 42, 42])
real nontiled_grad_out torch.Size([2, 3, 40, 40])
-----------------------------------------

-----------------------------------------

I am [2, 2]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([2, 3, 164, 164])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 162, 162])
padding info :: [2, 2, 2, 2]
grad_input torch.Size([2, 3, 160, 160])
grad_out size torch.Size([2, 3, 160, 160])
crop [80, 40, 80, 40] [0, 159, 0, 159] [80, 119, 80, 119]
## 80 120 80 120
A 80 120 80 120
B 80 122 80 122
csaved input torch.Size([2, 3, 42, 42])
cgrad_output torch.Size([2, 3, 40, 40])
real nontiled_activation torch.Size([2, 3, 42, 42])
real nontiled_grad_out torch.Size([2, 3, 40, 40])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 320, 320])
grad_out size torch.Size([2, 3, 160, 160])
arg size torch.Size([2, 3, 160, 160])
##############grad_in in maxp torch.Size([2, 3, 320, 320])
I am [2, 2]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 322, 322])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 320, 320])
grad_input torch.Size([2, 3, 322, 322])
padding info :: [1, 1, 1, 1]
grad_input old torch.Size([2, 3, 322, 322])
grad_input new torch.Size([2, 3, 322, 322])
new grad_input torch.Size([2, 3, 322, 322])
grad_out size torch.Size([2, 3, 320, 320])
crop [160, 80, 160, 80] [0, 319, 0, 319] [160, 239, 160, 239]
## 160 240 160 240
A 160 240 160 240
B 160 242 160 242
csaved input torch.Size([2, 3, 82, 82])
cgrad_output torch.Size([2, 3, 80, 80])
real nontiled_activation torch.Size([2, 3, 82, 82])
real nontiled_grad_out torch.Size([2, 3, 80, 80])
-----------------------------------------

-----------------------------------------

I am [2, 2]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([2, 3, 324, 324])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 322, 322])
final torch.Size([2, 3, 324, 324])
padding info :: [2, 2, 2, 2]
after crop g_in torch.Size([2, 3, 80, 80])
grad_out size torch.Size([2, 3, 320, 320])
crop [160, 80, 160, 80] [0, 319, 0, 319] [160, 239, 160, 239]
## 160 240 160 240
A 160 240 160 240
B 160 242 160 242
csaved input torch.Size([2, 3, 82, 82])
cgrad_output torch.Size([2, 3, 80, 80])
real nontiled_activation torch.Size([2, 3, 82, 82])
real nontiled_grad_out torch.Size([2, 3, 80, 80])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 16, 16])
grad_out size torch.Size([2, 3, 10, 10])
arg size torch.Size([2, 3, 8, 8])
new_grad_out torch.Size([2, 3, 8, 8])
##############grad_in in maxp torch.Size([2, 3, 16, 16])
I am [2, 1]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 18, 18])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 16, 16])
grad_input torch.Size([2, 3, 18, 18])
padding info :: [1, 0, 0, 1]
grad_input old torch.Size([2, 3, 18, 18])
grad_input new torch.Size([2, 3, 18, 18])
new grad_input torch.Size([2, 3, 18, 18])
grad_out size torch.Size([2, 3, 16, 16])
crop [5, 6, 6, 5] [0, 15, 4, 19] [5, 9, 10, 14]
## 6 11 5 10
A 10 15 5 10
B 10 17 5 12
csaved input torch.Size([2, 3, 7, 7])
cgrad_output torch.Size([2, 3, 5, 5])
real nontiled_activation torch.Size([2, 3, 7, 7])
real nontiled_grad_out torch.Size([2, 3, 5, 5])
-----------------------------------------

-----------------------------------------

I am [2, 1]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([2, 3, 20, 20])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 18, 18])
padding info :: [2, 0, 0, 2]
grad_input old torch.Size([2, 3, 20, 20])
grad_input new torch.Size([2, 3, 20, 20])
grad_input torch.Size([2, 3, 20, 20])
grad_out size torch.Size([2, 3, 17, 17])
crop [5, 7, 7, 5] [0, 16, 3, 19] [5, 9, 10, 14]
## 7 12 5 10
A 10 15 5 10
B 10 17 5 12
csaved input torch.Size([2, 3, 7, 7])
cgrad_output torch.Size([2, 3, 5, 5])
real nontiled_activation torch.Size([2, 3, 7, 7])
real nontiled_grad_out torch.Size([2, 3, 5, 5])
-----------------------------------------

-----------------------------------------

I am [2, 1]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([2, 3, 22, 22])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 20, 20])
padding info :: [3, 0, 0, 3]
grad_input torch.Size([2, 3, 19, 19])
grad_out size torch.Size([2, 3, 18, 18])
crop [5, 8, 8, 5] [0, 17, 2, 19] [5, 9, 10, 14]
## 8 13 5 10
A 10 15 5 10
B 10 17 5 12
csaved input torch.Size([2, 3, 7, 7])
cgrad_output torch.Size([2, 3, 5, 5])
real nontiled_activation torch.Size([2, 3, 7, 7])
real nontiled_grad_out torch.Size([2, 3, 5, 5])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 38, 38])
grad_out size torch.Size([2, 3, 19, 19])
arg size torch.Size([2, 3, 19, 19])
##############grad_in in maxp torch.Size([2, 3, 38, 38])
I am [2, 1]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 40, 40])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 38, 38])
grad_input torch.Size([2, 3, 40, 40])
padding info :: [1, 0, 0, 1]
grad_input old torch.Size([2, 3, 40, 40])
grad_input new torch.Size([2, 3, 40, 40])
new grad_input torch.Size([2, 3, 40, 40])
grad_out size torch.Size([2, 3, 38, 38])
crop [10, 18, 18, 10] [0, 37, 2, 39] [10, 19, 20, 29]
## 18 28 10 20
A 20 30 10 20
B 20 32 10 22
csaved input torch.Size([2, 3, 12, 12])
cgrad_output torch.Size([2, 3, 10, 10])
real nontiled_activation torch.Size([2, 3, 12, 12])
real nontiled_grad_out torch.Size([2, 3, 10, 10])
-----------------------------------------

-----------------------------------------

I am [2, 1]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([2, 3, 42, 42])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 40, 40])
padding info :: [2, 0, 0, 2]
grad_input old torch.Size([2, 3, 42, 42])
grad_input new torch.Size([2, 3, 42, 42])
grad_input torch.Size([2, 3, 42, 42])
grad_out size torch.Size([2, 3, 39, 39])
crop [10, 19, 19, 10] [0, 38, 1, 39] [10, 19, 20, 29]
## 19 29 10 20
A 20 30 10 20
B 20 32 10 22
csaved input torch.Size([2, 3, 12, 12])
cgrad_output torch.Size([2, 3, 10, 10])
real nontiled_activation torch.Size([2, 3, 12, 12])
real nontiled_grad_out torch.Size([2, 3, 10, 10])
-----------------------------------------

-----------------------------------------

I am [2, 1]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([2, 3, 44, 44])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 42, 42])
padding info :: [3, 1, 1, 3]
grad_input torch.Size([2, 3, 40, 40])
grad_out size torch.Size([2, 3, 40, 40])
crop [10, 20, 20, 10] [0, 39, 0, 39] [10, 19, 20, 29]
## 20 30 10 20
A 20 30 10 20
B 20 32 10 22
csaved input torch.Size([2, 3, 12, 12])
cgrad_output torch.Size([2, 3, 10, 10])
real nontiled_activation torch.Size([2, 3, 12, 12])
real nontiled_grad_out torch.Size([2, 3, 10, 10])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 80, 80])
grad_out size torch.Size([2, 3, 40, 40])
arg size torch.Size([2, 3, 40, 40])
##############grad_in in maxp torch.Size([2, 3, 80, 80])
I am [2, 1]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 82, 82])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 80, 80])
grad_input torch.Size([2, 3, 82, 82])
padding info :: [1, 1, 1, 1]
grad_input old torch.Size([2, 3, 82, 82])
grad_input new torch.Size([2, 3, 82, 82])
new grad_input torch.Size([2, 3, 82, 82])
grad_out size torch.Size([2, 3, 80, 80])
crop [20, 40, 40, 20] [0, 79, 0, 79] [20, 39, 40, 59]
## 40 60 20 40
A 40 60 20 40
B 40 62 20 42
csaved input torch.Size([2, 3, 22, 22])
cgrad_output torch.Size([2, 3, 20, 20])
real nontiled_activation torch.Size([2, 3, 22, 22])
real nontiled_grad_out torch.Size([2, 3, 20, 20])
-----------------------------------------

-----------------------------------------

I am [2, 1]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([2, 3, 84, 84])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 82, 82])
padding info :: [2, 2, 2, 2]
grad_input old torch.Size([2, 3, 84, 84])
grad_input new torch.Size([2, 3, 84, 84])
grad_input torch.Size([2, 3, 84, 84])
grad_out size torch.Size([2, 3, 80, 80])
crop [20, 40, 40, 20] [0, 79, 0, 79] [20, 39, 40, 59]
## 40 60 20 40
A 40 60 20 40
B 40 62 20 42
csaved input torch.Size([2, 3, 22, 22])
cgrad_output torch.Size([2, 3, 20, 20])
rc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackwareal nontiled_activation torch.Size([2, 3, 22, 22])
real nontiled_grad_out torch.Size([2, 3, 20, 20])
-----------------------------------------

-----------------------------------------

I am [2, 1]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([2, 3, 86, 86])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 84, 84])
padding info :: [3, 3, 3, 3]
grad_input torch.Size([2, 3, 80, 80])
grad_out size torch.Size([2, 3, 80, 80])
crop [20, 40, 40, 20] [0, 79, 0, 79] [20, 39, 40, 59]
## 40 60 20 40
A 40 60 20 40
B 40 62 20 42
csaved input torch.Size([2, 3, 22, 22])
cgrad_output torch.Size([2, 3, 20, 20])
real nontiled_activation torch.Size([2, 3, 22, 22])
real nontiled_grad_out torch.Size([2, 3, 20, 20])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 160, 160])
grad_out size torch.Size([2, 3, 80, 80])
arg size torch.Size([2, 3, 80, 80])
##############grad_in in maxp torch.Size([2, 3, 160, 160])
I am [2, 1]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 162, 162])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 160, 160])
grad_input torch.Size([2, 3, 162, 162])
padding info :: [1, 1, 1, 1]
grad_input old torch.Size([2, 3, 162, 162])
grad_input new torch.Size([2, 3, 162, 162])
new grad_input torch.Size([2, 3, 162, 162])
grad_out size torch.Size([2, 3, 160, 160])
crop [40, 80, 80, 40] [0, 159, 0, 159] [40, 79, 80, 119]
## 80 120 40 80
A 80 120 40 80
B 80 122 40 82
csaved input torch.Size([2, 3, 42, 42])
cgrad_output torch.Size([2, 3, 40, 40])
real nontiled_activation torch.Size([2, 3, 42, 42])
real nontiled_grad_out torch.Size([2, 3, 40, 40])
-----------------------------------------

-----------------------------------------

I am [2, 1]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([2, 3, 164, 164])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 162, 162])
padding info :: [2, 2, 2, 2]
grad_input torch.Size([2, 3, 160, 160])
grad_out size torch.Size([2, 3, 160, 160])
crop [40, 80, 80, 40] [0, 159, 0, 159] [40, 79, 80, 119]
## 80 120 40 80
A 80 120 40 80
B 80 122 40 82
csaved input torch.Size([2, 3, 42, 42])
cgrad_output torch.Size([2, 3, 40, 40])
real nontiled_activation torch.Size([2, 3, 42, 42])
real nontiled_grad_out torch.Size([2, 3, 40, 40])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 320, 320])
grad_out size torch.Size([2, 3, 160, 160])
arg size torch.Size([2, 3, 160, 160])
##############grad_in in maxp torch.Size([2, 3, 320, 320])
I am [2, 1]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 322, 322])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 320, 320])
grad_input torch.Size([2, 3, 322, 322])
padding info :: [1, 1, 1, 1]
grad_input old torch.Size([2, 3, 322, 322])
grad_input new torch.Size([2, 3, 322, 322])
new grad_input torch.Size([2, 3, 322, 322])
grad_out size torch.Size([2, 3, 320, 320])
crop [80, 160, 160, 80] [0, 319, 0, 319] [80, 159, 160, 239]
## 160 240 80 160
A 160 240 80 160
B 160 242 80 162
csaved input torch.Size([2, 3, 82, 82])
cgrad_output torch.Size([2, 3, 80, 80])
real nontiled_activation torch.Size([2, 3, 82, 82])
real nontiled_grad_out torch.Size([2, 3, 80, 80])
-----------------------------------------

-----------------------------------------

I am [2, 1]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([2, 3, 324, 324])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 322, 322])
final torch.Size([2, 3, 324, 324])
padding info :: [2, 2, 2, 2]
after crop g_in torch.Size([2, 3, 80, 80])
grad_out size torch.Size([2, 3, 320, 320])
crop [80, 160, 160, 80] [0, 319, 0, 319] [80, 159, 160, 239]
## 160 240 80 160
A 160 240 80 160
B 160 242 80 162
csaved input torch.Size([2, 3, 82, 82])
cgrad_output torch.Size([2, 3, 80, 80])
real nontiled_activation torch.Size([2, 3, 82, 82])
real nontiled_grad_out torch.Size([2, 3, 80, 80])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 16, 12])
grad_out size torch.Size([2, 3, 10, 10])
arg size torch.Size([2, 3, 8, 6])
new_grad_out torch.Size([2, 3, 8, 6])
##############grad_in in maxp torch.Size([2, 3, 16, 12])
I am [2, 0]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 18, 14])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 16, 12])
grad_input torch.Size([2, 3, 18, 14])
padding info :: [1, 0, 0, 1]
grad_input old torch.Size([2, 3, 18, 14])
grad_input new torch.Size([2, 3, 18, 14])
new grad_input torch.Size([2, 3, 18, 14])
grad_out size torch.Size([2, 3, 16, 12])
crop [0, 7, 6, 5] [0, 11, 4, 19] [0, 4, 10, 14]
## 6 11 0 5
A 10 15 0 5
B 10 17 0 7
csaved input torch.Size([2, 3, 7, 7])
cgrad_output torch.Size([2, 3, 5, 5])
real nontiled_activation torch.Size([2, 3, 7, 7])
real nontiled_grad_out torch.Size([2, 3, 5, 5])
-----------------------------------------

-----------------------------------------

I am [2, 0]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([2, 3, 20, 16])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 18, 14])
padding info :: [2, 0, 0, 2]
grad_input old torch.Size([2, 3, 20, 16])
grad_input new torch.Size([2, 3, 20, 16])
grad_input torch.Size([2, 3, 20, 16])
grad_out size torch.Size([2, 3, 17, 13])
crop [0, 8, 7, 5] [0, 12, 3, 19] [0, 4, 10, 14]
## 7 12 0 5
A 10 15 0 5
B 10 17 0 7
csaved input torch.Size([2, 3, 7, 7])
cgrad_output torch.Size([2, 3, 5, 5])
real nontiled_activation torch.Size([2, 3, 7, 7])
real nontiled_grad_out torch.Size([2, 3, 5, 5])
-----------------------------------------

-----------------------------------------

I am [2, 0]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([2, 3, 22, 18])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 20, 16])
padding info :: [3, 0, 0, 3]
grad_input torch.Size([2, 3, 19, 15])
grad_out size torch.Size([2, 3, 18, 14])
crop [0, 9, 8, 5] [0, 13, 2, 19] [0, 4, 10, 14]
## 8 13 0 5
A 10 15 0 5
B 10 17 0 7
csaved input torch.Size([2, 3, 7, 7])
cgrad_output torch.Size([2, 3, 5, 5])
real nontiled_activation torch.Size([2, 3, 7, 7])
real nontiled_grad_out torch.Size([2, 3, 5, 5])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 38, 30])
grad_out size torch.Size([2, 3, 19, 15])
arg size torch.Size([2, 3, 19, 15])
##############grad_in in maxp torch.Size([2, 3, 38, 30])
I am [2, 0]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 40, 32])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 38, 30])
grad_input torch.Size([2, 3, 40, 32])
padding info :: [1, 0, 0, 1]
grad_input old torch.Size([2, 3, 40, 32])
grad_input new torch.Size([2, 3, 40, 32])
new grad_input torch.Size([2, 3, 40, 32])
grad_out size torch.Size([2, 3, 38, 30])
crop [0, 20, 18, 10] [0, 29, 2, 39] [0, 9, 20, 29]
## 18 28 0 10
A 20 30 0 10
B 20 32 0 12
csaved input torch.Size([2, 3, 12, 12])
cgrad_output torch.Size([2, 3, 10, 10])
real nontiled_activation torch.Size([2, 3, 12, 12])
real nontiled_grad_out torch.Size([2, 3, 10, 10])
-----------------------------------------

-----------------------------------------

I am [2, 0]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([2, 3, 42, 34])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 40, 32])
padding info :: [2, 0, 0, 2]
grad_input old torch.Size([2, 3, 42, 34])
grad_input new torch.Size([2, 3, 42, 34])
grad_input torch.Size([2, 3, 42, 34])
grad_out size torch.Size([2, 3, 39, 31])
crop [0, 21, 19, 10] [0, 30, 1, 39] [0, 9, 20, 29]
## 19 29 0 10
A 20 30 0 10
B 20 32 0 12
csaved input torch.Size([2, 3, 12, 12])
cgrad_output torch.Size([2, 3, 10, 10])
real nontiled_activation torch.Size([2, 3, 12, 12])
real nontiled_grad_out torch.Size([2, 3, 10, 10])
-----------------------------------------

-----------------------------------------

I am [2, 0]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([2, 3, 44, 36])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 42, 34])
padding info :: [3, 0, 1, 3]
grad_input torch.Size([2, 3, 40, 33])
grad_out size torch.Size([2, 3, 40, 32])
crop [0, 22, 20, 10] [0, 31, 0, 39] [0, 9, 20, 29]
## 20 30 0 10
A 20 30 0 10
B 20 32 0 12
csaved input torch.Size([2, 3, 12, 12])
cgrad_output torch.Size([2, 3, 10, 10])
real nontiled_activation torch.Size([2, 3, 12, 12])
real nontiled_grad_out torch.Size([2, 3, 10, 10])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 80, 66])
grad_out size torch.Size([2, 3, 40, 33])
arg size torch.Size([2, 3, 40, 33])
##############grad_in in maxp torch.Size([2, 3, 80, 66])
I am [2, 0]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 82, 68])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 80, 66])
grad_input torch.Size([2, 3, 82, 68])
padding info :: [1, 0, 1, 1]
grad_input old torch.Size([2, 3, 82, 68])
grad_input new torch.Size([2, 3, 82, 68])
new grad_input torch.Size([2, 3, 82, 68])
grad_out size torch.Size([2, 3, 80, 66])
crop [0, 46, 40, 20] [0, 65, 0, 79] [0, 19, 40, 59]
## 40 60 0 20
A 40 60 0 20
B 40 62 0 22
csaved input torch.Size([2, 3, 22, 22])
cgrad_output torch.Size([2, 3, 20, 20])
real nontiled_activation torch.Size([2, 3, 22, 22])
real nontiled_grad_out torch.Size([2, 3, 20, 20])
-----------------------------------------

-----------------------------------------

I am [2, 0]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([2, 3, 84, 70])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 82, 68])
padding info :: [2, 0, 2, 2]
grad_input old torch.Size([2, 3, 84, 70])
grad_input new torch.Size([2, 3, 84, 70])
grad_input torch.Size([2, 3, 84, 70])
grad_out size torch.Size([2, 3, 80, 67])
crop [0, 47, 40, 20] [0, 66, 0, 79] [0, 19, 40, 59]
## 40 60 0 20
A 40 60 0 20
B 40 62 0 22
csaved input torch.Size([2, 3, 22, 22])
cgrad_output torch.Size([2, 3, 20, 20])
real nontiled_activation torch.Size([2, 3, 22, 22])
real nontiled_grad_out torch.Size([2, 3, 20, 20])
-----------------------------------------

-----------------------------------------

I am [2, 0]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([2, 3, 86, 72])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 84, 70])
padding info :: [3, 0, 3, 3]
grad_input torch.Size([2, 3, 80, 69])
grad_out size torch.Size([2, 3, 80, 68])
crop [0, 48, 40, 20] [0, 67, 0, 79] [0, 19, 40, 59]
## 40 60 0 20
A 40 60 0 20
B 40 62 0 22
csaved input torch.Size([2, 3, 22, 22])
cgrad_output torch.Size([2, 3, 20, 20])
real nontiled_activation torch.Size([2, 3, 22, 22])
real nontiled_grad_out torch.Size([2, 3, 20, 20])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 160, 138])
grad_out size torch.Size([2, 3, 80, 69])
arg size torch.Size([2, 3, 80, 69])
##############grad_in in maxp torch.Size([2, 3, 160, 138])
I am [2, 0]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 162, 140])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 160, 138])
grad_input torch.Size([2, 3, 162, 140])
padding info :: [1, 0, 1, 1]
grad_input old torch.Size([2, 3, 162, 140])
grad_input new torch.Size([2, 3, 162, 140])
new grad_input torch.Size([2, 3, 162, 140])
grad_out size torch.Size([2, 3, 160, 138])
crop [0, 98, 80, 40] [0, 137, 0, 159] [0, 39, 80, 119]
## 80 120 0 40
A 80 120 0 40
B 80 122 0 42
csaved input torch.Size([2, 3, 42, 42])
cgrad_output torch.Size([2, 3, 40, 40])
real nontiled_activation torch.Size([2, 3, 42, 42])
real nontiled_grad_out torch.Size([2, 3, 40, 40])
-----------------------------------------

-----------------------------------------

I am [2, 0]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([2, 3, 164, 142])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 162, 140])
padding info :: [2, 0, 2, 2]
grad_input torch.Size([2, 3, 160, 140])
grad_out size torch.Size([2, 3, 160, 139])
crop [0, 99, 80, 40] [0, 138, 0, 159] [0, 39, 80, 119]
## 80 120 0 40
A 80 120 0 40
B 80 122 0 42
csaved input torch.Size([2, 3, 42, 42])
cgrad_output torch.Size([2, 3, 40, 40])
real nontiled_activation torch.Size([2, 3, 42, 42])
real nontiled_grad_out torch.Size([2, 3, 40, 40])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 320, 280])
grad_out size torch.Size([2, 3, 160, 140])
arg size torch.Size([2, 3, 160, 140])
##############grad_in in maxp torch.Size([2, 3, 320, 280])
I am [2, 0]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 322, 282])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 320, 280])
grad_input torch.Size([2, 3, 322, 282])
padding info :: [1, 0, 1, 1]
grad_input old torch.Size([2, 3, 322, 282])
grad_input new torch.Size([2, 3, 322, 282])
new grad_input torch.Size([2, 3, 322, 282])
grad_out size torch.Size([2, 3, 320, 280])
crop [0, 200, 160, 80] [0, 279, 0, 319] [0, 79, 160, 239]
## 160 240 0 80
A 160 240 0 80
B 160 242 0 82
csaved input torch.Size([2, 3, 82, 82])
cgrad_output torch.Size([2, 3, 80, 80])
real nontiled_activation torch.Size([2, 3, 82, 82])
real nontiled_grad_out torch.Size([2, 3, 80, 80])
-----------------------------------------

-----------------------------------------

I am [2, 0]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([2, 3, 324, 284])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 322, 282])
final torch.Size([2, 3, 324, 284])
padding info :: [2, 0, 2, 2]
after crop g_in torch.Size([2, 3, 80, 80])
grad_out size torch.Size([2, 3, 320, 281])
crop [0, 201, 160, 80] [0, 280, 0, 319] [0, 79, 160, 239]
## 160 240 0 80
A 160 240 0 80
B 160 242 0 82
csaved input torch.Size([2, 3, 82, 82])
cgrad_output torch.Size([2, 3, 80, 80])
real nontiled_activation torch.Size([2, 3, 82, 82])
real nontiled_grad_out torch.Size([2, 3, 80, 80])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 16, 12])
grad_out size torch.Size([2, 3, 10, 10])
arg size torch.Size([2, 3, 8, 6])
new_grad_out torch.Size([2, 3, 8, 6])
##############grad_in in maxp torch.Size([2, 3, 16, 12])
I am [1, 3]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 18, 14])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 16, 12])
grad_input torch.Size([2, 3, 18, 14])
padding info :: [0, 1, 1, 0]
grad_input old torch.Size([2, 3, 18, 14])
grad_input new torch.Size([2, 3, 18, 14])
new grad_input torch.Size([2, 3, 18, 14])
grad_out size torch.Size([2, 3, 16, 12])
crop [7, 0, 5, 6] [8, 19, 0, 15] [15, 19, 5, 9]
## 5 10 7 12
A 5 10 15 20
B 5 12 15 22
csaved input torch.Size([2, 3, 7, 7])
cgrad_output torch.Size([2, 3, 5, 5])
real nontiled_activation torch.Size([2, 3, 7, 7])
real nontiled_grad_out torch.Size([2, 3, 5, 5])
-----------------------------------------

-----------------------------------------

I am [1, 3]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([2, 3, 20, 16])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 18, 14])
padding info :: [0, 2, 2, 0]
grad_input old torch.Size([2, 3, 20, 16])
grad_input new torch.Size([2, 3, 20, 16])
grad_input torch.Size([2, 3, 20, 16])
grad_out size torch.Size([2, 3, 17, 13])
crop [8, 0, 5, 7] [7, 19, 0, 16] [15, 19, 5, 9]
## 5 10 8 13
A 5 10 15 20
B 5 12 15 22
csaved input torch.Size([2, 3, 7, 7])
cgrad_output torch.Size([2, 3, 5, 5])
real nontiled_activation torch.Size([2, 3, 7, 7])
real nontiled_grad_out torch.Size([2, 3, 5, 5])
-----------------------------------------

-----------------------------------------

I am [1, 3]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([2, 3, 22, 18])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 20, 16])
padding info :: [0, 3, 3, 0]
grad_input torch.Size([2, 3, 19, 15])
grad_out size torch.Size([2, 3, 18, 14])
crop [9, 0, 5, 8] [6, 19, 0, 17] [15, 19, 5, 9]
## 5 10 9 14
A 5 10 15 20
B 5 12 15 22
csaved input torch.Size([2, 3, 7, 7])
cgrad_output torch.Size([2, 3, 5, 5])
real nontiled_activation torch.Size([2, 3, 7, 7])
real nontiled_grad_out torch.Size([2, 3, 5, 5])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 38, 30])
grad_out size torch.Size([2, 3, 19, 15])
arg size torch.Size([2, 3, 19, 15])
##############grad_in in maxp torch.Size([2, 3, 38, 30])
I am [1, 3]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 40, 32])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 38, 30])
grad_input torch.Size([2, 3, 40, 32])
padding info :: [0, 1, 1, 0]
grad_input old torch.Size([2, 3, 40, 32])
grad_input new torch.Size([2, 3, 40, 32])
new grad_input torch.Size([2, 3, 40, 32])
grad_out size torch.Size([2, 3, 38, 30])
crop [20, 0, 10, 18] [10, 39, 0, 37] [30, 39, 10, 19]
## 10 20 20 30
A 10 20 30 40
B 10 22 30 42
csaved input torch.Size([2, 3, 12, 12])
cgrad_output torch.Size([2, 3, 10, 10])
real nontiled_activation torch.Size([2, 3, 12, 12])
real nontiled_grad_out torch.Size([2, 3, 10, 10])
-----------------------------------------

-----------------------------------------

I am [1, 3]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([2, 3, 42, 34])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 40, 32])
padding info :: [0, 2, 2, 0]
grad_input old torch.Size([2, 3, 42, 34])
grad_input new torch.Size([2, 3, 42, 34])
grad_input torch.Size([2, 3, 42, 34])
grad_out size torch.Size([2, 3, 39, 31])
crop [21, 0, 10, 19] [9, 39, 0, 38] [30, 39, 10, 19]
## 10 20 21 31
A 10 20 30 40
B 10 22 30 42
csaved input torch.Size([2, 3, 12, 12])
cgrad_output torch.Size([2, 3, 10, 10])
real nontiled_activation torch.Size([2, 3, 12, 12])
real nontiled_grad_out torch.Size([2, 3, 10, 10])
-----------------------------------------

-----------------------------------------

I am [1, 3]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([2, 3, 44, 36])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 42, 34])
padding info :: [0, 3, 3, 1]
grad_input torch.Size([2, 3, 40, 33])
grad_out size torch.Size([2, 3, 40, 32])
crop [22, 0, 10, 20] [8, 39, 0, 39] [30, 39, 10, 19]
## 10 20 22 32
A 10 20 30 40
B 10 22 30 42
csaved input torch.Size([2, 3, 12, 12])
cgrad_output torch.Size([2, 3, 10, 10])
real nontiled_activation torch.Size([2, 3, 12, 12])
real nontiled_grad_out torch.Size([2, 3, 10, 10])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 80, 66])
grad_out size torch.Size([2, 3, 40, 33])
arg size torch.Size([2, 3, 40, 33])
##############grad_in in maxp torch.Size([2, 3, 80, 66])
I am [1, 3]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 82, 68])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 80, 66])
grad_input torch.Size([2, 3, 82, 68])
padding info :: [0, 1, 1, 1]
grad_input old torch.Size([2, 3, 82, 68])
grad_input new torch.Size([2, 3, 82, 68])
new grad_input torch.Size([2, 3, 82, 68])
grad_out size torch.Size([2, 3, 80, 66])
crop [46, 0, 20, 40] [14, 79, 0, 79] [60, 79, 20, 39]
## 20 40 46 66
A 20 40 60 80
B 20 42 60 82
csaved input torch.Size([2, 3, 22, 22])
cgrad_output torch.Size([2, 3, 20, 20])
real nontiled_activation torch.Size([2, 3, 22, 22])
real nontiled_grad_out torch.Size([2, 3, 20, 20])
-----------------------------------------

-----------------------------------------

I am [1, 3]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([2, 3, 84, 70])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 82, 68])
padding info :: [0, 2, 2, 2]
grad_input old torch.Size([2, 3, 84, 70])
grad_input new torch.Size([2, 3, 84, 70])
grad_input torch.Size([2, 3, 84, 70])
grad_out size torch.Size([2, 3, 80, 67])
crop [47, 0, 20, 40] [13, 79, 0, 79] [60, 79, 20, 39]
## 20 40 47 67
A 20 40 60 80
B 20 42 60 82
csaved input torch.Size([2, 3, 22, 22])
cgrad_output torch.Size([2, 3, 20, 20])
real nontiled_activation torch.Size([2, 3, 22, 22])
real nontiled_grad_out torch.Size([2, 3, 20, 20])
-----------------------------------------

-----------------------------------------

I am [1, 3]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([2, 3, 86, 72])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 84, 70])
padding info :: [0, 3, 3, 3]
grad_input torch.Size([2, 3, 80, 69])
grad_out size torch.Size([2, 3, 80, 68])
crop [48, 0, 20, 40] [12, 79, 0, 79] [60, 79, 20, 39]
## 20 40 48 68
A 20 40 60 80
B 20 42 60 82
csaved input torch.Size([2, 3, 22, 22])
cgrad_output torch.Size([2, 3, 20, 20])
real nontiled_activation torch.Size([2, 3, 22, 22])
real nontiled_grad_out torch.Size([2, 3, 20, 20])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 160, 138])
grad_out size torch.Size([2, 3, 80, 69])
arg size torch.Size([2, 3, 80, 69])
##############grad_in in maxp torch.Size([2, 3, 160, 138])
I am [1, 3]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 162, 140])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 160, 138])
grad_input torch.Size([2, 3, 162, 140])
padding info :: [0, 1, 1, 1]
grad_input old torch.Size([2, 3, 162, 140])
grad_input new torch.Size([2, 3, 162, 140])
new grad_input torch.Size([2, 3, 162, 140])
grad_out size torch.Size([2, 3, 160, 138])
crop [98, 0, 40, 80] [22, 159, 0, 159] [120, 159, 40, 79]
## 40 80 98 138
A 40 80 120 160
B 40 82 120 162
csaved input torch.Size([2, 3, 42, 42])
cgrad_output torch.Size([2, 3, 40, 40])
real nontiled_activation torch.Size([2, 3, 42, 42])
real nontiled_grad_out torch.Size([2, 3, 40, 40])
-----------------------------------------

-----------------------------------------

I am [1, 3]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([2, 3, 164, 142])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 162, 140])
padding info :: [0, 2, 2, 2]
grad_input torch.Size([2, 3, 160, 140])
grad_out size torch.Size([2, 3, 160, 139])
crop [99, 0, 40, 80] [21, 159, 0, 159] [120, 159, 40, 79]
## 40 80 99 139
A 40 80 120 160
B 40 82 120 162
csaved input torch.Size([2, 3, 42, 42])
cgrad_output torch.Size([2, 3, 40, 40])
real nontiled_activation torch.Size([2, 3, 42, 42])
real nontiled_grad_out torch.Size([2, 3, 40, 40])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 320, 280])
grad_out size torch.Size([2, 3, 160, 140])
arg size torch.Size([2, 3, 160, 140])
##############grad_in in maxp torch.Size([2, 3, 320, 280])
I am [1, 3]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 322, 282])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 320, 280])
grad_input torch.Size([2, 3, 322, 282])
padding info :: [0, 1, 1, 1]
grad_input old torch.Size([2, 3, 322, 282])
grad_input new torch.Size([2, 3, 322, 282])
new grad_input torch.Size([2, 3, 322, 282])
grad_out size torch.Size([2, 3, 320, 280])
crop [200, 0, 80, 160] [40, 319, 0, 319] [240, 319, 80, 159]
## 80 160 200 280
A 80 160 240 320
B 80 162 240 322
csaved input torch.Size([2, 3, 82, 82])
cgrad_output torch.Size([2, 3, 80, 80])
real nontiled_activation torch.Size([2, 3, 82, 82])
real nontiled_grad_out torch.Size([2, 3, 80, 80])
-----------------------------------------

-----------------------------------------

I am [1, 3]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([2, 3, 324, 284])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 322, 282])
final torch.Size([2, 3, 324, 284])
padding info :: [0, 2, 2, 2]
after crop g_in torch.Size([2, 3, 80, 80])
grad_out size torch.Size([2, 3, 320, 281])
crop [201, 0, 80, 160] [39, 319, 0, 319] [240, 319, 80, 159]
## 80 160 201 281
A 80 160 240 320
B 80 162 240 322
csaved input torch.Size([2, 3, 82, 82])
cgrad_output torch.Size([2, 3, 80, 80])
real nontiled_activation torch.Size([2, 3, 82, 82])
real nontiled_grad_out torch.Size([2, 3, 80, 80])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 16, 16])
grad_out size torch.Size([2, 3, 10, 10])
arg size torch.Size([2, 3, 8, 8])
new_grad_out torch.Size([2, 3, 8, 8])
##############grad_in in maxp torch.Size([2, 3, 16, 16])
I am [1, 2]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 18, 18])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 16, 16])
grad_input torch.Size([2, 3, 18, 18])
padding info :: [0, 1, 1, 0]
grad_input old torch.Size([2, 3, 18, 18])
grad_input new torch.Size([2, 3, 18, 18])
new grad_input torch.Size([2, 3, 18, 18])
grad_out size torch.Size([2, 3, 16, 16])
crop [6, 5, 5, 6] [4, 19, 0, 15] [10, 14, 5, 9]
## 5 10 6 11
A 5 10 10 15
B 5 12 10 17
csaved input torch.Size([2, 3, 7, 7])
cgrad_output torch.Size([2, 3, 5, 5])
real nontiled_activation torch.Size([2, 3, 7, 7])
real nontiled_grad_out torch.Size([2, 3, 5, 5])
-----------------------------------------

-----------------------------------------

I am [1, 2]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([2, 3, 20, 20])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 18, 18])
padding info :: [0, 2, 2, 0]
grad_input old torch.Size([2, 3, 20, 20])
grad_input new torch.Size([2, 3, 20, 20])
grad_input torch.Size([2, 3, 20, 20])
grad_out size torch.Size([2, 3, 17, 17])
crop [7, 5, 5, 7] [3, 19, 0, 16] [10, 14, 5, 9]
## 5 10 7 12
A 5 10 10 15
B 5 12 10 17
csaved input torch.Size([2, 3, 7, 7])
cgrad_output torch.Size([2, 3, 5, 5])
real nontiled_activation torch.Size([2, 3, 7, 7])
real nontiled_grad_out torch.Size([2, 3, 5, 5])
-----------------------------------------

-----------------------------------------

I am [1, 2]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([2, 3, 22, 22])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 20, 20])
padding info :: [0, 3, 3, 0]
grad_input torch.Size([2, 3, 19, 19])
grad_out size torch.Size([2, 3, 18, 18])
crop [8, 5, 5, 8] [2, 19, 0, 17] [10, 14, 5, 9]
## 5 10 8 13
A 5 10 10 15
B 5 12 10 17
csaved input torch.Size([2, 3, 7, 7])
cgrad_output torch.Size([2, 3, 5, 5])
real nontiled_activation torch.Size([2, 3, 7, 7])
real nontiled_grad_out torch.Size([2, 3, 5, 5])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 38, 38])
grad_out size torch.Size([2, 3, 19, 19])
arg size torch.Size([2, 3, 19, 19])
##############grad_in in maxp torch.Size([2, 3, 38, 38])
I am [1, 2]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 40, 40])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 38, 38])
grad_input torch.Size([2, 3, 40, 40])
padding info :: [0, 1, 1, 0]
grad_input old torch.Size([2, 3, 40, 40])
grad_input new torch.Size([2, 3, 40, 40])
new grad_input torch.Size([2, 3, 40, 40])
grad_out size torch.Size([2, 3, 38, 38])
crop [18, 10, 10, 18] [2, 39, 0, 37] [20, 29, 10, 19]
## 10 20 18 28
A 10 20 20 30
B 10 22 20 32
csaved input torch.Size([2, 3, 12, 12])
cgrad_output torch.Size([2, 3, 10, 10])
real nontiled_activation torch.Size([2, 3, 12, 12])
real nontiled_grad_out torch.Size([2, 3, 10, 10])
-----------------------------------------

-----------------------------------------

I am [1, 2]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([2, 3, 42, 42])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 40, 40])
padding info :: [0, 2, 2, 0]
grad_input old torch.Size([2, 3, 42, 42])
grad_input new torch.Size([2, 3, 42, 42])
grad_input torch.Size([2, 3, 42, 42])
grad_out size torch.Size([2, 3, 39, 39])
crop [19, 10, 10, 19] [1, 39, 0, 38] [20, 29, 10, 19]
## 10 20 19 29
A 10 20 20 30
B 10 22 20 32
csaved input torch.Size([2, 3, 12, 12])
cgrad_output torch.Size([2, 3, 10, 10])
real nontiled_activation torch.Size([2, 3, 12, 12])
real nontiled_grad_out torch.Size([2, 3, 10, 10])
-----------------------------------------

-----------------------------------------

I am [1, 2]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([2, 3, 44, 44])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 42, 42])
padding info :: [1, 3, 3, 1]
grad_input torch.Size([2, 3, 40, 40])
grad_out size torch.Size([2, 3, 40, 40])
crop [20, 10, 10, 20] [0, 39, 0, 39] [20, 29, 10, 19]
## 10 20 20 30
A 10 20 20 30
B 10 22 20 32
csaved input torch.Size([2, 3, 12, 12])
cgrad_output torch.Size([2, 3, 10, 10])
real nontiled_activation torch.Size([2, 3, 12, 12])
real nontiled_grad_out torch.Size([2, 3, 10, 10])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 80, 80])
grad_out size torch.Size([2, 3, 40, 40])
arg size torch.Size([2, 3, 40, 40])
##############grad_in in maxp torch.Size([2, 3, 80, 80])
I am [1, 2]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 82, 82])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 80, 80])
grad_input torch.Size([2, 3, 82, 82])
padding info :: [1, 1, 1, 1]
grad_input old torch.Size([2, 3, 82, 82])
grad_input new torch.Size([2, 3, 82, 82])
new grad_input torch.Size([2, 3, 82, 82])
grad_out size torch.Size([2, 3, 80, 80])
crop [40, 20, 20, 40] [0, 79, 0, 79] [40, 59, 20, 39]
## 20 40 40 60
A 20 40 40 60
B 20 42 40 62
csaved input torch.Size([2, 3, 22, 22])
cgrad_output torch.Size([2, 3, 20, 20])
real nontiled_activation torch.Size([2, 3, 22, 22])
real nontiled_grad_out torch.Size([2, 3, 20, 20])
-----------------------------------------

-----------------------------------------

I am [1, 2]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([2, 3, 84, 84])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 82, 82])
padding info :: [2, 2, 2, 2]
grad_input old torch.Size([2, 3, 84, 84])
grad_input new torch.Size([2, 3, 84, 84])
grad_input torch.Size([2, 3, 84, 84])
grad_out size torch.Size([2, 3, 80, 80])
crop [40, 20, 20, 40] [0, 79, 0, 79] [40, 59, 20, 39]
## 20 40 40 60
A 20 40 40 60
B 20 42 40 62
csaved input torch.Size([2, 3, 22, 22])
cgrad_output torch.Size([2, 3, 20, 20])
real nontiled_activation torch.Size([2, 3, 22, 22])
real nontiled_grad_out torch.Size([2, 3, 20, 20])
-----------------------------------------

-----------------------------------------

I am [1, 2]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([2, 3, 86, 86])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 84, 84])
padding info :: [3, 3, 3, 3]
grad_input torch.Size([2, 3, 80, 80])
grad_out size torch.Size([2, 3, 80, 80])
crop [40, 20, 20, 40] [0, 79, 0, 79] [40, 59, 20, 39]
## 20 40 40 60
A 20 40 40 60
B 20 42 40 62
csaved input torch.Size([2, 3, 22, 22])
cgrad_output torch.Size([2, 3, 20, 20])
real nontiled_activation torch.Size([2, 3, 22, 22])
real nontiled_grad_out torch.Size([2, 3, 20, 20])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 160, 160])
grad_out size torch.Size([2, 3, 80, 80])
arg size torch.Size([2, 3, 80, 80])
##############grad_in in maxp torch.Size([2, 3, 160, 160])
I am [1, 2]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 162, 162])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 160, 160])
grad_input torch.Size([2, 3, 162, 162])
padding info :: [1, 1, 1, 1]
grad_input old torch.Size([2, 3, 162, 162])
grad_input new torch.Size([2, 3, 162, 162])
new grad_input torch.Size([2, 3, 162, 162])
grad_out size torch.Size([2, 3, 160, 160])
crop [80, 40, 40, 80] [0, 159, 0, 159] [80, 119, 40, 79]
## 40 80 80 120
A 40 80 80 120
B 40 82 80 122
csaved input torch.Size([2, 3, 42, 42])
cgrad_output rd
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFutorch.Size([2, 3, 40, 40])
real nontiled_activation torch.Size([2, 3, 42, 42])
real nontiled_grad_out torch.Size([2, 3, 40, 40])
-----------------------------------------

-----------------------------------------

I am [1, 2]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([2, 3, 164, 164])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 162, 162])
padding info :: [2, 2, 2, 2]
grad_input torch.Size([2, 3, 160, 160])
grad_out size torch.Size([2, 3, 160, 160])
crop [80, 40, 40, 80] [0, 159, 0, 159] [80, 119, 40, 79]
## 40 80 80 120
A 40 80 80 120
B 40 82 80 122
csaved input torch.Size([2, 3, 42, 42])
cgrad_output torch.Size([2, 3, 40, 40])
real nontiled_activation torch.Size([2, 3, 42, 42])
real nontiled_grad_out torch.Size([2, 3, 40, 40])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 320, 320])
grad_out size torch.Size([2, 3, 160, 160])
arg size torch.Size([2, 3, 160, 160])
##############grad_in in maxp torch.Size([2, 3, 320, 320])
I am [1, 2]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 322, 322])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 320, 320])
grad_input torch.Size([2, 3, 322, 322])
padding info :: [1, 1, 1, 1]
grad_input old torch.Size([2, 3, 322, 322])
grad_input new torch.Size([2, 3, 322, 322])
new grad_input torch.Size([2, 3, 322, 322])
grad_out size torch.Size([2, 3, 320, 320])
crop [160, 80, 80, 160] [0, 319, 0, 319] [160, 239, 80, 159]
## 80 160 160 240
A 80 160 160 240
B 80 162 160 242
csaved input torch.Size([2, 3, 82, 82])
cgrad_output torch.Size([2, 3, 80, 80])
real nontiled_activation torch.Size([2, 3, 82, 82])
real nontiled_grad_out torch.Size([2, 3, 80, 80])
-----------------------------------------

-----------------------------------------

I am [1, 2]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([2, 3, 324, 324])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 322, 322])
final torch.Size([2, 3, 324, 324])
padding info :: [2, 2, 2, 2]
after crop g_in torch.Size([2, 3, 80, 80])
grad_out size torch.Size([2, 3, 320, 320])
crop [160, 80, 80, 160] [0, 319, 0, 319] [160, 239, 80, 159]
## 80 160 160 240
A 80 160 160 240
B 80 162 160 242
csaved input torch.Size([2, 3, 82, 82])
cgrad_output torch.Size([2, 3, 80, 80])
real nontiled_activation torch.Size([2, 3, 82, 82])
real nontiled_grad_out torch.Size([2, 3, 80, 80])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 16, 16])
grad_out size torch.Size([2, 3, 10, 10])
arg size torch.Size([2, 3, 8, 8])
new_grad_out torch.Size([2, 3, 8, 8])
##############grad_in in maxp torch.Size([2, 3, 16, 16])
I am [1, 1]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 18, 18])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 16, 16])
grad_input torch.Size([2, 3, 18, 18])
padding info :: [1, 0, 1, 0]
grad_input old torch.Size([2, 3, 18, 18])
grad_input new torch.Size([2, 3, 18, 18])
new grad_input torch.Size([2, 3, 18, 18])
grad_out size torch.Size([2, 3, 16, 16])
crop [5, 6, 5, 6] [0, 15, 0, 15] [5, 9, 5, 9]
## 5 10 5 10
A 5 10 5 10
B 5 12 5 12
csaved input torch.Size([2, 3, 7, 7])
cgrad_output torch.Size([2, 3, 5, 5])
real nontiled_activation torch.Size([2, 3, 7, 7])
real nontiled_grad_out torch.Size([2, 3, 5, 5])
-----------------------------------------

-----------------------------------------

I am [1, 1]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([2, 3, 20, 20])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 18, 18])
padding info :: [2, 0, 2, 0]
grad_input old torch.Size([2, 3, 20, 20])
grad_input new torch.Size([2, 3, 20, 20])
grad_input torch.Size([2, 3, 20, 20])
grad_out size torch.Size([2, 3, 17, 17])
crop [5, 7, 5, 7] [0, 16, 0, 16] [5, 9, 5, 9]
## 5 10 5 10
A 5 10 5 10
B 5 12 5 12
csaved input torch.Size([2, 3, 7, 7])
cgrad_output torch.Size([2, 3, 5, 5])
real nontiled_activation torch.Size([2, 3, 7, 7])
real nontiled_grad_out torch.Size([2, 3, 5, 5])
-----------------------------------------

-----------------------------------------

I am [1, 1]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([2, 3, 22, 22])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 20, 20])
padding info :: [3, 0, 3, 0]
grad_input torch.Size([2, 3, 19, 19])
grad_out size torch.Size([2, 3, 18, 18])
crop [5, 8, 5, 8] [0, 17, 0, 17] [5, 9, 5, 9]
## 5 10 5 10
A 5 10 5 10
B 5 12 5 12
csaved input torch.Size([2, 3, 7, 7])
cgrad_output torch.Size([2, 3, 5, 5])
real nontiled_activation torch.Size([2, 3, 7, 7])
real nontiled_grad_out torch.Size([2, 3, 5, 5])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 38, 38])
grad_out size torch.Size([2, 3, 19, 19])
arg size torch.Size([2, 3, 19, 19])
##############grad_in in maxp torch.Size([2, 3, 38, 38])
I am [1, 1]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 40, 40])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 38, 38])
grad_input torch.Size([2, 3, 40, 40])
padding info :: [1, 0, 1, 0]
grad_input old torch.Size([2, 3, 40, 40])
grad_input new torch.Size([2, 3, 40, 40])
new grad_input torch.Size([2, 3, 40, 40])
grad_out size torch.Size([2, 3, 38, 38])
crop [10, 18, 10, 18] [0, 37, 0, 37] [10, 19, 10, 19]
## 10 20 10 20
A 10 20 10 20
B 10 22 10 22
csaved input torch.Size([2, 3, 12, 12])
cgrad_output torch.Size([2, 3, 10, 10])
real nontiled_activation torch.Size([2, 3, 12, 12])
real nontiled_grad_out torch.Size([2, 3, 10, 10])
-----------------------------------------

-----------------------------------------

I am [1, 1]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([2, 3, 42, 42])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 40, 40])
padding info :: [2, 0, 2, 0]
grad_input old torch.Size([2, 3, 42, 42])
grad_input new torch.Size([2, 3, 42, 42])
grad_input torch.Size([2, 3, 42, 42])
grad_out size torch.Size([2, 3, 39, 39])
crop [10, 19, 10, 19] [0, 38, 0, 38] [10, 19, 10, 19]
## 10 20 10 20
A 10 20 10 20
B 10 22 10 22
csaved input torch.Size([2, 3, 12, 12])
cgrad_output torch.Size([2, 3, 10, 10])
real nontiled_activation torch.Size([2, 3, 12, 12])
real nontiled_grad_out torch.Size([2, 3, 10, 10])
-----------------------------------------

-----------------------------------------

I am [1, 1]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([2, 3, 44, 44])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 42, 42])
padding info :: [3, 1, 3, 1]
grad_input torch.Size([2, 3, 40, 40])
grad_out size torch.Size([2, 3, 40, 40])
crop [10, 20, 10, 20] [0, 39, 0, 39] [10, 19, 10, 19]
## 10 20 10 20
A 10 20 10 20
B 10 22 10 22
csaved input torch.Size([2, 3, 12, 12])
cgrad_output torch.Size([2, 3, 10, 10])
real nontiled_activation torch.Size([2, 3, 12, 12])
real nontiled_grad_out torch.Size([2, 3, 10, 10])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 80, 80])
grad_out size torch.Size([2, 3, 40, 40])
arg size torch.Size([2, 3, 40, 40])
##############grad_in in maxp torch.Size([2, 3, 80, 80])
I am [1, 1]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 82, 82])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 80, 80])
grad_input torch.Size([2, 3, 82, 82])
padding info :: [1, 1, 1, 1]
grad_input old torch.Size([2, 3, 82, 82])
grad_input new torch.Size([2, 3, 82, 82])
new grad_input torch.Size([2, 3, 82, 82])
grad_out size torch.Size([2, 3, 80, 80])
crop [20, 40, 20, 40] [0, 79, 0, 79] [20, 39, 20, 39]
## 20 40 20 40
A 20 40 20 40
B 20 42 20 42
csaved input torch.Size([2, 3, 22, 22])
cgrad_output torch.Size([2, 3, 20, 20])
real nontiled_activation torch.Size([2, 3, 22, 22])
real nontiled_grad_out torch.Size([2, 3, 20, 20])
-----------------------------------------

-----------------------------------------

I am [1, 1]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([2, 3, 84, 84])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 82, 82])
padding info :: [2, 2, 2, 2]
grad_input old torch.Size([2, 3, 84, 84])
grad_input new torch.Size([2, 3, 84, 84])
grad_input torch.Size([2, 3, 84, 84])
grad_out size torch.Size([2, 3, 80, 80])
crop [20, 40, 20, 40] [0, 79, 0, 79] [20, 39, 20, 39]
## 20 40 20 40
A 20 40 20 40
B 20 42 20 42
csaved input torch.Size([2, 3, 22, 22])
cgrad_output torch.Size([2, 3, 20, 20])
real nontiled_activation torch.Size([2, 3, 22, 22])
real nontiled_grad_out torch.Size([2, 3, 20, 20])
-----------------------------------------

-----------------------------------------

I am [1, 1]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([2, 3, 86, 86])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 84, 84])
padding info :: [3, 3, 3, 3]
grad_input torch.Size([2, 3, 80, 80])
grad_out size torch.Size([2, 3, 80, 80])
crop [20, 40, 20, 40] [0, 79, 0, 79] [20, 39, 20, 39]
## 20 40 20 40
A 20 40 20 40
B 20 42 20 42
csaved input torch.Size([2, 3, 22, 22])
cgrad_output torch.Size([2, 3, 20, 20])
real nontiled_activation torch.Size([2, 3, 22, 22])
real nontiled_grad_out torch.Size([2, 3, 20, 20])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 160, 160])
grad_out size torch.Size([2, 3, 80, 80])
arg size torch.Size([2, 3, 80, 80])
##############grad_in in maxp torch.Size([2, 3, 160, 160])
I am [1, 1]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 162, 162])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 160, 160])
grad_input torch.Size([2, 3, 162, 162])
padding info :: [1, 1, 1, 1]
grad_input old torch.Size([2, 3, 162, 162])
grad_input new torch.Size([2, 3, 162, 162])
new grad_input torch.Size([2, 3, 162, 162])
grad_out size torch.Size([2, 3, 160, 160])
crop [40, 80, 40, 80] [0, 159, 0, 159] [40, 79, 40, 79]
## 40 80 40 80
A 40 80 40 80
B 40 82 40 82
csaved input torch.Size([2, 3, 42, 42])
cgrad_output torch.Size([2, 3, 40, 40])
real nontiled_activation torch.Size([2, 3, 42, 42])
real nontiled_grad_out torch.Size([2, 3, 40, 40])
-----------------------------------------

-----------------------------------------

I am [1, 1]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([2, 3, 164, 164])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 162, 162])
padding info :: [2, 2, 2, 2]
grad_input torch.Size([2, 3, 160, 160])
grad_out size torch.Size([2, 3, 160, 160])
crop [40, 80, 40, 80] [0, 159, 0, 159] [40, 79, 40, 79]
## 40 80 40 80
A 40 80 40 80
B 40 82 40 82
csaved input torch.Size([2, 3, 42, 42])
cgrad_output torch.Size([2, 3, 40, 40])
real nontiled_activation torch.Size([2, 3, 42, 42])
real nontiled_grad_out torch.Size([2, 3, 40, 40])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 320, 320])
grad_out size torch.Size([2, 3, 160, 160])
arg size torch.Size([2, 3, 160, 160])
##############grad_in in maxp torch.Size([2, 3, 320, 320])
I am [1, 1]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 322, 322])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 320, 320])
grad_input torch.Size([2, 3, 322, 322])
padding info :: [1, 1, 1, 1]
grad_input old torch.Size([2, 3, 322, 322])
grad_input new torch.Size([2, 3, 322, 322])
new grad_input torch.Size([2, 3, 322, 322])
grad_out size torch.Size([2, 3, 320, 320])
crop [80, 160, 80, 160] [0, 319, 0, 319] [80, 159, 80, 159]
## 80 160 80 160
A 80 160 80 160
B 80 162 80 162
csaved input torch.Size([2, 3, 82, 82])
cgrad_output torch.Size([2, 3, 80, 80])
real nontiled_activation torch.Size([2, 3, 82, 82])
real nontiled_grad_out torch.Size([2, 3, 80, 80])
-----------------------------------------

-----------------------------------------

I am [1, 1]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([2, 3, 324, 324])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 322, 322])
final torch.Size([2, 3, 324, 324])
padding info :: [2, 2, 2, 2]
after crop g_in torch.Size([2, 3, 80, 80])
grad_out size torch.Size([2, 3, 320, 320])
crop [80, 160, 80, 160] [0, 319, 0, 319] [80, 159, 80, 159]
## 80 160 80 160
A 80 160 80 160
B 80 162 80 162
csaved input torch.Size([2, 3, 82, 82])
cgrad_output torch.Size([2, 3, 80, 80])
real nontiled_activation torch.Size([2, 3, 82, 82])
real nontiled_grad_out torch.Size([2, 3, 80, 80])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 16, 12])
grad_out size torch.Size([2, 3, 10, 10])
arg size torch.Size([2, 3, 8, 6])
new_grad_out torch.Size([2, 3, 8, 6])
##############grad_in in maxp torch.Size([2, 3, 16, 12])
I am [1, 0]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 18, 14])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 16, 12])
grad_input torch.Size([2, 3, 18, 14])
padding info :: [1, 0, 1, 0]
grad_input old torch.Size([2, 3, 18, 14])
grad_input new torch.Size([2, 3, 18, 14])
new grad_input torch.Size([2, 3, 18, 14])
grad_out size torch.Size([2, 3, 16, 12])
crop [0, 7, 5, 6] [0, 11, 0, 15] [0, 4, 5, 9]
## 5 10 0 5
A 5 10 0 5
B 5 12 0 7
csaved input torch.Size([2, 3, 7, 7])
cgrad_output torch.Size([2, 3, 5, 5])
real nontiled_activation torch.Size([2, 3, 7, 7])
real nontiled_grad_out torch.Size([2, 3, 5, 5])
-----------------------------------------

-----------------------------------------

I am [1, 0]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([2, 3, 20, 16])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 18, 14])
padding info :: [2, 0, 2, 0]
grad_input old torch.Size([2, 3, 20, 16])
grad_input new torch.Size([2, 3, 20, 16])
grad_input torch.Size([2, 3, 20, 16])
grad_out size torch.Size([2, 3, 17, 13])
crop [0, 8, 5, 7] [0, 12, 0, 16] [0, 4, 5, 9]
## 5 10 0 5
A 5 10 0 5
B 5 12 0 7
csaved input torch.Size([2, 3, 7, 7])
cgrad_output torch.Size([2, 3, 5, 5])
real nontiled_activation torch.Size([2, 3, 7, 7])
real nontiled_grad_out torch.Size([2, 3, 5, 5])
-----------------------------------------

-----------------------------------------

I am [1, 0]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([2, 3, 22, 18])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 20, 16])
padding info :: [3, 0, 3, 0]
grad_input torch.Size([2, 3, 19, 15])
grad_out size torch.Size([2, 3, 18, 14])
crop [0, 9, 5, 8] [0, 13, 0, 17] [0, 4, 5, 9]
## 5 10 0 5
A 5 10 0 5
B 5 12 0 7
csaved input torch.Size([2, 3, 7, 7])
cgrad_output torch.Size([2, 3, 5, 5])
real nontiled_activation torch.Size([2, 3, 7, 7])
real nontiled_grad_out torch.Size([2, 3, 5, 5])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 38, 30])
grad_out size torch.Size([2, 3, 19, 15])
arg size torch.Size([2, 3, 19, 15])
##############grad_in in maxp torch.Size([2, 3, 38, 30])
I am [1, 0]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 40, 32])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 38, 30])
grad_input torch.Size([2, 3, 40, 32])
padding info :: [1, 0, 1, 0]
grad_input old torch.Size([2, 3, 40, 32])
grad_input new torch.Size([2, 3, 40, 32])
new grad_input torch.Size([2, 3, 40, 32])
grad_out size torch.Size([2, 3, 38, 30])
crop [0, 20, 10, 18] [0, 29, 0, 37] [0, 9, 10, 19]
## 10 20 0 10
A 10 20 0 10
B 10 22 0 12
csaved input torch.Size([2, 3, 12, 12])
cgrad_output torch.Size([2, 3, 10, 10])
real nontiled_activation torch.Size([2, 3, 12, 12])
real nontiled_grad_out torch.Size([2, 3, 10, 10])
-----------------------------------------

-----------------------------------------

I am [1, 0]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([2, 3, 42, 34])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 40, 32])
padding info :: [2, 0, 2, 0]
grad_input old torch.Size([2, 3, 42, 34])
grad_input new torch.Size([2, 3, 42, 34])
grad_input torch.Size([2, 3, 42, 34])
grad_out size torch.Size([2, 3, 39, 31])
crop [0, 21, 10, 19] [0, 30, 0, 38] [0, 9, 10, 19]
## 10 20 0 10
A 10 20 0 10
B 10 22 0 12
csaved input torch.Size([2, 3, 12, 12])
cgrad_output torch.Size([2, 3, 10, 10])
real nontiled_activation torch.Size([2, 3, 12, 12])
real nontiled_grad_out torch.Size([2, 3, 10, 10])
-----------------------------------------

-----------------------------------------

I am [1, 0]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([2, 3, 44, 36])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 42, 34])
padding info :: [3, 0, 3, 1]
grad_input torch.Size([2, 3, 40, 33])
grad_out size torch.Size([2, 3, 40, 32])
crop [0, 22, 10, 20] [0, 31, 0, 39] [0, 9, 10, 19]
## 10 20 0 10
A 10 20 0 10
B 10 22 0 12
csaved input torch.Size([2, 3, 12, 12])
cgrad_output torch.Size([2, 3, 10, 10])
real nontiled_activation torch.Size([2, 3, 12, 12])
real nontiled_grad_out torch.Size([2, 3, 10, 10])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 80, 66])
grad_out size torch.Size([2, 3, 40, 33])
arg size torch.Size([2, 3, 40, 33])
##############grad_in in maxp torch.Size([2, 3, 80, 66])
I am [1, 0]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 82, 68])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 80, 66])
grad_input torch.Size([2, 3, 82, 68])
padding info :: [1, 0, 1, 1]
grad_input old torch.Size([2, 3, 82, 68])
grad_input new torch.Size([2, 3, 82, 68])
new grad_input torch.Size([2, 3, 82, 68])
grad_out size torch.Size([2, 3, 80, 66])
crop [0, 46, 20, 40] [0, 65, 0, 79] [0, 19, 20, 39]
## 20 40 0 20
A 20 40 0 20
B 20 42 0 22
csaved input torch.Size([2, 3, 22, 22])
cgrad_output torch.Size([2, 3, 20, 20])
real nontiled_activation torch.Size([2, 3, 22, 22])
real nontiled_grad_out torch.Size([2, 3, 20, 20])
-----------------------------------------

-----------------------------------------

I am [1, 0]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([2, 3, 84, 70])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 82, 68])
padding info :: [2, 0, 2, 2]
grad_input old torch.Size([2, 3, 84, 70])
grad_input new torch.Size([2, 3, 84, 70])
grad_input torch.Size([2, 3, 84, 70])
grad_out size torch.Size([2, 3, 80, 67])
crop [0, 47, 20, 40] [0, 66, 0, 79] [0, 19, 20, 39]
## 20 40 0 20
A 20 40 0 20
B 20 42 0 22
csaved input torch.Size([2, 3, 22, 22])
cgrad_output torch.Size([2, 3, 20, 20])
real nontiled_activation torch.Size([2, 3, 22, 22])
real nontiled_grad_out torch.Size([2, 3, 20, 20])
-----------------------------------------

-----------------------------------------

I am [1, 0]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([2, 3, 86, 72])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 84, 70])
padding info :: [3, 0, 3, 3]
grad_input torch.Size([2, 3, 80, 69])
grad_out size torch.Size([2, 3, 80, 68])
crop [0, 48, 20, 40] [0, 67, 0, 79] [0, 19, 20, 39]
## 20 40 0 20
A 20 40 0 20
B 20 42 0 22
csaved input torch.Size([2, 3, 22, 22])
cgrad_output torch.Size([2, 3, 20, 20])
real nontiled_activation torch.Size([2, 3, 22, 22])
real nontiled_grad_out torch.Size([2, 3, 20, 20])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 160, 138])
grad_out size torch.Size([2, 3, 80, 69])
arg size torch.Size([2, 3, 80, 69])
##############grad_in in maxp torch.Size([2, 3, 160, 138])
I am [1, 0]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 162, 140])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 160, 138])
grad_input torch.Size([2, 3, 162, 140])
padding info :: [1, 0, 1, 1]
grad_input old torch.Size([2, 3, 162, 140])
grad_input new torch.Size([2, 3, 162, 140])
new grad_input torch.Size([2, 3, 162, 140])
grad_out size torch.Size([2, 3, 160, 138])
crop [0, 98, 40, 80] [0, 137, 0, 159] [0, 39, 40, 79]
## 40 80 0 40
A 40 80 0 40
B 40 82 0 42
csaved input torch.Size([2, 3, 42, 42])
cgrad_output torch.Size([2, 3, 40, 40])
real nontiled_activation torch.Size([2, 3, 42, 42])
real nontiled_grad_out torch.Size([2, 3, 40, 40])
-----------------------------------------

-----------------------------------------

I am [1, 0]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([2, 3, 164, 142])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 162, 140])
padding info :: [2, 0, 2, 2]
grad_input torch.Size([2, 3, 160, 140])
grad_out size torch.Size([2, 3, 160, 139])
crop [0, 99, 40, 80] [0, 138, 0, 159] [0, 39, 40, 79]
## 40 80 0 40
A 40 80 0 40
B 40 82 0 42
csaved input torch.Size([2, 3, 42, 42])
cgrad_output torch.Size([2, 3, 40, 40])
real nontiled_activation torch.Size([2, 3, 42, 42])
real nontiled_grad_out torch.Size([2, 3, 40, 40])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 320, 280])
grad_out size torch.Size([2, 3, 160, 140])
arg size torch.Size([2, 3, 160, 140])
##############grad_in in maxp torch.Size([2, 3, 320, 280])
I am [1, 0]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 322, 282])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 320, 280])
grad_input torch.Size([2, 3, 322, 282])
padding info :: [1, 0, 1, 1]
grad_input old torch.Size([2, 3, 322, 282])
grad_input new torch.Size([2, 3, 322, 282])
new grad_input torch.Size([2, 3, 322, 282])
grad_out size torch.Size([2, 3, 320, 280])
crop [0, 200, 80, 160] [0, 279, 0, 319] [0, 79, 80, 159]
## 80 160 0 80
A 80 160 0 80
B 80 162 0 82
csaved input torch.Size([2, 3, 82, 82])
cgrad_output torch.Size([2, 3, 80, 80])
real nontiled_activation torch.Size([2, 3, 82, 82])
real nontiled_grad_out torch.Size([2, 3, 80, 80])
-----------------------------------------

-----------------------------------------

I am [1, 0]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([2, 3, 324, 284])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 322, 282])
final torch.Size([2, 3, 324, 284])
padding info :: [2, 0, 2, 2]
after crop g_in torch.Size([2, 3, 80, 80])
grad_out size torch.Size([2, 3, 320, 281])
crop [0, 201, 80, 160] [0, 280, 0, 319] [0, 79, 80, 159]
## 80 160 0 80
A 80 160 0 80
B 80 162 0 82
csaved input torch.Size([2, 3, 82, 82])
cgrad_output torch.Size([2, 3, 80, 80])
real nontiled_activation torch.Size([2, 3, 82, 82])
real nontiled_grad_out torch.Size([2, 3, 80, 80])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 12, 12])
grad_out size torch.Size([2, 3, 10, 10])
arg size torch.Size([2, 3, 6, 6])
new_grad_out torch.Size([2, 3, 6, 6])
##############grad_in in maxp torch.Size([2, 3, 12, 12])
I am [0, 3]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 14, 14])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 12, 12])
grad_input torch.Size([2, 3, 14, 14])
padding info :: [0, 1, 1, 0]
grad_input old torch.Size([2, 3, 14, 14])
grad_input new torch.Size([2, 3, 14, 14])
new grad_input torch.Size([2, 3, 14, 14])
grad_out size torch.Size([2, 3, 12, 12])
crop [7, 0, 0, 7] [8, 19, 0, 11] [15, 19, 0, 4]
## 0 5 7 12
A 0 5 15 20
B 0 7 15 22
csaved input torch.Size([2, 3, 7, 7])
cgrad_output torch.Size([2, 3, 5, 5])
real nontiled_activation torch.Size([2, 3, 7, 7])
real nontiled_grad_out torch.Size([2, 3, 5, 5])
-----------------------------------------

-----------------------------------------

I am [0, 3]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([2, 3, 16, 16])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 14, 14])
padding info :: [0, 2, 2, 0]
grad_input old torch.Size([2, 3, 16, 16])
grad_input new torch.Size([2, 3, 16, 16])
grad_input torch.Size([2, 3, 16, 16])
grad_out size nctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function ctorch.Size([2, 3, 13, 13])
crop [8, 0, 0, 8] [7, 19, 0, 12] [15, 19, 0, 4]
## 0 5 8 13
A 0 5 15 20
B 0 7 15 22
csaved input torch.Size([2, 3, 7, 7])
cgrad_output torch.Size([2, 3, 5, 5])
real nontiled_activation torch.Size([2, 3, 7, 7])
real nontiled_grad_out torch.Size([2, 3, 5, 5])
-----------------------------------------

-----------------------------------------

I am [0, 3]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([2, 3, 18, 18])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 16, 16])
padding info :: [0, 3, 3, 0]
grad_input torch.Size([2, 3, 15, 15])
grad_out size torch.Size([2, 3, 14, 14])
crop [9, 0, 0, 9] [6, 19, 0, 13] [15, 19, 0, 4]
## 0 5 9 14
A 0 5 15 20
B 0 7 15 22
csaved input torch.Size([2, 3, 7, 7])
cgrad_output torch.Size([2, 3, 5, 5])
real nontiled_activation torch.Size([2, 3, 7, 7])
real nontiled_grad_out torch.Size([2, 3, 5, 5])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 30, 30])
grad_out size torch.Size([2, 3, 15, 15])
arg size torch.Size([2, 3, 15, 15])
##############grad_in in maxp torch.Size([2, 3, 30, 30])
I am [0, 3]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 32, 32])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 30, 30])
grad_input torch.Size([2, 3, 32, 32])
padding info :: [0, 1, 1, 0]
grad_input old torch.Size([2, 3, 32, 32])
grad_input new torch.Size([2, 3, 32, 32])
new grad_input torch.Size([2, 3, 32, 32])
grad_out size torch.Size([2, 3, 30, 30])
crop [20, 0, 0, 20] [10, 39, 0, 29] [30, 39, 0, 9]
## 0 10 20 30
A 0 10 30 40
B 0 12 30 42
csaved input torch.Size([2, 3, 12, 12])
cgrad_output torch.Size([2, 3, 10, 10])
real nontiled_activation torch.Size([2, 3, 12, 12])
real nontiled_grad_out torch.Size([2, 3, 10, 10])
-----------------------------------------

-----------------------------------------

I am [0, 3]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([2, 3, 34, 34])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 32, 32])
padding info :: [0, 2, 2, 0]
grad_input old torch.Size([2, 3, 34, 34])
grad_input new torch.Size([2, 3, 34, 34])
grad_input torch.Size([2, 3, 34, 34])
grad_out size torch.Size([2, 3, 31, 31])
crop [21, 0, 0, 21] [9, 39, 0, 30] [30, 39, 0, 9]
## 0 10 21 31
A 0 10 30 40
B 0 12 30 42
csaved input torch.Size([2, 3, 12, 12])
cgrad_output torch.Size([2, 3, 10, 10])
real nontiled_activation torch.Size([2, 3, 12, 12])
real nontiled_grad_out torch.Size([2, 3, 10, 10])
-----------------------------------------

-----------------------------------------

I am [0, 3]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([2, 3, 36, 36])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 34, 34])
padding info :: [0, 3, 3, 0]
grad_input torch.Size([2, 3, 33, 33])
grad_out size torch.Size([2, 3, 32, 32])
crop [22, 0, 0, 22] [8, 39, 0, 31] [30, 39, 0, 9]
## 0 10 22 32
A 0 10 30 40
B 0 12 30 42
csaved input torch.Size([2, 3, 12, 12])
cgrad_output torch.Size([2, 3, 10, 10])
real nontiled_activation torch.Size([2, 3, 12, 12])
real nontiled_grad_out torch.Size([2, 3, 10, 10])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 66, 66])
grad_out size torch.Size([2, 3, 33, 33])
arg size torch.Size([2, 3, 33, 33])
##############grad_in in maxp torch.Size([2, 3, 66, 66])
I am [0, 3]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 68, 68])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 66, 66])
grad_input torch.Size([2, 3, 68, 68])
padding info :: [0, 1, 1, 0]
grad_input old torch.Size([2, 3, 68, 68])
grad_input new torch.Size([2, 3, 68, 68])
new grad_input torch.Size([2, 3, 68, 68])
grad_out size torch.Size([2, 3, 66, 66])
crop [46, 0, 0, 46] [14, 79, 0, 65] [60, 79, 0, 19]
## 0 20 46 66
A 0 20 60 80
B 0 22 60 82
csaved input torch.Size([2, 3, 22, 22])
cgrad_output torch.Size([2, 3, 20, 20])
real nontiled_activation torch.Size([2, 3, 22, 22])
real nontiled_grad_out torch.Size([2, 3, 20, 20])
-----------------------------------------

-----------------------------------------

I am [0, 3]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([2, 3, 70, 70])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 68, 68])
padding info :: [0, 2, 2, 0]
grad_input old torch.Size([2, 3, 70, 70])
grad_input new torch.Size([2, 3, 70, 70])
grad_input torch.Size([2, 3, 70, 70])
grad_out size torch.Size([2, 3, 67, 67])
crop [47, 0, 0, 47] [13, 79, 0, 66] [60, 79, 0, 19]
## 0 20 47 67
A 0 20 60 80
B 0 22 60 82
csaved input torch.Size([2, 3, 22, 22])
cgrad_output torch.Size([2, 3, 20, 20])
real nontiled_activation torch.Size([2, 3, 22, 22])
real nontiled_grad_out torch.Size([2, 3, 20, 20])
-----------------------------------------

-----------------------------------------

I am [0, 3]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([2, 3, 72, 72])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 70, 70])
padding info :: [0, 3, 3, 0]
grad_input torch.Size([2, 3, 69, 69])
grad_out size torch.Size([2, 3, 68, 68])
crop [48, 0, 0, 48] [12, 79, 0, 67] [60, 79, 0, 19]
## 0 20 48 68
A 0 20 60 80
B 0 22 60 82
csaved input torch.Size([2, 3, 22, 22])
cgrad_output torch.Size([2, 3, 20, 20])
real nontiled_activation torch.Size([2, 3, 22, 22])
real nontiled_grad_out torch.Size([2, 3, 20, 20])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 138, 138])
grad_out size torch.Size([2, 3, 69, 69])
arg size torch.Size([2, 3, 69, 69])
##############grad_in in maxp torch.Size([2, 3, 138, 138])
I am [0, 3]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 140, 140])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 138, 138])
grad_input torch.Size([2, 3, 140, 140])
padding info :: [0, 1, 1, 0]
grad_input old torch.Size([2, 3, 140, 140])
grad_input new torch.Size([2, 3, 140, 140])
new grad_input torch.Size([2, 3, 140, 140])
grad_out size torch.Size([2, 3, 138, 138])
crop [98, 0, 0, 98] [22, 159, 0, 137] [120, 159, 0, 39]
## 0 40 98 138
A 0 40 120 160
B 0 42 120 162
csaved input torch.Size([2, 3, 42, 42])
cgrad_output torch.Size([2, 3, 40, 40])
real nontiled_activation torch.Size([2, 3, 42, 42])
real nontiled_grad_out torch.Size([2, 3, 40, 40])
-----------------------------------------

-----------------------------------------

I am [0, 3]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([2, 3, 142, 142])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 140, 140])
padding info :: [0, 2, 2, 0]
grad_input torch.Size([2, 3, 140, 140])
grad_out size torch.Size([2, 3, 139, 139])
crop [99, 0, 0, 99] [21, 159, 0, 138] [120, 159, 0, 39]
## 0 40 99 139
A 0 40 120 160
B 0 42 120 162
csaved input torch.Size([2, 3, 42, 42])
cgrad_output torch.Size([2, 3, 40, 40])
real nontiled_activation torch.Size([2, 3, 42, 42])
real nontiled_grad_out torch.Size([2, 3, 40, 40])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 280, 280])
grad_out size torch.Size([2, 3, 140, 140])
arg size torch.Size([2, 3, 140, 140])
##############grad_in in maxp torch.Size([2, 3, 280, 280])
I am [0, 3]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 282, 282])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 280, 280])
grad_input torch.Size([2, 3, 282, 282])
padding info :: [0, 1, 1, 0]
grad_input old torch.Size([2, 3, 282, 282])
grad_input new torch.Size([2, 3, 282, 282])
new grad_input torch.Size([2, 3, 282, 282])
grad_out size torch.Size([2, 3, 280, 280])
crop [200, 0, 0, 200] [40, 319, 0, 279] [240, 319, 0, 79]
## 0 80 200 280
A 0 80 240 320
B 0 82 240 322
csaved input torch.Size([2, 3, 82, 82])
cgrad_output torch.Size([2, 3, 80, 80])
real nontiled_activation torch.Size([2, 3, 82, 82])
real nontiled_grad_out torch.Size([2, 3, 80, 80])
-----------------------------------------

-----------------------------------------

I am [0, 3]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([2, 3, 284, 284])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 282, 282])
final torch.Size([2, 3, 284, 284])
padding info :: [0, 2, 2, 0]
after crop g_in torch.Size([2, 3, 80, 80])
grad_out size torch.Size([2, 3, 281, 281])
crop [201, 0, 0, 201] [39, 319, 0, 280] [240, 319, 0, 79]
## 0 80 201 281
A 0 80 240 320
B 0 82 240 322
csaved input torch.Size([2, 3, 82, 82])
cgrad_output torch.Size([2, 3, 80, 80])
real nontiled_activation torch.Size([2, 3, 82, 82])
real nontiled_grad_out torch.Size([2, 3, 80, 80])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 12, 16])
grad_out size torch.Size([2, 3, 10, 10])
arg size torch.Size([2, 3, 6, 8])
new_grad_out torch.Size([2, 3, 6, 8])
##############grad_in in maxp torch.Size([2, 3, 12, 16])
I am [0, 2]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 14, 18])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 12, 16])
grad_input torch.Size([2, 3, 14, 18])
padding info :: [0, 1, 1, 0]
grad_input old torch.Size([2, 3, 14, 18])
grad_input new torch.Size([2, 3, 14, 18])
new grad_input torch.Size([2, 3, 14, 18])
grad_out size torch.Size([2, 3, 12, 16])
crop [6, 5, 0, 7] [4, 19, 0, 11] [10, 14, 0, 4]
## 0 5 6 11
A 0 5 10 15
B 0 7 10 17
csaved input torch.Size([2, 3, 7, 7])
cgrad_output torch.Size([2, 3, 5, 5])
real nontiled_activation torch.Size([2, 3, 7, 7])
real nontiled_grad_out torch.Size([2, 3, 5, 5])
-----------------------------------------

-----------------------------------------

I am [0, 2]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([2, 3, 16, 20])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 14, 18])
padding info :: [0, 2, 2, 0]
grad_input old torch.Size([2, 3, 16, 20])
grad_input new torch.Size([2, 3, 16, 20])
grad_input torch.Size([2, 3, 16, 20])
grad_out size torch.Size([2, 3, 13, 17])
crop [7, 5, 0, 8] [3, 19, 0, 12] [10, 14, 0, 4]
## 0 5 7 12
A 0 5 10 15
B 0 7 10 17
csaved input torch.Size([2, 3, 7, 7])
cgrad_output torch.Size([2, 3, 5, 5])
real nontiled_activation torch.Size([2, 3, 7, 7])
real nontiled_grad_out torch.Size([2, 3, 5, 5])
-----------------------------------------

-----------------------------------------

I am [0, 2]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([2, 3, 18, 22])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 16, 20])
padding info :: [0, 3, 3, 0]
grad_input torch.Size([2, 3, 15, 19])
grad_out size torch.Size([2, 3, 14, 18])
crop [8, 5, 0, 9] [2, 19, 0, 13] [10, 14, 0, 4]
## 0 5 8 13
A 0 5 10 15
B 0 7 10 17
csaved input torch.Size([2, 3, 7, 7])
cgrad_output torch.Size([2, 3, 5, 5])
real nontiled_activation torch.Size([2, 3, 7, 7])
real nontiled_grad_out torch.Size([2, 3, 5, 5])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 30, 38])
grad_out size torch.Size([2, 3, 15, 19])
arg size torch.Size([2, 3, 15, 19])
##############grad_in in maxp torch.Size([2, 3, 30, 38])
I am [0, 2]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 32, 40])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 30, 38])
grad_input torch.Size([2, 3, 32, 40])
padding info :: [0, 1, 1, 0]
grad_input old torch.Size([2, 3, 32, 40])
grad_input new torch.Size([2, 3, 32, 40])
new grad_input torch.Size([2, 3, 32, 40])
grad_out size torch.Size([2, 3, 30, 38])
crop [18, 10, 0, 20] [2, 39, 0, 29] [20, 29, 0, 9]
## 0 10 18 28
A 0 10 20 30
B 0 12 20 32
csaved input torch.Size([2, 3, 12, 12])
cgrad_output torch.Size([2, 3, 10, 10])
real nontiled_activation torch.Size([2, 3, 12, 12])
real nontiled_grad_out torch.Size([2, 3, 10, 10])
-----------------------------------------

-----------------------------------------

I am [0, 2]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([2, 3, 34, 42])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 32, 40])
padding info :: [0, 2, 2, 0]
grad_input old torch.Size([2, 3, 34, 42])
grad_input new torch.Size([2, 3, 34, 42])
grad_input torch.Size([2, 3, 34, 42])
grad_out size torch.Size([2, 3, 31, 39])
crop [19, 10, 0, 21] [1, 39, 0, 30] [20, 29, 0, 9]
## 0 10 19 29
A 0 10 20 30
B 0 12 20 32
csaved input torch.Size([2, 3, 12, 12])
cgrad_output torch.Size([2, 3, 10, 10])
real nontiled_activation torch.Size([2, 3, 12, 12])
real nontiled_grad_out torch.Size([2, 3, 10, 10])
-----------------------------------------

-----------------------------------------

I am [0, 2]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([2, 3, 36, 44])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 34, 42])
padding info :: [1, 3, 3, 0]
grad_input torch.Size([2, 3, 33, 40])
grad_out size torch.Size([2, 3, 32, 40])
crop [20, 10, 0, 22] [0, 39, 0, 31] [20, 29, 0, 9]
## 0 10 20 30
A 0 10 20 30
B 0 12 20 32
csaved input torch.Size([2, 3, 12, 12])
cgrad_output torch.Size([2, 3, 10, 10])
real nontiled_activation torch.Size([2, 3, 12, 12])
real nontiled_grad_out torch.Size([2, 3, 10, 10])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 66, 80])
grad_out size torch.Size([2, 3, 33, 40])
arg size torch.Size([2, 3, 33, 40])
##############grad_in in maxp torch.Size([2, 3, 66, 80])
I am [0, 2]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 68, 82])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 66, 80])
grad_input torch.Size([2, 3, 68, 82])
padding info :: [1, 1, 1, 0]
grad_input old torch.Size([2, 3, 68, 82])
grad_input new torch.Size([2, 3, 68, 82])
new grad_input torch.Size([2, 3, 68, 82])
grad_out size torch.Size([2, 3, 66, 80])
crop [40, 20, 0, 46] [0, 79, 0, 65] [40, 59, 0, 19]
## 0 20 40 60
A 0 20 40 60
B 0 22 40 62
csaved input torch.Size([2, 3, 22, 22])
cgrad_output torch.Size([2, 3, 20, 20])
real nontiled_activation torch.Size([2, 3, 22, 22])
real nontiled_grad_out torch.Size([2, 3, 20, 20])
-----------------------------------------

-----------------------------------------

I am [0, 2]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([2, 3, 70, 84])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 68, 82])
padding info :: [2, 2, 2, 0]
grad_input old torch.Size([2, 3, 70, 84])
grad_input new torch.Size([2, 3, 70, 84])
grad_input torch.Size([2, 3, 70, 84])
grad_out size torch.Size([2, 3, 67, 80])
crop [40, 20, 0, 47] [0, 79, 0, 66] [40, 59, 0, 19]
## 0 20 40 60
A 0 20 40 60
B 0 22 40 62
csaved input torch.Size([2, 3, 22, 22])
cgrad_output torch.Size([2, 3, 20, 20])
real nontiled_activation torch.Size([2, 3, 22, 22])
real nontiled_grad_out torch.Size([2, 3, 20, 20])
-----------------------------------------

-----------------------------------------

I am [0, 2]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([2, 3, 72, 86])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 70, 84])
padding info :: [3, 3, 3, 0]
grad_input torch.Size([2, 3, 69, 80])
grad_out size torch.Size([2, 3, 68, 80])
crop [40, 20, 0, 48] [0, 79, 0, 67] [40, 59, 0, 19]
## 0 20 40 60
A 0 20 40 60
B 0 22 40 62
csaved input torch.Size([2, 3, 22, 22])
cgrad_output torch.Size([2, 3, 20, 20])
real nontiled_activation torch.Size([2, 3, 22, 22])
real nontiled_grad_out torch.Size([2, 3, 20, 20])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 138, 160])
grad_out size torch.Size([2, 3, 69, 80])
arg size torch.Size([2, 3, 69, 80])
##############grad_in in maxp torch.Size([2, 3, 138, 160])
I am [0, 2]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 140, 162])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 138, 160])
grad_input torch.Size([2, 3, 140, 162])
padding info :: [1, 1, 1, 0]
grad_input old torch.Size([2, 3, 140, 162])
grad_input new torch.Size([2, 3, 140, 162])
new grad_input torch.Size([2, 3, 140, 162])
grad_out size torch.Size([2, 3, 138, 160])
crop [80, 40, 0, 98] [0, 159, 0, 137] [80, 119, 0, 39]
## 0 40 80 120
A 0 40 80 120
B 0 42 80 122
csaved input torch.Size([2, 3, 42, 42])
cgrad_output torch.Size([2, 3, 40, 40])
real nontiled_activation torch.Size([2, 3, 42, 42])
real nontiled_grad_out torch.Size([2, 3, 40, 40])
-----------------------------------------

-----------------------------------------

I am [0, 2]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([2, 3, 142, 164])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 140, 162])
padding info :: [2, 2, 2, 0]
grad_input torch.Size([2, 3, 140, 160])
grad_out size torch.Size([2, 3, 139, 160])
crop [80, 40, 0, 99] [0, 159, 0, 138] [80, 119, 0, 39]
## 0 40 80 120
A 0 40 80 120
B 0 42 80 122
csaved input torch.Size([2, 3, 42, 42])
cgrad_output torch.Size([2, 3, 40, 40])
real nontiled_activation torch.Size([2, 3, 42, 42])
real nontiled_grad_out torch.Size([2, 3, 40, 40])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 280, 320])
grad_out size torch.Size([2, 3, 140, 160])
arg size torch.Size([2, 3, 140, 160])
##############grad_in in maxp torch.Size([2, 3, 280, 320])
I am [0, 2]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 282, 322])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 280, 320])
grad_input torch.Size([2, 3, 282, 322])
padding info :: [1, 1, 1, 0]
grad_input old torch.Size([2, 3, 282, 322])
grad_input new torch.Size([2, 3, 282, 322])
new grad_input torch.Size([2, 3, 282, 322])
grad_out size torch.Size([2, 3, 280, 320])
crop [160, 80, 0, 200] [0, 319, 0, 279] [160, 239, 0, 79]
## 0 80 160 240
A 0 80 160 240
B 0 82 160 242
csaved input torch.Size([2, 3, 82, 82])
cgrad_output torch.Size([2, 3, 80, 80])
real nontiled_activation torch.Size([2, 3, 82, 82])
real nontiled_grad_out torch.Size([2, 3, 80, 80])
-----------------------------------------

-----------------------------------------

I am [0, 2]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([2, 3, 284, 324])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 282, 322])
final torch.Size([2, 3, 284, 324])
padding info :: [2, 2, 2, 0]
after crop g_in torch.Size([2, 3, 80, 80])
grad_out size torch.Size([2, 3, 281, 320])
crop [160, 80, 0, 201] [0, 319, 0, 280] [160, 239, 0, 79]
## 0 80 160 240
A 0 80 160 240
B 0 82 160 242
csaved input torch.Size([2, 3, 82, 82])
cgrad_output torch.Size([2, 3, 80, 80])
real nontiled_activation torch.Size([2, 3, 82, 82])
real nontiled_grad_out torch.Size([2, 3, 80, 80])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 12, 16])
grad_out size torch.Size([2, 3, 10, 10])
arg size torch.Size([2, 3, 6, 8])
new_grad_out torch.Size([2, 3, 6, 8])
##############grad_in in maxp torch.Size([2, 3, 12, 16])
I am [0, 1]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 14, 18])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 12, 16])
grad_input torch.Size([2, 3, 14, 18])
padding info :: [1, 0, 1, 0]
grad_input old torch.Size([2, 3, 14, 18])
grad_input new torch.Size([2, 3, 14, 18])
new grad_input torch.Size([2, 3, 14, 18])
grad_out size torch.Size([2, 3, 12, 16])
crop [5, 6, 0, 7] [0, 15, 0, 11] [5, 9, 0, 4]
## 0 5 5 10
A 0 5 5 10
B 0 7 5 12
csaved input torch.Size([2, 3, 7, 7])
cgrad_output torch.Size([2, 3, 5, 5])
real nontiled_activation torch.Size([2, 3, 7, 7])
real nontiled_grad_out torch.Size([2, 3, 5, 5])
-----------------------------------------

-----------------------------------------

I am [0, 1]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([2, 3, 16, 20])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 14, 18])
padding info :: [2, 0, 2, 0]
grad_input old torch.Size([2, 3, 16, 20])
grad_input new torch.Size([2, 3, 16, 20])
grad_input torch.Size([2, 3, 16, 20])
grad_out size torch.Size([2, 3, 13, 17])
crop [5, 7, 0, 8] [0, 16, 0, 12] [5, 9, 0, 4]
## 0 5 5 10
A 0 5 5 10
B 0 7 5 12
csaved input torch.Size([2, 3, 7, 7])
cgrad_output torch.Size([2, 3, 5, 5])
real nontiled_activation torch.Size([2, 3, 7, 7])
real nontiled_grad_out torch.Size([2, 3, 5, 5])
-----------------------------------------

-----------------------------------------

I am [0, 1]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([2, 3, 18, 22])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 16, 20])
padding info :: [3, 0, 3, 0]
grad_input torch.Size([2, 3, 15, 19])
grad_out size torch.Size([2, 3, 14, 18])
crop [5, 8, 0, 9] [0, 17, 0, 13] [5, 9, 0, 4]
## 0 5 5 10
A 0 5 5 10
B 0 7 5 12
csaved input torch.Size([2, 3, 7, 7])
cgrad_output torch.Size([2, 3, 5, 5])
real nontiled_activation torch.Size([2, 3, 7, 7])
real nontiled_grad_out torch.Size([2, 3, 5, 5])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 30, 38])
grad_out size torch.Size([2, 3, 15, 19])
arg size torch.Size([2, 3, 15, 19])
##############grad_in in maxp torch.Size([2, 3, 30, 38])
I am [0, 1]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 32, 40])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 30, 38])
grad_input torch.Size([2, 3, 32, 40])
padding info :: [1, 0, 1, 0]
grad_input old torch.Size([2, 3, 32, 40])
grad_input new torch.Size([2, 3, 32, 40])
new grad_input torch.Size([2, 3, 32, 40])
grad_out size torch.Size([2, 3, 30, 38])
crop [10, 18, 0, 20] [0, 37, 0, 29] [10, 19, 0, 9]
## 0 10 10 20
A 0 10 10 20
B 0 12 10 22
csaved input torch.Size([2, 3, 12, 12])
cgrad_output torch.Size([2, 3, 10, 10])
real nontiled_activation torch.Size([2, 3, 12, 12])
real nontiled_grad_out torch.Size([2, 3, 10, 10])
-----------------------------------------

-----------------------------------------

I am [0, 1]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([2, 3, 34, 42])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 32, 40])
padding info :: [2, 0, 2, 0]
grad_input old torch.Size([2, 3, 34, 42])
grad_input new torch.Size([2, 3, 34, 42])
grad_input torch.Size([2, 3, 34, 42])
grad_out size torch.Size([2, 3, 31, 39])
crop [10, 19, 0, 21] [0, 38, 0, 30] [10, 19, 0, 9]
## 0 10 10 20
A 0 10 10 20
B 0 12 10 22
csaved input torch.Size([2, 3, 12, 12])
cgrad_output torch.Size([2, 3, 10, 10])
real nontiled_activation torch.Size([2, 3, 12, 12])
real nontiled_grad_out torch.Size([2, 3, 10, 10])
-----------------------------------------

-----------------------------------------

I am [0, 1]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([2, 3, 36, 44])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 34, 42])
padding info :: [3, 1, 3, 0]
grad_input torch.Size([2, 3, 33, 40])
grad_out size torch.Size([2, 3, 32, 40])
crop [10, 20, 0, 22] [0, 39, 0, 31] [10, 19, 0, 9]
## 0 10 10 20
A 0 10 10 20
B 0 12 10 22
csaved input torch.Size([2, 3, 12, 12])
cgrad_output torch.Size([2, 3, 10, 10])
real nontiled_activation torch.Size([2, 3, 12, 12])
real nontiled_grad_out torch.Size([2, 3, 10, 10])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 66, 80])
grad_out size torch.Size([2, 3, 33, 40])
arg size torch.Size([2, 3, 33, 40])
##############grad_in in maxp torch.Size([2, 3, 66, 80])
I am [0, 1]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 68, 82])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 66, 80])
grad_input torch.Size([2, 3, 68, 82])
padding info :: [1, 1, 1, 0]
grad_input old torch.Size([2, 3, 68, 82])
grad_input new torch.Size([2, 3, 68, 82])
new grad_input torch.Size([2, 3, 68, 82])
grad_out size torch.Size([2, 3, 66, 80])
crop [20, 40, 0, 46] [0, 79, 0, 65] [20, 39, 0, 19]
## 0 20 20 40
A 0 20 20 40
B 0 22 20 42
csaved input torch.Size([2, 3, 22, 22])
cgrad_output torch.Size([2, 3, 20, 20])
real nontiled_activation torch.Size([2, 3, 22, 22])
real nontiled_grad_out torch.Size([2, 3, 20, 20])
-----------------------------------------

-----------------------------------------

I am [0, 1]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([2, 3, 70, 84])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 68, 82])
padding info :: [2, 2, 2, 0]
grad_input old torch.Size([2, 3, 70, 84])
grad_input new torch.Size([2, 3, 70, 84])
grad_input torch.Size([2, 3, 70, 84])
grad_out size torch.Size([2, 3, 67, 80])
crop [20, 40, 0, 47] [0, 79, 0, 66] [20, 39, 0, 19]
## 0 20 20 40
A 0 20 20 40
B 0 22 20 42
csaved input torch.Size([2, 3, 22, 22])
cgrad_output torch.Size([2, 3, 20, 20])
real nontiled_activation torch.Size([2, 3, 22, 22])
real nontiled_grad_out torch.Size([2, 3, 20, 20])
-----------------------------------------

-----------------------------------------

I am [0, 1]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([2, 3, 72, 86])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 70, 84])
padding info :: [3, 3, 3, 0]
grad_input torch.Size([2, 3, 69, 80])
grad_out size torch.Size([2, 3, 68, 80])
crop [20, 40, 0, 48] [0, 79, 0, 67] [20, 39, 0, 19]
## 0 20 20 40
A 0 20 20 40
B 0 22 20 42
csaved input torch.Size([2, 3, 22, 22])
cgrad_output torch.Size([2, 3, 20, 20])
real nontiled_activation torch.Size([2, 3, 22, 22])
real nontiled_grad_out torch.Size([2, 3, 20, 20])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 138, 160])
grad_out size torch.Size([2, 3, 69, 80])
arg size torch.Size([2, 3, 69, 80])
##############grad_in in maxp torch.Size([2, 3, 138, 160])
I am [0, 1]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 140, 162])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 138, 160])
grad_input torch.Size([2, 3, 140, 162])
padding info :: [1, 1, 1, 0]
grad_input old torch.Size([2, 3, 140, 162])
grad_input new torch.Size([2, 3, 140, 162])
new grad_input torch.Size([2, 3, 140, 162])
grad_out size torch.Size([2, 3, 138, 160])
crop [40, 80, 0, 98] [0, 159, 0, 137] [40, 79, 0, 39]
## 0 40 40 80
A 0 40 40 80
B 0 42 40 82
csaved input torch.Size([2, 3, 42, 42])
cgrad_output torch.Size([2, 3, 40, 40])
real nontiled_activation torch.Size([2, 3, 42, 42])
real nontiled_grad_out torch.Size([2, 3, 40, 40])
-----------------------------------------

-----------------------------------------

I am [0, 1]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([2, 3, 142, 164])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 140, 162])
padding info :: [2, 2, 2, 0]
grad_input torch.Size([2, 3, 140, 160])
grad_out size torch.Size([2, 3, 139, 160])
crop [40, 80, 0, 99] [0, 159, 0, 138] [40, 79, 0, 39]
## 0 40 40 80
A 0 40 40 80
B 0 42 40 82
csaved input torch.Size([2, 3, 42, 42])
cgrad_output torch.Size([2, 3, 40, 40])
real nontiled_activation torch.Size([2, 3, 42, 42])
real nontiled_grad_out torch.Size([2, 3, 40, 40])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 280, 320])
grad_out size torch.Size([2, 3, 140, 160])
arg size torch.Size([2, 3, 140, 160])
##############grad_in in maxp torch.Size([2, 3, 280, 320])
I am [0, 1]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 282, 322])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 280, 320])
grad_input torch.Size([2, 3, 282, 322])
padding info :: [1, 1, 1, 0]
grad_input old torch.Size([2, 3, 282, 322])
grad_input new torch.Size([2, 3, 282, 322])
new grad_input torch.Size([2, 3, 282, 322])
grad_out size torch.Size([2, 3, 280, 320])
crop [80, 160, 0, 200] [0, 319, 0, 279] [80, 159, 0, 79]
## 0 80 80 160
A 0 80 80 160
B 0 82 80 162
csaved input torch.Size([2, 3, 82, 82])
cgrad_output torch.Size([2, 3, 80, 80])
real nontiled_activation torch.Size([2, 3, 82, 82])
real nontiled_grad_out torch.Size([2, 3, 80, 80])
-----------------------------------------

-----------------------------------------

I am [0, 1]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([2, 3, 284, 324])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 282, 322])
final torch.Size([2, 3, 284, 324])
padding info :: [2, 2, 2, 0]
after crop g_in torch.Size([2, 3, 80, 80])
grad_out size torch.Size([2, 3, 281, 320])
crop [80, 160, 0, 201] [0, 319, 0, 280] [80, 159, 0, 79]
## 0 80 80 160
A 0 80 80 160
B 0 82 80 162
csaved input torch.Size([2, 3, 82, 82])
cgrad_output torch.Size([2, 3, 80, 80])
real nontiled_activation torch.Size([2, 3, 82, 82])
real nontiled_grad_out torch.Size([2, 3, 80, 80])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 12, 12])
grad_out size torch.Size([2, 3, 10, 10])
arg size torch.Size([2, 3, 6, 6])
new_grad_out torch.Size([2, 3, 6, 6])
##############grad_in in maxp torch.Size([2, 3, 12, 12])
I am [0, 0]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 14, 14])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 12, 12])
grad_input torch.Size([2, 3, 14, 14])
padding info :: [1, 0, 1, 0]
grad_input old torch.Size([2, 3, 14, 14])
grad_input new torch.Size([2, 3, 14, 14])
new grad_input torch.Size([2, 3, 14, 14])
grad_out size torch.Size([2, 3, 12, 12])
crop [0, 7, 0, 7] [0, 11, 0, 11] [0, 4, 0, 4]
## 0 5 0 5
A 0 5 0 5
B 0 7 0 7
csaved input torch.Size([2, 3, 7, 7])
cgrad_output torch.Size([2, 3, 5, 5])
real nontiled_activation torch.Size([2, 3, 7, 7])
real nontiled_grad_out torch.Size([2, 3, 5, 5])
-----------------------------------------

-----------------------------------------

I am [0, 0]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([2, 3, 16, 16])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 14, 14])
padding info :: [2, 0, 2, 0]
grad_input old torch.Size([2, 3, 16, 16])
grad_input new torch.Size([2, 3, 16, 16])
grad_input torch.Size([2, 3, 16, 16])
grad_out size torch.Size([2, 3, 13, 13])
crop [0, 8, 0, 8] [0, 12, 0, 12] [0, 4, 0, 4]
## 0 5 0 5
A 0 5 0 5
B 0 7 0 7
csaved input torch.Size([2, 3, 7, 7])
cgrad_output torch.Size([2, 3, 5, 5])
real nontiled_activation torch.Size([2, 3, 7, 7])
real nontiled_grad_out torch.Size([2, 3, 5, 5])
-----------------------------------------

-----------------------------------------

I am [0, 0]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([2, 3, 18, 18])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 16, 16])
padding info :: [3, 0, 3, 0]
grad_input torch.Size([2, 3, 15, 15])
grad_out size torch.Size([2, 3, 14, 14])
crop [0, 9, 0, 9] [0, 13, 0, 13] [0, 4, 0, 4]
## 0 5 0 5
A 0 5 0 5
B 0 7 0 7
csaved input torch.Size([2, 3, 7, 7])
cgrad_output torch.Size([2, 3, 5, 5])
real nontiled_activation torch.Size([2, 3, 7, 7])
real nontiled_grad_out torch.Size([2, 3, 5, 5])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 30, 30])
grad_out size torch.Size([2, 3, 15, 15])
arg size torch.Size([2, 3, 15, 15])
##############grad_in in maxp torch.Size([2, 3, 30, 30])
I am [0, 0]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 32, 32])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 30, 30])
grad_input torch.Size([2, 3, 32, 32])
padding info :: [1, 0, 1, 0]
grad_input old torch.Size([2, 3, 32, 32])
grad_input new torch.Size([2, 3, 32, 32])
new grad_input torch.Size([2, 3, 32, 32])
grad_out size torch.Size([2, 3, 30, 30])
crop [0, 20, 0, 20] [0, 29, 0, 29] [0, 9, 0, 9]
## 0 10 0 10
A 0 10 0 10
B 0 12 0 12
csaved input torch.Size([2, 3, 12, 12])
cgrad_output torch.Size([2, 3, 10, 10])
real nontiled_activation torch.Size([2, 3, 12, 12])
real nontiled_grad_out MaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autogradtorch.Size([2, 3, 10, 10])
-----------------------------------------

-----------------------------------------

I am [0, 0]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([2, 3, 34, 34])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 32, 32])
padding info :: [2, 0, 2, 0]
grad_input old torch.Size([2, 3, 34, 34])
grad_input new torch.Size([2, 3, 34, 34])
grad_input torch.Size([2, 3, 34, 34])
grad_out size torch.Size([2, 3, 31, 31])
crop [0, 21, 0, 21] [0, 30, 0, 30] [0, 9, 0, 9]
## 0 10 0 10
A 0 10 0 10
B 0 12 0 12
csaved input torch.Size([2, 3, 12, 12])
cgrad_output torch.Size([2, 3, 10, 10])
real nontiled_activation torch.Size([2, 3, 12, 12])
real nontiled_grad_out torch.Size([2, 3, 10, 10])
-----------------------------------------

-----------------------------------------

I am [0, 0]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([2, 3, 36, 36])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 34, 34])
padding info :: [3, 0, 3, 0]
grad_input torch.Size([2, 3, 33, 33])
grad_out size torch.Size([2, 3, 32, 32])
crop [0, 22, 0, 22] [0, 31, 0, 31] [0, 9, 0, 9]
## 0 10 0 10
A 0 10 0 10
B 0 12 0 12
csaved input torch.Size([2, 3, 12, 12])
cgrad_output torch.Size([2, 3, 10, 10])
real nontiled_activation torch.Size([2, 3, 12, 12])
real nontiled_grad_out torch.Size([2, 3, 10, 10])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 66, 66])
grad_out size torch.Size([2, 3, 33, 33])
arg size torch.Size([2, 3, 33, 33])
##############grad_in in maxp torch.Size([2, 3, 66, 66])
I am [0, 0]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 68, 68])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 66, 66])
grad_input torch.Size([2, 3, 68, 68])
padding info :: [1, 0, 1, 0]
grad_input old torch.Size([2, 3, 68, 68])
grad_input new torch.Size([2, 3, 68, 68])
new grad_input torch.Size([2, 3, 68, 68])
grad_out size torch.Size([2, 3, 66, 66])
crop [0, 46, 0, 46] [0, 65, 0, 65] [0, 19, 0, 19]
## 0 20 0 20
A 0 20 0 20
B 0 22 0 22
csaved input torch.Size([2, 3, 22, 22])
cgrad_output torch.Size([2, 3, 20, 20])
real nontiled_activation torch.Size([2, 3, 22, 22])
real nontiled_grad_out torch.Size([2, 3, 20, 20])
-----------------------------------------

-----------------------------------------

I am [0, 0]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([2, 3, 70, 70])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 68, 68])
padding info :: [2, 0, 2, 0]
grad_input old torch.Size([2, 3, 70, 70])
grad_input new torch.Size([2, 3, 70, 70])
grad_input torch.Size([2, 3, 70, 70])
grad_out size torch.Size([2, 3, 67, 67])
crop [0, 47, 0, 47] [0, 66, 0, 66] [0, 19, 0, 19]
## 0 20 0 20
A 0 20 0 20
B 0 22 0 22
csaved input torch.Size([2, 3, 22, 22])
cgrad_output torch.Size([2, 3, 20, 20])
real nontiled_activation torch.Size([2, 3, 22, 22])
real nontiled_grad_out torch.Size([2, 3, 20, 20])
-----------------------------------------

-----------------------------------------

I am [0, 0]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([2, 3, 72, 72])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 70, 70])
padding info :: [3, 0, 3, 0]
grad_input torch.Size([2, 3, 69, 69])
grad_out size torch.Size([2, 3, 68, 68])
crop [0, 48, 0, 48] [0, 67, 0, 67] [0, 19, 0, 19]
## 0 20 0 20
A 0 20 0 20
B 0 22 0 22
csaved input torch.Size([2, 3, 22, 22])
cgrad_output torch.Size([2, 3, 20, 20])
real nontiled_activation torch.Size([2, 3, 22, 22])
real nontiled_grad_out torch.Size([2, 3, 20, 20])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 138, 138])
grad_out size torch.Size([2, 3, 69, 69])
arg size torch.Size([2, 3, 69, 69])
##############grad_in in maxp torch.Size([2, 3, 138, 138])
I am [0, 0]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 140, 140])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 138, 138])
grad_input torch.Size([2, 3, 140, 140])
padding info :: [1, 0, 1, 0]
grad_input old torch.Size([2, 3, 140, 140])
grad_input new torch.Size([2, 3, 140, 140])
new grad_input torch.Size([2, 3, 140, 140])
grad_out size torch.Size([2, 3, 138, 138])
crop [0, 98, 0, 98] [0, 137, 0, 137] [0, 39, 0, 39]
## 0 40 0 40
A 0 40 0 40
B 0 42 0 42
csaved input torch.Size([2, 3, 42, 42])
cgrad_output torch.Size([2, 3, 40, 40])
real nontiled_activation torch.Size([2, 3, 42, 42])
real nontiled_grad_out torch.Size([2, 3, 40, 40])
-----------------------------------------

-----------------------------------------

I am [0, 0]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([2, 3, 142, 142])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 140, 140])
padding info :: [2, 0, 2, 0]
grad_input torch.Size([2, 3, 140, 140])
grad_out size torch.Size([2, 3, 139, 139])
crop [0, 99, 0, 99] [0, 138, 0, 138] [0, 39, 0, 39]
## 0 40 0 40
A 0 40 0 40
B 0 42 0 42
csaved input torch.Size([2, 3, 42, 42])
cgrad_output torch.Size([2, 3, 40, 40])
real nontiled_activation torch.Size([2, 3, 42, 42])
real nontiled_grad_out torch.Size([2, 3, 40, 40])
-----------------------------------------

-----------------------------------------


^^^^^cMaxPool2dFunction bwd
input size torch.Size([2, 3, 280, 280])
grad_out size torch.Size([2, 3, 140, 140])
arg size torch.Size([2, 3, 140, 140])
##############grad_in in maxp torch.Size([2, 3, 280, 280])
I am [0, 0]
@@@ using cudnn bkw
local last ++ input shape torch.Size([2, 3, 282, 282])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 280, 280])
grad_input torch.Size([2, 3, 282, 282])
padding info :: [1, 0, 1, 0]
grad_input old torch.Size([2, 3, 282, 282])
grad_input new torch.Size([2, 3, 282, 282])
new grad_input torch.Size([2, 3, 282, 282])
grad_out size torch.Size([2, 3, 280, 280])
crop [0, 200, 0, 200] [0, 279, 0, 279] [0, 79, 0, 79]
## 0 80 0 80
A 0 80 0 80
B 0 82 0 82
csaved input torch.Size([2, 3, 82, 82])
cgrad_output torch.Size([2, 3, 80, 80])
real nontiled_activation torch.Size([2, 3, 82, 82])
real nontiled_grad_out torch.Size([2, 3, 80, 80])
-----------------------------------------

-----------------------------------------

I am [0, 0]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([2, 3, 284, 284])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([2, 3, 282, 282])
final torch.Size([2, 3, 284, 284])
padding info :: [2, 0, 2, 0]
after crop g_in torch.Size([2, 3, 80, 80])
grad_out size torch.Size([2, 3, 281, 281])
crop [0, 201, 0, 201] [0, 280, 0, 280] [0, 79, 0, 79]
## 0 80 0 80
A 0 80 0 80
B 0 82 0 82
csaved input torch.Size([2, 3, 82, 82])
cgrad_output torch.Size([2, 3, 80, 80])
real nontiled_activation torch.Size([2, 3, 82, 82])
real nontiled_grad_out torch.Size([2, 3, 80, 80])
-----------------------------------------

-----------------------------------------

~~ check forward correctness ~~
-----------------------------------------

#### compare grad_in
-----------------------------------------

#### compare w1
-----------------------------------------

#### compare w2
-----------------------------------------

#### compare w3
-----------------------------------------

#### compare w4
-----------------------------------------

#### compare w5
-----------------------------------------

#### compare w6
-----------------------------------------

#### compare w7
-----------------------------------------

#### compare w8
-----------------------------------------

#### compare w9
-----------------------------------------

#### compare w10
-----------------------------------------

#### compare w11
-----------------------------------------

#### compare w12
-----------------------------------------

#### compare w13
-----------------------------------------

/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
