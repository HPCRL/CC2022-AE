[torch/csrc/autograd/engine.cpp] call_function SumBackward0
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function CudnnConvolutionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function CudnnConvolutionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function CudnnConvolutionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function CudnnConvolutionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function CudnnConvolutionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function CudnnConvolutionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function MaxPool2DWithIndicesBackward
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function CudnnConvolutionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function CudnnConvolutionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function CudnnConvolutionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function CudnnConvolutionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function CudnnConvolutionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function MaxPool2DWithIndicesBackward
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function CudnnConvolutionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd
&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&

Inside Conv2d forward
input size :  torch.Size([1, 3, 32, 32])
output size :  torch.Size([32, 32, 32])
Inside MaxPool2d forward
input size :  torch.Size([1, 32, 32, 32])
output size :  torch.Size([32, 16, 16])
Inside Conv2d forward
input size :  torch.Size([1, 32, 16, 16])
output size :  torch.Size([64, 16, 16])
Inside MaxPool2d forward
input size :  torch.Size([1, 64, 16, 16])
output size :  torch.Size([64, 8, 8])
Inside Conv2d forward
input size :  torch.Size([1, 64, 8, 8])
output size :  torch.Size([128, 8, 8])
Inside Conv2d forward
input size :  torch.Size([1, 128, 8, 8])
output size :  torch.Size([64, 8, 8])
Inside Conv2d forward
input size :  torch.Size([1, 64, 8, 8])
output size :  torch.Size([128, 8, 8])
Inside MaxPool2d forward
input size :  torch.Size([1, 128, 8, 8])
output size :  torch.Size([128, 4, 4])
Inside Conv2d forward
input size :  torch.Size([1, 128, 4, 4])
output size :  torch.Size([256, 4, 4])
Inside Conv2d forward
input size :  torch.Size([1, 256, 4, 4])
output size :  torch.Size([128, 4, 4])
Inside Conv2d forward
input size :  torch.Size([1, 128, 4, 4])
output size :  torch.Size([256, 4, 4])
Inside MaxPool2d forward
input size :  torch.Size([1, 256, 4, 4])
output size :  torch.Size([256, 2, 2])
Inside Conv2d forward
input size :  torch.Size([1, 256, 2, 2])
output size :  torch.Size([512, 2, 2])
Inside Conv2d forward
input size :  torch.Size([1, 512, 2, 2])
output size :  torch.Size([256, 2, 2])
Inside Conv2d forward
input size :  torch.Size([1, 256, 2, 2])
output size :  torch.Size([512, 2, 2])
Inside Conv2d forward
input size :  torch.Size([1, 512, 2, 2])
output size :  torch.Size([256, 2, 2])
Inside Conv2d forward
input size :  torch.Size([1, 256, 2, 2])
output size :  torch.Size([512, 2, 2])
Inside MaxPool2d forward
input size :  torch.Size([1, 512, 2, 2])
output size :  torch.Size([512, 1, 1])
Inside Conv2d forward
input size :  torch.Size([1, 512, 1, 1])
output size :  torch.Size([1024, 1, 1])
Inside Conv2d forward
input size :  torch.Size([1, 1024, 1, 1])
output size :  torch.Size([512, 1, 1])
Inside Conv2d forward
input size :  torch.Size([1, 512, 1, 1])
output size :  torch.Size([1024, 1, 1])
Inside Conv2d forward
input size :  torch.Size([1, 1024, 1, 1])
output size :  torch.Size([512, 1, 1])
Inside Conv2d forward
input size :  torch.Size([1, 512, 1, 1])
output size :  torch.Size([1024, 1, 1])
Inside Conv2d forward
input size :  torch.Size([1, 1024, 1, 1])
output size :  torch.Size([2048, 1, 1])
done ref
Inside Conv2d backward
grad_output size :  torch.Size([1, 2048, 1, 1])
grad_input size :  torch.Size([1, 1024, 1, 1])
Inside Conv2d backward
grad_output size :  torch.Size([1, 1024, 1, 1])
grad_input size :  torch.Size([1, 512, 1, 1])
Inside Conv2d backward
grad_output size :  torch.Size([1, 512, 1, 1])
grad_input size :  torch.Size([1, 1024, 1, 1])
Inside Conv2d backward
grad_output size :  torch.Size([1, 1024, 1, 1])
grad_input size :  torch.Size([1, 512, 1, 1])
Inside Conv2d backward
grad_output size :  torch.Size([1, 512, 1, 1])
grad_input size :  torch.Size([1, 1024, 1, 1])
Inside Conv2d backward
grad_output size :  torch.Size([1, 1024, 1, 1])
grad_input size :  torch.Size([1, 512, 1, 1])
Inside MaxPool2d backward
grad_output size :  torch.Size([1, 512, 1, 1])
grad_input size :  torch.Size([1, 512, 2, 2])
Inside Conv2d backward
grad_output size :  torch.Size([1, 512, 2, 2])
grad_input size :  torch.Size([1, 256, 2, 2])
Inside Conv2d backward
grad_output size :  torch.Size([1, 256, 2, 2])
grad_input size :  torch.Size([1, 512, 2, 2])
Inside Conv2d backward
grad_output size :  torch.Size([1, 512, 2, 2])
grad_input size :  torch.Size([1, 256, 2, 2])
Inside Conv2d backward
grad_output size :  torch.Size([1, 256, 2, 2])
grad_input size :  torch.Size([1, 512, 2, 2])
Inside Conv2d backward
grad_output size :  torch.Size([1, 512, 2, 2])
grad_input size :  torch.Size([1, 256, 2, 2])
Inside MaxPool2d backward
grad_output size :  torch.Size([1, 256, 2, 2])
grad_input size :  torch.Size([1, 256, 4, 4])
Inside Conv2d backward
grad_output size :  torch.Size([1, 256, 4, 4])
grad_input size :  torch.Size([1, 128, 4, 4])
Inside Conv2d backward
grad_output size :  torch.Size([1, 128, 4, 4])
grad_input size :  torch.Size([1, 256, 4, 4])
Inside Conv2d backward
grad_output size :  torch.Size([1, 256, 4, 4])
grad_input size :  torch.Size([1, 128, 4, 4])
Inside MaxPool2d backward
grad_output size :  torch.Size([1, 128, 4, 4])
grad_input size :  torch.Size([1, 128, 8, 8])
Inside Conv2d backward
grad_output size :  torch.Size([1, 128, 8, 8])
grad_input size :  torch.Size([1, 64, 8, 8])
Inside Conv2d backward
grad_output size :  torch.Size([1, 64, 8, 8])
grad_input size :  torch.Size([1, 128, 8, 8])
Inside Conv2d backward
grad_output size :  torch.Size([1, 128, 8, 8])
grad_input size :  torch.Size([1, 64, 8, 8])
Inside MaxPool2d backward
grad_output size :  torch.Size([1, 64, 8, 8])
grad_input size :  torch.Size([1, 64, 16, 16])
Inside Conv2d backward
grad_output size :  torch.Size([1, 64, 16, 16])
grad_input size :  torch.Size([1, 32, 16, 16])
Inside MaxPool2d backward
grad_output size :  torch.Size([1, 32, 16, 16])
grad_input size :  torch.Size([1, 32, 32, 32])
Inside Conv2d backward
grad_output size :  torch.Size([1, 32, 32, 32])
grad_input size :  torch.Size([1, 3, 32, 32])
done ref bkw

&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&

Input 1x3x32x32
L-->R current op 140450320871088
after conv2d 1x32x32x32
L-->R current op 140450320871184
after maxpool2d 1x32x16x16
L-->R current op 140448512502464
after conv2d 1x64x16x16
L-->R current op 140450320787440
after maxpool2d 1x64x8x8
L-->R current op 140450320787200
after conv2d 1x128x8x8
L-->R current op 140450320786864
after conv2d 1x64x8x8
L-->R current op 140450320786768
after conv2d 1x128x8x8
L-->R current op 140450320786672
after maxpool2d 1x128x4x4
L-->R current op 140450320786624
after conv2d 1x256x4x4
L-->R current op 140450320786528
after conv2d 1x128x4x4
L-->R current op 140448511540336
after conv2d 1x256x4x4
L-->R current op 140448511539472
after maxpool2d 1x256x2x2
L-->R current op 140448511540624
after conv2d 1x512x2x2
L-->R current op 140448511540672
after conv2d 1x256x2x2
L-->R current op 140448511540720
after conv2d 1x512x2x2
L-->R current op 140448511540768
after conv2d 1x256x2x2
L-->R current op 140448511540816
after conv2d 1x512x2x2
L-->R current op 140448511540864
after maxpool2d 1x512x1x1
L-->R current op 140448511540912
after conv2d 1x1024x1x1
L-->R current op 140448511540960
after conv2d 1x512x1x1
L-->R current op 140448511541008
after conv2d 1x1024x1x1
L-->R current op 140448511541056
after conv2d 1x512x1x1
L-->R current op 140448511541104
after conv2d 1x1024x1x1
L-->R current op 140448511541152
after conv2d 1x2048x1x1
!!!!!!! 1 1
torch.Size([1, 3, 32, 32])
torch.Size([1, 32, 32, 32])
torch.Size([1, 32, 16, 16])
torch.Size([1, 64, 16, 16])
torch.Size([1, 64, 8, 8])
torch.Size([1, 128, 8, 8])
torch.Size([1, 64, 8, 8])
torch.Size([1, 128, 8, 8])
torch.Size([1, 128, 4, 4])
torch.Size([1, 256, 4, 4])
torch.Size([1, 128, 4, 4])
torch.Size([1, 256, 4, 4])
torch.Size([1, 256, 2, 2])
torch.Size([1, 512, 2, 2])
torch.Size([1, 256, 2, 2])
torch.Size([1, 512, 2, 2])
torch.Size([1, 256, 2, 2])
torch.Size([1, 512, 2, 2])
torch.Size([1, 512, 1, 1])
torch.Size([1, 1024, 1, 1])
torch.Size([1, 512, 1, 1])
torch.Size([1, 1024, 1, 1])
torch.Size([1, 512, 1, 1])
torch.Size([1, 1024, 1, 1])
i 24 24
coord [0, 0]
bwd_out_shape  (1, 1)
fwd_out_shape  (1, 1)
info for [0, 0] [{-11: fake[-11,-11] PI( <0,0,>,
 <otileshape 1,1,>,
 <padding >,
 <inpslidx 0,0,0,0,>, 
 <internal >, 
 <realidx 0,0,0,0,>, 
 <ndtsize >, 
  local_first False
 next_id -11)
, 140448511541152: conv2d140448511541152[0,0] PI( <0,0,>,
 <otileshape 1,1,>,
 <padding 0,0,0,0,>,
 <inpslidx 0,0,0,0,>, 
 <internal 0,0,0,0,>, 
 <realidx 0,0,0,0,>, 
 <ndtsize 1,1,>, 
  local_first False
 next_id -99)
, 140448511541104: conv2d140448511541104[1,1] PI( <0,0,>,
 <otileshape 1,1,>,
 <padding 1,1,1,1,>,
 <inpslidx 0,0,0,0,>, 
 <internal 0,0,0,0,>, 
 <realidx -1,1,-1,1,>, 
 <ndtsize 1,1,>, 
  local_first False
 next_id 140448511541152)
, 140448511541056: conv2d140448511541056[2,2] PI( <0,0,>,
 <otileshape 1,1,>,
 <padding 1,1,1,1,>,
 <inpslidx 0,0,0,0,>, 
 <internal 0,0,0,0,>, 
 <realidx -1,1,-1,1,>, 
 <ndtsize 1,1,>, 
  local_first False
 next_id 140448511541104)
, 140448511541008: conv2d140448511541008[3,3] PI( <0,0,>,
 <otileshape 1,1,>,
 <padding 2,2,2,2,>,
 <inpslidx 0,0,0,0,>, 
 <internal 0,0,0,0,>, 
 <realidx -2,2,-2,2,>, 
 <ndtsize 1,1,>, 
  local_first False
 next_id 140448511541056)
, 140448511540960: conv2d140448511540960[4,4] PI( <0,0,>,
 <otileshape 1,1,>,
 <padding 2,2,2,2,>,
 <inpslidx 0,0,0,0,>, 
 <internal 0,0,0,0,>, 
 <realidx -2,2,-2,2,>, 
 <ndtsize 1,1,>, 
  local_first False
 next_id 140448511541008)
, 140448511540912: conv2d140448511540912[5,5] PI( <0,0,>,
 <otileshape 1,1,>,
 <padding 3,3,3,3,>,
 <inpslidx 0,0,0,0,>, 
 <internal 0,0,0,0,>, 
 <realidx -3,3,-3,3,>, 
 <ndtsize 1,1,>, 
  local_first True
 next_id 140448511540960)
, 140448511540864: maxpool2d140448511540864[6,-1] PI( <0,0,>,
 <otileshape 1,1,>,
 <padding >,
 <inpslidx 0,1,0,1,>, 
 <internal >, 
 <realidx 0,1,0,1,>, 
 <ndtsize 1,1,>, 
  local_first False
 next_id 140448511540912)
, 140448511540816: conv2d140448511540816[7,0] PI( <0,0,>,
 <otileshape 2,2,>,
 <padding 1,1,1,1,>,
 <inpslidx 0,1,0,1,>, 
 <internal 0,0,0,0,>, 
 <realidx -1,2,-1,2,>, 
 <ndtsize 2,2,>, 
  local_first False
 next_id 140448511540864)
, 140448511540768: conv2d140448511540768[8,1] PI( <0,0,>,
 <otileshape 2,2,>,
 <padding 1,1,1,1,>,
 <inpslidx 0,1,0,1,>, 
 <internal 0,0,0,0,>, 
 <realidx -1,2,-1,2,>, 
 <ndtsize 2,2,>, 
  local_first False
 next_id 140448511540816)
, 140448511540720: conv2d140448511540720[9,2] PI( <0,0,>,
 <otileshape 2,2,>,
 <padding 2,2,2,2,>,
 <inpslidx 0,1,0,1,>, 
 <internal 0,0,0,0,>, 
 <realidx -2,3,-2,3,>, 
 <ndtsize 2,2,>, 
  local_first False
 next_id 140448511540768)
, 140448511540672: conv2d140448511540672[10,3] PI( <0,0,>,
 <otileshape 2,2,>,
 <padding 2,2,2,2,>,
 <inpslidx 0,1,0,1,>, 
 <internal 0,0,0,0,>, 
 <realidx -2,3,-2,3,>, 
 <ndtsize 2,2,>, 
  local_first False
 next_id 140448511540720)
, 140448511540624: conv2d140448511540624[11,4] PI( <0,0,>,
 <otileshape 2,2,>,
 <padding 3,3,3,3,>,
 <inpslidx 0,1,0,1,>, 
 <internal 0,0,0,0,>, 
 <realidx -3,4,-3,4,>, 
 <ndtsize 2,2,>, 
  local_first True
 next_id 140448511540672)
, 140448511539472: maxpool2d140448511539472[12,-1] PI( <0,0,>,
 <otileshape 2,2,>,
 <padding >,
 <inpslidx 0,3,0,3,>, 
 <internal >, 
 <realidx 0,3,0,3,>, 
 <ndtsize 2,2,>, 
  local_first False
 next_id 140448511540624)
, 140448511540336: conv2d140448511540336[13,0] PI( <0,0,>,
 <otileshape 4,4,>,
 <padding 1,1,1,1,>,
 <inpslidx 0,3,0,3,>, 
 <internal 0,0,0,0,>, 
 <realidx -1,4,-1,4,>, 
 <ndtsize 4,4,>, 
  local_first False
 next_id 140448511539472)
, 140450320786528: conv2d140450320786528[14,1] PI( <0,0,>,
 <otileshape 4,4,>,
 <padding 1,1,1,1,>,
 <inpslidx 0,3,0,3,>, 
 <internal 0,0,0,0,>, 
 <realidx -1,4,-1,4,>, 
 <ndtsize 4,4,>, 
  local_first False
 next_id 140448511540336)
, 140450320786624: conv2d140450320786624[15,2] PI( <0,0,>,
 <otileshape 4,4,>,
 <padding 2,2,2,2,>,
 <inpslidx 0,3,0,3,>, 
 <internal 0,0,0,0,>, 
 <realidx -2,5,-2,5,>, 
 <ndtsize 4,4,>, 
  local_first True
 next_id 140450320786528)
, 140450320786672: maxpool2d140450320786672[16,-1] PI( <0,0,>,
 <otileshape 4,4,>,
 <padding >,
 <inpslidx 0,7,0,7,>, 
 <internal >, 
 <realidx 0,7,0,7,>, 
 <ndtsize 4,4,>, 
  local_first False
 next_id 140450320786624)
, 140450320786768: conv2d140450320786768[17,0] PI( <0,0,>,
 <otileshape 8,8,>,
 <padding 1,1,1,1,>,
 <inpslidx 0,7,0,7,>, 
 <internal 0,0,0,0,>, 
 <realidx -1,8,-1,8,>, 
 <ndtsize 8,8,>, 
  local_first False
 next_id 140450320786672)
, 140450320786864: conv2d140450320786864[18,1] PI( <0,0,>,
 <otileshape 8,8,>,
 <padding 1,1,1,1,>,
 <inpslidx 0,7,0,7,>, 
 <internal 0,0,0,0,>, 
 <realidx -1,8,-1,8,>, 
 <ndtsize 8,8,>, 
  local_first False
 next_id 140450320786768)
, 140450320787200: conv2d140450320787200[19,2] PI( <0,0,>,
 <otileshape 8,8,>,
 <padding 2,2,2,2,>,
 <inpslidx 0,7,0,7,>, 
 <internal 0,0,0,0,>, 
 <realidx -2,9,-2,9,>, 
 <ndtsize 8,8,>, 
  local_first True
 next_id 140450320786864)
, 140450320787440: maxpool2d140450320787440[20,-1] PI( <0,0,>,
 <otileshape 8,8,>,
 <padding >,
 <inpslidx 0,15,0,15,>, 
 <internal >, 
 <realidx 0,15,0,15,>, 
 <ndtsize 8,8,>, 
  local_first False
 next_id 140450320787200)
, 140448512502464: conv2d140448512502464[21,0] PI( <0,0,>,
 <otileshape 16,16,>,
 <padding 1,1,1,1,>,
 <inpslidx 0,15,0,15,>, 
 <internal 0,0,0,0,>, 
 <realidx -1,16,-1,16,>, 
 <ndtsize 16,16,>, 
  local_first True
 next_id 140450320787440)
, 140450320871184: maxpool2d140450320871184[22,-1] PI( <0,0,>,
 <otileshape 16,16,>,
 <padding >,
 <inpslidx 0,31,0,31,>, 
 <internal >, 
 <realidx 0,31,0,31,>, 
 <ndtsize 16,16,>, 
  local_first False
 next_id 140448512502464)
, 140450320871088: conv2d140450320871088[23,0] PI( <0,0,>,
 <otileshape 32,32,>,
 <padding 1,1,1,1,>,
 <inpslidx 0,31,0,31,>, 
 <internal 0,0,0,0,>, 
 <realidx -1,32,-1,32,>, 
 <ndtsize 32,32,>, 
  local_first True
 next_id 140450320871184)
}, {-11: fake-bp[-11,-11] PI( <0,0,>,
 <otileshape 32,32,>,
 <padding >,
 <inpslidx 0,31,0,31,>, 
 <internal >, 
 <realidx 0,31,0,31,>, 
 <ndtsize >, 
  local_first False
 next_id -11)
, 140450320871088: bk-conv2d140450320871088[0,0] PI( <0,0,>,
 <otileshape 32,32,>,
 <padding 1,1,1,1,>,
 <inpslidx 0,31,0,31,>, 
 <internal 0,0,0,0,>, 
 <realidx -1,32,-1,32,>, 
 <ndtsize >, 
  local_first False
 next_id -99)
, 140450320871184: bk-maxpool2d140450320871184[1,-1] PI( <0,0,>,
 <otileshape 16,16,>,
 <padding >,
 <inpslidx 0,15,0,15,>, 
 <internal >, 
 <realidx 0,15,0,15,>, 
 <ndtsize >, 
  local_first False
 next_id 140450320871088)
, 140448512502464: bk-conv2d140448512502464[2,0] PI( <0,0,>,
 <otileshape 16,16,>,
 <padding 1,1,1,1,>,
 <inpslidx 0,15,0,15,>, 
 <internal 0,0,0,0,>, 
 <realidx -1,16,-1,16,>, 
 <ndtsize >, 
  local_first False
 next_id 140450320871184)
, 140450320787440: bk-maxpool2d140450320787440[3,-1] PI( <0,0,>,
 <otileshape 8,8,>,
 <padding >,
 <inpslidx 0,7,0,7,>, 
 <internal >, 
 <realidx 0,7,0,7,>, 
 <ndtsize >, 
  local_first False
 next_id 140448512502464)
, 140450320787200: bk-conv2d140450320787200[4,0] PI( <0,0,>,
 <otileshape 8,8,>,
 <padding 1,1,1,1,>,
 <inpslidx 0,7,0,7,>, 
 <internal 0,0,0,0,>, 
 <realidx -1,8,-1,8,>, 
 <ndtsize >, 
  local_first False
 next_id 140450320787440)
, 140450320786864: bk-conv2d140450320786864[5,1] PI( <0,0,>,
 <otileshape 8,8,>,
 <padding 1,1,1,1,>,
 <inpslidx 0,7,0,7,>, 
 <internal 0,0,0,0,>, 
 <realidx -1,8,-1,8,>, 
 <ndtsize >, 
  local_first False
 next_id 140450320787200)
, 140450320786768: bk-conv2d140450320786768[6,2] PI( <0,0,>,
 <otileshape 8,8,>,
 <padding 2,2,2,2,>,
 <inpslidx 0,7,0,7,>, 
 <internal 0,0,0,0,>, 
 <realidx -2,9,-2,9,>, 
 <ndtsize >, 
  local_first False
 next_id 140450320786864)
, 140450320786672: bk-maxpool2d140450320786672[7,-1] PI( <0,0,>,
 <otileshape 4,4,>,
 <padding >,
 <inpslidx 0,3,0,3,>, 
 <internal >, 
 <realidx 0,3,0,3,>, 
 <ndtsize >, 
  local_first False
 next_id 140450320786768)
, 140450320786624: bk-conv2d140450320786624[8,0] PI( <0,0,>,
 <otileshape 4,4,>,
 <padding 1,1,1,1,>,
 <inpslidx 0,3,0,3,>, 
 <internal 0,0,0,0,>, 
 <realidx -1,4,-1,4,>, 
 <ndtsize >, 
  local_first False
 next_id 140450320786672)
, 140450320786528: bk-conv2d140450320786528[9,1] PI( <0,0,>,
 <otileshape 4,4,>,
 <padding 1,1,1,1,>,
 <inpslidx 0,3,0,3,>, 
 <internal 0,0,0,0,>, 
 <realidx -1,4,-1,4,>, 
 <ndtsize >, 
  local_first False
 next_id 140450320786624)
, 140448511540336: bk-conv2d140448511540336[10,2] PI( <0,0,>,
 <otileshape 4,4,>,
 <padding 2,2,2,2,>,
 <inpslidx 0,3,0,3,>, 
 <internal 0,0,0,0,>, 
 <realidx -2,5,-2,5,>, 
 <ndtsize >, 
  local_first False
 next_id 140450320786528)
, 140448511539472: bk-maxpool2d140448511539472[11,-1] PI( <0,0,>,
 <otileshape 2,2,>,
 <padding >,
 <inpslidx 0,1,0,1,>, 
 <internal >, 
 <realidx 0,1,0,1,>, 
 <ndtsize >, 
  local_first False
 next_id 140448511540336)
, 140448511540624: bk-conv2d140448511540624[12,0] PI( <0,0,>,
 <otileshape 2,2,>,
 <padding 1,1,1,1,>,
 <inpslidx 0,1,0,1,>, 
 <internal 0,0,0,0,>, 
 <realidx -1,2,-1,2,>, 
 <ndtsize >, 
  local_first False
 next_id 140448511539472)
, 140448511540672: bk-conv2d140448511540672[13,1] PI( <0,0,>,
 <otileshape 2,2,>,
 <padding 1,1,1,1,>,
 <inpslidx 0,1,0,1,>, 
 <internal 0,0,0,0,>, 
 <realidx -1,2,-1,2,>, 
 <ndtsize >, 
  local_first False
 next_id 140448511540624)
, 140448511540720: bk-conv2d140448511540720[14,2] PI( <0,0,>,
 <otileshape 2,2,>,
 <padding 2,2,2,2,>,
 <inpslidx 0,1,0,1,>, 
 <internal 0,0,0,0,>, 
 <realidx -2,3,-2,3,>, 
 <ndtsize >, 
  local_first False
 next_id 140448511540672)
, 140448511540768: bk-conv2d140448511540768[15,3] PI( <0,0,>,
 <otileshape 2,2,>,
 <padding 2,2,2,2,>,
 <inpslidx 0,1,0,1,>, 
 <internal 0,0,0,0,>, 
 <realidx -2,3,-2,3,>, 
 <ndtsize >, 
  local_first False
 next_id 140448511540720)
, 140448511540816: bk-conv2d140448511540816[16,4] PI( <0,0,>,
 <otileshape 2,2,>,
 <padding 3,3,3,3,>,
 <inpslidx 0,1,0,1,>, 
 <internal 0,0,0,0,>, 
 <realidx -3,4,-3,4,>, 
 <ndtsize >, 
  local_first False
 next_id 140448511540768)
, 140448511540864: bk-maxpool2d140448511540864[17,-1] PI( <0,0,>,
 <otileshape 1,1,>,
 <padding >,
 <inpslidx 0,0,0,0,>, 
 <internal >, 
 <realidx 0,0,0,0,>, 
 <ndtsize >, 
  local_first False
 next_id 140448511540816)
, 140448511540912: bk-conv2d140448511540912[18,0] PI( <0,0,>,
 <otileshape 1,1,>,
 <padding 1,1,1,1,>,
 <inpslidx 0,0,0,0,>, 
 <internal 0,0,0,0,>, 
 <realidx -1,1,-1,1,>, 
 <ndtsize >, 
  local_first False
 next_id 140448511540864)
, 140448511540960: bk-conv2d140448511540960[19,1] PI( <0,0,>,
 <otileshape 1,1,>,
 <padding 1,1,1,1,>,
 <inpslidx 0,0,0,0,>, 
 <internal 0,0,0,0,>, 
 <realidx -1,1,-1,1,>, 
 <ndtsize >, 
  local_first False
 next_id 140448511540912)
, 140448511541008: bk-conv2d140448511541008[20,2] PI( <0,0,>,
 <otileshape 1,1,>,
 <padding 2,2,2,2,>,
 <inpslidx 0,0,0,0,>, 
 <internal 0,0,0,0,>, 
 <realidx -2,2,-2,2,>, 
 <ndtsize >, 
  local_first False
 next_id 140448511540960)
, 140448511541056: bk-conv2d140448511541056[21,3] PI( <0,0,>,
 <otileshape 1,1,>,
 <padding 2,2,2,2,>,
 <inpslidx 0,0,0,0,>, 
 <internal 0,0,0,0,>, 
 <realidx -2,2,-2,2,>, 
 <ndtsize >, 
  local_first False
 next_id 140448511541008)
, 140448511541104: bk-conv2d140448511541104[22,4] PI( <0,0,>,
 <otileshape 1,1,>,
 <padding 3,3,3,3,>,
 <inpslidx 0,0,0,0,>, 
 <internal 0,0,0,0,>, 
 <realidx -3,3,-3,3,>, 
 <ndtsize >, 
  local_first False
 next_id 140448511541056)
, 140448511541152: bk-conv2d140448511541152[23,5] PI( <0,0,>,
 <otileshape 1,1,>,
 <padding 3,3,3,3,>,
 <inpslidx 0,0,0,0,>, 
 <internal 0,0,0,0,>, 
 <realidx -3,3,-3,3,>, 
 <ndtsize >, 
  local_first False
 next_id 140448511541104)
}]
++++++++++++++++++++++++++++++++++++++++++++++++
***input tile torch.Size([1, 3, 32, 32])
in customized Sequential 2
id 140450320871088
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([1, 32, 32, 32])
in customized Sequential 3
in customized Sequential 3
id 140448512502464
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([1, 64, 16, 16])
in customized Sequential 3
in customized Sequential 3
id 140450320787200
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([1, 128, 10, 10])
in customized Sequential 3
id 140450320786864
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([1, 64, 10, 10])
in customized Sequential 3
id 140450320786768
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([1, 128, 8, 8])
in customized Sequential 3
in customized Sequential 3
id 140450320786624
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([1, 256, 6, 6])
in customized Sequential 3
id 140450320786528
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([1, 128, 6, 6])
in customized Sequential 3
id 140448511540336
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([1, 256, 4, 4])
in customized Sequential 3
in customized Sequential 3
id 140448511540624
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([1, 512, 6, 6])
in customized Sequential 3
id 140448511540672
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([1, 256, 6, 6])
in customized Sequential 3
id 140448511540720
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([1, 512, 4, 4])
in customized Sequential 3
id 140448511540768
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([1, 256, 4, 4])
in customized Sequential 3
id 140448511540816
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([1, 512, 2, 2])
in customized Sequential 3
in customized Sequential 3
id 140448511540912
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([1, 1024, 5, 5])
in customized Sequential 3
id 140448511540960
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([1, 512, 5, 5])
in customized Sequential 3
id 140448511541008
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([1, 1024, 3, 3])
in customized Sequential 3
id 140448511541056
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([1, 512, 3, 3])
in customized Sequential 3
id 140448511541104
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([1, 1024, 1, 1])
in customized Sequential 3
id 140448511541152
== tiled conv2d forward
shape input_tile_for_next
 torch.Size([1, 2048, 1, 1])
(1, 1) [1, 1] [0, 0, 0, 0]
coord [0, 0, 0, 0]
out_temp torch.Size([1, 2048, 1, 1])

&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&


&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&

I am [0, 0]
@@@ using cudnn bkw conv2d140448511541152[0,0] PI( <0,0,>,
 <otileshape 1,1,>,
 <padding 0,0,0,0,>,
 <inpslidx 0,0,0,0,>, 
 <internal 0,0,0,0,>, 
 <realidx 0,0,0,0,>, 
 <ndtsize 1,1,>, 
  local_first False
 next_id -99)
 bk-conv2d140448511541152[23,5] PI( <0,0,>,
 <otileshape 1,1,>,
 <padding 3,3,3,3,>,
 <inpslidx 0,0,0,0,>, 
 <internal 0,0,0,0,>, 
 <realidx -3,3,-3,3,>, 
 <ndtsize >, 
  local_first False
 next_id 140448511541104)

ouput grad ++ input shape torch.Size([1, 1024, 1, 1])
weight shape torch.Size([2048, 1024, 1, 1])
grad_output shape torch.Size([1, 2048, 1, 1])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 1024, 1, 1])
grad_input new torch.Size([1, 1024, 1, 1])
grad_input torch.Size([1, 1024, 1, 1])
grad_out size torch.Size([1, 2048, 1, 1])
crop [0, 0, 0, 0] [0, 0, 0, 0] [0, 0, 0, 0]
## 0 1 0 1
A 0 1 0 1
B 0 1 0 1
csaved input torch.Size([1, 1024, 1, 1])
cgrad_output torch.Size([1, 2048, 1, 1])
real nontiled_activation torch.Size([1, 1024, 1, 1])
real nontiled_grad_out torch.Size([1, 2048, 1, 1])
##############return grad_in in conv2d torch.Size([1, 1024, 1, 1])
I am [0, 0]
@@@ using cudnn bkw conv2d140448511541104[1,1] PI( <0,0,>,
 <otileshape 1,1,>,
 <padding 1,1,1,1,>,
 <inpslidx 0,0,0,0,>, 
 <internal 0,0,0,0,>, 
 <realidx -1,1,-1,1,>, 
 <ndtsize 1,1,>, 
  local_first False
 next_id 140448511541152)
 bk-conv2d140448511541104[22,4] PI( <0,0,>,
 <otileshape 1,1,>,
 <padding 3,3,3,3,>,
 <inpslidx 0,0,0,0,>, 
 <internal 0,0,0,0,>, 
 <realidx -3,3,-3,3,>, 
 <ndtsize >, 
  local_first False
 next_id 140448511541056)

in the middle ++ input shape torch.Size([1, 512, 3, 3])
weight shape torch.Size([1024, 512, 3, 3])
grad_output shape torch.Size([1, 1024, 1, 1])
padding info :: [1, 1, 1, 1]
grad_input old torch.Size([1, 512, 3, 3])
grad_input new torch.Size([1, 512, 3, 3])
grad_input torch.Size([1, 512, 3, 3])
grad_out size torch.Size([1, 1024, 1, 1])
crop [0, 0, 0, 0] [0, 0, 0, 0] [0, 0, 0, 0]
## 0 1 0 1
A 0 1 0 1
B 0 3 0 3
csaved input torch.Size([1, 512, 3, 3])
cgrad_output torch.Size([1, 1024, 1, 1])
real nontiled_activation torch.Size([1, 512, 3, 3])
real nontiled_grad_out torch.Size([1, 1024, 1, 1])
##############return grad_in in conv2d torch.Size([1, 512, 3, 3])
I am [0, 0]
@@@ using cudnn bkw conv2d140448511541056[2,2] PI( <0,0,>,
 <otileshape 1,1,>,
 <padding 1,1,1,1,>,
 <inpslidx 0,0,0,0,>, 
 <internal 0,0,0,0,>, 
 <realidx -1,1,-1,1,>, 
 <ndtsize 1,1,>, 
  local_first False
 next_id 140448511541104)
 bk-conv2d140448511541056[21,3] PI( <0,0,>,
 <otileshape 1,1,>,
 <padding 2,2,2,2,>,
 <inpslidx 0,0,0,0,>, 
 <internal 0,0,0,0,>, 
 <realidx -2,2,-2,2,>, 
 <ndtsize >, 
  local_first False
 next_id 140448511541008)

in the middle ++ input shape torch.Size([1, 1024, 3, 3])
weight shape torch.Size([512, 1024, 1, 1])
grad_output shape torch.Size([1, 512, 3, 3])
padding info :: [1, 1, 1, 1]
grad_input old torch.Size([1, 1024, 3, 3])
grad_input new torch.Size([1, 1024, 3, 3])
grad_input torch.Size([1, 1024, 3, 3])
grad_out size torch.Size([1, 512, 1, 1])
crop [0, 0, 0, 0] [0, 0, 0, 0] [0, 0, 0, 0]
## 0 1 0 1
A 0 1 0 1
B 0 1 0 1
csaved input torch.Size([1, 1024, 1, 1])
cgrad_output torch.Size([1, 512, 1, 1])
real nontiled_activation torch.Size([1, 1024, 1, 1])
real nontiled_grad_out torch.Size([1, 512, 1, 1])
##############return grad_in in conv2d torch.Size([1, 1024, 3, 3])
I am [0, 0]
@@@ using cudnn bkw conv2d140448511541008[3,3] PI( <0,0,>,
 <otileshape 1,1,>,
 <padding 2,2,2,2,>,
 <inpslidx 0,0,0,0,>, 
 <internal 0,0,0,0,>, 
 <realidx -2,2,-2,2,>, 
 <ndtsize 1,1,>, 
  local_first False
 next_id 140448511541056)
 bk-conv2d140448511541008[20,2] PI( <0,0,>,
 <otileshape 1,1,>,
 <padding 2,2,2,2,>,
 <inpslidx 0,0,0,0,>, 
 <internal 0,0,0,0,>, 
 <realidx -2,2,-2,2,>, 
 <ndtsize >, 
  local_first False
 next_id 140448511540960)

in the middle ++ input shape torch.Size([1, 512, 5, 5])
weight shape torch.Size([1024, 512, 3, 3])
grad_output shape torch.Size([1, 1024, 3, 3])
padding info :: [2, 2, 2, 2]
grad_input old torch.Size([1, 512, 5, 5])
grad_input new torch.Size([1, 512, 5, 5])
grad_input torch.Size([1, 512, 5, 5])
grad_out size torch.Size([1, 1024, 1, 1])
crop [0, 0, 0, 0] [0, 0, 0, 0] [0, 0, 0, 0]
## 0 1 0 1
A 0 1 0 1
B 0 3 0 3
csaved input torch.Size([1, 512, 3, 3])
cgrad_output torch.Size([1, 1024, 1, 1])
real nontiled_activation torch.Size([1, 512, 3, 3])
real nontiled_grad_out torch.Size([1, 1024, 1, 1])
##############return grad_in in conv2d torch.Size([1, 512, 5, 5])
I am [0, 0]
@@@ using cudnn bkw conv2d140448511540960[4,4] PI( <0,0,>,
 <otileshape 1,1,>,
 <padding 2,2,2,2,>,
 <inpslidx 0,0,0,0,>, 
 <internal 0,0,0,0,>, 
 <realidx -2,2,-2,2,>, 
 <ndtsize 1,1,>, 
  local_first False
 next_id 140448511541008)
 bk-conv2d140448511540960[19,1] PI( <0,0,>,
 <otileshape 1,1,>,
 <padding 1,1,1,1,>,
 <inpslidx 0,0,0,0,>, 
 <internal 0,0,0,0,>, 
 <realidx -1,1,-1,1,>, 
 <ndtsize >, 
  local_first False
 next_id 140448511540912)

in the middle ++ input shape torch.Size([1, 1024, 5, 5])
weight shape torch.Size([512, 1024, 1, 1])
grad_output shape torch.Size([1, 512, 5, 5])
padding info :: [2, 2, 2, 2]
grad_input old torch.Size([1, 1024, 5, 5])
grad_input new torch.Size([1, 1024, 5, 5])
grad_input torch.Size([1, 1024, 5, 5])
grad_out size torch.Size([1, 512, 1, 1])
crop [0, 0, 0, 0] [0, 0, 0, 0] [0, 0, 0, 0]
## 0 1 0 1
A 0 1 0 1
B 0 1 0 1
csaved input torch.Size([1, 1024, 1, 1])
cgrad_output /engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function CudnnConvolutionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function CudnnConvolutionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function MaxPool2DWithIndicesBackward
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function CudnnConvolutionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function CudnnConvolutionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function CudnnConvolutionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function MaxPool2DWithIndicesBackward
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function CudnnConvolutionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function MaxPool2DWithIndicesBackward
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function CudnnConvolutionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function BackwardHookFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function SumBackward0
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::CopyBackwards
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/torch.Size([1, 512, 1, 1])
real nontiled_activation torch.Size([1, 1024, 1, 1])
real nontiled_grad_out torch.Size([1, 512, 1, 1])
##############return grad_in in conv2d torch.Size([1, 1024, 5, 5])
I am [0, 0]
@@@ using cudnn bkw conv2d140448511540912[5,5] PI( <0,0,>,
 <otileshape 1,1,>,
 <padding 3,3,3,3,>,
 <inpslidx 0,0,0,0,>, 
 <internal 0,0,0,0,>, 
 <realidx -3,3,-3,3,>, 
 <ndtsize 1,1,>, 
  local_first True
 next_id 140448511540960)
 bk-conv2d140448511540912[18,0] PI( <0,0,>,
 <otileshape 1,1,>,
 <padding 1,1,1,1,>,
 <inpslidx 0,0,0,0,>, 
 <internal 0,0,0,0,>, 
 <realidx -1,1,-1,1,>, 
 <ndtsize >, 
  local_first False
 next_id 140448511540864)

in the local first ++ input shape torch.Size([1, 512, 7, 7])
weight shape torch.Size([1024, 512, 3, 3])
grad_output shape torch.Size([1, 1024, 5, 5])
padding info :: [3, 3, 3, 3]
grad_input torch.Size([1, 512, 1, 1])
grad_out size torch.Size([1, 1024, 1, 1])
crop [0, 0, 0, 0] [0, 0, 0, 0] [0, 0, 0, 0]
## 0 1 0 1
A 0 1 0 1
B 0 3 0 3
csaved input torch.Size([1, 512, 3, 3])
cgrad_output torch.Size([1, 1024, 1, 1])
real nontiled_activation torch.Size([1, 512, 3, 3])
real nontiled_grad_out torch.Size([1, 1024, 1, 1])
##############return grad_in in conv2d torch.Size([1, 512, 1, 1])

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 512, 2, 2])
grad_out size torch.Size([1, 512, 1, 1])
arg size torch.Size([1, 512, 1, 1])
##############grad_in in maxp torch.Size([1, 512, 2, 2])
I am [0, 0]
@@@ using cudnn bkw conv2d140448511540816[7,0] PI( <0,0,>,
 <otileshape 2,2,>,
 <padding 1,1,1,1,>,
 <inpslidx 0,1,0,1,>, 
 <internal 0,0,0,0,>, 
 <realidx -1,2,-1,2,>, 
 <ndtsize 2,2,>, 
  local_first False
 next_id 140448511540864)
 bk-conv2d140448511540816[16,4] PI( <0,0,>,
 <otileshape 2,2,>,
 <padding 3,3,3,3,>,
 <inpslidx 0,1,0,1,>, 
 <internal 0,0,0,0,>, 
 <realidx -3,4,-3,4,>, 
 <ndtsize >, 
  local_first False
 next_id 140448511540768)

local last ++ input shape torch.Size([1, 256, 4, 4])
weight shape torch.Size([512, 256, 3, 3])
grad_output shape torch.Size([1, 512, 2, 2])
grad_input torch.Size([1, 256, 4, 4])
padding info :: [1, 1, 1, 1]
grad_input old torch.Size([1, 256, 4, 4])
grad_input new torch.Size([1, 256, 4, 4])
new grad_input torch.Size([1, 256, 4, 4])
grad_out size torch.Size([1, 512, 2, 2])
crop [0, 0, 0, 0] [0, 1, 0, 1] [0, 1, 0, 1]
## 0 2 0 2
A 0 2 0 2
B 0 4 0 4
csaved input torch.Size([1, 256, 4, 4])
cgrad_output torch.Size([1, 512, 2, 2])
real nontiled_activation torch.Size([1, 256, 4, 4])
real nontiled_grad_out torch.Size([1, 512, 2, 2])
##############return grad_in in conv2d torch.Size([1, 256, 4, 4])
I am [0, 0]
@@@ using cudnn bkw conv2d140448511540768[8,1] PI( <0,0,>,
 <otileshape 2,2,>,
 <padding 1,1,1,1,>,
 <inpslidx 0,1,0,1,>, 
 <internal 0,0,0,0,>, 
 <realidx -1,2,-1,2,>, 
 <ndtsize 2,2,>, 
  local_first False
 next_id 140448511540816)
 bk-conv2d140448511540768[15,3] PI( <0,0,>,
 <otileshape 2,2,>,
 <padding 2,2,2,2,>,
 <inpslidx 0,1,0,1,>, 
 <internal 0,0,0,0,>, 
 <realidx -2,3,-2,3,>, 
 <ndtsize >, 
  local_first False
 next_id 140448511540720)

in the middle ++ input shape torch.Size([1, 512, 4, 4])
weight shape torch.Size([256, 512, 1, 1])
grad_output shape torch.Size([1, 256, 4, 4])
padding info :: [1, 1, 1, 1]
grad_input old torch.Size([1, 512, 4, 4])
grad_input new torch.Size([1, 512, 4, 4])
grad_input torch.Size([1, 512, 4, 4])
grad_out size torch.Size([1, 256, 2, 2])
crop [0, 0, 0, 0] [0, 1, 0, 1] [0, 1, 0, 1]
## 0 2 0 2
A 0 2 0 2
B 0 2 0 2
csaved input torch.Size([1, 512, 2, 2])
cgrad_output torch.Size([1, 256, 2, 2])
real nontiled_activation torch.Size([1, 512, 2, 2])
real nontiled_grad_out torch.Size([1, 256, 2, 2])
##############return grad_in in conv2d torch.Size([1, 512, 4, 4])
I am [0, 0]
@@@ using cudnn bkw conv2d140448511540720[9,2] PI( <0,0,>,
 <otileshape 2,2,>,
 <padding 2,2,2,2,>,
 <inpslidx 0,1,0,1,>, 
 <internal 0,0,0,0,>, 
 <realidx -2,3,-2,3,>, 
 <ndtsize 2,2,>, 
  local_first False
 next_id 140448511540768)
 bk-conv2d140448511540720[14,2] PI( <0,0,>,
 <otileshape 2,2,>,
 <padding 2,2,2,2,>,
 <inpslidx 0,1,0,1,>, 
 <internal 0,0,0,0,>, 
 <realidx -2,3,-2,3,>, 
 <ndtsize >, 
  local_first False
 next_id 140448511540672)

in the middle ++ input shape torch.Size([1, 256, 6, 6])
weight shape torch.Size([512, 256, 3, 3])
grad_output shape torch.Size([1, 512, 4, 4])
padding info :: [2, 2, 2, 2]
grad_input old torch.Size([1, 256, 6, 6])
grad_input new torch.Size([1, 256, 6, 6])
grad_input torch.Size([1, 256, 6, 6])
grad_out size torch.Size([1, 512, 2, 2])
crop [0, 0, 0, 0] [0, 1, 0, 1] [0, 1, 0, 1]
## 0 2 0 2
A 0 2 0 2
B 0 4 0 4
csaved input torch.Size([1, 256, 4, 4])
cgrad_output torch.Size([1, 512, 2, 2])
real nontiled_activation torch.Size([1, 256, 4, 4])
real nontiled_grad_out torch.Size([1, 512, 2, 2])
##############return grad_in in conv2d torch.Size([1, 256, 6, 6])
I am [0, 0]
@@@ using cudnn bkw conv2d140448511540672[10,3] PI( <0,0,>,
 <otileshape 2,2,>,
 <padding 2,2,2,2,>,
 <inpslidx 0,1,0,1,>, 
 <internal 0,0,0,0,>, 
 <realidx -2,3,-2,3,>, 
 <ndtsize 2,2,>, 
  local_first False
 next_id 140448511540720)
 bk-conv2d140448511540672[13,1] PI( <0,0,>,
 <otileshape 2,2,>,
 <padding 1,1,1,1,>,
 <inpslidx 0,1,0,1,>, 
 <internal 0,0,0,0,>, 
 <realidx -1,2,-1,2,>, 
 <ndtsize >, 
  local_first False
 next_id 140448511540624)

in the middle ++ input shape torch.Size([1, 512, 6, 6])
weight shape torch.Size([256, 512, 1, 1])
grad_output shape torch.Size([1, 256, 6, 6])
padding info :: [2, 2, 2, 2]
grad_input old torch.Size([1, 512, 6, 6])
grad_input new torch.Size([1, 512, 6, 6])
grad_input torch.Size([1, 512, 6, 6])
grad_out size torch.Size([1, 256, 2, 2])
crop [0, 0, 0, 0] [0, 1, 0, 1] [0, 1, 0, 1]
## 0 2 0 2
A 0 2 0 2
B 0 2 0 2
csaved input torch.Size([1, 512, 2, 2])
cgrad_output torch.Size([1, 256, 2, 2])
real nontiled_activation torch.Size([1, 512, 2, 2])
real nontiled_grad_out torch.Size([1, 256, 2, 2])
##############return grad_in in conv2d torch.Size([1, 512, 6, 6])
I am [0, 0]
@@@ using cudnn bkw conv2d140448511540624[11,4] PI( <0,0,>,
 <otileshape 2,2,>,
 <padding 3,3,3,3,>,
 <inpslidx 0,1,0,1,>, 
 <internal 0,0,0,0,>, 
 <realidx -3,4,-3,4,>, 
 <ndtsize 2,2,>, 
  local_first True
 next_id 140448511540672)
 bk-conv2d140448511540624[12,0] PI( <0,0,>,
 <otileshape 2,2,>,
 <padding 1,1,1,1,>,
 <inpslidx 0,1,0,1,>, 
 <internal 0,0,0,0,>, 
 <realidx -1,2,-1,2,>, 
 <ndtsize >, 
  local_first False
 next_id 140448511539472)

in the local first ++ input shape torch.Size([1, 256, 8, 8])
weight shape torch.Size([512, 256, 3, 3])
grad_output shape torch.Size([1, 512, 6, 6])
padding info :: [3, 3, 3, 3]
grad_input torch.Size([1, 256, 2, 2])
grad_out size torch.Size([1, 512, 2, 2])
crop [0, 0, 0, 0] [0, 1, 0, 1] [0, 1, 0, 1]
## 0 2 0 2
A 0 2 0 2
B 0 4 0 4
csaved input torch.Size([1, 256, 4, 4])
cgrad_output torch.Size([1, 512, 2, 2])
real nontiled_activation torch.Size([1, 256, 4, 4])
real nontiled_grad_out torch.Size([1, 512, 2, 2])
##############return grad_in in conv2d torch.Size([1, 256, 2, 2])

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 256, 4, 4])
grad_out size torch.Size([1, 256, 2, 2])
arg size torch.Size([1, 256, 2, 2])
##############grad_in in maxp torch.Size([1, 256, 4, 4])
I am [0, 0]
@@@ using cudnn bkw conv2d140448511540336[13,0] PI( <0,0,>,
 <otileshape 4,4,>,
 <padding 1,1,1,1,>,
 <inpslidx 0,3,0,3,>, 
 <internal 0,0,0,0,>, 
 <realidx -1,4,-1,4,>, 
 <ndtsize 4,4,>, 
  local_first False
 next_id 140448511539472)
 bk-conv2d140448511540336[10,2] PI( <0,0,>,
 <otileshape 4,4,>,
 <padding 2,2,2,2,>,
 <inpslidx 0,3,0,3,>, 
 <internal 0,0,0,0,>, 
 <realidx -2,5,-2,5,>, 
 <ndtsize >, 
  local_first False
 next_id 140450320786528)

local last ++ input shape torch.Size([1, 128, 6, 6])
weight shape torch.Size([256, 128, 3, 3])
grad_output shape torch.Size([1, 256, 4, 4])
grad_input torch.Size([1, 128, 6, 6])
padding info :: [1, 1, 1, 1]
grad_input old torch.Size([1, 128, 6, 6])
grad_input new torch.Size([1, 128, 6, 6])
new grad_input torch.Size([1, 128, 6, 6])
grad_out size torch.Size([1, 256, 4, 4])
crop [0, 0, 0, 0] [0, 3, 0, 3] [0, 3, 0, 3]
## 0 4 0 4
A 0 4 0 4
B 0 6 0 6
csaved input torch.Size([1, 128, 6, 6])
cgrad_output torch.Size([1, 256, 4, 4])
real nontiled_activation torch.Size([1, 128, 6, 6])
real nontiled_grad_out torch.Size([1, 256, 4, 4])
##############return grad_in in conv2d torch.Size([1, 128, 6, 6])
I am [0, 0]
@@@ using cudnn bkw conv2d140450320786528[14,1] PI( <0,0,>,
 <otileshape 4,4,>,
 <padding 1,1,1,1,>,
 <inpslidx 0,3,0,3,>, 
 <internal 0,0,0,0,>, 
 <realidx -1,4,-1,4,>, 
 <ndtsize 4,4,>, 
  local_first False
 next_id 140448511540336)
 bk-conv2d140450320786528[9,1] PI( <0,0,>,
 <otileshape 4,4,>,
 <padding 1,1,1,1,>,
 <inpslidx 0,3,0,3,>, 
 <internal 0,0,0,0,>, 
 <realidx -1,4,-1,4,>, 
 <ndtsize >, 
  local_first False
 next_id 140450320786624)

in the middle ++ input shape torch.Size([1, 256, 6, 6])
weight shape torch.Size([128, 256, 1, 1])
grad_output shape torch.Size([1, 128, 6, 6])
padding info :: [1, 1, 1, 1]
grad_input old torch.Size([1, 256, 6, 6])
grad_input new torch.Size([1, 256, 6, 6])
grad_input torch.Size([1, 256, 6, 6])
grad_out size torch.Size([1, 128, 4, 4])
crop [0, 0, 0, 0] [0, 3, 0, 3] [0, 3, 0, 3]
## 0 4 0 4
A 0 4 0 4
B 0 4 0 4
csaved input torch.Size([1, 256, 4, 4])
cgrad_output torch.Size([1, 128, 4, 4])
real nontiled_activation torch.Size([1, 256, 4, 4])
real nontiled_grad_out torch.Size([1, 128, 4, 4])
##############return grad_in in conv2d torch.Size([1, 256, 6, 6])
I am [0, 0]
@@@ using cudnn bkw conv2d140450320786624[15,2] PI( <0,0,>,
 <otileshape 4,4,>,
 <padding 2,2,2,2,>,
 <inpslidx 0,3,0,3,>, 
 <internal 0,0,0,0,>, 
 <realidx -2,5,-2,5,>, 
 <ndtsize 4,4,>, 
  local_first True
 next_id 140450320786528)
 bk-conv2d140450320786624[8,0] PI( <0,0,>,
 <otileshape 4,4,>,
 <padding 1,1,1,1,>,
 <inpslidx 0,3,0,3,>, 
 <internal 0,0,0,0,>, 
 <realidx -1,4,-1,4,>, 
 <ndtsize >, 
  local_first False
 next_id 140450320786672)

in the local first ++ input shape torch.Size([1, 128, 8, 8])
weight shape torch.Size([256, 128, 3, 3])
grad_output shape torch.Size([1, 256, 6, 6])
padding info :: [2, 2, 2, 2]
grad_input torch.Size([1, 128, 4, 4])
grad_out size torch.Size([1, 256, 4, 4])
crop [0, 0, 0, 0] [0, 3, 0, 3] [0, 3, 0, 3]
## 0 4 0 4
A 0 4 0 4
B 0 6 0 6
csaved input torch.Size([1, 128, 6, 6])
cgrad_output torch.Size([1, 256, 4, 4])
real nontiled_activation torch.Size([1, 128, 6, 6])
real nontiled_grad_out torch.Size([1, 256, 4, 4])
##############return grad_in in conv2d torch.Size([1, 128, 4, 4])

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 128, 8, 8])
grad_out size torch.Size([1, 128, 4, 4])
arg size torch.Size([1, 128, 4, 4])
##############grad_in in maxp torch.Size([1, 128, 8, 8])
I am [0, 0]
@@@ using cudnn bkw conv2d140450320786768[17,0] PI( <0,0,>,
 <otileshape 8,8,>,
 <padding 1,1,1,1,>,
 <inpslidx 0,7,0,7,>, 
 <internal 0,0,0,0,>, 
 <realidx -1,8,-1,8,>, 
 <ndtsize 8,8,>, 
  local_first False
 next_id 140450320786672)
 bk-conv2d140450320786768[6,2] PI( <0,0,>,
 <otileshape 8,8,>,
 <padding 2,2,2,2,>,
 <inpslidx 0,7,0,7,>, 
 <internal 0,0,0,0,>, 
 <realidx -2,9,-2,9,>, 
 <ndtsize >, 
  local_first False
 next_id 140450320786864)

local last ++ input shape torch.Size([1, 64, 10, 10])
weight shape torch.Size([128, 64, 3, 3])
grad_output shape torch.Size([1, 128, 8, 8])
grad_input torch.Size([1, 64, 10, 10])
padding info :: [1, 1, 1, 1]
grad_input old torch.Size([1, 64, 10, 10])
grad_input new torch.Size([1, 64, 10, 10])
new grad_input torch.Size([1, 64, 10, 10])
grad_out size torch.Size([1, 128, 8, 8])
crop [0, 0, 0, 0] [0, 7, 0, 7] [0, 7, 0, 7]
## 0 8 0 8
A 0 8 0 8
B 0 10 0 10
csaved input torch.Size([1, 64, 10, 10])
cgrad_output torch.Size([1, 128, 8, 8])
real nontiled_activation torch.Size([1, 64, 10, 10])
real nontiled_grad_out torch.Size([1, 128, 8, 8])
##############return grad_in in conv2d torch.Size([1, 64, 10, 10])
I am [0, 0]
@@@ using cudnn bkw conv2d140450320786864[18,1] PI( <0,0,>,
 <otileshape 8,8,>,
 <padding 1,1,1,1,>,
 <inpslidx 0,7,0,7,>, 
 <internal 0,0,0,0,>, 
 <realidx -1,8,-1,8,>, 
 <ndtsize 8,8,>, 
  local_first False
 next_id 140450320786768)
 bk-conv2d140450320786864[5,1] PI( <0,0,>,
 <otileshape 8,8,>,
 <padding 1,1,1,1,>,
 <inpslidx 0,7,0,7,>, 
 <internal 0,0,0,0,>, 
 <realidx -1,8,-1,8,>, 
 <ndtsize >, 
  local_first False
 next_id 140450320787200)

in the middle ++ input shape torch.Size([1, 128, 10, 10])
weight shape torch.Size([64, 128, 1, 1])
grad_output shape torch.Size([1, 64, 10, 10])
padding info :: [1, 1, 1, 1]
grad_input old torch.Size([1, 128, 10, 10])
grad_input new torch.Size([1, 128, 10, 10])
grad_input torch.Size([1, 128, 10, 10])
grad_out size torch.Size([1, 64, 8, 8])
crop [0, 0, 0, 0] [0, 7, 0, 7] [0, 7, 0, 7]
## 0 8 0 8
A 0 8 0 8
B 0 8 0 8
csaved input torch.Size([1, 128, 8, 8])
cgrad_output torch.Size([1, 64, 8, 8])
real nontiled_activation torch.Size([1, 128, 8, 8])
real nontiled_grad_out torch.Size([1, 64, 8, 8])
##############return grad_in in conv2d torch.Size([1, 128, 10, 10])
I am [0, 0]
@@@ using cudnn bkw conv2d140450320787200[19,2] PI( <0,0,>,
 <otileshape 8,8,>,
 <padding 2,2,2,2,>,
 <inpslidx 0,7,0,7,>, 
 <internal 0,0,0,0,>, 
 <realidx -2,9,-2,9,>, 
 <ndtsize 8,8,>, 
  local_first True
 next_id 140450320786864)
 bk-conv2d140450320787200[4,0] PI( <0,0,>,
 <otileshape 8,8,>,
 <padding 1,1,1,1,>,
 <inpslidx 0,7,0,7,>, 
 <internal 0,0,0,0,>, 
 <realidx -1,8,-1,8,>, 
 <ndtsize >, 
  local_first False
 next_id 140450320787440)

in the local first ++ input shape torch.Size([1, 64, 12, 12])
weight shape torch.Size([128, 64, 3, 3])
grad_output shape torch.Size([1, 128, 10, 10])
padding info :: [2, 2, 2, 2]
grad_input torch.Size([1, 64, 8, 8])
grad_out size torch.Size([1, 128, 8, 8])
crop [0, 0, 0, 0] [0, 7, 0, 7] [0, 7, 0, 7]
## 0 8 0 8
A 0 8 0 8
B 0 10 0 10
csaved input torch.Size([1, 64, 10, 10])
cgrad_output torch.Size([1, 128, 8, 8])
real nontiled_activation torch.Size([1, 64, 10, 10])
real nontiled_grad_out torch.Size([1, 128, 8, 8])
##############return grad_in in conv2d torch.Size([1, 64, 8, 8])

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 64, 16, 16])
grad_out size torch.Size([1, 64, 8, 8])
arg size torch.Size([1, 64, 8, 8])
##############grad_in in maxp torch.Size([1, 64, 16, 16])
I am [0, 0]
@@@ using cudnn bkw conv2d140448512502464[21,0] PI( <0,0,>,
 <otileshape 16,16,>,
 <padding 1,1,1,1,>,
 <inpslidx 0,15,0,15,>, 
 <internal 0,0,0,0,>, 
 <realidx -1,16,-1,16,>, 
 <ndtsize 16,16,>, 
  local_first True
 next_id 140450320787440)
 bk-conv2d140448512502464[2,0] PI( <0,0,>,
 <otileshape 16,16,>,
 <padding 1,1,1,1,>,
 <inpslidx 0,15,0,15,>, 
 <internal 0,0,0,0,>, 
 <realidx -1,16,-1,16,>, 
 <ndtsize >, 
  local_first False
 next_id 140450320871184)

in the local first ++ input shape torch.Size([1, 32, 18, 18])
weight shape torch.Size([64, 32, 3, 3])
grad_output shape torch.Size([1, 64, 16, 16])
padding info :: [1, 1, 1, 1]
grad_input torch.Size([1, 32, 16, 16])
grad_out size torch.Size([1, 64, 16, 16])
crop [0, 0, 0, 0] [0, 15, 0, 15] [0, 15, 0, 15]
## 0 16 0 16
A 0 16 0 16
B 0 18 0 18
csaved input torch.Size([1, 32, 18, 18])
cgrad_output torch.Size([1, 64, 16, 16])
real nontiled_activation torch.Size([1, 32, 18, 18])
real nontiled_grad_out torch.Size([1, 64, 16, 16])
##############return grad_in in conv2d torch.Size([1, 32, 16, 16])

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 32, 32, 32])
grad_out size torch.Size([1, 32, 16, 16])
arg size torch.Size([1, 32, 16, 16])
##############grad_in in maxp torch.Size([1, 32, 32, 32])
I am [0, 0]
@@@ using cudnn bkw conv2d140450320871088[23,0] PI( <0,0,>,
 <otileshape 32,32,>,
 <padding 1,1,1,1,>,
 <inpslidx 0,31,0,31,>, 
 <internal 0,0,0,0,>, 
 <realidx -1,32,-1,32,>, 
 <ndtsize 32,32,>, 
  local_first True
 next_id 140450320871184)
 bk-conv2d140450320871088[0,0] PI( <0,0,>,
 <otileshape 32,32,>,
 <padding 1,1,1,1,>,
 <inpslidx 0,31,0,31,>, 
 <internal 0,0,0,0,>, 
 <realidx -1,32,-1,32,>, 
 <ndtsize >, 
  local_first False
 next_id -99)

input grad ++ input shape torch.Size([1, 3, 34, 34])
weight shape torch.Size([32, 3, 3, 3])
grad_output shape torch.Size([1, 32, 32, 32])
final torch.Size([1, 3, 34, 34])
padding info :: [1, 1, 1, 1]
after crop g_in torch.Size([1, 3, 32, 32])
grad_out size torch.Size([1, 32, 32, 32])
crop [0, 0, 0, 0] [0, 31, 0, 31] [0, 31, 0, 31]
## 0 32 0 32
A 0 32 0 32
B 0 34 0 34
csaved input torch.Size([1, 3, 34, 34])
cgrad_output torch.Size([1, 32, 32, 32])
real nontiled_activation torch.Size([1, 3, 34, 34])
real nontiled_grad_out torch.Size([1, 32, 32, 32])
##############return grad_in in conv2d torch.Size([1, 3, 32, 32])
~~ check forward correctness ~~
-----------------------------------------

#### compare grad_in
-----------------------------------------

engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
