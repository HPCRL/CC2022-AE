
&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&

==== our init ...
2.4GiB
7.7GiB
Input 1x3x10240x10240
L-->R current op 140045013207312
L-->R current op 140046822727744
after conv2d 1x3x10240x10240
L-->R current op 140045015171856
after conv2d 1x3x10240x10240
L-->R current op 140045014255792
after maxpool2d 1x3x5120x5120
L-->R current op 140045013800704
after conv2d 1x3x5120x5120
L-->R current op 140045013848944
after conv2d 1x3x5120x5120
L-->R current op 140045013207216
after maxpool2d 1x3x2560x2560
L-->R current op 140045013206640
after conv2d 1x3x2560x2560
L-->R current op 140045013206784
after conv2d 1x3x2560x2560
L-->R current op 140045013206880
after conv2d 1x3x2560x2560
L-->R current op 140045013206928
after maxpool2d 1x3x1280x1280
L-->R current op 140045013206352
after conv2d 1x3x1280x1280
L-->R current op 140045013206304
after conv2d 1x3x1280x1280
L-->R current op 140045013207072
after conv2d 1x3x1280x1280
L-->R current op 140045013206496
after maxpool2d 1x3x640x640
L-->R current op 140045013206400
after conv2d 1x3x640x640
L-->R current op 140045013206448
after conv2d 1x3x640x640
L-->R current op 140045013206976
after conv2d 1x3x640x640
L-->R current op 140045013205872
after maxpool2d 1x3x320x320
out shape torch.Size([1, 3, 320, 320])
coord [0, 0]
bwd_out_shape  (43, 43)
fwd_out_shape  (40, 40)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
in customized checkpoint forward 5
input size torch.Size([1, 3, 10240, 10240])
in customized Sequential 6
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([])
current fwd info [0, 0]
shape input_tile_for_next
 torch.Size([1, 3, 1466, 1466])
myctx [[ 140046822727744[ PI( <0,0,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744])
current fwd info [0, 0]
shape input_tile_for_next
 torch.Size([1, 3, 1464, 1464])
myctx [[ 140045015171856[ PI( <0,0,>,

in customized Sequential 3
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856])
current fwd info [0, 0]
shape input_tile_for_next
 torch.Size([1, 3, 732, 732])
myctx [[ 140045013800704[ PI( <0,0,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704])
current fwd info [0, 0]
shape input_tile_for_next
 torch.Size([1, 3, 730, 730])
myctx [[ 140045013848944[ PI( <0,0,>,

in customized Sequential 3
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944])
current fwd info [0, 0]
shape input_tile_for_next
 torch.Size([1, 3, 366, 366])
myctx [[ 140045013206640[ PI( <0,0,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640])
current fwd info [0, 0]
shape input_tile_for_next
 torch.Size([1, 3, 364, 364])
myctx [[ 140045013206784[ PI( <0,0,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784])
current fwd info [0, 0]
shape input_tile_for_next
 torch.Size([1, 3, 362, 362])
myctx [[ 140045013206880[ PI( <0,0,>,

in customized Sequential 3
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880])
current fwd info [0, 0]
shape input_tile_for_next
 torch.Size([1, 3, 182, 182])
myctx [[ 140045013206352[ PI( <0,0,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352])
current fwd info [0, 0]
shape input_tile_for_next
 torch.Size([1, 3, 180, 180])
myctx [[ 140045013206304[ PI( <0,0,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304])
current fwd info [0, 0]
shape input_tile_for_next
 torch.Size([1, 3, 178, 178])
myctx [[ 140045013207072[ PI( <0,0,>,

in customized Sequential 3
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072])
current fwd info [0, 0]
shape input_tile_for_next
 torch.Size([1, 3, 90, 90])
myctx [[ 140045013206400[ PI( <0,0,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400])
current fwd info [0, 0]
shape input_tile_for_next
 torch.Size([1, 3, 88, 88])
myctx [[ 140045013206448[ PI( <0,0,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448])
current fwd info [0, 0]
shape input_tile_for_next
 torch.Size([1, 3, 86, 86])
myctx [[ 140045013206976[ PI( <0,0,>,

in customized Sequential 3
coord [0, 42, 0, 42]
out_temp torch.Size([1, 3, 43, 43])
coord [0, 1]
bwd_out_shape  (46, 43)
fwd_out_shape  (40, 40)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
in customized checkpoint forward 5
input size torch.Size([1, 3, 10240, 10240])
in customized Sequential 6
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 1]
shape input_tile_for_next
 torch.Size([1, 3, 1466, 1650])
myctx [[ 140046822727744[ PI( <0,1,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 1]
shape input_tile_for_next
 torch.Size([1, 3, 1464, 1648])
myctx [[ 140045015171856[ PI( <0,1,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 1]
shape input_tile_for_next
 torch.Size([1, 3, 732, 822])
myctx [[ 140045013800704[ PI( <0,1,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 1]
shape input_tile_for_next
 torch.Size([1, 3, 730, 820])
myctx [[ 140045013848944[ PI( <0,1,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 1]
shape input_tile_for_next
 torch.Size([1, 3, 366, 408])
myctx [[ 140045013206640[ PI( <0,1,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 1]
shape input_tile_for_next
 torch.Size([1, 3, 364, 406])
myctx [[ 140045013206784[ PI( <0,1,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 1]
shape input_tile_for_next
 torch.Size([1, 3, 362, 404])
myctx [[ 140045013206880[ PI( <0,1,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 1]
shape input_tile_for_next
 torch.Size([1, 3, 182, 200])
myctx [[ 140045013206352[ PI( <0,1,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 1]
shape input_tile_for_next
 torch.Size([1, 3, 180, 198])
myctx [[ 140045013206304[ PI( <0,1,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 1]
shape input_tile_for_next
 torch.Size([1, 3, 178, 196])
myctx [[ 140045013207072[ PI( <0,1,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 1]
shape input_tile_for_next
 torch.Size([1, 3, 90, 96])
myctx [[ 140045013206400[ PI( <0,1,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 1]
shape input_tile_for_next
 torch.Size([1, 3, 88, 94])
myctx [[ 140045013206448[ PI( <0,1,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 1]
shape input_tile_for_next
 torch.Size([1, 3, 86, 92])
myctx [[ 140045013206976[ PI( <0,1,>,

in customized Sequential 3
need to get existing
coord [37, 82, 0, 42]
out_temp torch.Size([1, 3, 43, 46])
coord [0, 2]
bwd_out_shape  (46, 43)
fwd_out_shape  (40, 40)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
in customized checkpoint forward 5
input size torch.Size([1, 3, 10240, 10240])
in customized Sequential 6
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 2]
shape input_tile_for_next
 torch.Size([1, 3, 1466, 1650])
myctx [[ 140046822727744[ PI( <0,2,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 2]
shape input_tile_for_next
 torch.Size([1, 3, 1464, 1648])
myctx [[ 140045015171856[ PI( <0,2,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 2]
shape input_tile_for_next
 torch.Size([1, 3, 732, 822])
myctx [[ 140045013800704[ PI( <0,2,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 2]
shape input_tile_for_next
 torch.Size([1, 3, 730, 820])
myctx [[ 140045013848944[ PI( <0,2,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 2]
shape input_tile_for_next
 torch.Size([1, 3, 366, 408])
myctx [[ 140045013206640[ PI( <0,2,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 2]
shape input_tile_for_next
 torch.Size([1, 3, 364, 406])
myctx [[ 140045013206784[ PI( <0,2,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 2]
shape input_tile_for_next
 torch.Size([1, 3, 362, 404])
myctx [[ 140045013206880[ PI( <0,2,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 2]
shape input_tile_for_next
 torch.Size([1, 3, 182, 200])
myctx [[ 140045013206352[ PI( <0,2,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 2]
shape input_tile_for_next
 torch.Size([1, 3, 180, 198])
myctx [[ 140045013206304[ PI( <0,2,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 2]
shape input_tile_for_next
 torch.Size([1, 3, 178, 196])
myctx [[ 140045013207072[ PI( <0,2,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 2]
shape input_tile_for_next
 torch.Size([1, 3, 90, 96])
myctx [[ 140045013206400[ PI( <0,2,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 2]
shape input_tile_for_next
 torch.Size([1, 3, 88, 94])
myctx [[ 140045013206448[ PI( <0,2,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 2]
shape input_tile_for_next
 torch.Size([1, 3, 86, 92])
myctx [[ 140045013206976[ PI( <0,2,>,

in customized Sequential 3
need to get existing
coord [77, 122, 0, 42]
out_temp torch.Size([1, 3, 43, 46])
coord [0, 3]
bwd_out_shape  (46, 43)
fwd_out_shape  (40, 40)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
in customized checkpoint forward 5
input size torch.Size([1, 3, 10240, 10240])
in customized Sequential 6
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 3]
shape input_tile_for_next
 torch.Size([1, 3, 1466, 1650])
myctx [[ 140046822727744[ PI( <0,3,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 3]
shape input_tile_for_next
 torch.Size([1, 3, 1464, 1648])
myctx [[ 140045015171856[ PI( <0,3,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 3]
shape input_tile_for_next
 torch.Size([1, 3, 732, 822])
myctx [[ 140045013800704[ PI( <0,3,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 3]
shape input_tile_for_next
 torch.Size([1, 3, 730, 820])
myctx [[ 140045013848944[ PI( <0,3,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 3]
shape input_tile_for_next
 torch.Size([1, 3, 366, 408])
myctx [[ 140045013206640[ PI( <0,3,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 3]
shape input_tile_for_next
 torch.Size([1, 3, 364, 406])
myctx [[ 140045013206784[ PI( <0,3,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 3]
shape input_tile_for_next
 torch.Size([1, 3, 362, 404])
myctx [[ 140045013206880[ PI( <0,3,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 3]
shape input_tile_for_next
 torch.Size([1, 3, 182, 200])
myctx [[ 140045013206352[ PI( <0,3,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 3]
shape input_tile_for_next
 torch.Size([1, 3, 180, 198])
myctx [[ 140045013206304[ PI( <0,3,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 3]
shape input_tile_for_next
 torch.Size([1, 3, 178, 196])
myctx [[ 140045013207072[ PI( <0,3,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 3]
shape input_tile_for_next
 torch.Size([1, 3, 90, 96])
myctx [[ 140045013206400[ PI( <0,3,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 3]
shape input_tile_for_next
 torch.Size([1, 3, 88, 94])
myctx [[ 140045013206448[ PI( <0,3,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 3]
shape input_tile_for_next
 torch.Size([1, 3, 86, 92])
myctx [[ 140045013206976[ PI( <0,3,>,

in customized Sequential 3
need to get existing
coord [117, 162, 0, 42]
out_temp torch.Size([1, 3, 43, 46])
coord [0, 4]
bwd_out_shape  (46, 43)
fwd_out_shape  (40, 40)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
in customized checkpoint forward 5
input size torch.Size([1, 3, 10240, 10240])
in customized Sequential 6
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 4]
shape input_tile_for_next
 torch.Size([1, 3, 1466, 1650])
myctx [[ 140046822727744[ PI( <0,4,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 4]
shape input_tile_for_next
 torch.Size([1, 3, 1464, 1648])
myctx [[ 140045015171856[ PI( <0,4,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 4]
shape input_tile_for_next
 torch.Size([1, 3, 732, 822])
myctx [[ 140045013800704[ PI( <0,4,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 4]
shape input_tile_for_next
 torch.Size([1, 3, 730, 820])
myctx [[ 140045013848944[ PI( <0,4,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 4]
shape input_tile_for_next
 torch.Size([1, 3, 366, 408])
myctx [[ 140045013206640[ PI( <0,4,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 4]
shape input_tile_for_next
 torch.Size([1, 3, 364, 406])
myctx [[ 140045013206784[ PI( <0,4,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 4]
shape input_tile_for_next
 torch.Size([1, 3, 362, 404])
myctx [[ 140045013206880[ PI( <0,4,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 4]
shape input_tile_for_next
 torch.Size([1, 3, 182, 200])
myctx [[ 140045013206352[ PI( <0,4,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 4]
shape input_tile_for_next
 torch.Size([1, 3, 180, 198])
myctx [[ 140045013206304[ PI( <0,4,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 4]
shape input_tile_for_next
 torch.Size([1, 3, 178, 196])
myctx [[ 140045013207072[ PI( <0,4,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 4]
shape input_tile_for_next
 torch.Size([1, 3, 90, 96])
myctx [[ 140045013206400[ PI( <0,4,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 4]
shape input_tile_for_next
 torch.Size([1, 3, 88, 94])
myctx [[ 140045013206448[ PI( <0,4,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 4]
shape input_tile_for_next
 torch.Size([1, 3, 86, 92])
myctx [[ 140045013206976[ PI( <0,4,>,

in customized Sequential 3
need to get existing
coord [157, 202, 0, 42]
out_temp torch.Size([1, 3, 43, 46])
coord [0, 5]
bwd_out_shape  (46, 43)
fwd_out_shape  (40, 40)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
in customized checkpoint forward 5
input size torch.Size([1, 3, 10240, 10240])
in customized Sequential 6
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 5]
shape input_tile_for_next
 torch.Size([1, 3, 1466, 1650])
myctx [[ 140046822727744[ PI( <0,5,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 5]
shape input_tile_for_next
 torch.Size([1, 3, 1464, 1648])
myctx [[ 140045015171856[ PI( <0,5,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 5]
shape input_tile_for_next
 torch.Size([1, 3, 732, 822])
myctx [[ 140045013800704[ PI( <0,5,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 5]
shape input_tile_for_next
 torch.Size([1, 3, 730, 820])
myctx [[ 140045013848944[ PI( <0,5,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 5]
shape input_tile_for_next
 torch.Size([1, 3, 366, 408])
myctx [[ 140045013206640[ PI( <0,5,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 5]
shape input_tile_for_next
 torch.Size([1, 3, 364, 406])
myctx [[ 140045013206784[ PI( <0,5,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 5]
shape input_tile_for_next
 torch.Size([1, 3, 362, 404])
myctx [[ 140045013206880[ PI( <0,5,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 5]
shape input_tile_for_next
 torch.Size([1, 3, 182, 200])
myctx [[ 140045013206352[ PI( <0,5,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 5]
shape input_tile_for_next
 torch.Size([1, 3, 180, 198])
myctx [[ 140045013206304[ PI( <0,5,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 5]
shape input_tile_for_next
 torch.Size([1, 3, 178, 196])
myctx [[ 140045013207072[ PI( <0,5,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 5]
shape input_tile_for_next
 torch.Size([1, 3, 90, 96])
myctx [[ 140045013206400[ PI( <0,5,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 5]
shape input_tile_for_next
 torch.Size([1, 3, 88, 94])
myctx [[ 140045013206448[ PI( <0,5,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 5]
shape input_tile_for_next
 torch.Size([1, 3, 86, 92])
myctx [[ 140045013206976[ PI( <0,5,>,

in customized Sequential 3
need to get existing
coord [197, 242, 0, 42]
out_temp torch.Size([1, 3, 43, 46])
coord [0, 6]
bwd_out_shape  (46, 43)
fwd_out_shape  (40, 40)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
in customized checkpoint forward 5
input size torch.Size([1, 3, 10240, 10240])
in customized Sequential 6
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 6]
shape input_tile_for_next
 torch.Size([1, 3, 1466, 1650])
myctx [[ 140046822727744[ PI( <0,6,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 6]
shape input_tile_for_next
 torch.Size([1, 3, 1464, 1648])
myctx [[ 140045015171856[ PI( <0,6,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 6]
shape input_tile_for_next
 torch.Size([1, 3, 732, 822])
myctx [[ 140045013800704[ PI( <0,6,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 6]
shape input_tile_for_next
 torch.Size([1, 3, 730, 820])
myctx [[ 140045013848944[ PI( <0,6,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 6]
shape input_tile_for_next
 torch.Size([1, 3, 366, 408])
myctx [[ 140045013206640[ PI( <0,6,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 6]
shape input_tile_for_next
 torch.Size([1, 3, 364, 406])
myctx [[ 140045013206784[ PI( <0,6,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 6]
shape input_tile_for_next
 torch.Size([1, 3, 362, 404])
myctx [[ 140045013206880[ PI( <0,6,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 6]
shape input_tile_for_next
 torch.Size([1, 3, 182, 200])
myctx [[ 140045013206352[ PI( <0,6,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 6]
shape input_tile_for_next
 torch.Size([1, 3, 180, 198])
myctx [[ 140045013206304[ PI( <0,6,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 6]
shape input_tile_for_next
 torch.Size([1, 3, 178, 196])
myctx [[ 140045013207072[ PI( <0,6,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 6]
shape input_tile_for_next
 torch.Size([1, 3, 90, 96])
myctx [[ 140045013206400[ PI( <0,6,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 6]
shape input_tile_for_next
 torch.Size([1, 3, 88, 94])
myctx [[ 140045013206448[ PI( <0,6,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 6]
shape input_tile_for_next
 torch.Size([1, 3, 86, 92])
myctx [[ 140045013206976[ PI( <0,6,>,

in customized Sequential 3
need to get existing
coord [237, 282, 0, 42]
out_temp torch.Size([1, 3, 43, 46])
coord [0, 7]
bwd_out_shape  (43, 43)
fwd_out_shape  (40, 40)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
in customized checkpoint forward 5
input size torch.Size([1, 3, 10240, 10240])
in customized Sequential 6
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 7]
shape input_tile_for_next
 torch.Size([1, 3, 1466, 1466])
myctx [[ 140046822727744[ PI( <0,7,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 7]
shape input_tile_for_next
 torch.Size([1, 3, 1464, 1464])
myctx [[ 140045015171856[ PI( <0,7,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 7]
shape input_tile_for_next
 torch.Size([1, 3, 732, 732])
myctx [[ 140045013800704[ PI( <0,7,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 7]
shape input_tile_for_next
 torch.Size([1, 3, 730, 730])
myctx [[ 140045013848944[ PI( <0,7,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 7]
shape input_tile_for_next
 torch.Size([1, 3, 366, 366])
myctx [[ 140045013206640[ PI( <0,7,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 7]
shape input_tile_for_next
 torch.Size([1, 3, 364, 364])
myctx [[ 140045013206784[ PI( <0,7,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 7]
shape input_tile_for_next
 torch.Size([1, 3, 362, 362])
myctx [[ 140045013206880[ PI( <0,7,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 7]
shape input_tile_for_next
 torch.Size([1, 3, 182, 182])
myctx [[ 140045013206352[ PI( <0,7,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 7]
shape input_tile_for_next
 torch.Size([1, 3, 180, 180])
myctx [[ 140045013206304[ PI( <0,7,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 7]
shape input_tile_for_next
 torch.Size([1, 3, 178, 178])
myctx [[ 140045013207072[ PI( <0,7,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 7]
shape input_tile_for_next
 torch.Size([1, 3, 90, 90])
myctx [[ 140045013206400[ PI( <0,7,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 7]
shape input_tile_for_next
 torch.Size([1, 3, 88, 88])
myctx [[ 140045013206448[ PI( <0,7,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 7]
shape input_tile_for_next
 torch.Size([1, 3, 86, 86])
myctx [[ 140045013206976[ PI( <0,7,>,

in customized Sequential 3
need to get existing
coord [277, 319, 0, 42]
out_temp torch.Size([1, 3, 43, 43])
coord [1, 0]
bwd_out_shape  (43, 46)
fwd_out_shape  (40, 40)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
in customized checkpoint forward 5
input size torch.Size([1, 3, 10240, 10240])
in customized Sequential 6
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 0]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1466])
myctx [[ 140046822727744[ PI( <1,0,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 0]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1464])
myctx [[ 140045015171856[ PI( <1,0,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 0]
shape input_tile_for_next
 torch.Size([1, 3, 822, 732])
myctx [[ 140045013800704[ PI( <1,0,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 0]
shape input_tile_for_next
 torch.Size([1, 3, 820, 730])
myctx [[ 140045013848944[ PI( <1,0,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 0]
shape input_tile_for_next
 torch.Size([1, 3, 408, 366])
myctx [[ 140045013206640[ PI( <1,0,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 0]
shape input_tile_for_next
 torch.Size([1, 3, 406, 364])
myctx [[ 140045013206784[ PI( <1,0,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 0]
shape input_tile_for_next
 torch.Size([1, 3, 404, 362])
myctx [[ 140045013206880[ PI( <1,0,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 0]
shape input_tile_for_next
 torch.Size([1, 3, 200, 182])
myctx [[ 140045013206352[ PI( <1,0,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 0]
shape input_tile_for_next
 torch.Size([1, 3, 198, 180])
myctx [[ 140045013206304[ PI( <1,0,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 0]
shape input_tile_for_next
 torch.Size([1, 3, 196, 178])
myctx [[ 140045013207072[ PI( <1,0,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 0]
shape input_tile_for_next
 torch.Size([1, 3, 96, 90])
myctx [[ 140045013206400[ PI( <1,0,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 0]
shape input_tile_for_next
 torch.Size([1, 3, 94, 88])
myctx [[ 140045013206448[ PI( <1,0,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 0]
shape input_tile_for_next
 torch.Size([1, 3, 92, 86])
myctx [[ 140045013206976[ PI( <1,0,>,

in customized Sequential 3
need to get existing
coord [0, 42, 37, 82]
out_temp torch.Size([1, 3, 46, 43])
coord [1, 1]
bwd_out_shape  (46, 46)
fwd_out_shape  (40, 40)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
in customized checkpoint forward 5
input size torch.Size([1, 3, 10240, 10240])
in customized Sequential 6
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 1]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <1,1,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 1]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <1,1,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 1]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <1,1,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 1]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <1,1,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 1]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <1,1,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 1]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <1,1,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 1]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <1,1,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 1]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <1,1,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 1]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <1,1,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 1]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <1,1,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 1]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <1,1,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 1]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <1,1,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 1]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <1,1,>,

in customized Sequential 3
need to get existing
coord [37, 82, 37, 82]
out_temp torch.Size([1, 3, 46, 46])
coord [1, 2]
bwd_out_shape  (46, 46)
fwd_out_shape  (40, 40)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
in customized checkpoint forward 5
input size torch.Size([1, 3, 10240, 10240])
in customized Sequential 6
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 2]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <1,2,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 2]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <1,2,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 2]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <1,2,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 2]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <1,2,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 2]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <1,2,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 2]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <1,2,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 2]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <1,2,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 2]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <1,2,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 2]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <1,2,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 2]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <1,2,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 2]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <1,2,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 2]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <1,2,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 2]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <1,2,>,

in customized Sequential 3
need to get existing
coord [77, 122, 37, 82]
out_temp torch.Size([1, 3, 46, 46])
coord [1, 3]
bwd_out_shape  (46, 46)
fwd_out_shape  (40, 40)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
in customized checkpoint forward 5
input size torch.Size([1, 3, 10240, 10240])
in customized Sequential 6
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 3]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <1,3,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 3]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <1,3,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 3]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <1,3,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 3]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <1,3,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 3]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <1,3,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 3]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <1,3,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 3]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <1,3,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 3]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <1,3,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 3]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <1,3,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 3]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <1,3,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 3]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <1,3,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 3]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <1,3,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 3]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <1,3,>,

in customized Sequential 3
need to get existing
coord [117, 162, 37, 82]
out_temp torch.Size([1, 3, 46, 46])
coord [1, 4]
bwd_out_shape  (46, 46)
fwd_out_shape  (40, 40)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
in customized checkpoint forward 5
input size torch.Size([1, 3, 10240, 10240])
in customized Sequential 6
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 4]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <1,4,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 4]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <1,4,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 4]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <1,4,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 4]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <1,4,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 4]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <1,4,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 4]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <1,4,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 4]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <1,4,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 4]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <1,4,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 4]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <1,4,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 4]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <1,4,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 4]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <1,4,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 4]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <1,4,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 4]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <1,4,>,

in customized Sequential 3
need to get existing
coord [157, 202, 37, 82]
out_temp torch.Size([1, 3, 46, 46])
coord [1, 5]
bwd_out_shape  (46, 46)
fwd_out_shape  (40, 40)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
in customized checkpoint forward 5
input size torch.Size([1, 3, 10240, 10240])
in customized Sequential 6
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 5]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <1,5,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 5]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <1,5,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 5]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <1,5,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 5]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <1,5,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 5]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <1,5,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 5]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <1,5,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 5]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <1,5,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 5]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <1,5,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 5]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <1,5,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 5]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <1,5,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 5]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <1,5,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 5]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <1,5,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 5]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <1,5,>,

in customized Sequential 3
need to get existing
coord [197, 242, 37, 82]
out_temp torch.Size([1, 3, 46, 46])
coord [1, 6]
bwd_out_shape  (46, 46)
fwd_out_shape  (40, 40)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
in customized checkpoint forward 5
input size torch.Size([1, 3, 10240, 10240])
in customized Sequential 6
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 6]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <1,6,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 6]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <1,6,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 6]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <1,6,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 6]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <1,6,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 6]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <1,6,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 6]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <1,6,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 6]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <1,6,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 6]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <1,6,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 6]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <1,6,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 6]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <1,6,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 6]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <1,6,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 6]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <1,6,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 6]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <1,6,>,

in customized Sequential 3
need to get existing
coord [237, 282, 37, 82]
out_temp torch.Size([1, 3, 46, 46])
coord [1, 7]
bwd_out_shape  (43, 46)
fwd_out_shape  (40, 40)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
in customized checkpoint forward 5
input size torch.Size([1, 3, 10240, 10240])
in customized Sequential 6
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 7]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1466])
myctx [[ 140046822727744[ PI( <1,7,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 7]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1464])
myctx [[ 140045015171856[ PI( <1,7,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 7]
shape input_tile_for_next
 torch.Size([1, 3, 822, 732])
myctx [[ 140045013800704[ PI( <1,7,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 7]
shape input_tile_for_next
 torch.Size([1, 3, 820, 730])
myctx [[ 140045013848944[ PI( <1,7,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 7]
shape input_tile_for_next
 torch.Size([1, 3, 408, 366])
myctx [[ 140045013206640[ PI( <1,7,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 7]
shape input_tile_for_next
 torch.Size([1, 3, 406, 364])
myctx [[ 140045013206784[ PI( <1,7,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 7]
shape input_tile_for_next
 torch.Size([1, 3, 404, 362])
myctx [[ 140045013206880[ PI( <1,7,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 7]
shape input_tile_for_next
 torch.Size([1, 3, 200, 182])
myctx [[ 140045013206352[ PI( <1,7,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 7]
shape input_tile_for_next
 torch.Size([1, 3, 198, 180])
myctx [[ 140045013206304[ PI( <1,7,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 7]
shape input_tile_for_next
 torch.Size([1, 3, 196, 178])
myctx [[ 140045013207072[ PI( <1,7,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 7]
shape input_tile_for_next
 torch.Size([1, 3, 96, 90])
myctx [[ 140045013206400[ PI( <1,7,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 7]
shape input_tile_for_next
 torch.Size([1, 3, 94, 88])
myctx [[ 140045013206448[ PI( <1,7,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 7]
shape input_tile_for_next
 torch.Size([1, 3, 92, 86])
myctx [[ 140045013206976[ PI( <1,7,>,

in customized Sequential 3
need to get existing
coord [277, 319, 37, 82]
out_temp torch.Size([1, 3, 46, 43])
coord [2, 0]
bwd_out_shape  (43, 46)
fwd_out_shape  (40, 40)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
in customized checkpoint forward 5
input size torch.Size([1, 3, 10240, 10240])
in customized Sequential 6
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 0]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1466])
myctx [[ 140046822727744[ PI( <2,0,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 0]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1464])
myctx [[ 140045015171856[ PI( <2,0,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 0]
shape input_tile_for_next
 torch.Size([1, 3, 822, 732])
myctx [[ 140045013800704[ PI( <2,0,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 0]
shape input_tile_for_next
 torch.Size([1, 3, 820, 730])
myctx [[ 140045013848944[ PI( <2,0,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 0]
shape input_tile_for_next
 torch.Size([1, 3, 408, 366])
myctx [[ 140045013206640[ PI( <2,0,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 0]
shape input_tile_for_next
 torch.Size([1, 3, 406, 364])
myctx [[ 140045013206784[ PI( <2,0,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 0]
shape input_tile_for_next
 torch.Size([1, 3, 404, 362])
myctx [[ 140045013206880[ PI( <2,0,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 0]
shape input_tile_for_next
 torch.Size([1, 3, 200, 182])
myctx [[ 140045013206352[ PI( <2,0,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 0]
shape input_tile_for_next
 torch.Size([1, 3, 198, 180])
myctx [[ 140045013206304[ PI( <2,0,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 0]
shape input_tile_for_next
 torch.Size([1, 3, 196, 178])
myctx [[ 140045013207072[ PI( <2,0,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 0]
shape input_tile_for_next
 torch.Size([1, 3, 96, 90])
myctx [[ 140045013206400[ PI( <2,0,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 0]
shape input_tile_for_next
 torch.Size([1, 3, 94, 88])
myctx [[ 140045013206448[ PI( <2,0,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 0]
shape input_tile_for_next
 torch.Size([1, 3, 92, 86])
myctx [[ 140045013206976[ PI( <2,0,>,

in customized Sequential 3
need to get existing
coord [0, 42, 77, 122]
out_temp torch.Size([1, 3, 46, 43])
coord [2, 1]
bwd_out_shape  (46, 46)
fwd_out_shape  (40, 40)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
in customized checkpoint forward 5
input size torch.Size([1, 3, 10240, 10240])
in customized Sequential 6
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 1]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <2,1,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 1]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <2,1,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 1]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <2,1,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 1]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <2,1,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 1]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <2,1,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 1]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <2,1,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 1]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <2,1,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 1]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <2,1,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 1]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <2,1,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 1]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <2,1,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 1]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <2,1,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 1]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <2,1,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 1]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <2,1,>,

in customized Sequential 3
need to get existing
coord [37, 82, 77, 122]
out_temp torch.Size([1, 3, 46, 46])
coord [2, 2]
bwd_out_shape  (46, 46)
fwd_out_shape  (40, 40)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
in customized checkpoint forward 5
input size torch.Size([1, 3, 10240, 10240])
in customized Sequential 6
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 2]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <2,2,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 2]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <2,2,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 2]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <2,2,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 2]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <2,2,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 2]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <2,2,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 2]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <2,2,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 2]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <2,2,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 2]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <2,2,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 2]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <2,2,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 2]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <2,2,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 2]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <2,2,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 2]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <2,2,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 2]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <2,2,>,

in customized Sequential 3
need to get existing
coord [77, 122, 77, 122]
out_temp torch.Size([1, 3, 46, 46])
coord [2, 3]
bwd_out_shape  (46, 46)
fwd_out_shape  (40, 40)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
in customized checkpoint forward 5
input size torch.Size([1, 3, 10240, 10240])
in customized Sequential 6
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 3]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <2,3,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 3]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <2,3,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 3]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <2,3,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 3]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <2,3,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 3]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <2,3,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 3]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <2,3,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 3]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <2,3,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 3]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <2,3,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 3]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <2,3,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 3]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <2,3,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 3]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <2,3,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 3]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <2,3,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 3]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <2,3,>,

in customized Sequential 3
need to get existing
coord [117, 162, 77, 122]
out_temp torch.Size([1, 3, 46, 46])
coord [2, 4]
bwd_out_shape  (46, 46)
fwd_out_shape  (40, 40)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
in customized checkpoint forward 5
input size torch.Size([1, 3, 10240, 10240])
in customized Sequential 6
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 4]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <2,4,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 4]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <2,4,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 4]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <2,4,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 4]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <2,4,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 4]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <2,4,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 4]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <2,4,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 4]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <2,4,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 4]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <2,4,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 4]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <2,4,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 4]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <2,4,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 4]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <2,4,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 4]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <2,4,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 4]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <2,4,>,

in customized Sequential 3
need to get existing
coord [157, 202, 77, 122]
out_temp torch.Size([1, 3, 46, 46])
coord [2, 5]
bwd_out_shape  (46, 46)
fwd_out_shape  (40, 40)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
in customized checkpoint forward 5
input size torch.Size([1, 3, 10240, 10240])
in customized Sequential 6
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 5]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <2,5,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 5]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <2,5,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 5]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <2,5,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 5]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <2,5,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 5]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <2,5,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 5]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <2,5,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 5]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <2,5,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 5]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <2,5,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 5]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <2,5,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 5]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <2,5,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 5]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <2,5,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 5]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <2,5,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 5]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <2,5,>,

in customized Sequential 3
need to get existing
coord [197, 242, 77, 122]
out_temp torch.Size([1, 3, 46, 46])
coord [2, 6]
bwd_out_shape  (46, 46)
fwd_out_shape  (40, 40)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
in customized checkpoint forward 5
input size torch.Size([1, 3, 10240, 10240])
in customized Sequential 6
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 6]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <2,6,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 6]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <2,6,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 6]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <2,6,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 6]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <2,6,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 6]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <2,6,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 6]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <2,6,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 6]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <2,6,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 6]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <2,6,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 6]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <2,6,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 6]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <2,6,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 6]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <2,6,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 6]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <2,6,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 6]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <2,6,>,

in customized Sequential 3
need to get existing
coord [237, 282, 77, 122]
out_temp torch.Size([1, 3, 46, 46])
coord [2, 7]
bwd_out_shape  (43, 46)
fwd_out_shape  (40, 40)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
in customized checkpoint forward 5
input size torch.Size([1, 3, 10240, 10240])
in customized Sequential 6
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 7]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1466])
myctx [[ 140046822727744[ PI( <2,7,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 7]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1464])
myctx [[ 140045015171856[ PI( <2,7,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 7]
shape input_tile_for_next
 torch.Size([1, 3, 822, 732])
myctx [[ 140045013800704[ PI( <2,7,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 7]
shape input_tile_for_next
 torch.Size([1, 3, 820, 730])
myctx [[ 140045013848944[ PI( <2,7,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 7]
shape input_tile_for_next
 torch.Size([1, 3, 408, 366])
myctx [[ 140045013206640[ PI( <2,7,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 7]
shape input_tile_for_next
 torch.Size([1, 3, 406, 364])
myctx [[ 140045013206784[ PI( <2,7,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 7]
shape input_tile_for_next
 torch.Size([1, 3, 404, 362])
myctx [[ 140045013206880[ PI( <2,7,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 7]
shape input_tile_for_next
 torch.Size([1, 3, 200, 182])
myctx [[ 140045013206352[ PI( <2,7,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 7]
shape input_tile_for_next
 torch.Size([1, 3, 198, 180])
myctx [[ 140045013206304[ PI( <2,7,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 7]
shape input_tile_for_next
 torch.Size([1, 3, 196, 178])
myctx [[ 140045013207072[ PI( <2,7,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 7]
shape input_tile_for_next
 torch.Size([1, 3, 96, 90])
myctx [[ 140045013206400[ PI( <2,7,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 7]
shape input_tile_for_next
 torch.Size([1, 3, 94, 88])
myctx [[ 140045013206448[ PI( <2,7,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 7]
shape input_tile_for_next
 torch.Size([1, 3, 92, 86])
myctx [[ 140045013206976[ PI( <2,7,>,

in customized Sequential 3
need to get existing
coord [277, 319, 77, 122]
out_temp torch.Size([1, 3, 46, 43])
coord [3, 0]
bwd_out_shape  (43, 46)
fwd_out_shape  (40, 40)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
in customized checkpoint forward 5
input size torch.Size([1, 3, 10240, 10240])
in customized Sequential 6
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 0]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1466])
myctx [[ 140046822727744[ PI( <3,0,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 0]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1464])
myctx [[ 140045015171856[ PI( <3,0,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 0]
shape input_tile_for_next
 torch.Size([1, 3, 822, 732])
myctx [[ 140045013800704[ PI( <3,0,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 0]
shape input_tile_for_next
 torch.Size([1, 3, 820, 730])
myctx [[ 140045013848944[ PI( <3,0,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 0]
shape input_tile_for_next
 torch.Size([1, 3, 408, 366])
myctx [[ 140045013206640[ PI( <3,0,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 0]
shape input_tile_for_next
 torch.Size([1, 3, 406, 364])
myctx [[ 140045013206784[ PI( <3,0,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 0]
shape input_tile_for_next
 torch.Size([1, 3, 404, 362])
myctx [[ 140045013206880[ PI( <3,0,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 0]
shape input_tile_for_next
 torch.Size([1, 3, 200, 182])
myctx [[ 140045013206352[ PI( <3,0,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 0]
shape input_tile_for_next
 torch.Size([1, 3, 198, 180])
myctx [[ 140045013206304[ PI( <3,0,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 0]
shape input_tile_for_next
 torch.Size([1, 3, 196, 178])
myctx [[ 140045013207072[ PI( <3,0,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 0]
shape input_tile_for_next
 torch.Size([1, 3, 96, 90])
myctx [[ 140045013206400[ PI( <3,0,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 0]
shape input_tile_for_next
 torch.Size([1, 3, 94, 88])
myctx [[ 140045013206448[ PI( <3,0,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 0]
shape input_tile_for_next
 torch.Size([1, 3, 92, 86])
myctx [[ 140045013206976[ PI( <3,0,>,

in customized Sequential 3
need to get existing
coord [0, 42, 117, 162]
out_temp torch.Size([1, 3, 46, 43])
coord [3, 1]
bwd_out_shape  (46, 46)
fwd_out_shape  (40, 40)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
in customized checkpoint forward 5
input size torch.Size([1, 3, 10240, 10240])
in customized Sequential 6
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 1]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <3,1,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 1]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <3,1,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 1]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <3,1,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 1]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <3,1,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 1]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <3,1,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 1]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <3,1,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 1]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <3,1,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 1]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <3,1,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 1]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <3,1,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 1]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <3,1,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 1]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <3,1,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 1]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <3,1,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 1]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <3,1,>,

in customized Sequential 3
need to get existing
coord [37, 82, 117, 162]
out_temp torch.Size([1, 3, 46, 46])
coord [3, 2]
bwd_out_shape  (46, 46)
fwd_out_shape  (40, 40)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
in customized checkpoint forward 5
input size torch.Size([1, 3, 10240, 10240])
in customized Sequential 6
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 2]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <3,2,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 2]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <3,2,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 2]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <3,2,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 2]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <3,2,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 2]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <3,2,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 2]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <3,2,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 2]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <3,2,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 2]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <3,2,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 2]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <3,2,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 2]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <3,2,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 2]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <3,2,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 2]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <3,2,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 2]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <3,2,>,

in customized Sequential 3
need to get existing
coord [77, 122, 117, 162]
out_temp torch.Size([1, 3, 46, 46])
coord [3, 3]
bwd_out_shape  (46, 46)
fwd_out_shape  (40, 40)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
in customized checkpoint forward 5
input size torch.Size([1, 3, 10240, 10240])
in customized Sequential 6
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 3]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <3,3,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 3]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <3,3,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 3]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <3,3,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 3]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <3,3,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 3]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <3,3,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 3]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <3,3,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 3]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <3,3,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 3]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <3,3,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 3]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <3,3,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 3]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <3,3,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 3]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <3,3,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 3]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <3,3,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 3]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <3,3,>,

in customized Sequential 3
need to get existing
coord [117, 162, 117, 162]
out_temp torch.Size([1, 3, 46, 46])
coord [3, 4]
bwd_out_shape  (46, 46)
fwd_out_shape  (40, 40)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
in customized checkpoint forward 5
input size torch.Size([1, 3, 10240, 10240])
in customized Sequential 6
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 4]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <3,4,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 4]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <3,4,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 4]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <3,4,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 4]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <3,4,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 4]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <3,4,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 4]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <3,4,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 4]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <3,4,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 4]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <3,4,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 4]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <3,4,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 4]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <3,4,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 4]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <3,4,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 4]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <3,4,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 4]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <3,4,>,

in customized Sequential 3
need to get existing
coord [157, 202, 117, 162]
out_temp torch.Size([1, 3, 46, 46])
coord [3, 5]
bwd_out_shape  (46, 46)
fwd_out_shape  (40, 40)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
in customized checkpoint forward 5
input size torch.Size([1, 3, 10240, 10240])
in customized Sequential 6
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 5]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <3,5,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 5]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <3,5,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 5]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <3,5,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 5]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <3,5,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 5]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <3,5,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 5]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <3,5,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 5]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <3,5,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 5]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <3,5,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 5]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <3,5,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 5]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <3,5,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 5]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <3,5,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 5]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <3,5,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 5]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <3,5,>,

in customized Sequential 3
need to get existing
coord [197, 242, 117, 162]
out_temp torch.Size([1, 3, 46, 46])
coord [3, 6]
bwd_out_shape  (46, 46)
fwd_out_shape  (40, 40)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
in customized checkpoint forward 5
input size torch.Size([1, 3, 10240, 10240])
in customized Sequential 6
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 6]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <3,6,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 6]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <3,6,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 6]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <3,6,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 6]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <3,6,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 6]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <3,6,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 6]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <3,6,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 6]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <3,6,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 6]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <3,6,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 6]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <3,6,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 6]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <3,6,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 6]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <3,6,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 6]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <3,6,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 6]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <3,6,>,

in customized Sequential 3
need to get existing
coord [237, 282, 117, 162]
out_temp torch.Size([1, 3, 46, 46])
coord [3, 7]
bwd_out_shape  (43, 46)
fwd_out_shape  (40, 40)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
in customized checkpoint forward 5
input size torch.Size([1, 3, 10240, 10240])
in customized Sequential 6
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 7]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1466])
myctx [[ 140046822727744[ PI( <3,7,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 7]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1464])
myctx [[ 140045015171856[ PI( <3,7,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 7]
shape input_tile_for_next
 torch.Size([1, 3, 822, 732])
myctx [[ 140045013800704[ PI( <3,7,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 7]
shape input_tile_for_next
 torch.Size([1, 3, 820, 730])
myctx [[ 140045013848944[ PI( <3,7,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 7]
shape input_tile_for_next
 torch.Size([1, 3, 408, 366])
myctx [[ 140045013206640[ PI( <3,7,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 7]
shape input_tile_for_next
 torch.Size([1, 3, 406, 364])
myctx [[ 140045013206784[ PI( <3,7,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 7]
shape input_tile_for_next
 torch.Size([1, 3, 404, 362])
myctx [[ 140045013206880[ PI( <3,7,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 7]
shape input_tile_for_next
 torch.Size([1, 3, 200, 182])
myctx [[ 140045013206352[ PI( <3,7,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 7]
shape input_tile_for_next
 torch.Size([1, 3, 198, 180])
myctx [[ 140045013206304[ PI( <3,7,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 7]
shape input_tile_for_next
 torch.Size([1, 3, 196, 178])
myctx [[ 140045013207072[ PI( <3,7,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 7]
shape input_tile_for_next
 torch.Size([1, 3, 96, 90])
myctx [[ 140045013206400[ PI( <3,7,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 7]
shape input_tile_for_next
 torch.Size([1, 3, 94, 88])
myctx [[ 140045013206448[ PI( <3,7,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 7]
shape input_tile_for_next
 torch.Size([1, 3, 92, 86])
myctx [[ 140045013206976[ PI( <3,7,>,

in customized Sequential 3
need to get existing
coord [277, 319, 117, 162]
out_temp torch.Size([1, 3, 46, 43])
coord [4, 0]
bwd_out_shape  (43, 46)
fwd_out_shape  (40, 40)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
in customized checkpoint forward 5
input size torch.Size([1, 3, 10240, 10240])
in customized Sequential 6
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 0]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1466])
myctx [[ 140046822727744[ PI( <4,0,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 0]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1464])
myctx [[ 140045015171856[ PI( <4,0,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 0]
shape input_tile_for_next
 torch.Size([1, 3, 822, 732])
myctx [[ 140045013800704[ PI( <4,0,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 0]
shape input_tile_for_next
 torch.Size([1, 3, 820, 730])
myctx [[ 140045013848944[ PI( <4,0,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 0]
shape input_tile_for_next
 torch.Size([1, 3, 408, 366])
myctx [[ 140045013206640[ PI( <4,0,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 0]
shape input_tile_for_next
 torch.Size([1, 3, 406, 364])
myctx [[ 140045013206784[ PI( <4,0,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 0]
shape input_tile_for_next
 torch.Size([1, 3, 404, 362])
myctx [[ 140045013206880[ PI( <4,0,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 0]
shape input_tile_for_next
 torch.Size([1, 3, 200, 182])
myctx [[ 140045013206352[ PI( <4,0,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 0]
shape input_tile_for_next
 torch.Size([1, 3, 198, 180])
myctx [[ 140045013206304[ PI( <4,0,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 0]
shape input_tile_for_next
 torch.Size([1, 3, 196, 178])
myctx [[ 140045013207072[ PI( <4,0,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 0]
shape input_tile_for_next
 torch.Size([1, 3, 96, 90])
myctx [[ 140045013206400[ PI( <4,0,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 0]
shape input_tile_for_next
 torch.Size([1, 3, 94, 88])
myctx [[ 140045013206448[ PI( <4,0,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 0]
shape input_tile_for_next
 torch.Size([1, 3, 92, 86])
myctx [[ 140045013206976[ PI( <4,0,>,

in customized Sequential 3
need to get existing
coord [0, 42, 157, 202]
out_temp torch.Size([1, 3, 46, 43])
coord [4, 1]
bwd_out_shape  (46, 46)
fwd_out_shape  (40, 40)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
in customized checkpoint forward 5
input size torch.Size([1, 3, 10240, 10240])
in customized Sequential 6
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 1]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <4,1,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 1]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <4,1,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 1]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <4,1,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 1]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <4,1,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 1]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <4,1,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 1]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <4,1,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 1]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <4,1,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 1]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <4,1,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 1]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <4,1,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 1]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <4,1,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 1]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <4,1,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 1]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <4,1,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 1]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <4,1,>,

in customized Sequential 3
need to get existing
coord [37, 82, 157, 202]
out_temp torch.Size([1, 3, 46, 46])
coord [4, 2]
bwd_out_shape  (46, 46)
fwd_out_shape  (40, 40)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
in customized checkpoint forward 5
input size torch.Size([1, 3, 10240, 10240])
in customized Sequential 6
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 2]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <4,2,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 2]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <4,2,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 2]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <4,2,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 2]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <4,2,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 2]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <4,2,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 2]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <4,2,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 2]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <4,2,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 2]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <4,2,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 2]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <4,2,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 2]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <4,2,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 2]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <4,2,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 2]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <4,2,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 2]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <4,2,>,

in customized Sequential 3
need to get existing
coord [77, 122, 157, 202]
out_temp torch.Size([1, 3, 46, 46])
coord [4, 3]
bwd_out_shape  (46, 46)
fwd_out_shape  (40, 40)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
in customized checkpoint forward 5
input size torch.Size([1, 3, 10240, 10240])
in customized Sequential 6
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 3]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <4,3,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 3]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <4,3,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 3]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <4,3,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 3]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <4,3,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 3]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <4,3,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 3]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <4,3,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 3]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <4,3,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 3]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <4,3,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 3]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <4,3,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 3]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <4,3,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 3]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <4,3,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 3]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <4,3,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 3]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <4,3,>,

in customized Sequential 3
need to get existing
coord [117, 162, 157, 202]
out_temp torch.Size([1, 3, 46, 46])
coord [4, 4]
bwd_out_shape  (46, 46)
fwd_out_shape  (40, 40)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
in customized checkpoint forward 5
input size torch.Size([1, 3, 10240, 10240])
in customized Sequential 6
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 4]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <4,4,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 4]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <4,4,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 4]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <4,4,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 4]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <4,4,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 4]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <4,4,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 4]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <4,4,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 4]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <4,4,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 4]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <4,4,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 4]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <4,4,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 4]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <4,4,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 4]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <4,4,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 4]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <4,4,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 4]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <4,4,>,

in customized Sequential 3
need to get existing
coord [157, 202, 157, 202]
out_temp torch.Size([1, 3, 46, 46])
coord [4, 5]
bwd_out_shape  (46, 46)
fwd_out_shape  (40, 40)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
in customized checkpoint forward 5
input size torch.Size([1, 3, 10240, 10240])
in customized Sequential 6
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 5]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <4,5,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 5]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <4,5,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 5]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <4,5,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 5]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <4,5,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 5]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <4,5,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 5]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <4,5,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 5]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <4,5,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 5]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <4,5,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 5]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <4,5,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 5]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <4,5,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 5]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <4,5,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 5]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <4,5,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 5]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <4,5,>,

in customized Sequential 3
need to get existing
coord [197, 242, 157, 202]
out_temp torch.Size([1, 3, 46, 46])
coord [4, 6]
bwd_out_shape  (46, 46)
fwd_out_shape  (40, 40)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
in customized checkpoint forward 5
input size torch.Size([1, 3, 10240, 10240])
in customized Sequential 6
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 6]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <4,6,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 6]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <4,6,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 6]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <4,6,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 6]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <4,6,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 6]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <4,6,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 6]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <4,6,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 6]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <4,6,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 6]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <4,6,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 6]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <4,6,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 6]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <4,6,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 6]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <4,6,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 6]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <4,6,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 6]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <4,6,>,

in customized Sequential 3
need to get existing
coord [237, 282, 157, 202]
out_temp torch.Size([1, 3, 46, 46])
coord [4, 7]
bwd_out_shape  (43, 46)
fwd_out_shape  (40, 40)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
in customized checkpoint forward 5
input size torch.Size([1, 3, 10240, 10240])
in customized Sequential 6
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 7]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1466])
myctx [[ 140046822727744[ PI( <4,7,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 7]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1464])
myctx [[ 140045015171856[ PI( <4,7,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 7]
shape input_tile_for_next
 torch.Size([1, 3, 822, 732])
myctx [[ 140045013800704[ PI( <4,7,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 7]
shape input_tile_for_next
 torch.Size([1, 3, 820, 730])
myctx [[ 140045013848944[ PI( <4,7,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 7]
shape input_tile_for_next
 torch.Size([1, 3, 408, 366])
myctx [[ 140045013206640[ PI( <4,7,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 7]
shape input_tile_for_next
 torch.Size([1, 3, 406, 364])
myctx [[ 140045013206784[ PI( <4,7,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 7]
shape input_tile_for_next
 torch.Size([1, 3, 404, 362])
myctx [[ 140045013206880[ PI( <4,7,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 7]
shape input_tile_for_next
 torch.Size([1, 3, 200, 182])
myctx [[ 140045013206352[ PI( <4,7,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 7]
shape input_tile_for_next
 torch.Size([1, 3, 198, 180])
myctx [[ 140045013206304[ PI( <4,7,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 7]
shape input_tile_for_next
 torch.Size([1, 3, 196, 178])
myctx [[ 140045013207072[ PI( <4,7,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 7]
shape input_tile_for_next
 torch.Size([1, 3, 96, 90])
myctx [[ 140045013206400[ PI( <4,7,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 7]
shape input_tile_for_next
 torch.Size([1, 3, 94, 88])
myctx [[ 140045013206448[ PI( <4,7,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 7]
shape input_tile_for_next
 torch.Size([1, 3, 92, 86])
myctx [[ 140045013206976[ PI( <4,7,>,

in customized Sequential 3
need to get existing
coord [277, 319, 157, 202]
out_temp torch.Size([1, 3, 46, 43])
coord [5, 0]
bwd_out_shape  (43, 46)
fwd_out_shape  (40, 40)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
in customized checkpoint forward 5
input size torch.Size([1, 3, 10240, 10240])
in customized Sequential 6
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 0]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1466])
myctx [[ 140046822727744[ PI( <5,0,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 0]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1464])
myctx [[ 140045015171856[ PI( <5,0,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 0]
shape input_tile_for_next
 torch.Size([1, 3, 822, 732])
myctx [[ 140045013800704[ PI( <5,0,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 0]
shape input_tile_for_next
 torch.Size([1, 3, 820, 730])
myctx [[ 140045013848944[ PI( <5,0,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 0]
shape input_tile_for_next
 torch.Size([1, 3, 408, 366])
myctx [[ 140045013206640[ PI( <5,0,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 0]
shape input_tile_for_next
 torch.Size([1, 3, 406, 364])
myctx [[ 140045013206784[ PI( <5,0,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 0]
shape input_tile_for_next
 torch.Size([1, 3, 404, 362])
myctx [[ 140045013206880[ PI( <5,0,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 0]
shape input_tile_for_next
 torch.Size([1, 3, 200, 182])
myctx [[ 140045013206352[ PI( <5,0,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 0]
shape input_tile_for_next
 torch.Size([1, 3, 198, 180])
myctx [[ 140045013206304[ PI( <5,0,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 0]
shape input_tile_for_next
 torch.Size([1, 3, 196, 178])
myctx [[ 140045013207072[ PI( <5,0,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 0]
shape input_tile_for_next
 torch.Size([1, 3, 96, 90])
myctx [[ 140045013206400[ PI( <5,0,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 0]
shape input_tile_for_next
 torch.Size([1, 3, 94, 88])
myctx [[ 140045013206448[ PI( <5,0,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 0]
shape input_tile_for_next
 torch.Size([1, 3, 92, 86])
myctx [[ 140045013206976[ PI( <5,0,>,

in customized Sequential 3
need to get existing
coord [0, 42, 197, 242]
out_temp torch.Size([1, 3, 46, 43])
coord [5, 1]
bwd_out_shape  (46, 46)
fwd_out_shape  (40, 40)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
in customized checkpoint forward 5
input size torch.Size([1, 3, 10240, 10240])
in customized Sequential 6
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 1]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <5,1,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 1]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <5,1,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 1]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <5,1,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 1]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <5,1,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 1]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <5,1,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 1]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <5,1,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 1]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <5,1,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 1]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <5,1,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 1]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <5,1,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 1]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <5,1,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 1]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <5,1,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 1]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <5,1,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 1]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <5,1,>,

in customized Sequential 3
need to get existing
coord [37, 82, 197, 242]
out_temp torch.Size([1, 3, 46, 46])
coord [5, 2]
bwd_out_shape  (46, 46)
fwd_out_shape  (40, 40)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
in customized checkpoint forward 5
input size torch.Size([1, 3, 10240, 10240])
in customized Sequential 6
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 2]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <5,2,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 2]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <5,2,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 2]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <5,2,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 2]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <5,2,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 2]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <5,2,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 2]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <5,2,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 2]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <5,2,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 2]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <5,2,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 2]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <5,2,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 2]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <5,2,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 2]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <5,2,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 2]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <5,2,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 2]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <5,2,>,

in customized Sequential 3
need to get existing
coord [77, 122, 197, 242]
out_temp torch.Size([1, 3, 46, 46])
coord [5, 3]
bwd_out_shape  (46, 46)
fwd_out_shape  (40, 40)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
in customized checkpoint forward 5
input size torch.Size([1, 3, 10240, 10240])
in customized Sequential 6
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 3]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <5,3,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 3]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <5,3,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 3]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <5,3,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 3]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <5,3,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 3]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <5,3,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 3]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <5,3,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 3]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <5,3,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 3]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <5,3,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 3]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <5,3,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 3]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <5,3,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 3]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <5,3,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 3]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <5,3,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 3]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <5,3,>,

in customized Sequential 3
need to get existing
coord [117, 162, 197, 242]
out_temp torch.Size([1, 3, 46, 46])
coord [5, 4]
bwd_out_shape  (46, 46)
fwd_out_shape  (40, 40)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
in customized checkpoint forward 5
input size torch.Size([1, 3, 10240, 10240])
in customized Sequential 6
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 4]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <5,4,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 4]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <5,4,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 4]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <5,4,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 4]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <5,4,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 4]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <5,4,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 4]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <5,4,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 4]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <5,4,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 4]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <5,4,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 4]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <5,4,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 4]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <5,4,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 4]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <5,4,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 4]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <5,4,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 4]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <5,4,>,

in customized Sequential 3
need to get existing
coord [157, 202, 197, 242]
out_temp torch.Size([1, 3, 46, 46])
coord [5, 5]
bwd_out_shape  (46, 46)
fwd_out_shape  (40, 40)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
in customized checkpoint forward 5
input size torch.Size([1, 3, 10240, 10240])
in customized Sequential 6
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 5]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <5,5,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 5]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <5,5,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 5]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <5,5,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 5]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <5,5,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 5]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <5,5,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 5]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <5,5,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 5]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <5,5,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 5]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <5,5,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 5]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <5,5,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 5]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <5,5,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 5]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <5,5,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 5]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <5,5,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 5]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <5,5,>,

in customized Sequential 3
need to get existing
coord [197, 242, 197, 242]
out_temp torch.Size([1, 3, 46, 46])
coord [5, 6]
bwd_out_shape  (46, 46)
fwd_out_shape  (40, 40)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
in customized checkpoint forward 5
input size torch.Size([1, 3, 10240, 10240])
in customized Sequential 6
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 6]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <5,6,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 6]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <5,6,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 6]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <5,6,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 6]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <5,6,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 6]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <5,6,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 6]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <5,6,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 6]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <5,6,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 6]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <5,6,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 6]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <5,6,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 6]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <5,6,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 6]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <5,6,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 6]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <5,6,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 6]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <5,6,>,

in customized Sequential 3
need to get existing
coord [237, 282, 197, 242]
out_temp torch.Size([1, 3, 46, 46])
coord [5, 7]
bwd_out_shape  (43, 46)
fwd_out_shape  (40, 40)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
in customized checkpoint forward 5
input size torch.Size([1, 3, 10240, 10240])
in customized Sequential 6
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 7]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1466])
myctx [[ 140046822727744[ PI( <5,7,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 7]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1464])
myctx [[ 140045015171856[ PI( <5,7,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 7]
shape input_tile_for_next
 torch.Size([1, 3, 822, 732])
myctx [[ 140045013800704[ PI( <5,7,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 7]
shape input_tile_for_next
 torch.Size([1, 3, 820, 730])
myctx [[ 140045013848944[ PI( <5,7,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 7]
shape input_tile_for_next
 torch.Size([1, 3, 408, 366])
myctx [[ 140045013206640[ PI( <5,7,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 7]
shape input_tile_for_next
 torch.Size([1, 3, 406, 364])
myctx [[ 140045013206784[ PI( <5,7,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 7]
shape input_tile_for_next
 torch.Size([1, 3, 404, 362])
myctx [[ 140045013206880[ PI( <5,7,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 7]
shape input_tile_for_next
 torch.Size([1, 3, 200, 182])
myctx [[ 140045013206352[ PI( <5,7,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 7]
shape input_tile_for_next
 torch.Size([1, 3, 198, 180])
myctx [[ 140045013206304[ PI( <5,7,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 7]
shape input_tile_for_next
 torch.Size([1, 3, 196, 178])
myctx [[ 140045013207072[ PI( <5,7,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 7]
shape input_tile_for_next
 torch.Size([1, 3, 96, 90])
myctx [[ 140045013206400[ PI( <5,7,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 7]
shape input_tile_for_next
 torch.Size([1, 3, 94, 88])
myctx [[ 140045013206448[ PI( <5,7,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 7]
shape input_tile_for_next
 torch.Size([1, 3, 92, 86])
myctx [[ 140045013206976[ PI( <5,7,>,

in customized Sequential 3
need to get existing
coord [277, 319, 197, 242]
out_temp torch.Size([1, 3, 46, 43])
coord [6, 0]
bwd_out_shape  (43, 46)
fwd_out_shape  (40, 40)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
in customized checkpoint forward 5
input size torch.Size([1, 3, 10240, 10240])
in customized Sequential 6
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 0]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1466])
myctx [[ 140046822727744[ PI( <6,0,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 0]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1464])
myctx [[ 140045015171856[ PI( <6,0,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 0]
shape input_tile_for_next
 torch.Size([1, 3, 822, 732])
myctx [[ 140045013800704[ PI( <6,0,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 0]
shape input_tile_for_next
 torch.Size([1, 3, 820, 730])
myctx [[ 140045013848944[ PI( <6,0,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 0]
shape input_tile_for_next
 torch.Size([1, 3, 408, 366])
myctx [[ 140045013206640[ PI( <6,0,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 0]
shape input_tile_for_next
 torch.Size([1, 3, 406, 364])
myctx [[ 140045013206784[ PI( <6,0,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 0]
shape input_tile_for_next
 torch.Size([1, 3, 404, 362])
myctx [[ 140045013206880[ PI( <6,0,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 0]
shape input_tile_for_next
 torch.Size([1, 3, 200, 182])
myctx [[ 140045013206352[ PI( <6,0,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 0]
shape input_tile_for_next
 torch.Size([1, 3, 198, 180])
myctx [[ 140045013206304[ PI( <6,0,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 0]
shape input_tile_for_next
 torch.Size([1, 3, 196, 178])
myctx [[ 140045013207072[ PI( <6,0,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 0]
shape input_tile_for_next
 torch.Size([1, 3, 96, 90])
myctx [[ 140045013206400[ PI( <6,0,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 0]
shape input_tile_for_next
 torch.Size([1, 3, 94, 88])
myctx [[ 140045013206448[ PI( <6,0,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 0]
shape input_tile_for_next
 torch.Size([1, 3, 92, 86])
myctx [[ 140045013206976[ PI( <6,0,>,

in customized Sequential 3
need to get existing
coord [0, 42, 237, 282]
out_temp torch.Size([1, 3, 46, 43])
coord [6, 1]
bwd_out_shape  (46, 46)
fwd_out_shape  (40, 40)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
in customized checkpoint forward 5
input size torch.Size([1, 3, 10240, 10240])
in customized Sequential 6
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 1]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <6,1,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 1]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <6,1,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 1]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <6,1,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 1]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <6,1,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 1]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <6,1,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 1]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <6,1,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 1]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <6,1,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 1]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <6,1,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 1]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <6,1,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 1]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <6,1,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 1]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <6,1,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 1]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <6,1,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 1]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <6,1,>,

in customized Sequential 3
need to get existing
coord [37, 82, 237, 282]
out_temp torch.Size([1, 3, 46, 46])
coord [6, 2]
bwd_out_shape  (46, 46)
fwd_out_shape  (40, 40)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
in customized checkpoint forward 5
input size torch.Size([1, 3, 10240, 10240])
in customized Sequential 6
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 2]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <6,2,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 2]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <6,2,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 2]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <6,2,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 2]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <6,2,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 2]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <6,2,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 2]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <6,2,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 2]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <6,2,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 2]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <6,2,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 2]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <6,2,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 2]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <6,2,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 2]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <6,2,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 2]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <6,2,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 2]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <6,2,>,

in customized Sequential 3
need to get existing
coord [77, 122, 237, 282]
out_temp torch.Size([1, 3, 46, 46])
coord [6, 3]
bwd_out_shape  (46, 46)
fwd_out_shape  (40, 40)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
in customized checkpoint forward 5
input size torch.Size([1, 3, 10240, 10240])
in customized Sequential 6
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 3]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <6,3,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 3]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <6,3,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 3]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <6,3,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 3]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <6,3,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 3]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <6,3,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 3]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <6,3,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 3]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <6,3,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 3]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <6,3,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 3]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <6,3,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 3]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <6,3,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 3]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <6,3,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 3]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <6,3,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 3]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <6,3,>,

in customized Sequential 3
need to get existing
coord [117, 162, 237, 282]
out_temp torch.Size([1, 3, 46, 46])
coord [6, 4]
bwd_out_shape  (46, 46)
fwd_out_shape  (40, 40)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
in customized checkpoint forward 5
input size torch.Size([1, 3, 10240, 10240])
in customized Sequential 6
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 4]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <6,4,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 4]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <6,4,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 4]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <6,4,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 4]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <6,4,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 4]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <6,4,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 4]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <6,4,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 4]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <6,4,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 4]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <6,4,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 4]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <6,4,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 4]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <6,4,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 4]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <6,4,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 4]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <6,4,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 4]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <6,4,>,

in customized Sequential 3
need to get existing
coord [157, 202, 237, 282]
out_temp torch.Size([1, 3, 46, 46])
coord [6, 5]
bwd_out_shape  (46, 46)
fwd_out_shape  (40, 40)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
in customized checkpoint forward 5
input size torch.Size([1, 3, 10240, 10240])
in customized Sequential 6
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 5]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <6,5,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 5]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <6,5,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 5]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <6,5,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 5]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <6,5,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 5]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <6,5,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 5]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <6,5,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 5]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <6,5,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 5]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <6,5,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 5]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <6,5,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 5]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <6,5,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 5]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <6,5,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 5]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <6,5,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 5]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <6,5,>,

in customized Sequential 3
need to get existing
coord [197, 242, 237, 282]
out_temp torch.Size([1, 3, 46, 46])
coord [6, 6]
bwd_out_shape  (46, 46)
fwd_out_shape  (40, 40)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
in customized checkpoint forward 5
input size torch.Size([1, 3, 10240, 10240])
in customized Sequential 6
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 6]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <6,6,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 6]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <6,6,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 6]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <6,6,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 6]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <6,6,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 6]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <6,6,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 6]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <6,6,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 6]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <6,6,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 6]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <6,6,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 6]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <6,6,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 6]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <6,6,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 6]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <6,6,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 6]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <6,6,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 6]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <6,6,>,

in customized Sequential 3
need to get existing
coord [237, 282, 237, 282]
out_temp torch.Size([1, 3, 46, 46])
coord [6, 7]
bwd_out_shape  (43, 46)
fwd_out_shape  (40, 40)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
in customized checkpoint forward 5
input size torch.Size([1, 3, 10240, 10240])
in customized Sequential 6
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 7]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1466])
myctx [[ 140046822727744[ PI( <6,7,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 7]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1464])
myctx [[ 140045015171856[ PI( <6,7,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 7]
shape input_tile_for_next
 torch.Size([1, 3, 822, 732])
myctx [[ 140045013800704[ PI( <6,7,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 7]
shape input_tile_for_next
 torch.Size([1, 3, 820, 730])
myctx [[ 140045013848944[ PI( <6,7,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 7]
shape input_tile_for_next
 torch.Size([1, 3, 408, 366])
myctx [[ 140045013206640[ PI( <6,7,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 7]
shape input_tile_for_next
 torch.Size([1, 3, 406, 364])
myctx [[ 140045013206784[ PI( <6,7,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 7]
shape input_tile_for_next
 torch.Size([1, 3, 404, 362])
myctx [[ 140045013206880[ PI( <6,7,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 7]
shape input_tile_for_next
 torch.Size([1, 3, 200, 182])
myctx [[ 140045013206352[ PI( <6,7,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 7]
shape input_tile_for_next
 torch.Size([1, 3, 198, 180])
myctx [[ 140045013206304[ PI( <6,7,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 7]
shape input_tile_for_next
 torch.Size([1, 3, 196, 178])
myctx [[ 140045013207072[ PI( <6,7,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 7]
shape input_tile_for_next
 torch.Size([1, 3, 96, 90])
myctx [[ 140045013206400[ PI( <6,7,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 7]
shape input_tile_for_next
 torch.Size([1, 3, 94, 88])
myctx [[ 140045013206448[ PI( <6,7,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 7]
shape input_tile_for_next
 torch.Size([1, 3, 92, 86])
myctx [[ 140045013206976[ PI( <6,7,>,

in customized Sequential 3
need to get existing
coord [277, 319, 237, 282]
out_temp torch.Size([1, 3, 46, 43])
coord [7, 0]
bwd_out_shape  (43, 43)
fwd_out_shape  (40, 40)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
in customized checkpoint forward 5
input size torch.Size([1, 3, 10240, 10240])
in customized Sequential 6
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 0]
shape input_tile_for_next
 torch.Size([1, 3, 1466, 1466])
myctx [[ 140046822727744[ PI( <7,0,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 0]
shape input_tile_for_next
 torch.Size([1, 3, 1464, 1464])
myctx [[ 140045015171856[ PI( <7,0,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 0]
shape input_tile_for_next
 torch.Size([1, 3, 732, 732])
myctx [[ 140045013800704[ PI( <7,0,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 0]
shape input_tile_for_next
 torch.Size([1, 3, 730, 730])
myctx [[ 140045013848944[ PI( <7,0,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 0]
shape input_tile_for_next
 torch.Size([1, 3, 366, 366])
myctx [[ 140045013206640[ PI( <7,0,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 0]
shape input_tile_for_next
 torch.Size([1, 3, 364, 364])
myctx [[ 140045013206784[ PI( <7,0,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 0]
shape input_tile_for_next
 torch.Size([1, 3, 362, 362])
myctx [[ 140045013206880[ PI( <7,0,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 0]
shape input_tile_for_next
 torch.Size([1, 3, 182, 182])
myctx [[ 140045013206352[ PI( <7,0,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 0]
shape input_tile_for_next
 torch.Size([1, 3, 180, 180])
myctx [[ 140045013206304[ PI( <7,0,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 0]
shape input_tile_for_next
 torch.Size([1, 3, 178, 178])
myctx [[ 140045013207072[ PI( <7,0,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 0]
shape input_tile_for_next
 torch.Size([1, 3, 90, 90])
myctx [[ 140045013206400[ PI( <7,0,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 0]
shape input_tile_for_next
 torch.Size([1, 3, 88, 88])
myctx [[ 140045013206448[ PI( <7,0,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 0]
shape input_tile_for_next
 torch.Size([1, 3, 86, 86])
myctx [[ 140045013206976[ PI( <7,0,>,

in customized Sequential 3
need to get existing
coord [0, 42, 277, 319]
out_temp torch.Size([1, 3, 43, 43])
coord [7, 1]
bwd_out_shape  (46, 43)
fwd_out_shape  (40, 40)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
in customized checkpoint forward 5
input size torch.Size([1, 3, 10240, 10240])
in customized Sequential 6
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 1]
shape input_tile_for_next
 torch.Size([1, 3, 1466, 1650])
myctx [[ 140046822727744[ PI( <7,1,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 1]
shape input_tile_for_next
 torch.Size([1, 3, 1464, 1648])
myctx [[ 140045015171856[ PI( <7,1,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 1]
shape input_tile_for_next
 torch.Size([1, 3, 732, 822])
myctx [[ 140045013800704[ PI( <7,1,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 1]
shape input_tile_for_next
 torch.Size([1, 3, 730, 820])
myctx [[ 140045013848944[ PI( <7,1,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 1]
shape input_tile_for_next
 torch.Size([1, 3, 366, 408])
myctx [[ 140045013206640[ PI( <7,1,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 1]
shape input_tile_for_next
 torch.Size([1, 3, 364, 406])
myctx [[ 140045013206784[ PI( <7,1,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 1]
shape input_tile_for_next
 torch.Size([1, 3, 362, 404])
myctx [[ 140045013206880[ PI( <7,1,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 1]
shape input_tile_for_next
 torch.Size([1, 3, 182, 200])
myctx [[ 140045013206352[ PI( <7,1,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 1]
shape input_tile_for_next
 torch.Size([1, 3, 180, 198])
myctx [[ 140045013206304[ PI( <7,1,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 1]
shape input_tile_for_next
 torch.Size([1, 3, 178, 196])
myctx [[ 140045013207072[ PI( <7,1,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 1]
shape input_tile_for_next
 torch.Size([1, 3, 90, 96])
myctx [[ 140045013206400[ PI( <7,1,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 1]
shape input_tile_for_next
 torch.Size([1, 3, 88, 94])
myctx [[ 140045013206448[ PI( <7,1,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 1]
shape input_tile_for_next
 torch.Size([1, 3, 86, 92])
myctx [[ 140045013206976[ PI( <7,1,>,

in customized Sequential 3
need to get existing
coord [37, 82, 277, 319]
out_temp torch.Size([1, 3, 43, 46])
coord [7, 2]
bwd_out_shape  (46, 43)
fwd_out_shape  (40, 40)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
in customized checkpoint forward 5
input size torch.Size([1, 3, 10240, 10240])
in customized Sequential 6
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 2]
shape input_tile_for_next
 torch.Size([1, 3, 1466, 1650])
myctx [[ 140046822727744[ PI( <7,2,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 2]
shape input_tile_for_next
 torch.Size([1, 3, 1464, 1648])
myctx [[ 140045015171856[ PI( <7,2,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 2]
shape input_tile_for_next
 torch.Size([1, 3, 732, 822])
myctx [[ 140045013800704[ PI( <7,2,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 2]
shape input_tile_for_next
 torch.Size([1, 3, 730, 820])
myctx [[ 140045013848944[ PI( <7,2,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 2]
shape input_tile_for_next
 torch.Size([1, 3, 366, 408])
myctx [[ 140045013206640[ PI( <7,2,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 2]
shape input_tile_for_next
 torch.Size([1, 3, 364, 406])
myctx [[ 140045013206784[ PI( <7,2,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 2]
shape input_tile_for_next
 torch.Size([1, 3, 362, 404])
myctx [[ 140045013206880[ PI( <7,2,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 2]
shape input_tile_for_next
 torch.Size([1, 3, 182, 200])
myctx [[ 140045013206352[ PI( <7,2,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 2]
shape input_tile_for_next
 torch.Size([1, 3, 180, 198])
myctx [[ 140045013206304[ PI( <7,2,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 2]
shape input_tile_for_next
 torch.Size([1, 3, 178, 196])
myctx [[ 140045013207072[ PI( <7,2,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 2]
shape input_tile_for_next
 torch.Size([1, 3, 90, 96])
myctx [[ 140045013206400[ PI( <7,2,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 2]
shape input_tile_for_next
 torch.Size([1, 3, 88, 94])
myctx [[ 140045013206448[ PI( <7,2,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 2]
shape input_tile_for_next
 torch.Size([1, 3, 86, 92])
myctx [[ 140045013206976[ PI( <7,2,>,

in customized Sequential 3
need to get existing
coord [77, 122, 277, 319]
out_temp torch.Size([1, 3, 43, 46])
coord [7, 3]
bwd_out_shape  (46, 43)
fwd_out_shape  (40, 40)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
in customized checkpoint forward 5
input size torch.Size([1, 3, 10240, 10240])
in customized Sequential 6
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 3]
shape input_tile_for_next
 torch.Size([1, 3, 1466, 1650])
myctx [[ 140046822727744[ PI( <7,3,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 3]
shape input_tile_for_next
 torch.Size([1, 3, 1464, 1648])
myctx [[ 140045015171856[ PI( <7,3,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 3]
shape input_tile_for_next
 torch.Size([1, 3, 732, 822])
myctx [[ 140045013800704[ PI( <7,3,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 3]
shape input_tile_for_next
 torch.Size([1, 3, 730, 820])
myctx [[ 140045013848944[ PI( <7,3,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 3]
shape input_tile_for_next
 torch.Size([1, 3, 366, 408])
myctx [[ 140045013206640[ PI( <7,3,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 3]
shape input_tile_for_next
 torch.Size([1, 3, 364, 406])
myctx [[ 140045013206784[ PI( <7,3,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 3]
shape input_tile_for_next
 torch.Size([1, 3, 362, 404])
myctx [[ 140045013206880[ PI( <7,3,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 3]
shape input_tile_for_next
 torch.Size([1, 3, 182, 200])
myctx [[ 140045013206352[ PI( <7,3,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 3]
shape input_tile_for_next
 torch.Size([1, 3, 180, 198])
myctx [[ 140045013206304[ PI( <7,3,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 3]
shape input_tile_for_next
 torch.Size([1, 3, 178, 196])
myctx [[ 140045013207072[ PI( <7,3,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 3]
shape input_tile_for_next
 torch.Size([1, 3, 90, 96])
myctx [[ 140045013206400[ PI( <7,3,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 3]
shape input_tile_for_next
 torch.Size([1, 3, 88, 94])
myctx [[ 140045013206448[ PI( <7,3,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 3]
shape input_tile_for_next
 torch.Size([1, 3, 86, 92])
myctx [[ 140045013206976[ PI( <7,3,>,

in customized Sequential 3
need to get existing
coord [117, 162, 277, 319]
out_temp torch.Size([1, 3, 43, 46])
coord [7, 4]
bwd_out_shape  (46, 43)
fwd_out_shape  (40, 40)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
in customized checkpoint forward 5
input size torch.Size([1, 3, 10240, 10240])
in customized Sequential 6
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 4]
shape input_tile_for_next
 torch.Size([1, 3, 1466, 1650])
myctx [[ 140046822727744[ PI( <7,4,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 4]
shape input_tile_for_next
 torch.Size([1, 3, 1464, 1648])
myctx [[ 140045015171856[ PI( <7,4,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 4]
shape input_tile_for_next
 torch.Size([1, 3, 732, 822])
myctx [[ 140045013800704[ PI( <7,4,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 4]
shape input_tile_for_next
 torch.Size([1, 3, 730, 820])
myctx [[ 140045013848944[ PI( <7,4,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 4]
shape input_tile_for_next
 torch.Size([1, 3, 366, 408])
myctx [[ 140045013206640[ PI( <7,4,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 4]
shape input_tile_for_next
 torch.Size([1, 3, 364, 406])
myctx [[ 140045013206784[ PI( <7,4,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 4]
shape input_tile_for_next
 torch.Size([1, 3, 362, 404])
myctx [[ 140045013206880[ PI( <7,4,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 4]
shape input_tile_for_next
 torch.Size([1, 3, 182, 200])
myctx [[ 140045013206352[ PI( <7,4,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 4]
shape input_tile_for_next
 torch.Size([1, 3, 180, 198])
myctx [[ 140045013206304[ PI( <7,4,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 4]
shape input_tile_for_next
 torch.Size([1, 3, 178, 196])
myctx [[ 140045013207072[ PI( <7,4,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 4]
shape input_tile_for_next
 torch.Size([1, 3, 90, 96])
myctx [[ 140045013206400[ PI( <7,4,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 4]
shape input_tile_for_next
 torch.Size([1, 3, 88, 94])
myctx [[ 140045013206448[ PI( <7,4,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 4]
shape input_tile_for_next
 torch.Size([1, 3, 86, 92])
myctx [[ 140045013206976[ PI( <7,4,>,

in customized Sequential 3
need to get existing
coord [157, 202, 277, 319]
out_temp torch.Size([1, 3, 43, 46])
coord [7, 5]
bwd_out_shape  (46, 43)
fwd_out_shape  (40, 40)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
in customized checkpoint forward 5
input size torch.Size([1, 3, 10240, 10240])
in customized Sequential 6
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 5]
shape input_tile_for_next
 torch.Size([1, 3, 1466, 1650])
myctx [[ 140046822727744[ PI( <7,5,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 5]
shape input_tile_for_next
 torch.Size([1, 3, 1464, 1648])
myctx [[ 140045015171856[ PI( <7,5,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 5]
shape input_tile_for_next
 torch.Size([1, 3, 732, 822])
myctx [[ 140045013800704[ PI( <7,5,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 5]
shape input_tile_for_next
 torch.Size([1, 3, 730, 820])
myctx [[ 140045013848944[ PI( <7,5,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 5]
shape input_tile_for_next
 torch.Size([1, 3, 366, 408])
myctx [[ 140045013206640[ PI( <7,5,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 5]
shape input_tile_for_next
 torch.Size([1, 3, 364, 406])
myctx [[ 140045013206784[ PI( <7,5,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 5]
shape input_tile_for_next
 torch.Size([1, 3, 362, 404])
myctx [[ 140045013206880[ PI( <7,5,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 5]
shape input_tile_for_next
 torch.Size([1, 3, 182, 200])
myctx [[ 140045013206352[ PI( <7,5,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 5]
shape input_tile_for_next
 torch.Size([1, 3, 180, 198])
myctx [[ 140045013206304[ PI( <7,5,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 5]
shape input_tile_for_next
 torch.Size([1, 3, 178, 196])
myctx [[ 140045013207072[ PI( <7,5,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 5]
shape input_tile_for_next
 torch.Size([1, 3, 90, 96])
myctx [[ 140045013206400[ PI( <7,5,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 5]
shape input_tile_for_next
 torch.Size([1, 3, 88, 94])
myctx [[ 140045013206448[ PI( <7,5,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 5]
shape input_tile_for_next
 torch.Size([1, 3, 86, 92])
myctx [[ 140045013206976[ PI( <7,5,>,

in customized Sequential 3
need to get existing
coord [197, 242, 277, 319]
out_temp torch.Size([1, 3, 43, 46])
coord [7, 6]
bwd_out_shape  (46, 43)
fwd_out_shape  (40, 40)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
in customized checkpoint forward 5
input size torch.Size([1, 3, 10240, 10240])
in customized Sequential 6
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 6]
shape input_tile_for_next
 torch.Size([1, 3, 1466, 1650])
myctx [[ 140046822727744[ PI( <7,6,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 6]
shape input_tile_for_next
 torch.Size([1, 3, 1464, 1648])
myctx [[ 140045015171856[ PI( <7,6,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 6]
shape input_tile_for_next
 torch.Size([1, 3, 732, 822])
myctx [[ 140045013800704[ PI( <7,6,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 6]
shape input_tile_for_next
 torch.Size([1, 3, 730, 820])
myctx [[ 140045013848944[ PI( <7,6,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 6]
shape input_tile_for_next
 torch.Size([1, 3, 366, 408])
myctx [[ 140045013206640[ PI( <7,6,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 6]
shape input_tile_for_next
 torch.Size([1, 3, 364, 406])
myctx [[ 140045013206784[ PI( <7,6,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 6]
shape input_tile_for_next
 torch.Size([1, 3, 362, 404])
myctx [[ 140045013206880[ PI( <7,6,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 6]
shape input_tile_for_next
 torch.Size([1, 3, 182, 200])
myctx [[ 140045013206352[ PI( <7,6,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 6]
shape input_tile_for_next
 torch.Size([1, 3, 180, 198])
myctx [[ 140045013206304[ PI( <7,6,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 6]
shape input_tile_for_next
 torch.Size([1, 3, 178, 196])
myctx [[ 140045013207072[ PI( <7,6,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 6]
shape input_tile_for_next
 torch.Size([1, 3, 90, 96])
myctx [[ 140045013206400[ PI( <7,6,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 6]
shape input_tile_for_next
 torch.Size([1, 3, 88, 94])
myctx [[ 140045013206448[ PI( <7,6,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 6]
shape input_tile_for_next
 torch.Size([1, 3, 86, 92])
myctx [[ 140045013206976[ PI( <7,6,>,

in customized Sequential 3
need to get existing
coord [237, 282, 277, 319]
out_temp torch.Size([1, 3, 43, 46])
coord [7, 7]
bwd_out_shape  (43, 43)
fwd_out_shape  (40, 40)
Yes, fwd is smaller
++++++++++++++++++++++++++++++++++++++++++++++++
in customized checkpoint forward 5
input size torch.Size([1, 3, 10240, 10240])
in customized Sequential 6
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 7]
shape input_tile_for_next
 torch.Size([1, 3, 1466, 1466])
myctx [[ 140046822727744[ PI( <7,7,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 7]
shape input_tile_for_next
 torch.Size([1, 3, 1464, 1464])
myctx [[ 140045015171856[ PI( <7,7,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 7]
shape input_tile_for_next
 torch.Size([1, 3, 732, 732])
myctx [[ 140045013800704[ PI( <7,7,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 7]
shape input_tile_for_next
 torch.Size([1, 3, 730, 730])
myctx [[ 140045013848944[ PI( <7,7,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 7]
shape input_tile_for_next
 torch.Size([1, 3, 366, 366])
myctx [[ 140045013206640[ PI( <7,7,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 7]
shape input_tile_for_next
 torch.Size([1, 3, 364, 364])
myctx [[ 140045013206784[ PI( <7,7,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() [torch/csrc/autograd/python_engine.cpp] THPEngine_run_backward
[torch/csrc/autograd/engine.cpp] call_function SumBackward0
[torch/csrc/autograd/engine.cpp] call_function MmBackward
[torch/csrc/autograd/engine.cpp] call_function TBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function MmBackward
[torch/csrc/autograd/engine.cpp] call_function TBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function ViewBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function cCheckpointBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/enginedict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 7]
shape input_tile_for_next
 torch.Size([1, 3, 362, 362])
myctx [[ 140045013206880[ PI( <7,7,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 7]
shape input_tile_for_next
 torch.Size([1, 3, 182, 182])
myctx [[ 140045013206352[ PI( <7,7,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 7]
shape input_tile_for_next
 torch.Size([1, 3, 180, 180])
myctx [[ 140045013206304[ PI( <7,7,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 7]
shape input_tile_for_next
 torch.Size([1, 3, 178, 178])
myctx [[ 140045013207072[ PI( <7,7,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 7]
shape input_tile_for_next
 torch.Size([1, 3, 90, 90])
myctx [[ 140045013206400[ PI( <7,7,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 7]
shape input_tile_for_next
 torch.Size([1, 3, 88, 88])
myctx [[ 140045013206448[ PI( <7,7,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 7]
shape input_tile_for_next
 torch.Size([1, 3, 86, 86])
myctx [[ 140045013206976[ PI( <7,7,>,

in customized Sequential 3
need to get existing
coord [277, 319, 277, 319]
out_temp torch.Size([1, 3, 43, 43])
==== our_fwd done ...
221.7MiB
avail our 7.0GiB
max our 2.9GiB 3076650496

&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&


&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&


############# Enter checkpointing bkward ####
inputs len 5 4
in customized Sequential 5
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 7]
shape input_tile_for_next
 torch.Size([1, 3, 1466, 1466])
myctx [[ 140046822727744[ PI( <7,7,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 7]
shape input_tile_for_next
 torch.Size([1, 3, 1464, 1464])
myctx [[ 140045015171856[ PI( <7,7,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 7]
shape input_tile_for_next
 torch.Size([1, 3, 732, 732])
myctx [[ 140045013800704[ PI( <7,7,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 7]
shape input_tile_for_next
 torch.Size([1, 3, 730, 730])
myctx [[ 140045013848944[ PI( <7,7,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 7]
shape input_tile_for_next
 torch.Size([1, 3, 366, 366])
myctx [[ 140045013206640[ PI( <7,7,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 7]
shape input_tile_for_next
 torch.Size([1, 3, 364, 364])
myctx [[ 140045013206784[ PI( <7,7,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 7]
shape input_tile_for_next
 torch.Size([1, 3, 362, 362])
myctx [[ 140045013206880[ PI( <7,7,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 7]
shape input_tile_for_next
 torch.Size([1, 3, 182, 182])
myctx [[ 140045013206352[ PI( <7,7,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 7]
shape input_tile_for_next
 torch.Size([1, 3, 180, 180])
myctx [[ 140045013206304[ PI( <7,7,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() .cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledCopyFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::CopyBackwards
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/python_engine.cpp] THPEngine_run_backward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 7]
shape input_tile_for_next
 torch.Size([1, 3, 178, 178])
myctx [[ 140045013207072[ PI( <7,7,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 7]
shape input_tile_for_next
 torch.Size([1, 3, 90, 90])
myctx [[ 140045013206400[ PI( <7,7,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 7]
shape input_tile_for_next
 torch.Size([1, 3, 88, 88])
myctx [[ 140045013206448[ PI( <7,7,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 7]
shape input_tile_for_next
 torch.Size([1, 3, 86, 86])
myctx [[ 140045013206976[ PI( <7,7,>,

in customized Sequential 3
need to get existing

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 86, 86])
grad_out size torch.Size([1, 3, 320, 320])
arg size torch.Size([1, 3, 43, 43])
new_grad_out torch.Size([1, 3, 43, 43])
##############grad_in in maxp torch.Size([1, 3, 86, 86])
I am [7, 7]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 88, 88])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 86, 86])
grad_input torch.Size([1, 3, 88, 88])
padding info :: [0, 1, 0, 1]
grad_input old torch.Size([1, 3, 88, 88])
grad_input new torch.Size([1, 3, 88, 88])
new grad_input torch.Size([1, 3, 88, 88])
grad_out size torch.Size([1, 3, 86, 86])
crop [6, 0, 6, 0] [554, 639, 554, 639] [560, 639, 560, 639]
## 6 86 6 86
##############return grad_in in conv2d tensor([ 0.0000,  0.0000,  0.0004,  0.0042, -0.0008, -0.0005,  0.0028, -0.0022,
        -0.0019, -0.0056], device='cuda:0')
I am [7, 7]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 90, 90])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 88, 88])
padding info :: [0, 2, 0, 2]
grad_input old torch.Size([1, 3, 90, 90])
grad_input new torch.Size([1, 3, 90, 90])
grad_input torch.Size([1, 3, 90, 90])
grad_out size torch.Size([1, 3, 87, 87])
crop [7, 0, 7, 0] [553, 639, 553, 639] [560, 639, 560, 639]
## 7 87 7 87
##############return grad_in in conv2d tensor([ 0.0000,  0.0000,  0.0009, -0.0011,  0.0008, -0.0006,  0.0005, -0.0003,
         0.0005, -0.0004], device='cuda:0')
I am [7, 7]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 92, 92])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 90, 90])
padding info :: [0, 3, 0, 3]
grad_input torch.Size([1, 3, 89, 89])
grad_out size torch.Size([1, 3, 88, 88])
crop [8, 0, 8, 0] [552, 639, 552, 639] [560, 639, 560, 639]
## 8 88 8 88
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  1.6293e-04, -3.1132e-04, -1.0676e-04,
         2.8200e-04, -8.1636e-05,  1.2424e-04, -1.0779e-04,  2.5170e-04],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 178, 178])
grad_out size torch.Size([1, 3, 89, 89])
arg size torch.Size([1, 3, 89, 89])
##############grad_in in maxp torch.Size([1, 3, 178, 178])
I am [7, 7]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 180, 180])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 178, 178])
grad_input torch.Size([1, 3, 180, 180])
padding info :: [0, 1, 0, 1]
grad_input old torch.Size([1, 3, 180, 180])
grad_input new torch.Size([1, 3, 180, 180])
new grad_input torch.Size([1, 3, 180, 180])
grad_out size torch.Size([1, 3, 178, 178])
crop [18, 0, 18, 0] [1102, 1279, 1102, 1279] [1120, 1279, 1120, 1279]
## 18 178 18 178
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.7749e-05,
        -8.4156e-06, -9.5186e-06, -6.6110e-05,  3.0065e-05,  3.2976e-05],
       device='cuda:0')
I am [7, 7]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 182, 182])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 180, 180])
padding info :: [0, 2, 0, 2]
grad_input old torch.Size([1, 3, 182, 182])
grad_input new torch.Size([1, 3, 182, 182])
grad_input torch.Size([1, 3, 182, 182])
grad_out size torch.Size([1, 3, 179, 179])
crop [19, 0, 19, 0] [1101, 1279, 1101, 1279] [1120, 1279, 1120, 1279]
## 19 179 19 179
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  5.0572e-06,
        -1.1759e-07,  6.0020e-06, -2.5851e-05,  2.0544e-05, -1.1145e-05],
       device='cuda:0')
I am [7, 7]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 184, 184])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 182, 182])
padding info :: [0, 3, 0, 3]
grad_input torch.Size([1, 3, 181, 181])
grad_out size torch.Size([1, 3, 180, 180])
crop [20, 0, 20, 0] [1100, 1279, 1100, 1279] [1120, 1279, 1120, 1279]
## 20 180 20 180
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -3.9544e-07,
        -1.4432e-06, -3.1138e-07,  2.7255e-06,  2.6919e-06,  5.4537e-07],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 362, 362])
grad_out size torch.Size([1, 3, 181, 181])
arg size torch.Size([1, 3, 181, 181])
##############grad_in in maxp torch.Size([1, 3, 362, 362])
I am [7, 7]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 364, 364])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 362, 362])
grad_input torch.Size([1, 3, 364, 364])
padding info :: [0, 1, 0, 1]
grad_input old torch.Size([1, 3, 364, 364])
grad_input new torch.Size([1, 3, 364, 364])
new grad_input torch.Size([1, 3, 364, 364])
grad_out size torch.Size([1, 3, 362, 362])
crop [42, 0, 42, 0] [2198, 2559, 2198, 2559] [2240, 2559, 2240, 2559]
## 42 362 42 362
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00, -5.6080e-08, -5.4829e-08],
       device='cuda:0')
I am [7, 7]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 366, 366])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 364, 364])
padding info :: [0, 2, 0, 2]
grad_input old torch.Size([1, 3, 366, 366])
grad_input new torch.Size([1, 3, 366, 366])
grad_input torch.Size([1, 3, 366, 366])
grad_out size torch.Size([1, 3, 363, 363])
crop [43, 0, 43, 0] [2197, 2559, 2197, 2559] [2240, 2559, 2240, 2559]
## 43 363 43 363
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0003e-08, -3.0236e-08],
       device='cuda:0')
I am [7, 7]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 368, 368])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 366, 366])
padding info :: [0, 3, 0, 3]
grad_input torch.Size([1, 3, 365, 365])
grad_out size torch.Size([1, 3, 364, 364])
crop [44, 0, 44, 0] [2196, 2559, 2196, 2559] [2240, 2559, 2240, 2559]
## 44 364 44 364
##############return grad_in in conv2d [torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cCheckpointBackward
tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00, -1.1626e-09,  2.6979e-09],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 730, 730])
grad_out size torch.Size([1, 3, 365, 365])
arg size torch.Size([1, 3, 365, 365])
##############grad_in in maxp torch.Size([1, 3, 730, 730])
I am [7, 7]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 732, 732])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 730, 730])
grad_input torch.Size([1, 3, 732, 732])
padding info :: [0, 1, 0, 1]
grad_input old torch.Size([1, 3, 732, 732])
grad_input new torch.Size([1, 3, 732, 732])
new grad_input torch.Size([1, 3, 732, 732])
grad_out size torch.Size([1, 3, 730, 730])
crop [90, 0, 90, 0] [4390, 5119, 4390, 5119] [4480, 5119, 4480, 5119]
## 90 730 90 730
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [7, 7]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 734, 734])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 732, 732])
padding info :: [0, 2, 0, 2]
grad_input torch.Size([1, 3, 732, 732])
grad_out size torch.Size([1, 3, 731, 731])
crop [91, 0, 91, 0] [4389, 5119, 4389, 5119] [4480, 5119, 4480, 5119]
## 91 731 91 731
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 1464, 1464])
grad_out size torch.Size([1, 3, 732, 732])
arg size torch.Size([1, 3, 732, 732])
##############grad_in in maxp torch.Size([1, 3, 1464, 1464])
I am [7, 7]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 1466, 1466])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1464, 1464])
grad_input torch.Size([1, 3, 1466, 1466])
padding info :: [0, 1, 0, 1]
grad_input old torch.Size([1, 3, 1466, 1466])
grad_input new torch.Size([1, 3, 1466, 1466])
new grad_input torch.Size([1, 3, 1466, 1466])
grad_out size torch.Size([1, 3, 1464, 1464])
crop [184, 0, 184, 0] [8776, 10239, 8776, 10239] [8960, 10239, 8960, 10239]
## 184 1464 184 1464
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [7, 7]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 3, 1468, 1468])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1466, 1466])
final torch.Size([1, 3, 1468, 1468])
padding info :: [0, 2, 0, 2]
reshape_for_final ## 186 1466 186 1466
after crop g_in torch.Size([1, 3, 1280, 1280])
grad_out size torch.Size([1, 3, 1465, 1465])
crop [185, 0, 185, 0] [8775, 10239, 8775, 10239] [8960, 10239, 8960, 10239]
## 185 1465 185 1465
##############return grad_in in conv2d tensor([-8.6249e-08,  2.6738e-09,  1.2093e-07,  2.5345e-07, -1.6326e-07,
         3.2307e-08,  3.9761e-07,  8.2954e-09,  6.6876e-09, -1.6208e-07],
       device='cuda:0')
[8, 8]
HREREERE torch.Size([1, 3, 10240, 10240])

############# Enter checkpointing bkward ####
inputs len 5 4
in customized Sequential 5
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 6]
shape input_tile_for_next
 torch.Size([1, 3, 1466, 1650])
myctx [[ 140046822727744[ PI( <7,6,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 6]
shape input_tile_for_next
 torch.Size([1, 3, 1464, 1648])
myctx [[ 140045015171856[ PI( <7,6,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 6]
shape input_tile_for_next
 torch.Size([1, 3, 732, 822])
myctx [[ 140045013800704[ PI( <7,6,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 6]
shape input_tile_for_next
 torch.Size([1, 3, 730, 820])
myctx [[ 140045013848944[ PI( <7,6,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 6]
shape input_tile_for_next
 torch.Size([1, 3, 366, 408])
myctx [[ 140045013206640[ PI( <7,6,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 6]
shape input_tile_for_next
 torch.Size([1, 3, 364, 406])
myctx [[ 140045013206784[ PI( <7,6,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 6]
shape input_tile_for_next
 torch.Size([1, 3, 362, 404])
myctx [[ 140045013206880[ PI( <7,6,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 6]
shape input_tile_for_next
 torch.Size([1, 3, 182, 200])
myctx [[ 140045013206352[ PI( <7,6,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 6]
shape input_tile_for_next
 torch.Size([1, 3, 180, 198])
myctx [[ 140045013206304[ PI( <7,6,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 6]
shape input_tile_for_next
 torch.Size([1, 3, 178, 196])
myctx [[ 140045013207072[ PI( <7,6,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() [torch/csrc/autograd/python_engine.cpp] THPEngine_run_backward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 6]
shape input_tile_for_next
 torch.Size([1, 3, 90, 96])
myctx [[ 140045013206400[ PI( <7,6,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 6]
shape input_tile_for_next
 torch.Size([1, 3, 88, 94])
myctx [[ 140045013206448[ PI( <7,6,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 6]
shape input_tile_for_next
 torch.Size([1, 3, 86, 92])
myctx [[ 140045013206976[ PI( <7,6,>,

in customized Sequential 3
need to get existing

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 86, 92])
grad_out size torch.Size([1, 3, 320, 320])
arg size torch.Size([1, 3, 43, 46])
new_grad_out torch.Size([1, 3, 43, 46])
##############grad_in in maxp torch.Size([1, 3, 86, 92])
I am [7, 6]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 88, 94])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 86, 92])
grad_input torch.Size([1, 3, 88, 94])
padding info :: [0, 0, 0, 1]
grad_input old torch.Size([1, 3, 88, 94])
grad_input new torch.Size([1, 3, 88, 94])
new grad_input torch.Size([1, 3, 88, 94])
grad_out size torch.Size([1, 3, 86, 92])
crop [6, 6, 6, 0] [474, 565, 554, 639] [480, 559, 560, 639]
## 6 86 6 86
##############return grad_in in conv2d tensor([ 0.0005,  0.0028, -0.0011, -0.0004, -0.0014, -0.0065,  0.0024, -0.0004,
         0.0033,  0.0029], device='cuda:0')
I am [7, 6]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 90, 96])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 88, 94])
padding info :: [0, 0, 0, 2]
grad_input old torch.Size([1, 3, 90, 96])
grad_input new torch.Size([1, 3, 90, 96])
grad_input torch.Size([1, 3, 90, 96])
grad_out size torch.Size([1, 3, 87, 94])
crop [7, 7, 7, 0] [473, 566, 553, 639] [480, 559, 560, 639]
## 7 87 7 87
##############return grad_in in conv2d tensor([ 0.0003, -0.0006,  0.0002, -0.0008, -0.0006,  0.0011, -0.0004,  0.0015,
         0.0001, -0.0012], device='cuda:0')
I am [7, 6]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 92, 98])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 90, 96])
padding info :: [0, 0, 0, 3]
grad_input torch.Size([1, 3, 89, 98])
grad_out size torch.Size([1, 3, 88, 96])
crop [8, 8, 8, 0] [472, 567, 552, 639] [480, 559, 560, 639]
## 8 88 8 88
##############return grad_in in conv2d tensor([ 4.8978e-05, -1.4437e-04,  1.8090e-05,  1.3222e-05, -1.5018e-04,
         4.8006e-04, -6.2908e-05, -3.1484e-05,  9.1381e-05, -4.5970e-04],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 178, 196])
grad_out size torch.Size([1, 3, 89, 98])
arg size torch.Size([1, 3, 89, 98])
##############grad_in in maxp torch.Size([1, 3, 178, 196])
I am [7, 6]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 180, 198])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 178, 196])
grad_input torch.Size([1, 3, 180, 198])
padding info :: [0, 0, 0, 1]
grad_input old torch.Size([1, 3, 180, 198])
grad_input new torch.Size([1, 3, 180, 198])
new grad_input torch.Size([1, 3, 180, 198])
grad_out size torch.Size([1, 3, 178, 196])
crop [18, 18, 18, 0] [942, 1137, 1102, 1279] [960, 1119, 1120, 1279]
## 18 178 18 178
##############return grad_in in conv2d tensor([ 4.4180e-06, -1.5548e-06, -3.3114e-06, -1.5301e-05,  1.5383e-05,
         4.4690e-06,  0.0000e+00, -2.5725e-05,  3.6673e-05,  2.0477e-06],
       device='cuda:0')
I am [7, 6]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 182, 200])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 180, 198])
padding info :: [0, 0, 0, 2]
grad_input old torch.Size([1, 3, 182, 200])
grad_input new torch.Size([1, 3, 182, 200])
grad_input torch.Size([1, 3, 182, 200])
grad_out size torch.Size([1, 3, 179, 198])
crop [19, 19, 19, 0] [941, 1138, 1101, 1279] [960, 1119, 1120, 1279]
## 19 179 19 179
##############return grad_in in conv2d tensor([ 1.2432e-06,  2.1983e-07,  1.3006e-06, -6.1502e-06,  2.6000e-06,
        -7.7191e-06,  8.7900e-06, -9.8494e-06,  1.5060e-05, -1.1788e-05],
       device='cuda:0')
I am [7, 6]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 184, 202])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 182, 200])
padding info :: [0, 0, 0, 3]
grad_input torch.Size([1, 3, 181, 202])
grad_out size torch.Size([1, 3, 180, 200])
crop [20, 20, 20, 0] [940, 1139, 1100, 1279] [960, 1119, 1120, 1279]
## 20 180 20 180
##############return grad_in in conv2d tensor([-9.6931e-08, -3.8488e-07, -1.2739e-07,  8.7951e-07,  1.2092e-06,
        -1.1547e-07, -1.7749e-06,  6.7901e-07,  9.8707e-07, -1.0797e-06],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 362, 404])
grad_out size torch.Size([1, 3, 181, 202])
arg size torch.Size([1, 3, 181, 202])
##############grad_in in maxp torch.Size([1, 3, 362, 404])
I am [7, 6]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 364, 406])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 362, 404])
grad_input torch.Size([1, 3, 364, 406])
padding info :: [0, 0, 0, 1]
grad_input old torch.Size([1, 3, 364, 406])
grad_input new torch.Size([1, 3, 364, 406])
new grad_input torch.Size([1, 3, 364, 406])
grad_out size torch.Size([1, 3, 362, 404])
crop [42, 42, 42, 0] [1878, 2281, 2198, 2559] [1920, 2239, 2240, 2559]
## 42 362 42 362
##############return grad_in in conv2d tensor([-3.8383e-08,  1.2233e-08,  6.4638e-10, -8.1755e-08,  4.7601e-08,
         1.6727e-08,  2.7022e-07, -1.3019e-07,  3.3673e-08,  4.6188e-08],
       device='cuda:0')
I am [7, 6]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 366, 408])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 364, 406])
padding info :: [0, 0, 0, 2]
grad_input old torch.Size([1, 3, 366, 408])
grad_input new torch.Size([1, 3, 366, 408])
grad_input torch.Size([1, 3, 366, 408])
grad_out size torch.Size([1, 3, 363, 406])
crop [43, 43, 43, 0] [1877, 2282, 2197, 2559] [1920, 2239, 2240, 2559]
## 43 363 43 363
##############return grad_in in conv2d tensor([ 8.0635e-10,  3.1005e-09, -2.1631e-08, -1.4618e-08,  1.1244e-08,
        -2.6492e-08,  1.1360e-08, -1.8002e-08,  6.8076e-08,  1.3617e-07],
       device='cuda:0')
I am [7, 6]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 368, 410])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 366, 408])
padding info :: [0, 0, 0, 3]
grad_input torch.Size([1, 3, 365, 410])
grad_out size torch.Size([1, 3, 364, 408])
crop [44, 44, 44, 0] [1876, 2283, 2196, 2559] [1920, 2239, 2240, 2559]
## 44 364 44 364
##############return grad_in in conv2d tensor([-1.6250e-10, -1.8796e-09,  3.0854e-10,  4.5494e-09,  3.3625e-09,
         7.8963e-09,  4.8107e-09,  7.3154e-09,  6.6364e-10, -2.9198e-08],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 730, 820])
grad_out size torch.Size([1, 3, 365, 410])
arg size torch.Size([1, 3, 365, 410])
##############grad_in in maxp torch.Size([1, 3, 730, 820])
I am [7, 6]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 732, 822])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 730, 820])
grad_input torch.Size([1, 3, 732, 822])
padding info :: [torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cCheckpointBackward
[0, 0, 0, 1]
grad_input old torch.Size([1, 3, 732, 822])
grad_input new torch.Size([1, 3, 732, 822])
new grad_input torch.Size([1, 3, 732, 822])
grad_out size torch.Size([1, 3, 730, 820])
crop [90, 90, 90, 0] [3750, 4569, 4390, 5119] [3840, 4479, 4480, 5119]
## 90 730 90 730
##############return grad_in in conv2d tensor([-3.0434e-11, -2.5516e-11,  2.3266e-11, -9.5486e-11, -5.8172e-11,
        -1.7463e-09, -3.6302e-10, -5.6595e-10, -9.3817e-11,  2.1565e-10],
       device='cuda:0')
I am [7, 6]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 734, 824])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 732, 822])
padding info :: [0, 0, 0, 2]
grad_input torch.Size([1, 3, 732, 824])
grad_out size torch.Size([1, 3, 731, 822])
crop [91, 91, 91, 0] [3749, 4570, 4389, 5119] [3840, 4479, 4480, 5119]
## 91 731 91 731
##############return grad_in in conv2d tensor([ 1.8449e-13, -1.0521e-12, -7.6983e-12, -1.4628e-11, -3.6307e-11,
        -2.8025e-10, -4.9884e-10, -2.2383e-10,  2.3128e-10,  1.1867e-10],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 1464, 1648])
grad_out size torch.Size([1, 3, 732, 824])
arg size torch.Size([1, 3, 732, 824])
##############grad_in in maxp torch.Size([1, 3, 1464, 1648])
I am [7, 6]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 1466, 1650])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1464, 1648])
grad_input torch.Size([1, 3, 1466, 1650])
padding info :: [0, 0, 0, 1]
grad_input old torch.Size([1, 3, 1466, 1650])
grad_input new torch.Size([1, 3, 1466, 1650])
new grad_input torch.Size([1, 3, 1466, 1650])
grad_out size torch.Size([1, 3, 1464, 1648])
crop [184, 184, 184, 0] [7496, 9143, 8776, 10239] [7680, 8959, 8960, 10239]
## 184 1464 184 1464
##############return grad_in in conv2d tensor([ 1.4537e-13,  8.3011e-13,  6.5178e-14,  1.8729e-13,  9.5814e-13,
        -2.3077e-12,  3.9489e-12, -1.0148e-13, -1.1205e-12,  4.9513e-12],
       device='cuda:0')
I am [7, 6]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 3, 1468, 1652])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1466, 1650])
final torch.Size([1, 3, 1468, 1652])
padding info :: [0, 0, 0, 2]
reshape_for_final ## 186 1466 186 1466
after crop g_in torch.Size([1, 3, 1280, 1280])
grad_out size torch.Size([1, 3, 1465, 1650])
crop [185, 185, 185, 0] [7495, 9144, 8775, 10239] [7680, 8959, 8960, 10239]
## 185 1465 185 1465
##############return grad_in in conv2d tensor([ 2.1620e-07,  1.1480e-07, -2.1480e-07, -7.7271e-07, -3.9098e-07,
         6.5391e-07, -2.5910e-07,  1.0415e-07,  3.5535e-07,  1.1485e-07],
       device='cuda:0')
[8, 8]
HREREERE torch.Size([1, 3, 10240, 10240])

############# Enter checkpointing bkward ####
inputs len 5 4
in customized Sequential 5
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 5]
shape input_tile_for_next
 torch.Size([1, 3, 1466, 1650])
myctx [[ 140046822727744[ PI( <7,5,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 5]
shape input_tile_for_next
 torch.Size([1, 3, 1464, 1648])
myctx [[ 140045015171856[ PI( <7,5,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 5]
shape input_tile_for_next
 torch.Size([1, 3, 732, 822])
myctx [[ 140045013800704[ PI( <7,5,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 5]
shape input_tile_for_next
 torch.Size([1, 3, 730, 820])
myctx [[ 140045013848944[ PI( <7,5,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 5]
shape input_tile_for_next
 torch.Size([1, 3, 366, 408])
myctx [[ 140045013206640[ PI( <7,5,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 5]
shape input_tile_for_next
 torch.Size([1, 3, 364, 406])
myctx [[ 140045013206784[ PI( <7,5,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 5]
shape input_tile_for_next
 torch.Size([1, 3, 362, 404])
myctx [[ 140045013206880[ PI( <7,5,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 5]
shape input_tile_for_next
 torch.Size([1, 3, 182, 200])
myctx [[ 140045013206352[ PI( <7,5,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 5]
shape input_tile_for_next
 torch.Size([1, 3, 180, 198])
myctx [[ 140045013206304[ PI( <7,5,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 5]
shape input_tile_for_next
 torch.Size([1, 3, 178, 196])
myctx [[ 140045013207072[ PI( <7,5,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 5]
shape input_tile_for_next
 torch.Size([1, 3, 90, 96])
myctx [[ 140045013206400[ PI( <7,5,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
[torch/csrc/autograd/python_engine.cpp] THPEngine_run_backward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 5]
shape input_tile_for_next
 torch.Size([1, 3, 88, 94])
myctx [[ 140045013206448[ PI( <7,5,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 5]
shape input_tile_for_next
 torch.Size([1, 3, 86, 92])
myctx [[ 140045013206976[ PI( <7,5,>,

in customized Sequential 3
need to get existing

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 86, 92])
grad_out size torch.Size([1, 3, 320, 320])
arg size torch.Size([1, 3, 43, 46])
new_grad_out torch.Size([1, 3, 43, 46])
##############grad_in in maxp torch.Size([1, 3, 86, 92])
I am [7, 5]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 88, 94])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 86, 92])
grad_input torch.Size([1, 3, 88, 94])
padding info :: [0, 0, 0, 1]
grad_input old torch.Size([1, 3, 88, 94])
grad_input new torch.Size([1, 3, 88, 94])
new grad_input torch.Size([1, 3, 88, 94])
grad_out size torch.Size([1, 3, 86, 92])
crop [6, 6, 6, 0] [394, 485, 554, 639] [400, 479, 560, 639]
## 6 86 6 86
##############return grad_in in conv2d tensor([ 3.3797e-05, -1.5706e-03, -8.1065e-04,  4.6695e-03,  3.0528e-03,
        -3.9317e-03,  5.6777e-03, -2.6887e-03,  3.7725e-04, -4.6397e-04],
       device='cuda:0')
I am [7, 5]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 90, 96])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 88, 94])
padding info :: [0, 0, 0, 2]
grad_input old torch.Size([1, 3, 90, 96])
grad_input new torch.Size([1, 3, 90, 96])
grad_input torch.Size([1, 3, 90, 96])
grad_out size torch.Size([1, 3, 87, 94])
crop [7, 7, 7, 0] [393, 486, 553, 639] [400, 479, 560, 639]
## 7 87 7 87
##############return grad_in in conv2d tensor([ 1.9286e-05, -3.3436e-04, -7.6079e-04, -7.8175e-04, -1.8222e-03,
        -1.6910e-03, -1.2181e-03, -1.1518e-04, -1.8606e-03,  1.0373e-03],
       device='cuda:0')
I am [7, 5]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 92, 98])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 90, 96])
padding info :: [0, 0, 0, 3]
grad_input torch.Size([1, 3, 89, 98])
grad_out size torch.Size([1, 3, 88, 96])
crop [8, 8, 8, 0] [392, 487, 552, 639] [400, 479, 560, 639]
## 8 88 8 88
##############return grad_in in conv2d tensor([ 3.1159e-06,  1.3264e-06, -7.7827e-05, -6.6519e-05, -7.1969e-05,
        -1.3898e-04, -2.1920e-04,  5.3363e-04, -1.9874e-04,  6.3068e-05],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 178, 196])
grad_out size torch.Size([1, 3, 89, 98])
arg size torch.Size([1, 3, 89, 98])
##############grad_in in maxp torch.Size([1, 3, 178, 196])
I am [7, 5]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 180, 198])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 178, 196])
grad_input torch.Size([1, 3, 180, 198])
padding info :: [0, 0, 0, 1]
grad_input old torch.Size([1, 3, 180, 198])
grad_input new torch.Size([1, 3, 180, 198])
new grad_input torch.Size([1, 3, 180, 198])
grad_out size torch.Size([1, 3, 178, 196])
crop [18, 18, 18, 0] [782, 977, 1102, 1279] [800, 959, 1120, 1279]
## 18 178 18 178
##############return grad_in in conv2d tensor([ 2.8107e-07, -9.8918e-08, -2.4797e-06, -3.1278e-06, -3.5016e-06,
        -3.3592e-05,  1.1472e-06,  4.8089e-07, -8.7352e-06,  1.2005e-05],
       device='cuda:0')
I am [7, 5]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 182, 200])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 180, 198])
padding info :: [0, 0, 0, 2]
grad_input old torch.Size([1, 3, 182, 200])
grad_input new torch.Size([1, 3, 182, 200])
grad_input torch.Size([1, 3, 182, 200])
grad_out size torch.Size([1, 3, 179, 198])
crop [19, 19, 19, 0] [781, 978, 1101, 1279] [800, 959, 1120, 1279]
## 19 179 19 179
##############return grad_in in conv2d tensor([ 7.9090e-08,  1.3985e-08, -6.8112e-07, -2.4544e-06, -9.1797e-07,
        -1.2625e-05, -2.4457e-06, -7.5910e-06, -6.8500e-06,  4.7828e-06],
       device='cuda:0')
I am [7, 5]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 184, 202])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 182, 200])
padding info :: [0, 0, 0, 3]
grad_input torch.Size([1, 3, 181, 202])
grad_out size torch.Size([1, 3, 180, 200])
crop [20, 20, 20, 0] [780, 979, 1100, 1279] [800, 959, 1120, 1279]
## 20 180 20 180
##############return grad_in in conv2d tensor([-6.1667e-09, -2.4486e-08,  1.2045e-07,  3.6417e-07, -1.3073e-08,
         1.4939e-06,  2.8373e-06, -3.2510e-07,  5.0886e-07,  1.8886e-06],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 362, 404])
grad_out size torch.Size([1, 3, 181, 202])
arg size torch.Size([1, 3, 181, 202])
##############grad_in in maxp torch.Size([1, 3, 362, 404])
I am [7, 5]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 364, 406])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 362, 404])
grad_input torch.Size([1, 3, 364, 406])
padding info :: [0, 0, 0, 1]
grad_input old torch.Size([1, 3, 364, 406])
grad_input new torch.Size([1, 3, 364, 406])
new grad_input torch.Size([1, 3, 364, 406])
grad_out size torch.Size([1, 3, 362, 404])
crop [42, 42, 42, 0] [1558, 1961, 2198, 2559] [1600, 1919, 2240, 2559]
## 42 362 42 362
##############return grad_in in conv2d tensor([ 0.0000e+00, -3.3503e-10,  1.1946e-09, -2.9027e-09,  7.7585e-10,
         2.9605e-09,  4.2841e-08, -4.4711e-08, -5.5230e-08, -4.6316e-08],
       device='cuda:0')
I am [7, 5]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 366, 408])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 364, 406])
padding info :: [0, 0, 0, 2]
grad_input old torch.Size([1, 3, 366, 408])
grad_input new torch.Size([1, 3, 366, 408])
grad_input torch.Size([1, 3, 366, 408])
grad_out size torch.Size([1, 3, 363, 406])
crop [43, 43, 43, 0] [1557, 1962, 2197, 2559] [1600, 1919, 2240, 2559]
## 43 363 43 363
##############return grad_in in conv2d tensor([ 0.0000e+00, -1.3312e-10,  4.2315e-12,  9.8725e-10,  3.1787e-10,
        -1.1855e-09, -5.6955e-09, -1.7304e-08,  1.5501e-08,  1.2240e-08],
       device='cuda:0')
I am [7, 5]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 368, 410])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 366, 408])
padding info :: [0, 0, 0, 3]
grad_input torch.Size([1, 3, 365, 410])
grad_out size torch.Size([1, 3, 364, 408])
crop [44, 44, 44, 0] [1556, 1963, 2196, 2559] [1600, 1919, 2240, 2559]
## 44 364 44 364
##############return grad_in in conv2d tensor([ 0.0000e+00, -1.4287e-11,  5.0643e-11, -4.5000e-11, -2.1513e-10,
        -9.5389e-11, -2.3508e-10,  4.3369e-09,  2.0896e-09,  1.5513e-09],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 730, 820])
grad_out size torch.Size([1, 3, 365, 410])
arg size torch.Size([1, 3, 365, 410])
##############grad_in in maxp torch.Size([1, 3, 730, 820])
I am [7, 5]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 732, 822])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 730, 820])
grad_input torch.Size([1, 3, 732, 822])
padding info :: [0, 0, 0, 1]
grad_input old torch.Size([1, 3, 732, 822])
grad_input new torch.Size([1, 3, 732, 822])
new grad_input torch.Size([1, 3, 732, 822])
grad_out size torch.Size([1, 3, 730, 820])
crop [90, 90, 90, 0] [3110, 3929, 4390, 5119] [3200, 3839, 4480, 5119]
## 90 730 90 730
##############return grad_in in conv2d [torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cCheckpointBackward
tensor([ 0.0000e+00,  0.0000e+00, -5.8857e-12, -8.6901e-12,  8.8558e-13,
         8.0949e-12, -7.0592e-12,  8.0512e-13,  0.0000e+00, -4.0291e-11],
       device='cuda:0')
I am [7, 5]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 734, 824])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 732, 822])
padding info :: [0, 0, 0, 2]
grad_input torch.Size([1, 3, 732, 824])
grad_out size torch.Size([1, 3, 731, 822])
crop [91, 91, 91, 0] [3109, 3930, 4389, 5119] [3200, 3839, 4480, 5119]
## 91 731 91 731
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00, -9.2366e-13, -3.2680e-12, -2.7332e-12,
         7.1661e-13,  2.3974e-12, -1.0721e-13, -1.5039e-12,  3.0137e-13],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 1464, 1648])
grad_out size torch.Size([1, 3, 732, 824])
arg size torch.Size([1, 3, 732, 824])
##############grad_in in maxp torch.Size([1, 3, 1464, 1648])
I am [7, 5]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 1466, 1650])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1464, 1648])
grad_input torch.Size([1, 3, 1466, 1650])
padding info :: [0, 0, 0, 1]
grad_input old torch.Size([1, 3, 1466, 1650])
grad_input new torch.Size([1, 3, 1466, 1650])
new grad_input torch.Size([1, 3, 1466, 1650])
grad_out size torch.Size([1, 3, 1464, 1648])
crop [184, 184, 184, 0] [6216, 7863, 8776, 10239] [6400, 7679, 8960, 10239]
## 184 1464 184 1464
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         3.5429e-14,  2.2278e-13,  3.9807e-13, -3.7548e-14, -9.4408e-14],
       device='cuda:0')
I am [7, 5]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 3, 1468, 1652])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1466, 1650])
final torch.Size([1, 3, 1468, 1652])
padding info :: [0, 0, 0, 2]
reshape_for_final ## 186 1466 186 1466
after crop g_in torch.Size([1, 3, 1280, 1280])
grad_out size torch.Size([1, 3, 1465, 1650])
crop [185, 185, 185, 0] [6215, 7864, 8775, 10239] [6400, 7679, 8960, 10239]
## 185 1465 185 1465
##############return grad_in in conv2d tensor([-2.1672e-08,  4.9118e-07, -3.3859e-07, -4.7779e-07, -4.2008e-07,
        -3.1530e-07,  4.1533e-07, -2.1675e-08, -4.9438e-07,  2.2621e-07],
       device='cuda:0')
[8, 8]
HREREERE torch.Size([1, 3, 10240, 10240])

############# Enter checkpointing bkward ####
inputs len 5 4
in customized Sequential 5
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 4]
shape input_tile_for_next
 torch.Size([1, 3, 1466, 1650])
myctx [[ 140046822727744[ PI( <7,4,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 4]
shape input_tile_for_next
 torch.Size([1, 3, 1464, 1648])
myctx [[ 140045015171856[ PI( <7,4,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 4]
shape input_tile_for_next
 torch.Size([1, 3, 732, 822])
myctx [[ 140045013800704[ PI( <7,4,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 4]
shape input_tile_for_next
 torch.Size([1, 3, 730, 820])
myctx [[ 140045013848944[ PI( <7,4,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 4]
shape input_tile_for_next
 torch.Size([1, 3, 366, 408])
myctx [[ 140045013206640[ PI( <7,4,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 4]
shape input_tile_for_next
 torch.Size([1, 3, 364, 406])
myctx [[ 140045013206784[ PI( <7,4,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 4]
shape input_tile_for_next
 torch.Size([1, 3, 362, 404])
myctx [[ 140045013206880[ PI( <7,4,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 4]
shape input_tile_for_next
 torch.Size([1, 3, 182, 200])
myctx [[ 140045013206352[ PI( <7,4,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 4]
shape input_tile_for_next
 torch.Size([1, 3, 180, 198])
myctx [[ 140045013206304[ PI( <7,4,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 4]
shape input_tile_for_next
 torch.Size([1, 3, 178, 196])
myctx [[ 140045013207072[ PI( <7,4,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 4]
shape input_tile_for_next
 torch.Size([1, 3, 90, 96])
myctx [[ 140045013206400[ PI( <7,4,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 4]
shape input_tile_for_next
 [torch/csrc/autograd/python_engine.cpp] THPEngine_run_backward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
torch.Size([1, 3, 88, 94])
myctx [[ 140045013206448[ PI( <7,4,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 4]
shape input_tile_for_next
 torch.Size([1, 3, 86, 92])
myctx [[ 140045013206976[ PI( <7,4,>,

in customized Sequential 3
need to get existing

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 86, 92])
grad_out size torch.Size([1, 3, 320, 320])
arg size torch.Size([1, 3, 43, 46])
new_grad_out torch.Size([1, 3, 43, 46])
##############grad_in in maxp torch.Size([1, 3, 86, 92])
I am [7, 4]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 88, 94])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 86, 92])
grad_input torch.Size([1, 3, 88, 94])
padding info :: [0, 0, 0, 1]
grad_input old torch.Size([1, 3, 88, 94])
grad_input new torch.Size([1, 3, 88, 94])
new grad_input torch.Size([1, 3, 88, 94])
grad_out size torch.Size([1, 3, 86, 92])
crop [6, 6, 6, 0] [314, 405, 554, 639] [320, 399, 560, 639]
## 6 86 6 86
##############return grad_in in conv2d tensor([ 0.0000,  0.0000,  0.0022, -0.0010, -0.0025,  0.0026,  0.0018,  0.0046,
        -0.0011,  0.0008], device='cuda:0')
I am [7, 4]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 90, 96])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 88, 94])
padding info :: [0, 0, 0, 2]
grad_input old torch.Size([1, 3, 90, 96])
grad_input new torch.Size([1, 3, 90, 96])
grad_input torch.Size([1, 3, 90, 96])
grad_out size torch.Size([1, 3, 87, 94])
crop [7, 7, 7, 0] [313, 406, 553, 639] [320, 399, 560, 639]
## 7 87 7 87
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00, -2.4430e-04,  7.6292e-05, -1.3652e-03,
         1.4670e-04,  1.8550e-04, -2.0623e-03,  7.4515e-04, -6.5274e-04],
       device='cuda:0')
I am [7, 4]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 92, 98])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 90, 96])
padding info :: [0, 0, 0, 3]
grad_input torch.Size([1, 3, 89, 98])
grad_out size torch.Size([1, 3, 88, 96])
crop [8, 8, 8, 0] [312, 407, 552, 639] [320, 399, 560, 639]
## 8 88 8 88
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00, -7.6747e-05,  1.3193e-04, -1.2312e-04,
        -1.6174e-04,  4.5660e-04, -5.6050e-04, -1.2474e-04,  4.2172e-04],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 178, 196])
grad_out size torch.Size([1, 3, 89, 98])
arg size torch.Size([1, 3, 89, 98])
##############grad_in in maxp torch.Size([1, 3, 178, 196])
I am [7, 4]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 180, 198])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 178, 196])
grad_input torch.Size([1, 3, 180, 198])
padding info :: [0, 0, 0, 1]
grad_input old torch.Size([1, 3, 180, 198])
grad_input new torch.Size([1, 3, 180, 198])
new grad_input torch.Size([1, 3, 180, 198])
grad_out size torch.Size([1, 3, 178, 196])
crop [18, 18, 18, 0] [622, 817, 1102, 1279] [640, 799, 1120, 1279]
## 18 178 18 178
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00, -5.0924e-05,  1.5508e-05],
       device='cuda:0')
I am [7, 4]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 182, 200])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 180, 198])
padding info :: [0, 0, 0, 2]
grad_input old torch.Size([1, 3, 182, 200])
grad_input new torch.Size([1, 3, 182, 200])
grad_input torch.Size([1, 3, 182, 200])
grad_out size torch.Size([1, 3, 179, 198])
crop [19, 19, 19, 0] [621, 818, 1101, 1279] [640, 799, 1120, 1279]
## 19 179 19 179
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00, -1.5028e-05,  9.0402e-06],
       device='cuda:0')
I am [7, 4]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 184, 202])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 182, 200])
padding info :: [0, 0, 0, 3]
grad_input torch.Size([1, 3, 181, 202])
grad_out size torch.Size([1, 3, 180, 200])
crop [20, 20, 20, 0] [620, 819, 1100, 1279] [640, 799, 1120, 1279]
## 20 180 20 180
##############return grad_in in conv2d tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 8.8579e-07, 2.6079e-06], device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 362, 404])
grad_out size torch.Size([1, 3, 181, 202])
arg size torch.Size([1, 3, 181, 202])
##############grad_in in maxp torch.Size([1, 3, 362, 404])
I am [7, 4]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 364, 406])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 362, 404])
grad_input torch.Size([1, 3, 364, 406])
padding info :: [0, 0, 0, 1]
grad_input old torch.Size([1, 3, 364, 406])
grad_input new torch.Size([1, 3, 364, 406])
new grad_input torch.Size([1, 3, 364, 406])
grad_out size torch.Size([1, 3, 362, 404])
crop [42, 42, 42, 0] [1238, 1641, 2198, 2559] [1280, 1599, 2240, 2559]
## 42 362 42 362
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [7, 4]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 366, 408])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 364, 406])
padding info :: [0, 0, 0, 2]
grad_input old torch.Size([1, 3, 366, 408])
grad_input new torch.Size([1, 3, 366, 408])
grad_input torch.Size([1, 3, 366, 408])
grad_out size torch.Size([1, 3, 363, 406])
crop [43, 43, 43, 0] [1237, 1642, 2197, 2559] [1280, 1599, 2240, 2559]
## 43 363 43 363
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [7, 4]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 368, 410])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 366, 408])
padding info :: [0, 0, 0, 3]
grad_input torch.Size([1, 3, 365, 410])
grad_out size torch.Size([1, 3, 364, 408])
crop [44, 44, 44, 0] [1236, 1643, 2196, 2559] [1280, 1599, 2240, 2559]
## 44 364 44 364
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 730, 820])
grad_out size torch.Size([1, 3, 365, 410])
arg size torch.Size([1, 3, 365, 410])
##############grad_in in maxp torch.Size([1, 3, 730, 820])
I am [7, 4]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 732, 822])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 730, 820])
grad_input torch.Size([1, 3, 732, 822])
padding info :: [0, 0, 0, 1]
grad_input old torch.Size([1, 3, 732, 822])
grad_input new torch.Size([1, 3, 732, 822])
new grad_input torch.Size([1, 3, 732, 822])
grad_out size torch.Size([1, 3, 730, 820])
crop [90, 90, 90, 0] [2470, 3289, 4390, 5119] [2560, 3199, 4480, 5119]
## 90 730 90 730
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [7, 4]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 734, 824])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 732, 822])
padding info :: [0, 0, 0, 2]
grad_input torch.Size([1, 3, 732, 824])
grad_out size torch.Size([1, 3, 731, 822])
crop [91, 91, 91, 0] [2469, 3290, 4389, 5119] [2560, 3199, 4480, 5119]
## 91 731 91 731
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 1464, 1648])
grad_out size torch.Size([1, 3, 732, 824])
arg size torch.Size([1, 3, 732, 824])
##############grad_in in maxp [torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cCheckpointBackward
torch.Size([1, 3, 1464, 1648])
I am [7, 4]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 1466, 1650])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1464, 1648])
grad_input torch.Size([1, 3, 1466, 1650])
padding info :: [0, 0, 0, 1]
grad_input old torch.Size([1, 3, 1466, 1650])
grad_input new torch.Size([1, 3, 1466, 1650])
new grad_input torch.Size([1, 3, 1466, 1650])
grad_out size torch.Size([1, 3, 1464, 1648])
crop [184, 184, 184, 0] [4936, 6583, 8776, 10239] [5120, 6399, 8960, 10239]
## 184 1464 184 1464
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [7, 4]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 3, 1468, 1652])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1466, 1650])
final torch.Size([1, 3, 1468, 1652])
padding info :: [0, 0, 0, 2]
reshape_for_final ## 186 1466 186 1466
after crop g_in torch.Size([1, 3, 1280, 1280])
grad_out size torch.Size([1, 3, 1465, 1650])
crop [185, 185, 185, 0] [4935, 6584, 8775, 10239] [5120, 6399, 8960, 10239]
## 185 1465 185 1465
##############return grad_in in conv2d tensor([-5.6458e-08, -1.5294e-08, -3.5004e-07, -4.1745e-07,  1.0143e-06,
        -4.4220e-07, -2.6161e-07, -4.7986e-07, -3.1680e-07, -5.1999e-07],
       device='cuda:0')
[8, 8]
HREREERE torch.Size([1, 3, 10240, 10240])

############# Enter checkpointing bkward ####
inputs len 5 4
in customized Sequential 5
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 3]
shape input_tile_for_next
 torch.Size([1, 3, 1466, 1650])
myctx [[ 140046822727744[ PI( <7,3,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 3]
shape input_tile_for_next
 torch.Size([1, 3, 1464, 1648])
myctx [[ 140045015171856[ PI( <7,3,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 3]
shape input_tile_for_next
 torch.Size([1, 3, 732, 822])
myctx [[ 140045013800704[ PI( <7,3,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 3]
shape input_tile_for_next
 torch.Size([1, 3, 730, 820])
myctx [[ 140045013848944[ PI( <7,3,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 3]
shape input_tile_for_next
 torch.Size([1, 3, 366, 408])
myctx [[ 140045013206640[ PI( <7,3,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 3]
shape input_tile_for_next
 torch.Size([1, 3, 364, 406])
myctx [[ 140045013206784[ PI( <7,3,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 3]
shape input_tile_for_next
 torch.Size([1, 3, 362, 404])
myctx [[ 140045013206880[ PI( <7,3,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 3]
shape input_tile_for_next
 torch.Size([1, 3, 182, 200])
myctx [[ 140045013206352[ PI( <7,3,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 3]
shape input_tile_for_next
 torch.Size([1, 3, 180, 198])
myctx [[ 140045013206304[ PI( <7,3,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 3]
shape input_tile_for_next
 torch.Size([1, 3, 178, 196])
myctx [[ 140045013207072[ PI( <7,3,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 3]
shape input_tile_for_next
 torch.Size([1, 3, 90, 96])
myctx [[ 140045013206400[ PI( <7,3,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 3]
shape input_tile_for_next
 torch.Size([1, 3, 88, 94])
myctx [[ 140045013206448[ PI( <7,3,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 3]
shape input_tile_for_next
 torch.Size([1, 3, 86, 92])
myctx [[ 140045013206976[ PI( <7,3,>,

in customized Sequential 3
need to get existing

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 86, 92])
grad_out size torch.Size([1, 3, 320, 320])
arg size torch.Size([1, 3, 43, 46])
new_grad_out torch.Size([1, 3, 43, 46])
##############grad_in in maxp torch.Size([1, 3, 86, 92])
I am [7, 3]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 88, 94])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 86, 92])
grad_input torch.Size([1, 3, 88, 94])
padding info :: [0, 0, 0, 1]
[torch/csrc/autograd/python_engine.cpp] THPEngine_run_backward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
grad_input old torch.Size([1, 3, 88, 94])
grad_input new torch.Size([1, 3, 88, 94])
new grad_input torch.Size([1, 3, 88, 94])
grad_out size torch.Size([1, 3, 86, 92])
crop [6, 6, 6, 0] [234, 325, 554, 639] [240, 319, 560, 639]
## 6 86 6 86
##############return grad_in in conv2d tensor([ 0.0000,  0.0050, -0.0049,  0.0072, -0.0066, -0.0030,  0.0007,  0.0022,
        -0.0014,  0.0012], device='cuda:0')
I am [7, 3]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 90, 96])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 88, 94])
padding info :: [0, 0, 0, 2]
grad_input old torch.Size([1, 3, 90, 96])
grad_input new torch.Size([1, 3, 90, 96])
grad_input torch.Size([1, 3, 90, 96])
grad_out size torch.Size([1, 3, 87, 94])
crop [7, 7, 7, 0] [233, 326, 553, 639] [240, 319, 560, 639]
## 7 87 7 87
##############return grad_in in conv2d tensor([ 0.0000,  0.0009,  0.0012,  0.0010,  0.0033,  0.0005,  0.0036,  0.0002,
         0.0003, -0.0008], device='cuda:0')
I am [7, 3]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 92, 98])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 90, 96])
padding info :: [0, 0, 0, 3]
grad_input torch.Size([1, 3, 89, 98])
grad_out size torch.Size([1, 3, 88, 96])
crop [8, 8, 8, 0] [232, 327, 552, 639] [240, 319, 560, 639]
## 8 88 8 88
##############return grad_in in conv2d tensor([ 0.0000e+00, -2.2330e-05,  3.5889e-04, -5.0329e-04,  5.6280e-04,
        -3.8150e-05,  2.3803e-05,  3.0710e-04, -4.4480e-04, -3.3240e-04],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 178, 196])
grad_out size torch.Size([1, 3, 89, 98])
arg size torch.Size([1, 3, 89, 98])
##############grad_in in maxp torch.Size([1, 3, 178, 196])
I am [7, 3]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 180, 198])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 178, 196])
grad_input torch.Size([1, 3, 180, 198])
padding info :: [0, 0, 0, 1]
grad_input old torch.Size([1, 3, 180, 198])
grad_input new torch.Size([1, 3, 180, 198])
new grad_input torch.Size([1, 3, 180, 198])
grad_out size torch.Size([1, 3, 178, 196])
crop [18, 18, 18, 0] [462, 657, 1102, 1279] [480, 639, 1120, 1279]
## 18 178 18 178
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  2.5848e-05,  1.7141e-05,
        -7.4229e-06, -6.5515e-05, -9.1774e-06,  3.1989e-05,  2.5476e-04],
       device='cuda:0')
I am [7, 3]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 182, 200])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 180, 198])
padding info :: [0, 0, 0, 2]
grad_input old torch.Size([1, 3, 182, 200])
grad_input new torch.Size([1, 3, 182, 200])
grad_input torch.Size([1, 3, 182, 200])
grad_out size torch.Size([1, 3, 179, 198])
crop [19, 19, 19, 0] [461, 658, 1101, 1279] [480, 639, 1120, 1279]
## 19 179 19 179
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  7.8478e-06,  5.5183e-06,
         2.4408e-07, -8.8700e-06,  6.4227e-06, -1.1998e-05,  7.6449e-05],
       device='cuda:0')
I am [7, 3]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 184, 202])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 182, 200])
padding info :: [0, 0, 0, 3]
grad_input torch.Size([1, 3, 181, 202])
grad_out size torch.Size([1, 3, 180, 200])
crop [20, 20, 20, 0] [460, 659, 1100, 1279] [480, 639, 1120, 1279]
## 20 180 20 180
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00, -6.2213e-07, -1.9053e-06,
        -7.8412e-07, -1.4750e-07,  2.9041e-06,  4.4060e-06, -9.5833e-06],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 362, 404])
grad_out size torch.Size([1, 3, 181, 202])
arg size torch.Size([1, 3, 181, 202])
##############grad_in in maxp torch.Size([1, 3, 362, 404])
I am [7, 3]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 364, 406])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 362, 404])
grad_input torch.Size([1, 3, 364, 406])
padding info :: [0, 0, 0, 1]
grad_input old torch.Size([1, 3, 364, 406])
grad_input new torch.Size([1, 3, 364, 406])
new grad_input torch.Size([1, 3, 364, 406])
grad_out size torch.Size([1, 3, 362, 404])
crop [42, 42, 42, 0] [918, 1321, 2198, 2559] [960, 1279, 2240, 2559]
## 42 362 42 362
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00, -8.8229e-08, -5.2713e-08,  3.6950e-08,  0.0000e+00],
       device='cuda:0')
I am [7, 3]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 366, 408])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 364, 406])
padding info :: [0, 0, 0, 2]
grad_input old torch.Size([1, 3, 366, 408])
grad_input new torch.Size([1, 3, 366, 408])
grad_input torch.Size([1, 3, 366, 408])
grad_out size torch.Size([1, 3, 363, 406])
crop [43, 43, 43, 0] [917, 1322, 2197, 2559] [960, 1279, 2240, 2559]
## 43 363 43 363
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00, -1.5738e-08, -3.4239e-08, -1.8339e-08, -2.2732e-08],
       device='cuda:0')
I am [7, 3]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 368, 410])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 366, 408])
padding info :: [0, 0, 0, 3]
grad_input torch.Size([1, 3, 365, 410])
grad_out size torch.Size([1, 3, 364, 408])
crop [44, 44, 44, 0] [916, 1323, 2196, 2559] [960, 1279, 2240, 2559]
## 44 364 44 364
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00, -1.8292e-09,  5.6751e-09,  1.2129e-08,  1.6875e-08],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 730, 820])
grad_out size torch.Size([1, 3, 365, 410])
arg size torch.Size([1, 3, 365, 410])
##############grad_in in maxp torch.Size([1, 3, 730, 820])
I am [7, 3]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 732, 822])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 730, 820])
grad_input torch.Size([1, 3, 732, 822])
padding info :: [0, 0, 0, 1]
grad_input old torch.Size([1, 3, 732, 822])
grad_input new torch.Size([1, 3, 732, 822])
new grad_input torch.Size([1, 3, 732, 822])
grad_out size torch.Size([1, 3, 730, 820])
crop [90, 90, 90, 0] [1830, 2649, 4390, 5119] [1920, 2559, 4480, 5119]
## 90 730 90 730
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [7, 3]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 734, 824])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 732, 822])
padding info :: [0, 0, 0, 2]
grad_input torch.Size([1, 3, 732, 824])
grad_out size torch.Size([1, 3, 731, 822])
crop [91, 91, 91, 0] [1829, 2650, 4389, 5119] [1920, 2559, 4480, 5119]
## 91 731 91 731
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 1464, 1648])
grad_out size torch.Size([1, 3, 732, 824])
arg size torch.Size([1, 3, 732, 824])
##############grad_in in maxp torch.Size([1, 3, 1464, 1648])
I am [7, 3]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 1466, 1650])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1464, 1648])
grad_input torch.Size([1, 3, 1466, 1650])
padding info :: [0, 0, 0, 1]
grad_input old torch.Size([1, 3, 1466, 1650])
grad_input new torch.Size([1, 3, 1466, 1650])
new grad_input torch.Size([1, 3, 1466, 1650])
grad_out size torch.Size([1, 3, 1464, 1648])
crop [184, 184, 184, 0] [3656, 5303, 8776, 10239] [3840, 5119, 8960, 10239]
## 184 1464 184 1464
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [7, 3]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 3, 1468, 1652])
weight shape [torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cCheckpointBackward
[torch/csrc/autograd/python_engine.cpp] THPEngine_run_backward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1466, 1650])
final torch.Size([1, 3, 1468, 1652])
padding info :: [0, 0, 0, 2]
reshape_for_final ## 186 1466 186 1466
after crop g_in torch.Size([1, 3, 1280, 1280])
grad_out size torch.Size([1, 3, 1465, 1650])
crop [185, 185, 185, 0] [3655, 5304, 8775, 10239] [3840, 5119, 8960, 10239]
## 185 1465 185 1465
##############return grad_in in conv2d tensor([ 6.3396e-07, -3.4869e-07, -2.0671e-07, -3.1519e-08, -1.0669e-08,
        -3.6300e-07, -1.7338e-07, -7.1694e-10, -2.9686e-08,  4.3055e-07],
       device='cuda:0')
[8, 8]
HREREERE torch.Size([1, 3, 10240, 10240])

############# Enter checkpointing bkward ####
inputs len 5 4
in customized Sequential 5
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 2]
shape input_tile_for_next
 torch.Size([1, 3, 1466, 1650])
myctx [[ 140046822727744[ PI( <7,2,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 2]
shape input_tile_for_next
 torch.Size([1, 3, 1464, 1648])
myctx [[ 140045015171856[ PI( <7,2,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 2]
shape input_tile_for_next
 torch.Size([1, 3, 732, 822])
myctx [[ 140045013800704[ PI( <7,2,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 2]
shape input_tile_for_next
 torch.Size([1, 3, 730, 820])
myctx [[ 140045013848944[ PI( <7,2,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 2]
shape input_tile_for_next
 torch.Size([1, 3, 366, 408])
myctx [[ 140045013206640[ PI( <7,2,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 2]
shape input_tile_for_next
 torch.Size([1, 3, 364, 406])
myctx [[ 140045013206784[ PI( <7,2,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 2]
shape input_tile_for_next
 torch.Size([1, 3, 362, 404])
myctx [[ 140045013206880[ PI( <7,2,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 2]
shape input_tile_for_next
 torch.Size([1, 3, 182, 200])
myctx [[ 140045013206352[ PI( <7,2,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 2]
shape input_tile_for_next
 torch.Size([1, 3, 180, 198])
myctx [[ 140045013206304[ PI( <7,2,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 2]
shape input_tile_for_next
 torch.Size([1, 3, 178, 196])
myctx [[ 140045013207072[ PI( <7,2,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 2]
shape input_tile_for_next
 torch.Size([1, 3, 90, 96])
myctx [[ 140045013206400[ PI( <7,2,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 2]
shape input_tile_for_next
 torch.Size([1, 3, 88, 94])
myctx [[ 140045013206448[ PI( <7,2,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 2]
shape input_tile_for_next
 torch.Size([1, 3, 86, 92])
myctx [[ 140045013206976[ PI( <7,2,>,

in customized Sequential 3
need to get existing

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 86, 92])
grad_out size torch.Size([1, 3, 320, 320])
arg size torch.Size([1, 3, 43, 46])
new_grad_out torch.Size([1, 3, 43, 46])
##############grad_in in maxp torch.Size([1, 3, 86, 92])
I am [7, 2]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 88, 94])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 86, 92])
grad_input torch.Size([1, 3, 88, 94])
padding info :: [0, 0, 0, 1]
grad_input old torch.Size([1, 3, 88, 94])
grad_input new torch.Size([1, 3, 88, 94])
new grad_input torch.Size([1, 3, 88, 94])
grad_out size torch.Size([1, 3, 86, 92])
crop [6, 6, 6, 0] [154, 245, 554, 639] [160, 239, 560, 639]
## 6 86 6 86
##############return grad_in in conv2d tensor([ 0.0024,  0.0038,  0.0003,  0.0008, -0.0006, -0.0015,  0.0052,  0.0018,
         0.0041, -0.0055], device='cuda:0')
I am [7, 2]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 90, 96])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 88, 94])
padding info :: [0, 0, 0, 2]
grad_input old torch.Size([1, 3, 90, 96])
grad_input new torch.Size([1, 3, 90, 96])
grad_input torch.Size([1, 3, 90, 96])
grad_out size torch.Size([1, 3, 87, 94])
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cCheckpointBackward
crop [7, 7, 7, 0] [153, 246, 553, 639] [160, 239, 560, 639]
## 7 87 7 87
##############return grad_in in conv2d tensor([ 1.3877e-03, -1.5349e-03,  6.4239e-04,  6.8656e-04, -1.0362e-04,
         5.5689e-04, -5.4234e-04, -1.0601e-03,  7.4656e-04,  9.1273e-05],
       device='cuda:0')
I am [7, 2]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 92, 98])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 90, 96])
padding info :: [0, 0, 0, 3]
grad_input torch.Size([1, 3, 89, 98])
grad_out size torch.Size([1, 3, 88, 96])
crop [8, 8, 8, 0] [152, 247, 552, 639] [160, 239, 560, 639]
## 8 88 8 88
##############return grad_in in conv2d tensor([ 2.2420e-04, -3.4239e-04, -4.6863e-04,  6.0839e-04,  1.9200e-04,
        -4.9238e-05, -2.1504e-04, -3.3945e-04,  3.3979e-05,  4.7465e-04],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 178, 196])
grad_out size torch.Size([1, 3, 89, 98])
arg size torch.Size([1, 3, 89, 98])
##############grad_in in maxp torch.Size([1, 3, 178, 196])
I am [7, 2]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 180, 198])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 178, 196])
grad_input torch.Size([1, 3, 180, 198])
padding info :: [0, 0, 0, 1]
grad_input old torch.Size([1, 3, 180, 198])
grad_input new torch.Size([1, 3, 180, 198])
new grad_input torch.Size([1, 3, 180, 198])
grad_out size torch.Size([1, 3, 178, 196])
crop [18, 18, 18, 0] [302, 497, 1102, 1279] [320, 479, 1120, 1279]
## 18 178 18 178
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00, -9.0355e-05,  4.4356e-05,
         1.3408e-04,  1.9088e-05, -6.8181e-05, -2.3938e-05, -2.4230e-05],
       device='cuda:0')
I am [7, 2]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 182, 200])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 180, 198])
padding info :: [0, 0, 0, 2]
grad_input old torch.Size([1, 3, 182, 200])
grad_input new torch.Size([1, 3, 182, 200])
grad_input torch.Size([1, 3, 182, 200])
grad_out size torch.Size([1, 3, 179, 198])
crop [19, 19, 19, 0] [301, 498, 1101, 1279] [320, 479, 1120, 1279]
## 19 179 19 179
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00, -2.6235e-05,  2.8374e-05,
         9.9197e-06,  7.7433e-06,  1.1164e-05, -4.6495e-06, -2.3486e-05],
       device='cuda:0')
I am [7, 2]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 184, 202])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 182, 200])
padding info :: [0, 0, 0, 3]
grad_input torch.Size([1, 3, 181, 202])
grad_out size torch.Size([1, 3, 180, 200])
crop [20, 20, 20, 0] [300, 499, 1100, 1279] [320, 479, 1120, 1279]
## 20 180 20 180
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.1655e-06,  3.6667e-06,
        -1.6919e-07, -1.1849e-05, -4.6791e-06,  8.0781e-06,  4.4238e-06],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 362, 404])
grad_out size torch.Size([1, 3, 181, 202])
arg size torch.Size([1, 3, 181, 202])
##############grad_in in maxp torch.Size([1, 3, 362, 404])
I am [7, 2]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 364, 406])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 362, 404])
grad_input torch.Size([1, 3, 364, 406])
padding info :: [0, 0, 0, 1]
grad_input old torch.Size([1, 3, 364, 406])
grad_input new torch.Size([1, 3, 364, 406])
new grad_input torch.Size([1, 3, 364, 406])
grad_out size torch.Size([1, 3, 362, 404])
crop [42, 42, 42, 0] [598, 1001, 2198, 2559] [640, 959, 2240, 2559]
## 42 362 42 362
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  1.0906e-07, -3.8888e-07,  5.2681e-07,  4.0532e-07],
       device='cuda:0')
I am [7, 2]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 366, 408])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 364, 406])
padding info :: [0, 0, 0, 2]
grad_input old torch.Size([1, 3, 366, 408])
grad_input new torch.Size([1, 3, 366, 408])
grad_input torch.Size([1, 3, 366, 408])
grad_out size torch.Size([1, 3, 363, 406])
crop [43, 43, 43, 0] [597, 1002, 2197, 2559] [640, 959, 2240, 2559]
## 43 363 43 363
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  4.3336e-08, -1.3775e-09, -2.2749e-07,  1.5260e-07],
       device='cuda:0')
I am [7, 2]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 368, 410])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 366, 408])
padding info :: [0, 0, 0, 3]
grad_input torch.Size([1, 3, 365, 410])
grad_out size torch.Size([1, 3, 364, 408])
crop [44, 44, 44, 0] [596, 1003, 2196, 2559] [640, 959, 2240, 2559]
## 44 364 44 364
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  4.6508e-09, -1.6486e-08,  2.0198e-08,  1.4301e-08],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 730, 820])
grad_out size torch.Size([1, 3, 365, 410])
arg size torch.Size([1, 3, 365, 410])
##############grad_in in maxp torch.Size([1, 3, 730, 820])
I am [7, 2]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 732, 822])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 730, 820])
grad_input torch.Size([1, 3, 732, 822])
padding info :: [0, 0, 0, 1]
grad_input old torch.Size([1, 3, 732, 822])
grad_input new torch.Size([1, 3, 732, 822])
new grad_input torch.Size([1, 3, 732, 822])
grad_out size torch.Size([1, 3, 730, 820])
crop [90, 90, 90, 0] [1190, 2009, 4390, 5119] [1280, 1919, 4480, 5119]
## 90 730 90 730
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [7, 2]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 734, 824])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 732, 822])
padding info :: [0, 0, 0, 2]
grad_input torch.Size([1, 3, 732, 824])
grad_out size torch.Size([1, 3, 731, 822])
crop [91, 91, 91, 0] [1189, 2010, 4389, 5119] [1280, 1919, 4480, 5119]
## 91 731 91 731
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 1464, 1648])
grad_out size torch.Size([1, 3, 732, 824])
arg size torch.Size([1, 3, 732, 824])
##############grad_in in maxp torch.Size([1, 3, 1464, 1648])
I am [7, 2]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 1466, 1650])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1464, 1648])
grad_input torch.Size([1, 3, 1466, 1650])
padding info :: [0, 0, 0, 1]
grad_input old torch.Size([1, 3, 1466, 1650])
grad_input new torch.Size([1, 3, 1466, 1650])
new grad_input torch.Size([1, 3, 1466, 1650])
grad_out size torch.Size([1, 3, 1464, 1648])
crop [184, 184, 184, 0] [2376, 4023, 8776, 10239] [2560, 3839, 8960, 10239]
## 184 1464 184 1464
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [7, 2]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 3, 1468, 1652])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1466, 1650])
final torch.Size([1, 3, 1468, 1652])
padding info :: [0, 0, 0, 2]
reshape_for_final ## 186 1466 186 1466
after crop g_in torch.Size([1, 3, 1280, 1280])
grad_out size torch.Size([1, 3, 1465, 1650])
crop [185, 185, 185, 0] [2375, 4024, 8775, 10239] [2560, 3839, 8960, 10239]
## 185 1465 185 1465
##############return grad_in in conv2d tensor([-8.3650e-07, -2.0874e-07, -2.6031e-07,  1.0829e-07,  6.2016e-07,
         3.9277e-07,  3.5340e-07,  7.5693e-08, -1.3984e-07,  8.1870e-08],
       device='cuda:0')
[8, 8]
HREREERE torch.Size([1, 3, 10240, 10240])

############# Enter checkpointing bkward ####
inputs len 5 4
in customized Sequential 5
in customized Sequential 2
id[torch/csrc/autograd/python_engine.cpp] THPEngine_run_backward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 1]
shape input_tile_for_next
 torch.Size([1, 3, 1466, 1650])
myctx [[ 140046822727744[ PI( <7,1,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 1]
shape input_tile_for_next
 torch.Size([1, 3, 1464, 1648])
myctx [[ 140045015171856[ PI( <7,1,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 1]
shape input_tile_for_next
 torch.Size([1, 3, 732, 822])
myctx [[ 140045013800704[ PI( <7,1,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 1]
shape input_tile_for_next
 torch.Size([1, 3, 730, 820])
myctx [[ 140045013848944[ PI( <7,1,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 1]
shape input_tile_for_next
 torch.Size([1, 3, 366, 408])
myctx [[ 140045013206640[ PI( <7,1,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 1]
shape input_tile_for_next
 torch.Size([1, 3, 364, 406])
myctx [[ 140045013206784[ PI( <7,1,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 1]
shape input_tile_for_next
 torch.Size([1, 3, 362, 404])
myctx [[ 140045013206880[ PI( <7,1,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 1]
shape input_tile_for_next
 torch.Size([1, 3, 182, 200])
myctx [[ 140045013206352[ PI( <7,1,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 1]
shape input_tile_for_next
 torch.Size([1, 3, 180, 198])
myctx [[ 140045013206304[ PI( <7,1,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 1]
shape input_tile_for_next
 torch.Size([1, 3, 178, 196])
myctx [[ 140045013207072[ PI( <7,1,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 1]
shape input_tile_for_next
 torch.Size([1, 3, 90, 96])
myctx [[ 140045013206400[ PI( <7,1,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 1]
shape input_tile_for_next
 torch.Size([1, 3, 88, 94])
myctx [[ 140045013206448[ PI( <7,1,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 1]
shape input_tile_for_next
 torch.Size([1, 3, 86, 92])
myctx [[ 140045013206976[ PI( <7,1,>,

in customized Sequential 3
need to get existing

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 86, 92])
grad_out size torch.Size([1, 3, 320, 320])
arg size torch.Size([1, 3, 43, 46])
new_grad_out torch.Size([1, 3, 43, 46])
##############grad_in in maxp torch.Size([1, 3, 86, 92])
I am [7, 1]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 88, 94])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 86, 92])
grad_input torch.Size([1, 3, 88, 94])
padding info :: [0, 0, 0, 1]
grad_input old torch.Size([1, 3, 88, 94])
grad_input new torch.Size([1, 3, 88, 94])
new grad_input torch.Size([1, 3, 88, 94])
grad_out size torch.Size([1, 3, 86, 92])
crop [6, 6, 6, 0] [74, 165, 554, 639] [80, 159, 560, 639]
## 6 86 6 86
##############return grad_in in conv2d tensor([ 1.5574e-03, -2.2636e-03, -1.7369e-03,  8.1319e-04, -1.3522e-04,
        -1.8499e-03,  1.0223e-03, -3.7914e-03,  5.0310e-03,  4.4594e-05],
       device='cuda:0')
I am [7, 1]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 90, 96])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 88, 94])
padding info :: [0, 0, 0, 2]
grad_input old torch.Size([1, 3, 90, 96])
grad_input new torch.Size([1, 3, 90, 96])
grad_input torch.Size([1, 3, 90, 96])
grad_out size torch.Size([1, 3, 87, 94])
crop [7, 7, 7, 0] [73, 166, 553, 639] [80, 159, 560, 639]
## 7 87 7 87
##############return grad_in in conv2d tensor([ 2.8937e-04,  1.6360e-04,  6.0388e-04,  6.0009e-04,  2.8545e-04,
         2.5988e-04,  5.1414e-05, -4.7686e-04, -6.1535e-04, -8.6644e-04],
       device='cuda:0')
I am [7, 1]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 92, 98])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 90, 96])
padding info :: [0, 0, 0, 3]
grad_input torch.Size([1, 3, 89, 98])
grad_out size torch.Size([1, 3, 88, 96])
crop [8, 8, 8, 0] [72, 167, 552, 639] [80, 159, 560, 639]
## 8 88 8 88
##############return grad_in in conv2d [torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cCheckpointBackward
tensor([-6.9823e-06,  8.2542e-05, -2.9985e-05,  4.9913e-05,  3.6399e-05,
         3.3003e-05, -8.5223e-05, -2.1603e-05, -2.8389e-04,  1.7960e-04],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 178, 196])
grad_out size torch.Size([1, 3, 89, 98])
arg size torch.Size([1, 3, 89, 98])
##############grad_in in maxp torch.Size([1, 3, 178, 196])
I am [7, 1]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 180, 198])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 178, 196])
grad_input torch.Size([1, 3, 180, 198])
padding info :: [0, 0, 0, 1]
grad_input old torch.Size([1, 3, 180, 198])
grad_input new torch.Size([1, 3, 180, 198])
new grad_input torch.Size([1, 3, 180, 198])
grad_out size torch.Size([1, 3, 178, 196])
crop [18, 18, 18, 0] [142, 337, 1102, 1279] [160, 319, 1120, 1279]
## 18 178 18 178
##############return grad_in in conv2d tensor([ 1.0198e-06,  3.3824e-06,  2.4116e-05, -9.2247e-06, -4.5354e-06,
         3.3188e-06,  8.3723e-07,  6.3998e-06, -9.9802e-07, -7.3296e-06],
       device='cuda:0')
I am [7, 1]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 182, 200])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 180, 198])
padding info :: [0, 0, 0, 2]
grad_input old torch.Size([1, 3, 182, 200])
grad_input new torch.Size([1, 3, 182, 200])
grad_input torch.Size([1, 3, 182, 200])
grad_out size torch.Size([1, 3, 179, 198])
crop [19, 19, 19, 0] [141, 338, 1101, 1279] [160, 319, 1120, 1279]
## 19 179 19 179
##############return grad_in in conv2d tensor([ 3.9518e-07,  2.5237e-06,  6.6384e-06,  1.3614e-06,  8.3140e-06,
        -2.7551e-06,  1.3695e-06,  3.8643e-06, -1.8632e-06,  4.0090e-07],
       device='cuda:0')
I am [7, 1]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 184, 202])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 182, 200])
padding info :: [0, 0, 0, 3]
grad_input torch.Size([1, 3, 181, 202])
grad_out size torch.Size([1, 3, 180, 200])
crop [20, 20, 20, 0] [140, 339, 1100, 1279] [160, 319, 1120, 1279]
## 20 180 20 180
##############return grad_in in conv2d tensor([-1.0681e-07, -2.6286e-07, -4.3309e-07, -2.1661e-06, -5.6161e-07,
         1.4795e-06, -1.0109e-06, -1.0882e-07, -1.1316e-07, -1.5077e-07],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 362, 404])
grad_out size torch.Size([1, 3, 181, 202])
arg size torch.Size([1, 3, 181, 202])
##############grad_in in maxp torch.Size([1, 3, 362, 404])
I am [7, 1]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 364, 406])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 362, 404])
grad_input torch.Size([1, 3, 364, 406])
padding info :: [0, 0, 0, 1]
grad_input old torch.Size([1, 3, 364, 406])
grad_input new torch.Size([1, 3, 364, 406])
new grad_input torch.Size([1, 3, 364, 406])
grad_out size torch.Size([1, 3, 362, 404])
crop [42, 42, 42, 0] [278, 681, 2198, 2559] [320, 639, 2240, 2559]
## 42 362 42 362
##############return grad_in in conv2d tensor([ 0.0000e+00, -1.5148e-08, -4.6328e-08, -5.3870e-08,  5.5210e-08,
        -4.2626e-08, -2.3967e-07, -3.0492e-07, -9.2560e-09,  4.4128e-08],
       device='cuda:0')
I am [7, 1]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 366, 408])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 364, 406])
padding info :: [0, 0, 0, 2]
grad_input old torch.Size([1, 3, 366, 408])
grad_input new torch.Size([1, 3, 366, 408])
grad_input torch.Size([1, 3, 366, 408])
grad_out size torch.Size([1, 3, 363, 406])
crop [43, 43, 43, 0] [277, 682, 2197, 2559] [320, 639, 2240, 2559]
## 43 363 43 363
##############return grad_in in conv2d tensor([ 0.0000e+00, -2.7020e-09, -1.2528e-08, -1.2597e-08,  4.5617e-09,
        -4.2664e-08, -7.5432e-08, -5.0105e-08, -1.1758e-07, -7.0718e-08],
       device='cuda:0')
I am [7, 1]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 368, 410])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 366, 408])
padding info :: [0, 0, 0, 3]
grad_input torch.Size([1, 3, 365, 410])
grad_out size torch.Size([1, 3, 364, 408])
crop [44, 44, 44, 0] [276, 683, 2196, 2559] [320, 639, 2240, 2559]
## 44 364 44 364
##############return grad_in in conv2d tensor([ 0.0000e+00, -3.1404e-10,  2.0150e-10,  4.6688e-09,  4.1112e-09,
         3.6915e-09,  3.6012e-09,  1.9806e-08,  4.9536e-08,  5.4777e-08],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 730, 820])
grad_out size torch.Size([1, 3, 365, 410])
arg size torch.Size([1, 3, 365, 410])
##############grad_in in maxp torch.Size([1, 3, 730, 820])
I am [7, 1]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 732, 822])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 730, 820])
grad_input torch.Size([1, 3, 732, 822])
padding info :: [0, 0, 0, 1]
grad_input old torch.Size([1, 3, 732, 822])
grad_input new torch.Size([1, 3, 732, 822])
new grad_input torch.Size([1, 3, 732, 822])
grad_out size torch.Size([1, 3, 730, 820])
crop [90, 90, 90, 0] [550, 1369, 4390, 5119] [640, 1279, 4480, 5119]
## 90 730 90 730
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00, -5.8815e-11, -1.4296e-10, -2.7868e-12,
        -9.3505e-10, -5.2555e-10, -8.0906e-11,  4.5562e-10, -1.3410e-10],
       device='cuda:0')
I am [7, 1]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 734, 824])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 732, 822])
padding info :: [0, 0, 0, 2]
grad_input torch.Size([1, 3, 732, 824])
grad_out size torch.Size([1, 3, 731, 822])
crop [91, 91, 91, 0] [549, 1370, 4389, 5119] [640, 1279, 4480, 5119]
## 91 731 91 731
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  3.5652e-13, -1.5181e-11, -2.4574e-11,
        -1.2383e-10, -2.4857e-10, -1.8724e-10, -4.9912e-11,  3.0651e-10],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 1464, 1648])
grad_out size torch.Size([1, 3, 732, 824])
arg size torch.Size([1, 3, 732, 824])
##############grad_in in maxp torch.Size([1, 3, 1464, 1648])
I am [7, 1]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 1466, 1650])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1464, 1648])
grad_input torch.Size([1, 3, 1466, 1650])
padding info :: [0, 0, 0, 1]
grad_input old torch.Size([1, 3, 1466, 1650])
grad_input new torch.Size([1, 3, 1466, 1650])
new grad_input torch.Size([1, 3, 1466, 1650])
grad_out size torch.Size([1, 3, 1464, 1648])
crop [184, 184, 184, 0] [1096, 2743, 8776, 10239] [1280, 2559, 8960, 10239]
## 184 1464 184 1464
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
        -1.3675e-14, -2.5668e-14,  1.6242e-12, -1.6225e-12, -9.1929e-13],
       device='cuda:0')
I am [7, 1]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 3, 1468, 1652])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1466, 1650])
final torch.Size([1, 3, 1468, 1652])
padding info :: [0, 0, 0, 2]
reshape_for_final ## 186 1466 186 1466
after crop g_in torch.Size([1, 3, 1280, 1280])
grad_out size torch.Size([1, 3, 1465, 1650])
crop [185, 185, 185, 0] [1095, 2744, 8775, 10239] [1280, 2559, 8960, 10239]
## 185 1465 185 1465
##############return grad_in in conv2d tensor([-2.9048e-07, -7.8960e-08, -2.3524e-07, -1.8260e-07,  1.2824e-07,
        -1.2337e-07, -2.7904e-07, -2.9575e-07,  1.6830e-07,  1.2072e-07],
       device='cuda:0')
[8, 8]
HREREERE torch.Size([1, 3, 10240, 10240])

############# Enter checkpointing bkward ####
inputs len 5 4
in customized Sequential 5
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 0]
shape input_tile_for_next
 [torch/csrc/autograd/python_engine.cpp] THPEngine_run_backward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
torch.Size([1, 3, 1466, 1466])
myctx [[ 140046822727744[ PI( <7,0,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 0]
shape input_tile_for_next
 torch.Size([1, 3, 1464, 1464])
myctx [[ 140045015171856[ PI( <7,0,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 0]
shape input_tile_for_next
 torch.Size([1, 3, 732, 732])
myctx [[ 140045013800704[ PI( <7,0,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 0]
shape input_tile_for_next
 torch.Size([1, 3, 730, 730])
myctx [[ 140045013848944[ PI( <7,0,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 0]
shape input_tile_for_next
 torch.Size([1, 3, 366, 366])
myctx [[ 140045013206640[ PI( <7,0,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 0]
shape input_tile_for_next
 torch.Size([1, 3, 364, 364])
myctx [[ 140045013206784[ PI( <7,0,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 0]
shape input_tile_for_next
 torch.Size([1, 3, 362, 362])
myctx [[ 140045013206880[ PI( <7,0,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 0]
shape input_tile_for_next
 torch.Size([1, 3, 182, 182])
myctx [[ 140045013206352[ PI( <7,0,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 0]
shape input_tile_for_next
 torch.Size([1, 3, 180, 180])
myctx [[ 140045013206304[ PI( <7,0,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 0]
shape input_tile_for_next
 torch.Size([1, 3, 178, 178])
myctx [[ 140045013207072[ PI( <7,0,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 0]
shape input_tile_for_next
 torch.Size([1, 3, 90, 90])
myctx [[ 140045013206400[ PI( <7,0,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 0]
shape input_tile_for_next
 torch.Size([1, 3, 88, 88])
myctx [[ 140045013206448[ PI( <7,0,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [7, 0]
shape input_tile_for_next
 torch.Size([1, 3, 86, 86])
myctx [[ 140045013206976[ PI( <7,0,>,

in customized Sequential 3
need to get existing

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 86, 86])
grad_out size torch.Size([1, 3, 320, 320])
arg size torch.Size([1, 3, 43, 43])
new_grad_out torch.Size([1, 3, 43, 43])
##############grad_in in maxp torch.Size([1, 3, 86, 86])
I am [7, 0]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 88, 88])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 86, 86])
grad_input torch.Size([1, 3, 88, 88])
padding info :: [1, 0, 0, 1]
grad_input old torch.Size([1, 3, 88, 88])
grad_input new torch.Size([1, 3, 88, 88])
new grad_input torch.Size([1, 3, 88, 88])
grad_out size torch.Size([1, 3, 86, 86])
crop [0, 6, 6, 0] [0, 85, 554, 639] [0, 79, 560, 639]
## 6 86 0 80
##############return grad_in in conv2d tensor([ 0.0000,  0.0031,  0.0016, -0.0042, -0.0025,  0.0034,  0.0026,  0.0028,
        -0.0045, -0.0002], device='cuda:0')
I am [7, 0]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 90, 90])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 88, 88])
padding info :: [2, 0, 0, 2]
grad_input old torch.Size([1, 3, 90, 90])
grad_input new torch.Size([1, 3, 90, 90])
grad_input torch.Size([1, 3, 90, 90])
grad_out size torch.Size([1, 3, 87, 87])
crop [0, 7, 7, 0] [0, 86, 553, 639] [0, 79, 560, 639]
## 7 87 0 80
##############return grad_in in conv2d tensor([ 0.0000,  0.0000,  0.0004, -0.0003,  0.0008,  0.0014, -0.0015,  0.0003,
         0.0003,  0.0009], device='cuda:0')
I am [7, 0]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 92, 92])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 90, 90])
padding info :: [3, 0, 0, 3]
grad_input torch.Size([1, 3, 89, 89])
grad_out size torch.Size([1, 3, 88, 88])
crop [0, 8, 8, 0] [0, 87, 552, 639] [0, 79, 560, 639]
## 8 88 0 80
##############return grad_in in conv2d tensor([ 5.8156e-05, -9.5988e-06,  3.7943e-04, -2.4572e-04, -5.5265e-04,
         4.0343e-04, -4.2555e-05,  4.4552e-04, -5.6245e-05, -4.1986e-04],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 178, 178])
grad_out size torch.Size([1, 3, 89, 89])
arg size torch.Size([1, 3, 89, 89])
##############grad_in in maxp torch.Size([1, 3, 178, 178])
I am [7, 0]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 180, 180])
weight shape [torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cCheckpointBackward
torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 178, 178])
grad_input torch.Size([1, 3, 180, 180])
padding info :: [1, 0, 0, 1]
grad_input old torch.Size([1, 3, 180, 180])
grad_input new torch.Size([1, 3, 180, 180])
new grad_input torch.Size([1, 3, 180, 180])
grad_out size torch.Size([1, 3, 178, 178])
crop [0, 18, 18, 0] [0, 177, 1102, 1279] [0, 159, 1120, 1279]
## 18 178 0 160
##############return grad_in in conv2d tensor([ 0.0000e+00,  4.8667e-06,  5.5678e-06, -6.1309e-05,  3.0862e-06,
         2.4213e-05, -4.0019e-05, -6.1107e-05,  4.0534e-05,  1.2102e-05],
       device='cuda:0')
I am [7, 0]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 182, 182])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 180, 180])
padding info :: [2, 0, 0, 2]
grad_input old torch.Size([1, 3, 182, 182])
grad_input new torch.Size([1, 3, 182, 182])
grad_input torch.Size([1, 3, 182, 182])
grad_out size torch.Size([1, 3, 179, 179])
crop [0, 19, 19, 0] [0, 178, 1101, 1279] [0, 159, 1120, 1279]
## 19 179 0 160
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  1.2080e-06, -1.5210e-05,  1.2345e-06,
        -7.8829e-06, -1.4288e-05, -4.1691e-06,  2.7221e-06, -1.0011e-05],
       device='cuda:0')
I am [7, 0]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 184, 184])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 182, 182])
padding info :: [3, 0, 0, 3]
grad_input torch.Size([1, 3, 181, 181])
grad_out size torch.Size([1, 3, 180, 180])
crop [0, 20, 20, 0] [0, 179, 1100, 1279] [0, 159, 1120, 1279]
## 20 180 0 160
##############return grad_in in conv2d tensor([ 2.1426e-08,  3.3566e-06,  4.0665e-07, -1.1208e-06,  4.0396e-06,
         3.6286e-06,  1.3987e-06, -5.9870e-06, -3.8035e-06, -9.3448e-06],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 362, 362])
grad_out size torch.Size([1, 3, 181, 181])
arg size torch.Size([1, 3, 181, 181])
##############grad_in in maxp torch.Size([1, 3, 362, 362])
I am [7, 0]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 364, 364])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 362, 362])
grad_input torch.Size([1, 3, 364, 364])
padding info :: [1, 0, 0, 1]
grad_input old torch.Size([1, 3, 364, 364])
grad_input new torch.Size([1, 3, 364, 364])
new grad_input torch.Size([1, 3, 364, 364])
grad_out size torch.Size([1, 3, 362, 362])
crop [0, 42, 42, 0] [0, 361, 2198, 2559] [0, 319, 2240, 2559]
## 42 362 0 320
##############return grad_in in conv2d tensor([ 0.0000e+00,  2.9434e-07, -3.9085e-07, -1.2188e-08, -2.8484e-07,
         1.0440e-07, -1.4143e-07, -2.3463e-07,  6.5666e-07, -5.4115e-07],
       device='cuda:0')
I am [7, 0]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 366, 366])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 364, 364])
padding info :: [2, 0, 0, 2]
grad_input old torch.Size([1, 3, 366, 366])
grad_input new torch.Size([1, 3, 366, 366])
grad_input torch.Size([1, 3, 366, 366])
grad_out size torch.Size([1, 3, 363, 363])
crop [0, 43, 43, 0] [0, 362, 2197, 2559] [0, 319, 2240, 2559]
## 43 363 0 320
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00, -1.2652e-07, -6.2836e-08,  1.7646e-07,
        -1.3074e-07, -4.7203e-09, -4.6641e-08, -1.3240e-07, -3.4844e-07],
       device='cuda:0')
I am [7, 0]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 368, 368])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 366, 366])
padding info :: [3, 0, 0, 3]
grad_input torch.Size([1, 3, 365, 365])
grad_out size torch.Size([1, 3, 364, 364])
crop [0, 44, 44, 0] [0, 363, 2196, 2559] [0, 319, 2240, 2559]
## 44 364 0 320
##############return grad_in in conv2d tensor([ 2.9077e-08,  8.7786e-09,  3.5644e-08,  1.1057e-09,  4.3955e-09,
        -1.6028e-09,  8.7840e-08,  6.3349e-08,  1.8436e-08,  7.9165e-08],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 730, 730])
grad_out size torch.Size([1, 3, 365, 365])
arg size torch.Size([1, 3, 365, 365])
##############grad_in in maxp torch.Size([1, 3, 730, 730])
I am [7, 0]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 732, 732])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 730, 730])
grad_input torch.Size([1, 3, 732, 732])
padding info :: [1, 0, 0, 1]
grad_input old torch.Size([1, 3, 732, 732])
grad_input new torch.Size([1, 3, 732, 732])
new grad_input torch.Size([1, 3, 732, 732])
grad_out size torch.Size([1, 3, 730, 730])
crop [0, 90, 90, 0] [0, 729, 4390, 5119] [0, 639, 4480, 5119]
## 90 730 0 640
##############return grad_in in conv2d tensor([ 0.0000e+00,  5.4456e-09,  4.2389e-09,  3.8429e-09,  2.3946e-09,
         1.0275e-08,  2.2786e-09, -5.1002e-09,  1.6120e-10, -1.6464e-10],
       device='cuda:0')
I am [7, 0]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 734, 734])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 732, 732])
padding info :: [2, 0, 0, 2]
grad_input torch.Size([1, 3, 732, 732])
grad_out size torch.Size([1, 3, 731, 731])
crop [0, 91, 91, 0] [0, 730, 4389, 5119] [0, 639, 4480, 5119]
## 91 731 0 640
##############return grad_in in conv2d tensor([ 1.4240e-10,  2.3582e-09,  2.4123e-09,  6.0655e-10, -3.6160e-10,
         1.6218e-09, -2.1561e-10, -1.0510e-09, -4.3385e-13, -9.1630e-10],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 1464, 1464])
grad_out size torch.Size([1, 3, 732, 732])
arg size torch.Size([1, 3, 732, 732])
##############grad_in in maxp torch.Size([1, 3, 1464, 1464])
I am [7, 0]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 1466, 1466])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1464, 1464])
grad_input torch.Size([1, 3, 1466, 1466])
padding info :: [1, 0, 0, 1]
grad_input old torch.Size([1, 3, 1466, 1466])
grad_input new torch.Size([1, 3, 1466, 1466])
new grad_input torch.Size([1, 3, 1466, 1466])
grad_out size torch.Size([1, 3, 1464, 1464])
crop [0, 184, 184, 0] [0, 1463, 8776, 10239] [0, 1279, 8960, 10239]
## 184 1464 0 1280
##############return grad_in in conv2d tensor([ 0.0000e+00, -5.4619e-12, -1.0071e-10,  3.7785e-11, -2.1912e-10,
        -5.2220e-10,  1.6279e-10,  1.4086e-10,  0.0000e+00,  0.0000e+00],
       device='cuda:0')
I am [7, 0]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 3, 1468, 1468])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1466, 1466])
final torch.Size([1, 3, 1468, 1468])
padding info :: [2, 0, 0, 2]
reshape_for_final ## 186 1466 0 1280
after crop g_in torch.Size([1, 3, 1280, 1280])
grad_out size torch.Size([1, 3, 1465, 1465])
crop [0, 185, 185, 0] [0, 1464, 8775, 10239] [0, 1279, 8960, 10239]
## 185 1465 0 1280
##############return grad_in in conv2d tensor([ 1.1660e-08,  1.2479e-07, -1.1658e-07, -3.3896e-07,  2.4970e-07,
        -1.2733e-07,  4.4390e-07,  2.3592e-07, -2.1446e-08,  4.0968e-08],
       device='cuda:0')
[8, 8]
HREREERE torch.Size([1, 3, 10240, 10240])

############# Enter checkpointing bkward ####
inputs len 5 4
in customized Sequential 5
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 7]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1466])
myctx [[ 140046822727744[ PI( <6,7,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 7]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1464])
myctx [[ 140045015171856[ PI( <6,7,>,

in customized Sequential[torch/csrc/autograd/python_engine.cpp] THPEngine_run_backward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 7]
shape input_tile_for_next
 torch.Size([1, 3, 822, 732])
myctx [[ 140045013800704[ PI( <6,7,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 7]
shape input_tile_for_next
 torch.Size([1, 3, 820, 730])
myctx [[ 140045013848944[ PI( <6,7,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 7]
shape input_tile_for_next
 torch.Size([1, 3, 408, 366])
myctx [[ 140045013206640[ PI( <6,7,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 7]
shape input_tile_for_next
 torch.Size([1, 3, 406, 364])
myctx [[ 140045013206784[ PI( <6,7,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 7]
shape input_tile_for_next
 torch.Size([1, 3, 404, 362])
myctx [[ 140045013206880[ PI( <6,7,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 7]
shape input_tile_for_next
 torch.Size([1, 3, 200, 182])
myctx [[ 140045013206352[ PI( <6,7,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 7]
shape input_tile_for_next
 torch.Size([1, 3, 198, 180])
myctx [[ 140045013206304[ PI( <6,7,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 7]
shape input_tile_for_next
 torch.Size([1, 3, 196, 178])
myctx [[ 140045013207072[ PI( <6,7,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 7]
shape input_tile_for_next
 torch.Size([1, 3, 96, 90])
myctx [[ 140045013206400[ PI( <6,7,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 7]
shape input_tile_for_next
 torch.Size([1, 3, 94, 88])
myctx [[ 140045013206448[ PI( <6,7,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 7]
shape input_tile_for_next
 torch.Size([1, 3, 92, 86])
myctx [[ 140045013206976[ PI( <6,7,>,

in customized Sequential 3
need to get existing

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 92, 86])
grad_out size torch.Size([1, 3, 320, 320])
arg size torch.Size([1, 3, 46, 43])
new_grad_out torch.Size([1, 3, 46, 43])
##############grad_in in maxp torch.Size([1, 3, 92, 86])
I am [6, 7]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 94, 88])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 92, 86])
grad_input torch.Size([1, 3, 94, 88])
padding info :: [0, 1, 0, 0]
grad_input old torch.Size([1, 3, 94, 88])
grad_input new torch.Size([1, 3, 94, 88])
new grad_input torch.Size([1, 3, 94, 88])
grad_out size torch.Size([1, 3, 92, 86])
crop [6, 0, 6, 6] [554, 639, 474, 565] [560, 639, 480, 559]
## 6 86 6 86
##############return grad_in in conv2d tensor([-0.0014,  0.0021, -0.0006,  0.0003,  0.0002,  0.0010,  0.0029, -0.0012,
         0.0034, -0.0014], device='cuda:0')
I am [6, 7]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 96, 90])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 94, 88])
padding info :: [0, 2, 0, 0]
grad_input old torch.Size([1, 3, 96, 90])
grad_input new torch.Size([1, 3, 96, 90])
grad_input torch.Size([1, 3, 96, 90])
grad_out size torch.Size([1, 3, 94, 87])
crop [7, 0, 7, 7] [553, 639, 473, 566] [560, 639, 480, 559]
## 7 87 7 87
##############return grad_in in conv2d tensor([-0.0003, -0.0002, -0.0003, -0.0006,  0.0005,  0.0004, -0.0004, -0.0003,
        -0.0005, -0.0005], device='cuda:0')
I am [6, 7]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 98, 92])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 96, 90])
padding info :: [0, 3, 0, 0]
grad_input torch.Size([1, 3, 98, 89])
grad_out size torch.Size([1, 3, 96, 88])
crop [8, 0, 8, 8] [552, 639, 472, 567] [560, 639, 480, 559]
## 8 88 8 88
##############return grad_in in conv2d tensor([ 6.4222e-06, -7.5920e-05,  1.0790e-04, -1.8411e-04,  4.3621e-05,
         2.4599e-04, -2.0799e-04, -1.4789e-04, -1.1712e-05,  2.7359e-04],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 196, 178])
grad_out size torch.Size([1, 3, 98, 89])
arg size torch.Size([1, 3, 98, 89])
##############grad_in in maxp torch.Size([1, 3, 196, 178])
I am [6, 7]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 198, 180])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 196, 178])
grad_input torch.Size([1, 3, 198, 180])
padding info :: [0, 1, 0, 0]
grad_input old torch.Size([1, 3, 198, 180])
grad_input new torch.Size([1, 3, 198, 180])
new grad_input torch.Size([1, 3, 198, 180])
grad_out size torch.Size([1, 3, 196, 178])
crop [18, 0, 18, 18] [1102, 1279, 942, 1137] [1120, 1279, 960, 1119]
## 18 178 18 178
##############return grad_in in conv2d [torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cCheckpointBackward
tensor([ 0.0000e+00,  6.8815e-06, -3.3781e-06, -2.2293e-05,  7.3464e-06,
         2.3924e-05, -1.0935e-05,  7.1584e-08,  2.7739e-06, -4.5997e-06],
       device='cuda:0')
I am [6, 7]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 200, 182])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 198, 180])
padding info :: [0, 2, 0, 0]
grad_input old torch.Size([1, 3, 200, 182])
grad_input new torch.Size([1, 3, 200, 182])
grad_input torch.Size([1, 3, 200, 182])
grad_out size torch.Size([1, 3, 198, 179])
crop [19, 0, 19, 19] [1101, 1279, 941, 1138] [1120, 1279, 960, 1119]
## 19 179 19 179
##############return grad_in in conv2d tensor([ 0.0000e+00,  1.9981e-06, -2.1610e-06, -4.3389e-06, -5.3274e-07,
        -6.0076e-07,  3.4934e-08,  2.2026e-06,  1.1937e-06,  9.2290e-09],
       device='cuda:0')
I am [6, 7]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 202, 184])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 200, 182])
padding info :: [0, 3, 0, 0]
grad_input torch.Size([1, 3, 202, 181])
grad_out size torch.Size([1, 3, 200, 180])
crop [20, 0, 20, 20] [1100, 1279, 940, 1139] [1120, 1279, 960, 1119]
## 20 180 20 180
##############return grad_in in conv2d tensor([ 0.0000e+00, -8.8762e-08, -2.7926e-07,  3.5997e-07,  1.8845e-06,
        -3.6075e-07, -2.0248e-06,  1.4092e-06, -6.8232e-07, -1.4053e-06],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 404, 362])
grad_out size torch.Size([1, 3, 202, 181])
arg size torch.Size([1, 3, 202, 181])
##############grad_in in maxp torch.Size([1, 3, 404, 362])
I am [6, 7]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 406, 364])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 404, 362])
grad_input torch.Size([1, 3, 406, 364])
padding info :: [0, 1, 0, 0]
grad_input old torch.Size([1, 3, 406, 364])
grad_input new torch.Size([1, 3, 406, 364])
new grad_input torch.Size([1, 3, 406, 364])
grad_out size torch.Size([1, 3, 404, 362])
crop [42, 0, 42, 42] [2198, 2559, 1878, 2281] [2240, 2559, 1920, 2239]
## 42 362 42 362
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00, -8.3063e-09,  1.7029e-08, -8.7247e-08,
        -1.8390e-08,  1.4105e-07, -4.6116e-08, -1.1285e-07,  0.0000e+00],
       device='cuda:0')
I am [6, 7]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 408, 366])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 406, 364])
padding info :: [0, 2, 0, 0]
grad_input old torch.Size([1, 3, 408, 366])
grad_input new torch.Size([1, 3, 408, 366])
grad_input torch.Size([1, 3, 408, 366])
grad_out size torch.Size([1, 3, 406, 363])
crop [43, 0, 43, 43] [2197, 2559, 1877, 2282] [2240, 2559, 1920, 2239]
## 43 363 43 363
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00, -3.3005e-09, -2.1405e-09,  5.3761e-09,
        -2.6013e-08, -1.6075e-08, -1.9076e-08,  4.6194e-08,  2.5114e-08],
       device='cuda:0')
I am [6, 7]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 410, 368])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 408, 366])
padding info :: [0, 3, 0, 0]
grad_input torch.Size([1, 3, 410, 365])
grad_out size torch.Size([1, 3, 408, 364])
crop [44, 0, 44, 44] [2196, 2559, 1876, 2283] [2240, 2559, 1920, 2239]
## 44 364 44 364
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00, -3.5421e-10,  9.9459e-10, -1.5496e-09,
         3.6373e-09,  7.0430e-09,  1.2799e-08, -1.4877e-10, -6.8628e-09],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 820, 730])
grad_out size torch.Size([1, 3, 410, 365])
arg size torch.Size([1, 3, 410, 365])
##############grad_in in maxp torch.Size([1, 3, 820, 730])
I am [6, 7]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 822, 732])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 820, 730])
grad_input torch.Size([1, 3, 822, 732])
padding info :: [0, 1, 0, 0]
grad_input old torch.Size([1, 3, 822, 732])
grad_input new torch.Size([1, 3, 822, 732])
new grad_input torch.Size([1, 3, 822, 732])
grad_out size torch.Size([1, 3, 820, 730])
crop [90, 0, 90, 90] [4390, 5119, 3750, 4569] [4480, 5119, 3840, 4479]
## 90 730 90 730
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.2653e-10,
        -1.3085e-10, -5.3574e-11,  2.3901e-10,  1.4499e-10, -1.4809e-10],
       device='cuda:0')
I am [6, 7]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 824, 734])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 822, 732])
padding info :: [0, 2, 0, 0]
grad_input torch.Size([1, 3, 824, 732])
grad_out size torch.Size([1, 3, 822, 731])
crop [91, 0, 91, 91] [4389, 5119, 3749, 4570] [4480, 5119, 3840, 4479]
## 91 731 91 731
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.7764e-11,
        -1.2698e-11,  1.8253e-11, -1.4022e-11, -4.3030e-12,  5.6913e-11],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 1648, 1464])
grad_out size torch.Size([1, 3, 824, 732])
arg size torch.Size([1, 3, 824, 732])
##############grad_in in maxp torch.Size([1, 3, 1648, 1464])
I am [6, 7]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 1650, 1466])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1648, 1464])
grad_input torch.Size([1, 3, 1650, 1466])
padding info :: [0, 1, 0, 0]
grad_input old torch.Size([1, 3, 1650, 1466])
grad_input new torch.Size([1, 3, 1650, 1466])
new grad_input torch.Size([1, 3, 1650, 1466])
grad_out size torch.Size([1, 3, 1648, 1464])
crop [184, 0, 184, 184] [8776, 10239, 7496, 9143] [8960, 10239, 7680, 8959]
## 184 1464 184 1464
##############return grad_in in conv2d tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 6.8138e-13], device='cuda:0')
I am [6, 7]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 3, 1652, 1468])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1650, 1466])
final torch.Size([1, 3, 1652, 1468])
padding info :: [0, 2, 0, 0]
reshape_for_final ## 186 1466 186 1466
after crop g_in torch.Size([1, 3, 1280, 1280])
grad_out size torch.Size([1, 3, 1650, 1465])
crop [185, 0, 185, 185] [8775, 10239, 7495, 9144] [8960, 10239, 7680, 8959]
## 185 1465 185 1465
##############return grad_in in conv2d tensor([ 1.1520e-07,  2.0572e-07, -3.8814e-07,  2.8931e-07,  7.8377e-08,
        -8.7626e-08, -2.9120e-09, -9.7674e-08,  2.0237e-07,  4.9117e-08],
       device='cuda:0')
[8, 8]
HREREERE torch.Size([1, 3, 10240, 10240])

############# Enter checkpointing bkward ####
inputs len 5 4
in customized Sequential 5
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 6]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <6,6,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 6]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <6,6,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
[torch/csrc/autograd/python_engine.cpp] THPEngine_run_backward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
need to get existing
current fwd info [6, 6]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <6,6,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 6]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <6,6,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 6]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <6,6,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 6]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <6,6,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 6]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <6,6,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 6]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <6,6,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 6]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <6,6,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 6]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <6,6,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 6]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <6,6,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 6]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <6,6,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 6]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <6,6,>,

in customized Sequential 3
need to get existing

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 92, 92])
grad_out size torch.Size([1, 3, 320, 320])
arg size torch.Size([1, 3, 46, 46])
new_grad_out torch.Size([1, 3, 46, 46])
##############grad_in in maxp torch.Size([1, 3, 92, 92])
I am [6, 6]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 94, 94])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 92, 92])
grad_input torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 94, 94])
grad_input new torch.Size([1, 3, 94, 94])
new grad_input torch.Size([1, 3, 94, 94])
grad_out size torch.Size([1, 3, 92, 92])
crop [6, 6, 6, 6] [474, 565, 474, 565] [480, 559, 480, 559]
## 6 86 6 86
##############return grad_in in conv2d tensor([-2.1279e-03,  3.5775e-03, -3.4320e-03,  7.4527e-05,  5.7855e-03,
        -2.8451e-03, -2.1835e-03,  1.0899e-03, -3.5344e-04, -5.7152e-04],
       device='cuda:0')
I am [6, 6]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 96, 96])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 96, 96])
grad_input new torch.Size([1, 3, 96, 96])
grad_input torch.Size([1, 3, 96, 96])
grad_out size torch.Size([1, 3, 94, 94])
crop [7, 7, 7, 7] [473, 566, 473, 566] [480, 559, 480, 559]
## 7 87 7 87
##############return grad_in in conv2d tensor([-3.9536e-04, -9.7665e-04,  2.6902e-04, -1.9139e-03,  4.1678e-05,
        -2.9197e-04, -2.3055e-03,  5.4224e-04, -7.5594e-04,  2.4411e-05],
       device='cuda:0')
I am [6, 6]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 98, 98])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 96, 96])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 98, 98])
grad_out size torch.Size([1, 3, 96, 96])
crop [8, 8, 8, 8] [472, 567, 472, 567] [480, 559, 480, 559]
## 8 88 8 88
##############return grad_in in conv2d tensor([ 9.5397e-06, -2.6039e-04,  4.0105e-04, -1.7681e-04, -4.3103e-04,
         6.1850e-04, -3.4471e-04, -1.3944e-04,  4.1071e-04, -1.0385e-04],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 196, 196])
grad_out size torch.Size([1, 3, 98, 98])
arg size torch.Size([1, 3, 98, 98])
##############grad_in in maxp torch.Size([1, 3, 196, 196])
I am [6, 6]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 198, 198])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 196, 196])
grad_input torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 198, 198])
grad_input new torch.Size([1, 3, 198, 198])
new grad_input torch.Size([1, 3, 198, 198])
grad_out size torch.Size([1, 3, 196, 196])
crop [18, 18, 18, 18] [942, 1137, 942, 1137] [960, 1119, 960, 1119]
## 18 178 18 178
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00, -4.5480e-05,  8.0625e-05,
        -1.7233e-05, -6.6358e-05,  1.9589e-05,  2.0872e-05, -2.9069e-05],
       device='cuda:0')
I am [6, 6]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 200, 200])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 198, 198])
padding info :: [torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cCheckpointBackward
[0, 0, 0, 0]
grad_input old torch.Size([1, 3, 200, 200])
grad_input new torch.Size([1, 3, 200, 200])
grad_input torch.Size([1, 3, 200, 200])
grad_out size torch.Size([1, 3, 198, 198])
crop [19, 19, 19, 19] [941, 1138, 941, 1138] [960, 1119, 960, 1119]
## 19 179 19 179
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00, -1.3251e-05,  1.9172e-05,
        -3.4371e-05,  1.0334e-05, -1.3624e-05, -1.6159e-05, -1.2787e-05],
       device='cuda:0')
I am [6, 6]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 202, 202])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 200, 200])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 202, 202])
grad_out size torch.Size([1, 3, 200, 200])
crop [20, 20, 20, 20] [940, 1139, 940, 1139] [960, 1119, 960, 1119]
## 20 180 20 180
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0413e-06,  2.5879e-06,
        -2.1207e-06, -2.6285e-06,  8.8418e-06,  4.4304e-07, -2.8082e-06],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 404, 404])
grad_out size torch.Size([1, 3, 202, 202])
arg size torch.Size([1, 3, 202, 202])
##############grad_in in maxp torch.Size([1, 3, 404, 404])
I am [6, 6]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 406, 406])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 404, 404])
grad_input torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 406, 406])
grad_input new torch.Size([1, 3, 406, 406])
new grad_input torch.Size([1, 3, 406, 406])
grad_out size torch.Size([1, 3, 404, 404])
crop [42, 42, 42, 42] [1878, 2281, 1878, 2281] [1920, 2239, 1920, 2239]
## 42 362 42 362
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  3.1478e-07, -4.6828e-07, -5.5450e-08,  0.0000e+00],
       device='cuda:0')
I am [6, 6]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 408, 408])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 408, 408])
grad_input new torch.Size([1, 3, 408, 408])
grad_input torch.Size([1, 3, 408, 408])
grad_out size torch.Size([1, 3, 406, 406])
crop [43, 43, 43, 43] [1877, 2282, 1877, 2282] [1920, 2239, 1920, 2239]
## 43 363 43 363
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00, -1.2282e-08, -1.1153e-07, -1.9751e-08,  9.5819e-08],
       device='cuda:0')
I am [6, 6]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 410, 410])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 408, 408])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 410, 410])
grad_out size torch.Size([1, 3, 408, 408])
crop [44, 44, 44, 44] [1876, 2283, 1876, 2283] [1920, 2239, 1920, 2239]
## 44 364 44 364
##############return grad_in in conv2d tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        1.0738e-09, 1.8355e-08, 2.3376e-08, 1.1995e-08], device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 820, 820])
grad_out size torch.Size([1, 3, 410, 410])
arg size torch.Size([1, 3, 410, 410])
##############grad_in in maxp torch.Size([1, 3, 820, 820])
I am [6, 6]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 822, 822])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 820, 820])
grad_input torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 822, 822])
grad_input new torch.Size([1, 3, 822, 822])
new grad_input torch.Size([1, 3, 822, 822])
grad_out size torch.Size([1, 3, 820, 820])
crop [90, 90, 90, 90] [3750, 4569, 3750, 4569] [3840, 4479, 3840, 4479]
## 90 730 90 730
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [6, 6]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 824, 824])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 824, 824])
grad_out size torch.Size([1, 3, 822, 822])
crop [91, 91, 91, 91] [3749, 4570, 3749, 4570] [3840, 4479, 3840, 4479]
## 91 731 91 731
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 1648, 1648])
grad_out size torch.Size([1, 3, 824, 824])
arg size torch.Size([1, 3, 824, 824])
##############grad_in in maxp torch.Size([1, 3, 1648, 1648])
I am [6, 6]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 1650, 1650])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1648, 1648])
grad_input torch.Size([1, 3, 1650, 1650])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 1650, 1650])
grad_input new torch.Size([1, 3, 1650, 1650])
new grad_input torch.Size([1, 3, 1650, 1650])
grad_out size torch.Size([1, 3, 1648, 1648])
crop [184, 184, 184, 184] [7496, 9143, 7496, 9143] [7680, 8959, 7680, 8959]
## 184 1464 184 1464
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [6, 6]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 3, 1652, 1652])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1650, 1650])
final torch.Size([1, 3, 1652, 1652])
padding info :: [0, 0, 0, 0]
reshape_for_final ## 186 1466 186 1466
after crop g_in torch.Size([1, 3, 1280, 1280])
grad_out size torch.Size([1, 3, 1650, 1650])
crop [185, 185, 185, 185] [7495, 9144, 7495, 9144] [7680, 8959, 7680, 8959]
## 185 1465 185 1465
##############return grad_in in conv2d tensor([ 6.1222e-07,  7.5930e-08, -7.5337e-08,  4.5533e-07,  2.4756e-07,
         1.0849e-07, -1.2507e-07, -2.8904e-07,  1.7166e-07,  1.2000e-07],
       device='cuda:0')
[8, 8]
HREREERE torch.Size([1, 3, 10240, 10240])

############# Enter checkpointing bkward ####
inputs len 5 4
in customized Sequential 5
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 5]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <6,5,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 5]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <6,5,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 5]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <6,5,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 5]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <6,5,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id [torch/csrc/autograd/python_engine.cpp] THPEngine_run_backward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 5]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <6,5,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 5]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <6,5,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 5]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <6,5,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 5]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <6,5,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 5]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <6,5,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 5]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <6,5,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 5]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <6,5,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 5]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <6,5,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 5]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <6,5,>,

in customized Sequential 3
need to get existing

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 92, 92])
grad_out size torch.Size([1, 3, 320, 320])
arg size torch.Size([1, 3, 46, 46])
new_grad_out torch.Size([1, 3, 46, 46])
##############grad_in in maxp torch.Size([1, 3, 92, 92])
I am [6, 5]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 94, 94])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 92, 92])
grad_input torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 94, 94])
grad_input new torch.Size([1, 3, 94, 94])
new grad_input torch.Size([1, 3, 94, 94])
grad_out size torch.Size([1, 3, 92, 92])
crop [6, 6, 6, 6] [394, 485, 474, 565] [400, 479, 480, 559]
## 6 86 6 86
##############return grad_in in conv2d tensor([ 0.0015, -0.0022, -0.0025,  0.0058,  0.0012,  0.0009,  0.0006, -0.0018,
         0.0013, -0.0026], device='cuda:0')
I am [6, 5]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 96, 96])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 96, 96])
grad_input new torch.Size([1, 3, 96, 96])
grad_input torch.Size([1, 3, 96, 96])
grad_out size torch.Size([1, 3, 94, 94])
crop [7, 7, 7, 7] [393, 486, 473, 566] [400, 479, 480, 559]
## 7 87 7 87
##############return grad_in in conv2d tensor([ 2.8074e-04,  1.5872e-04, -6.4104e-05,  1.3572e-04, -3.9995e-04,
        -2.6753e-03,  4.1540e-04, -2.2958e-04,  5.8189e-04,  5.5825e-04],
       device='cuda:0')
I am [6, 5]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 98, 98])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 96, 96])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 98, 98])
grad_out size torch.Size([1, 3, 96, 96])
crop [8, 8, 8, 8] [392, 487, 472, 567] [400, 479, 480, 559]
## 8 88 8 88
##############return grad_in in conv2d tensor([-6.7741e-06,  8.0081e-05, -7.7005e-05, -5.8478e-05,  3.0055e-04,
        -5.8559e-04, -2.5327e-04,  5.9880e-04,  1.3941e-05,  2.3794e-04],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 196, 196])
grad_out size torch.Size([1, 3, 98, 98])
arg size torch.Size([1, 3, 98, 98])
##############grad_in in maxp torch.Size([1, 3, 196, 196])
I am [6, 5]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 198, 198])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 196, 196])
grad_input torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 198, 198])
grad_input new torch.Size([1, 3, 198, 198])
new grad_input torch.Size([1, 3, 198, 198])
grad_out size torch.Size([1, 3, 196, 196])
crop [18, 18, 18, 18] [782, 977, 942, 1137] [800, 959, 960, 1119]
## 18 178 18 178
##############return grad_in in conv2d tensor([ 0.0000e+00, -4.0668e-07,  2.4093e-06, -6.2553e-07, -5.5356e-06,
         2.0982e-05, -3.0517e-05,  2.7868e-08,  6.7699e-06,  5.6793e-05],
       device='cuda:0')
I am [6, 5]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 200, 200])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 200, 200])
grad_input new torch.Size([1, 3, 200, 200])
grad_input torch.Size([1, 3, 200, 200])
grad_out size torch.Size([1, 3, 198, 198])
crop [19, 19, 19, 19] [781, 978, 941, 1138] [800, 959, 960, 1119]
## 19 179 19 179
##############return grad_in in conv2d tensor([ 0.0000e+00, -1.1022e-07,  6.1293e-07, -8.2496e-07, -5.3190e-07,
         4.7069e-06, -1.5474e-05,  7.7115e-06, -9.7535e-06,  1.5038e-05],
       device='cuda:0')
I am [6, 5]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 202, 202])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 200, 200])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 202, 202])
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cCheckpointBackward
grad_out size torch.Size([1, 3, 200, 200])
crop [20, 20, 20, 20] [780, 979, 940, 1139] [800, 959, 960, 1119]
## 20 180 20 180
##############return grad_in in conv2d tensor([ 0.0000e+00,  8.5189e-09,  1.1222e-08, -7.2712e-08, -1.0198e-08,
         3.5190e-07,  9.5319e-08,  6.2197e-07,  1.1115e-06, -1.5980e-06],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 404, 404])
grad_out size torch.Size([1, 3, 202, 202])
arg size torch.Size([1, 3, 202, 202])
##############grad_in in maxp torch.Size([1, 3, 404, 404])
I am [6, 5]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 406, 406])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 404, 404])
grad_input torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 406, 406])
grad_input new torch.Size([1, 3, 406, 406])
new grad_input torch.Size([1, 3, 406, 406])
grad_out size torch.Size([1, 3, 404, 404])
crop [42, 42, 42, 42] [1558, 1961, 1878, 2281] [1600, 1919, 1920, 2239]
## 42 362 42 362
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  2.2144e-09, -2.3111e-09, -1.1986e-08,
         1.8770e-08, -1.6052e-08, -6.8274e-09,  4.3186e-09,  3.6774e-09],
       device='cuda:0')
I am [6, 5]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 408, 408])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 408, 408])
grad_input new torch.Size([1, 3, 408, 408])
grad_input torch.Size([1, 3, 408, 408])
grad_out size torch.Size([1, 3, 406, 406])
crop [43, 43, 43, 43] [1557, 1962, 1877, 2282] [1600, 1919, 1920, 2239]
## 43 363 43 363
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00, -2.9288e-10, -9.4625e-10,  4.5351e-10,
         3.3464e-09,  1.4041e-09, -7.5795e-09,  5.2459e-10, -3.4830e-09],
       device='cuda:0')
I am [6, 5]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 410, 410])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 408, 408])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 410, 410])
grad_out size torch.Size([1, 3, 408, 408])
crop [44, 44, 44, 44] [1556, 1963, 1876, 2283] [1600, 1919, 1920, 2239]
## 44 364 44 364
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00, -1.1010e-11,  2.2824e-10,  1.5312e-11,
        -9.1193e-11, -9.9068e-10,  7.8329e-11,  1.0950e-10,  1.7606e-09],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 820, 820])
grad_out size torch.Size([1, 3, 410, 410])
arg size torch.Size([1, 3, 410, 410])
##############grad_in in maxp torch.Size([1, 3, 820, 820])
I am [6, 5]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 822, 822])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 820, 820])
grad_input torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 822, 822])
grad_input new torch.Size([1, 3, 822, 822])
new grad_input torch.Size([1, 3, 822, 822])
grad_out size torch.Size([1, 3, 820, 820])
crop [90, 90, 90, 90] [3110, 3929, 3750, 4569] [3200, 3839, 3840, 4479]
## 90 730 90 730
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
        -7.7626e-12, -3.9579e-12,  4.2627e-11,  4.4062e-11, -2.5616e-11],
       device='cuda:0')
I am [6, 5]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 824, 824])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 824, 824])
grad_out size torch.Size([1, 3, 822, 822])
crop [91, 91, 91, 91] [3109, 3930, 3749, 4570] [3200, 3839, 3840, 4479]
## 91 731 91 731
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
        -1.0899e-12, -8.0373e-13,  9.6719e-13,  2.8456e-12,  1.1075e-11],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 1648, 1648])
grad_out size torch.Size([1, 3, 824, 824])
arg size torch.Size([1, 3, 824, 824])
##############grad_in in maxp torch.Size([1, 3, 1648, 1648])
I am [6, 5]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 1650, 1650])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1648, 1648])
grad_input torch.Size([1, 3, 1650, 1650])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 1650, 1650])
grad_input new torch.Size([1, 3, 1650, 1650])
new grad_input torch.Size([1, 3, 1650, 1650])
grad_out size torch.Size([1, 3, 1648, 1648])
crop [184, 184, 184, 184] [6216, 7863, 7496, 9143] [6400, 7679, 7680, 8959]
## 184 1464 184 1464
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [6, 5]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 3, 1652, 1652])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1650, 1650])
final torch.Size([1, 3, 1652, 1652])
padding info :: [0, 0, 0, 0]
reshape_for_final ## 186 1466 186 1466
after crop g_in torch.Size([1, 3, 1280, 1280])
grad_out size torch.Size([1, 3, 1650, 1650])
crop [185, 185, 185, 185] [6215, 7864, 7495, 9144] [6400, 7679, 7680, 8959]
## 185 1465 185 1465
##############return grad_in in conv2d tensor([ 1.2240e-07, -2.5986e-07, -3.8875e-07, -1.2129e-07,  4.4797e-08,
        -4.3660e-07,  7.7512e-07,  3.8570e-07,  3.1264e-07, -4.5956e-08],
       device='cuda:0')
[8, 8]
HREREERE torch.Size([1, 3, 10240, 10240])

############# Enter checkpointing bkward ####
inputs len 5 4
in customized Sequential 5
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 4]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <6,4,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 4]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <6,4,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 4]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <6,4,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 4]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <6,4,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 4]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <6,4,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
[torch/csrc/autograd/python_engine.cpp] THPEngine_run_backward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 4]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <6,4,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 4]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <6,4,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 4]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <6,4,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 4]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <6,4,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 4]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <6,4,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 4]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <6,4,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 4]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <6,4,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 4]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <6,4,>,

in customized Sequential 3
need to get existing

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 92, 92])
grad_out size torch.Size([1, 3, 320, 320])
arg size torch.Size([1, 3, 46, 46])
new_grad_out torch.Size([1, 3, 46, 46])
##############grad_in in maxp torch.Size([1, 3, 92, 92])
I am [6, 4]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 94, 94])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 92, 92])
grad_input torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 94, 94])
grad_input new torch.Size([1, 3, 94, 94])
new grad_input torch.Size([1, 3, 94, 94])
grad_out size torch.Size([1, 3, 92, 92])
crop [6, 6, 6, 6] [314, 405, 474, 565] [320, 399, 480, 559]
## 6 86 6 86
##############return grad_in in conv2d tensor([-0.0021,  0.0032,  0.0004, -0.0001,  0.0000, -0.0017, -0.0046,  0.0024,
         0.0029, -0.0007], device='cuda:0')
I am [6, 4]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 96, 96])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 96, 96])
grad_input new torch.Size([1, 3, 96, 96])
grad_input torch.Size([1, 3, 96, 96])
grad_out size torch.Size([1, 3, 94, 94])
crop [7, 7, 7, 7] [313, 406, 473, 566] [320, 399, 480, 559]
## 7 87 7 87
##############return grad_in in conv2d tensor([-0.0004, -0.0001, -0.0007, -0.0008,  0.0002, -0.0010,  0.0013,  0.0002,
        -0.0004,  0.0003], device='cuda:0')
I am [6, 4]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 98, 98])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 96, 96])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 98, 98])
grad_out size torch.Size([1, 3, 96, 96])
crop [8, 8, 8, 8] [312, 407, 472, 567] [320, 399, 480, 559]
## 8 88 8 88
##############return grad_in in conv2d tensor([ 9.3083e-06, -9.6504e-05,  9.4538e-05, -2.2392e-04,  3.1749e-05,
         4.2910e-05,  3.0825e-04,  3.0483e-04, -5.0258e-04, -1.7159e-04],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 196, 196])
grad_out size torch.Size([1, 3, 98, 98])
arg size torch.Size([1, 3, 98, 98])
##############grad_in in maxp torch.Size([1, 3, 196, 196])
I am [6, 4]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 198, 198])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 196, 196])
grad_input torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 198, 198])
grad_input new torch.Size([1, 3, 198, 198])
new grad_input torch.Size([1, 3, 198, 198])
grad_out size torch.Size([1, 3, 196, 196])
crop [18, 18, 18, 18] [622, 817, 942, 1137] [640, 799, 960, 1119]
## 18 178 18 178
##############return grad_in in conv2d tensor([-8.0071e-07, -5.4907e-06, -5.4919e-06,  0.0000e+00,  3.2139e-05,
        -1.0872e-05, -1.6704e-05, -1.5522e-05,  2.1845e-05,  1.0724e-05],
       device='cuda:0')
I am [6, 4]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 200, 200])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 200, 200])
grad_input new torch.Size([1, 3, 200, 200])
grad_input torch.Size([1, 3, 200, 200])
grad_out size torch.Size([1, 3, 198, 198])
crop [19, 19, 19, 19] [621, 818, 941, 1138] [640, 799, 960, 1119]
## 19 179 19 179
##############return grad_in in conv2d tensor([-3.7536e-07, -3.5303e-06, -5.1588e-07, -3.9671e-06,  7.2669e-06,
        -1.1432e-06,  6.4211e-06, -1.0139e-05,  2.7397e-06,  6.7740e-06],
       device='cuda:0')
I am [6, 4]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 202, 202])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 200, 200])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 202, 202])
grad_out size torch.Size([1, 3, 200, 200])
crop [20, 20, 20, 20] [620, 819, 940, 1139] [640, 799, 960, 1119]
## 20 180 20 180
##############return grad_in in conv2d tensor([ 1.3069e-07,  3.0496e-07, -4.1936e-08,  1.1183e-06,  7.4157e-09,
        -2.8977e-06,  3.0760e-07,  2.5805e-06,  1.7262e-07, -1.9865e-06],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 404, 404])
grad_out size torch.Size([1, 3, 202, 202])
arg size torch.Size([1, 3, 202, 202])
##############grad_in in maxp torch.Size([1, 3, 404, 404])
I am [6, 4]
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cCheckpointBackward
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 406, 406])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 404, 404])
grad_input torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 406, 406])
grad_input new torch.Size([1, 3, 406, 406])
new grad_input torch.Size([1, 3, 406, 406])
grad_out size torch.Size([1, 3, 404, 404])
crop [42, 42, 42, 42] [1238, 1641, 1878, 2281] [1280, 1599, 1920, 2239]
## 42 362 42 362
##############return grad_in in conv2d tensor([ 1.4419e-09, -5.1413e-09,  6.9649e-09,  0.0000e+00,  0.0000e+00,
        -6.3571e-08,  2.2494e-07,  1.7396e-07, -6.6419e-08, -1.0153e-07],
       device='cuda:0')
I am [6, 4]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 408, 408])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 408, 408])
grad_input new torch.Size([1, 3, 408, 408])
grad_input torch.Size([1, 3, 408, 408])
grad_out size torch.Size([1, 3, 406, 406])
crop [43, 43, 43, 43] [1237, 1642, 1877, 2282] [1280, 1599, 1920, 2239]
## 43 363 43 363
##############return grad_in in conv2d tensor([ 5.7293e-10, -1.8211e-11, -3.0076e-09,  1.3936e-09,  6.9376e-10,
         8.4079e-09,  5.5454e-08,  3.8054e-08,  2.2607e-08,  9.2000e-08],
       device='cuda:0')
I am [6, 4]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 410, 410])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 408, 408])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 410, 410])
grad_out size torch.Size([1, 3, 408, 408])
crop [44, 44, 44, 44] [1236, 1643, 1876, 2283] [1280, 1599, 1920, 2239]
## 44 364 44 364
##############return grad_in in conv2d tensor([ 6.1488e-11, -2.1796e-10,  2.6703e-10,  1.1120e-10,  2.6078e-10,
         2.9215e-10, -3.3230e-09, -1.3415e-08, -2.3694e-08, -3.5426e-08],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 820, 820])
grad_out size torch.Size([1, 3, 410, 410])
arg size torch.Size([1, 3, 410, 410])
##############grad_in in maxp torch.Size([1, 3, 820, 820])
I am [6, 4]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 822, 822])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 820, 820])
grad_input torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 822, 822])
grad_input new torch.Size([1, 3, 822, 822])
new grad_input torch.Size([1, 3, 822, 822])
grad_out size torch.Size([1, 3, 820, 820])
crop [90, 90, 90, 90] [2470, 3289, 3750, 4569] [2560, 3199, 3840, 4479]
## 90 730 90 730
##############return grad_in in conv2d tensor([ 0.0000e+00,  2.5331e-11,  1.5437e-11, -1.4880e-11, -3.1708e-11,
         8.2466e-11,  1.0032e-10,  1.8477e-11,  7.8763e-11,  3.2283e-11],
       device='cuda:0')
I am [6, 4]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 824, 824])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 824, 824])
grad_out size torch.Size([1, 3, 822, 822])
crop [91, 91, 91, 91] [2469, 3290, 3749, 4570] [2560, 3199, 3840, 4479]
## 91 731 91 731
##############return grad_in in conv2d tensor([ 0.0000e+00,  3.9752e-12,  1.0981e-11,  9.5073e-12,  9.3278e-13,
        -8.5640e-12,  1.1024e-11,  4.5576e-11,  2.2838e-11,  1.4008e-12],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 1648, 1648])
grad_out size torch.Size([1, 3, 824, 824])
arg size torch.Size([1, 3, 824, 824])
##############grad_in in maxp torch.Size([1, 3, 1648, 1648])
I am [6, 4]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 1650, 1650])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1648, 1648])
grad_input torch.Size([1, 3, 1650, 1650])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 1650, 1650])
grad_input new torch.Size([1, 3, 1650, 1650])
new grad_input torch.Size([1, 3, 1650, 1650])
grad_out size torch.Size([1, 3, 1648, 1648])
crop [184, 184, 184, 184] [4936, 6583, 7496, 9143] [5120, 6399, 7680, 8959]
## 184 1464 184 1464
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00, -1.5248e-13, -6.8664e-13,  2.1466e-13,
        -4.4383e-13, -6.3407e-13, -9.2899e-13, -4.3159e-13,  7.7170e-13],
       device='cuda:0')
I am [6, 4]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 3, 1652, 1652])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1650, 1650])
final torch.Size([1, 3, 1652, 1652])
padding info :: [0, 0, 0, 0]
reshape_for_final ## 186 1466 186 1466
after crop g_in torch.Size([1, 3, 1280, 1280])
grad_out size torch.Size([1, 3, 1650, 1650])
crop [185, 185, 185, 185] [4935, 6584, 7495, 9144] [5120, 6399, 7680, 8959]
## 185 1465 185 1465
##############return grad_in in conv2d tensor([ 5.0795e-07, -3.6534e-07,  4.0549e-07, -9.5008e-08,  2.9767e-07,
        -8.0022e-08,  6.9514e-08,  1.6763e-07, -1.5634e-07,  1.3922e-07],
       device='cuda:0')
[8, 8]
HREREERE torch.Size([1, 3, 10240, 10240])

############# Enter checkpointing bkward ####
inputs len 5 4
in customized Sequential 5
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 3]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <6,3,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 3]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <6,3,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 3]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <6,3,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 3]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <6,3,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 3]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <6,3,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 3]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <6,3,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
[torch/csrc/autograd/python_engine.cpp] THPEngine_run_backward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 3]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <6,3,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 3]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <6,3,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 3]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <6,3,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 3]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <6,3,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 3]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <6,3,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 3]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <6,3,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 3]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <6,3,>,

in customized Sequential 3
need to get existing

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 92, 92])
grad_out size torch.Size([1, 3, 320, 320])
arg size torch.Size([1, 3, 46, 46])
new_grad_out torch.Size([1, 3, 46, 46])
##############grad_in in maxp torch.Size([1, 3, 92, 92])
I am [6, 3]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 94, 94])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 92, 92])
grad_input torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 94, 94])
grad_input new torch.Size([1, 3, 94, 94])
new grad_input torch.Size([1, 3, 94, 94])
grad_out size torch.Size([1, 3, 92, 92])
crop [6, 6, 6, 6] [234, 325, 474, 565] [240, 319, 480, 559]
## 6 86 6 86
##############return grad_in in conv2d tensor([-3.0210e-03,  5.7181e-03,  1.1847e-04,  1.3438e-03, -3.9786e-04,
        -7.7615e-05, -5.9988e-05, -4.5305e-03,  7.5089e-03, -2.9290e-03],
       device='cuda:0')
I am [6, 3]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 96, 96])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 96, 96])
grad_input new torch.Size([1, 3, 96, 96])
grad_input torch.Size([1, 3, 96, 96])
grad_out size torch.Size([1, 3, 94, 94])
crop [7, 7, 7, 7] [233, 326, 473, 566] [240, 319, 480, 559]
## 7 87 7 87
##############return grad_in in conv2d tensor([-9.6580e-04, -4.1189e-04, -1.5159e-03, -2.0283e-03,  3.1365e-04,
        -1.6713e-04,  1.5333e-04,  6.0944e-05,  7.0413e-04,  1.4957e-03],
       device='cuda:0')
I am [6, 3]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 98, 98])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 96, 96])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 98, 98])
grad_out size torch.Size([1, 3, 96, 96])
crop [8, 8, 8, 8] [232, 327, 472, 567] [240, 319, 480, 559]
## 8 88 8 88
##############return grad_in in conv2d tensor([-2.8395e-05, -1.4936e-04,  2.1943e-04, -5.6759e-04,  7.1939e-05,
         4.4761e-04,  7.3357e-05,  1.3081e-04, -3.1259e-04,  4.4005e-04],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 196, 196])
grad_out size torch.Size([1, 3, 98, 98])
arg size torch.Size([1, 3, 98, 98])
##############grad_in in maxp torch.Size([1, 3, 196, 196])
I am [6, 3]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 198, 198])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 196, 196])
grad_input torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 198, 198])
grad_input new torch.Size([1, 3, 198, 198])
new grad_input torch.Size([1, 3, 198, 198])
grad_out size torch.Size([1, 3, 196, 196])
crop [18, 18, 18, 18] [462, 657, 942, 1137] [480, 639, 960, 1119]
## 18 178 18 178
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00, -8.9670e-06, -1.5173e-05,  1.2802e-06,
         8.6469e-06, -2.3138e-05, -4.4610e-05,  7.6924e-05, -3.6584e-05],
       device='cuda:0')
I am [6, 3]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 200, 200])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 200, 200])
grad_input new torch.Size([1, 3, 200, 200])
grad_input torch.Size([1, 3, 200, 200])
grad_out size torch.Size([1, 3, 198, 198])
crop [19, 19, 19, 19] [461, 658, 941, 1138] [480, 639, 960, 1119]
## 19 179 19 179
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00, -2.4303e-06, -6.7864e-06, -1.0258e-05,
         5.7436e-06, -1.3028e-05, -7.4243e-06,  6.8710e-06, -2.9463e-05],
       device='cuda:0')
I am [6, 3]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 202, 202])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 200, 200])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 202, 202])
grad_out size torch.Size([1, 3, 200, 200])
crop [20, 20, 20, 20] [460, 659, 940, 1139] [480, 639, 960, 1119]
## 20 180 20 180
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  1.8783e-07,  1.5735e-06,  2.2208e-06,
        -2.1498e-06,  3.9921e-07,  3.9610e-06,  2.2232e-06, -2.8041e-06],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 404, 404])
grad_out size torch.Size([1, 3, 202, 202])
arg size torch.Size([1, 3, 202, 202])
##############grad_in in maxp torch.Size([1, 3, 404, 404])
I am [6, 3]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 406, 406])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 404, 404])
grad_input torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 406, 406])
grad_input new torch.Size([1, 3, 406, 406])
new grad_input torch.Size([1, 3, 406, 406])
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cCheckpointBackward
grad_out size torch.Size([1, 3, 404, 404])
crop [42, 42, 42, 42] [918, 1321, 1878, 2281] [960, 1279, 1920, 2239]
## 42 362 42 362
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0441e-08,
        -1.0591e-08,  2.8949e-07,  1.4021e-07, -1.5781e-07,  2.5908e-07],
       device='cuda:0')
I am [6, 3]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 408, 408])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 408, 408])
grad_input new torch.Size([1, 3, 408, 408])
grad_input torch.Size([1, 3, 408, 408])
grad_out size torch.Size([1, 3, 406, 406])
crop [43, 43, 43, 43] [917, 1322, 1877, 2282] [960, 1279, 1920, 2239]
## 43 363 43 363
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  4.1487e-09,
         4.6198e-09,  2.8363e-08,  1.0940e-07,  5.8040e-08, -8.5750e-09],
       device='cuda:0')
I am [6, 3]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 410, 410])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 408, 408])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 410, 410])
grad_out size torch.Size([1, 3, 408, 408])
crop [44, 44, 44, 44] [916, 1323, 1876, 2283] [960, 1279, 1920, 2239]
## 44 364 44 364
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  4.4524e-10,
        -1.0260e-09,  4.8464e-09, -1.6440e-08, -3.6610e-08, -4.2067e-08],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 820, 820])
grad_out size torch.Size([1, 3, 410, 410])
arg size torch.Size([1, 3, 410, 410])
##############grad_in in maxp torch.Size([1, 3, 820, 820])
I am [6, 3]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 822, 822])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 820, 820])
grad_input torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 822, 822])
grad_input new torch.Size([1, 3, 822, 822])
new grad_input torch.Size([1, 3, 822, 822])
grad_out size torch.Size([1, 3, 820, 820])
crop [90, 90, 90, 90] [1830, 2649, 3750, 4569] [1920, 2559, 3840, 4479]
## 90 730 90 730
##############return grad_in in conv2d tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 8.3387e-11, 2.4833e-10], device='cuda:0')
I am [6, 3]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 824, 824])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 824, 824])
grad_out size torch.Size([1, 3, 822, 822])
crop [91, 91, 91, 91] [1829, 2650, 3749, 4570] [1920, 2559, 3840, 4479]
## 91 731 91 731
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00, -5.0547e-13,  3.0966e-11],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 1648, 1648])
grad_out size torch.Size([1, 3, 824, 824])
arg size torch.Size([1, 3, 824, 824])
##############grad_in in maxp torch.Size([1, 3, 1648, 1648])
I am [6, 3]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 1650, 1650])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1648, 1648])
grad_input torch.Size([1, 3, 1650, 1650])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 1650, 1650])
grad_input new torch.Size([1, 3, 1650, 1650])
new grad_input torch.Size([1, 3, 1650, 1650])
grad_out size torch.Size([1, 3, 1648, 1648])
crop [184, 184, 184, 184] [3656, 5303, 7496, 9143] [3840, 5119, 7680, 8959]
## 184 1464 184 1464
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [6, 3]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 3, 1652, 1652])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1650, 1650])
final torch.Size([1, 3, 1652, 1652])
padding info :: [0, 0, 0, 0]
reshape_for_final ## 186 1466 186 1466
after crop g_in torch.Size([1, 3, 1280, 1280])
grad_out size torch.Size([1, 3, 1650, 1650])
crop [185, 185, 185, 185] [3655, 5304, 7495, 9144] [3840, 5119, 7680, 8959]
## 185 1465 185 1465
##############return grad_in in conv2d tensor([-2.8743e-07, -6.0145e-07, -2.5089e-07,  2.7430e-07,  3.0175e-07,
         1.2715e-06,  9.5270e-07,  1.0476e-06, -6.8091e-07, -4.3866e-07],
       device='cuda:0')
[8, 8]
HREREERE torch.Size([1, 3, 10240, 10240])

############# Enter checkpointing bkward ####
inputs len 5 4
in customized Sequential 5
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 2]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <6,2,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 2]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <6,2,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 2]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <6,2,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 2]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <6,2,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 2]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <6,2,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 2]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <6,2,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 2]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <6,2,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
[torch/csrc/autograd/python_engine.cpp] THPEngine_run_backward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 2]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <6,2,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 2]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <6,2,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 2]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <6,2,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 2]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <6,2,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 2]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <6,2,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 2]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <6,2,>,

in customized Sequential 3
need to get existing

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 92, 92])
grad_out size torch.Size([1, 3, 320, 320])
arg size torch.Size([1, 3, 46, 46])
new_grad_out torch.Size([1, 3, 46, 46])
##############grad_in in maxp torch.Size([1, 3, 92, 92])
I am [6, 2]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 94, 94])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 92, 92])
grad_input torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 94, 94])
grad_input new torch.Size([1, 3, 94, 94])
new grad_input torch.Size([1, 3, 94, 94])
grad_out size torch.Size([1, 3, 92, 92])
crop [6, 6, 6, 6] [154, 245, 474, 565] [160, 239, 480, 559]
## 6 86 6 86
##############return grad_in in conv2d tensor([ 0.0000,  0.0032, -0.0004, -0.0014,  0.0009, -0.0019, -0.0014, -0.0015,
        -0.0044,  0.0017], device='cuda:0')
I am [6, 2]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 96, 96])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 96, 96])
grad_input new torch.Size([1, 3, 96, 96])
grad_input torch.Size([1, 3, 96, 96])
grad_out size torch.Size([1, 3, 94, 94])
crop [7, 7, 7, 7] [153, 246, 473, 566] [160, 239, 480, 559]
## 7 87 7 87
##############return grad_in in conv2d tensor([ 0.0000, -0.0001, -0.0002, -0.0015,  0.0003, -0.0011,  0.0003, -0.0003,
         0.0012,  0.0005], device='cuda:0')
I am [6, 2]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 98, 98])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 96, 96])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 98, 98])
grad_out size torch.Size([1, 3, 96, 96])
crop [8, 8, 8, 8] [152, 247, 472, 567] [160, 239, 480, 559]
## 8 88 8 88
##############return grad_in in conv2d tensor([ 0.0000e+00, -6.7070e-05,  1.0698e-04, -2.3047e-04, -3.2702e-05,
         2.5664e-04, -1.1212e-04,  1.5535e-04,  1.9478e-04,  1.3324e-04],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 196, 196])
grad_out size torch.Size([1, 3, 98, 98])
arg size torch.Size([1, 3, 98, 98])
##############grad_in in maxp torch.Size([1, 3, 196, 196])
I am [6, 2]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 198, 198])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 196, 196])
grad_input torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 198, 198])
grad_input new torch.Size([1, 3, 198, 198])
new grad_input torch.Size([1, 3, 198, 198])
grad_out size torch.Size([1, 3, 196, 196])
crop [18, 18, 18, 18] [302, 497, 942, 1137] [320, 479, 960, 1119]
## 18 178 18 178
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00, -1.0816e-05,  5.5515e-06,  1.6927e-05,
         3.9485e-06, -2.0521e-05, -5.1459e-05, -1.0161e-05,  1.6875e-05],
       device='cuda:0')
I am [6, 2]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 200, 200])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 200, 200])
grad_input new torch.Size([1, 3, 200, 200])
grad_input torch.Size([1, 3, 200, 200])
grad_out size torch.Size([1, 3, 198, 198])
crop [19, 19, 19, 19] [301, 498, 941, 1138] [320, 479, 960, 1119]
## 19 179 19 179
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00, -3.1865e-06, -2.2705e-07,  2.0966e-07,
         4.4954e-07, -1.4862e-06, -1.1360e-05, -6.9681e-06, -7.5848e-06],
       device='cuda:0')
I am [6, 2]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 202, 202])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 200, 200])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 202, 202])
grad_out size torch.Size([1, 3, 200, 200])
crop [20, 20, 20, 20] [300, 499, 940, 1139] [320, 479, 960, 1119]
## 20 180 20 180
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  2.8899e-07,  8.7995e-07, -2.4298e-07,
        -1.3596e-06, -3.9138e-07,  1.9404e-06,  4.4035e-06,  1.3887e-06],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 404, 404])
grad_out size torch.Size([1, 3, 202, 202])
arg size torch.Size([1, 3, 202, 202])
##############grad_in in maxp torch.Size([1, 3, 404, 404])
I am [6, 2]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 406, 406])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 404, 404])
grad_input torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 406, 406])
grad_input new torch.Size([1, 3, 406, 406])
new grad_input torch.Size([1, 3, 406, 406])
grad_out size torch.Size([1, 3, 404, 404])
crop [42, 42, 42, 42] [598, 1001, 1878, 2281] [640, 959, 1920, 2239]
## 42 362 42 362
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  6.8336e-08,
        -7.1319e-08, -9.7476e-08,  2.1170e-07, -1.0875e-07, -2.3382e-08],
       device='cuda:0')
I am [6, 2]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 408, 408])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 408, 408])[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cCheckpointBackward

grad_input new torch.Size([1, 3, 408, 408])
grad_input torch.Size([1, 3, 408, 408])
grad_out size torch.Size([1, 3, 406, 406])
crop [43, 43, 43, 43] [597, 1002, 1877, 2282] [640, 959, 1920, 2239]
## 43 363 43 363
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -9.0380e-09,
        -2.9201e-08,  2.0351e-08,  2.7872e-08,  2.5714e-08,  6.3606e-08],
       device='cuda:0')
I am [6, 2]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 410, 410])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 408, 408])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 410, 410])
grad_out size torch.Size([1, 3, 408, 408])
crop [44, 44, 44, 44] [596, 1003, 1876, 2283] [640, 959, 1920, 2239]
## 44 364 44 364
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -3.3975e-10,
         7.0433e-09,  2.9289e-09,  6.2726e-09,  5.2253e-10, -2.2520e-08],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 820, 820])
grad_out size torch.Size([1, 3, 410, 410])
arg size torch.Size([1, 3, 410, 410])
##############grad_in in maxp torch.Size([1, 3, 820, 820])
I am [6, 2]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 822, 822])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 820, 820])
grad_input torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 822, 822])
grad_input new torch.Size([1, 3, 822, 822])
new grad_input torch.Size([1, 3, 822, 822])
grad_out size torch.Size([1, 3, 820, 820])
crop [90, 90, 90, 90] [1190, 2009, 3750, 4569] [1280, 1919, 3840, 4479]
## 90 730 90 730
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -3.0318e-10],
       device='cuda:0')
I am [6, 2]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 824, 824])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 824, 824])
grad_out size torch.Size([1, 3, 822, 822])
crop [91, 91, 91, 91] [1189, 2010, 3749, 4570] [1280, 1919, 3840, 4479]
## 91 731 91 731
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -3.3247e-11],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 1648, 1648])
grad_out size torch.Size([1, 3, 824, 824])
arg size torch.Size([1, 3, 824, 824])
##############grad_in in maxp torch.Size([1, 3, 1648, 1648])
I am [6, 2]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 1650, 1650])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1648, 1648])
grad_input torch.Size([1, 3, 1650, 1650])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 1650, 1650])
grad_input new torch.Size([1, 3, 1650, 1650])
new grad_input torch.Size([1, 3, 1650, 1650])
grad_out size torch.Size([1, 3, 1648, 1648])
crop [184, 184, 184, 184] [2376, 4023, 7496, 9143] [2560, 3839, 7680, 8959]
## 184 1464 184 1464
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [6, 2]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 3, 1652, 1652])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1650, 1650])
final torch.Size([1, 3, 1652, 1652])
padding info :: [0, 0, 0, 0]
reshape_for_final ## 186 1466 186 1466
after crop g_in torch.Size([1, 3, 1280, 1280])
grad_out size torch.Size([1, 3, 1650, 1650])
crop [185, 185, 185, 185] [2375, 4024, 7495, 9144] [2560, 3839, 7680, 8959]
## 185 1465 185 1465
##############return grad_in in conv2d tensor([8.6704e-08, 3.5985e-08, 2.0553e-07, 1.8443e-07, 1.9155e-07, 3.7971e-08,
        6.1596e-08, 9.9174e-08, 2.4040e-07, 3.0041e-07], device='cuda:0')
[8, 8]
HREREERE torch.Size([1, 3, 10240, 10240])

############# Enter checkpointing bkward ####
inputs len 5 4
in customized Sequential 5
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 1]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <6,1,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 1]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <6,1,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 1]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <6,1,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 1]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <6,1,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 1]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <6,1,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 1]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <6,1,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 1]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <6,1,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 1]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <6,1,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() [torch/csrc/autograd/python_engine.cpp] THPEngine_run_backward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 1]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <6,1,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 1]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <6,1,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 1]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <6,1,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 1]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <6,1,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 1]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <6,1,>,

in customized Sequential 3
need to get existing

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 92, 92])
grad_out size torch.Size([1, 3, 320, 320])
arg size torch.Size([1, 3, 46, 46])
new_grad_out torch.Size([1, 3, 46, 46])
##############grad_in in maxp torch.Size([1, 3, 92, 92])
I am [6, 1]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 94, 94])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 92, 92])
grad_input torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 94, 94])
grad_input new torch.Size([1, 3, 94, 94])
new grad_input torch.Size([1, 3, 94, 94])
grad_out size torch.Size([1, 3, 92, 92])
crop [6, 6, 6, 6] [74, 165, 474, 565] [80, 159, 480, 559]
## 6 86 6 86
##############return grad_in in conv2d tensor([-0.0025,  0.0097, -0.0083, -0.0019,  0.0007,  0.0005,  0.0035, -0.0016,
         0.0004,  0.0003], device='cuda:0')
I am [6, 1]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 96, 96])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 96, 96])
grad_input new torch.Size([1, 3, 96, 96])
grad_input torch.Size([1, 3, 96, 96])
grad_out size torch.Size([1, 3, 94, 94])
crop [7, 7, 7, 7] [73, 166, 473, 566] [80, 159, 480, 559]
## 7 87 7 87
##############return grad_in in conv2d tensor([ 0.0008,  0.0005,  0.0024,  0.0011,  0.0031,  0.0003, -0.0007,  0.0004,
        -0.0019,  0.0008], device='cuda:0')
I am [6, 1]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 98, 98])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 96, 96])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 98, 98])
grad_out size torch.Size([1, 3, 96, 96])
crop [8, 8, 8, 8] [72, 167, 472, 567] [80, 159, 480, 559]
## 8 88 8 88
##############return grad_in in conv2d tensor([ 1.8403e-04, -3.6087e-04,  4.2798e-04, -5.4299e-05,  2.3728e-04,
         2.2110e-04, -7.4506e-04,  7.2500e-05, -2.2604e-04, -8.2103e-05],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 196, 196])
grad_out size torch.Size([1, 3, 98, 98])
arg size torch.Size([1, 3, 98, 98])
##############grad_in in maxp torch.Size([1, 3, 196, 196])
I am [6, 1]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 198, 198])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 196, 196])
grad_input torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 198, 198])
grad_input new torch.Size([1, 3, 198, 198])
new grad_input torch.Size([1, 3, 198, 198])
grad_out size torch.Size([1, 3, 196, 196])
crop [18, 18, 18, 18] [142, 337, 942, 1137] [160, 319, 960, 1119]
## 18 178 18 178
##############return grad_in in conv2d tensor([ 0.0000e+00,  3.6988e-06, -1.8158e-06,  5.2106e-06,  1.3743e-04,
        -3.8680e-05, -5.4041e-05, -4.6965e-05,  1.5849e-05,  1.1063e-04],
       device='cuda:0')
I am [6, 1]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 200, 200])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 200, 200])
grad_input new torch.Size([1, 3, 200, 200])
grad_input torch.Size([1, 3, 200, 200])
grad_out size torch.Size([1, 3, 198, 198])
crop [19, 19, 19, 19] [141, 338, 941, 1138] [160, 319, 960, 1119]
## 19 179 19 179
##############return grad_in in conv2d tensor([ 0.0000e+00,  1.0740e-06, -1.1615e-06,  3.4557e-06,  3.5486e-05,
        -4.1019e-05,  4.3946e-05, -3.4274e-05, -8.8335e-06,  2.4348e-05],
       device='cuda:0')
I am [6, 1]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 202, 202])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 200, 200])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 202, 202])
grad_out size torch.Size([1, 3, 200, 200])
crop [20, 20, 20, 20] [140, 339, 940, 1139] [160, 319, 960, 1119]
## 20 180 20 180
##############return grad_in in conv2d tensor([ 0.0000e+00, -4.7710e-08, -1.5010e-07, -2.7472e-07, -1.1523e-06,
        -4.3880e-06, -6.3243e-06,  7.8158e-06,  6.1049e-06, -5.5149e-06],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 404, 404])
grad_out size torch.Size([1, 3, 202, 202])
arg size torch.Size([1, 3, 202, 202])
##############grad_in in maxp torch.Size([1, 3, 404, 404])
I am [6, 1]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 406, 406])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 404, 404])
grad_input torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 406, 406])
grad_input new torch.Size([1, 3, 406, 406])
new grad_input torch.Size([1, 3, 406, 406])
grad_out size torch.Size([1, 3, 404, 404])
crop [42, 42, 42, 42] [278, 681, 1878, 2281] [320, 639, 1920, 2239]
## 42 362 42 362
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00, -4.4647e-09,  1.5920e-08, -4.2853e-08,
        -1.2718e-08, -3.0045e-08, -9.1730e-08, -5.3964e-08,  1.8933e-07],
       device='cuda:0')
I am [6, 1]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 408, 408])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 408, 408])
grad_input new torch.Size([1, 3, 408, 408])
grad_input torch.Size([1, 3, 408, 408])
grad_out size torch.Size([1, 3, 406, 406])
crop [43, 43, 43, 43] [277, 682, 1877, 2282] [320, 639, 1920, 2239]
## 43 363 43 363
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00, -1.7740e-09,  5.6389e-11,  5.5154e-09,
        -1.2576e-08, -1.3522e-08, -1.1551e-08, -3.0615e-08,  1.9529e-08],
       device='cuda:0')
I am [6, 1]
@@@ using cudnn bkw
in the local first ++ input shape [torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cCheckpointBackward
torch.Size([1, 3, 410, 410])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 408, 408])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 410, 410])
grad_out size torch.Size([1, 3, 408, 408])
crop [44, 44, 44, 44] [276, 683, 1876, 2283] [320, 639, 1920, 2239]
## 44 364 44 364
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00, -1.9039e-10,  6.7487e-10, -1.2681e-09,
         1.0249e-09,  1.3111e-09,  6.9919e-09, -6.2018e-09,  2.7899e-08],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 820, 820])
grad_out size torch.Size([1, 3, 410, 410])
arg size torch.Size([1, 3, 410, 410])
##############grad_in in maxp torch.Size([1, 3, 820, 820])
I am [6, 1]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 822, 822])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 820, 820])
grad_input torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 822, 822])
grad_input new torch.Size([1, 3, 822, 822])
new grad_input torch.Size([1, 3, 822, 822])
grad_out size torch.Size([1, 3, 820, 820])
crop [90, 90, 90, 90] [550, 1369, 3750, 4569] [640, 1279, 3840, 4479]
## 90 730 90 730
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
        -6.8009e-11, -3.4675e-11,  1.2535e-10,  2.4755e-10,  2.0884e-10],
       device='cuda:0')
I am [6, 1]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 824, 824])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 824, 824])
grad_out size torch.Size([1, 3, 822, 822])
crop [91, 91, 91, 91] [549, 1370, 3749, 4570] [640, 1279, 3840, 4479]
## 91 731 91 731
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
        -9.5482e-12, -7.0415e-12,  9.9776e-12,  2.8581e-11,  1.2151e-10],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 1648, 1648])
grad_out size torch.Size([1, 3, 824, 824])
arg size torch.Size([1, 3, 824, 824])
##############grad_in in maxp torch.Size([1, 3, 1648, 1648])
I am [6, 1]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 1650, 1650])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1648, 1648])
grad_input torch.Size([1, 3, 1650, 1650])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 1650, 1650])
grad_input new torch.Size([1, 3, 1650, 1650])
new grad_input torch.Size([1, 3, 1650, 1650])
grad_out size torch.Size([1, 3, 1648, 1648])
crop [184, 184, 184, 184] [1096, 2743, 7496, 9143] [1280, 2559, 7680, 8959]
## 184 1464 184 1464
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [6, 1]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 3, 1652, 1652])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1650, 1650])
final torch.Size([1, 3, 1652, 1652])
padding info :: [0, 0, 0, 0]
reshape_for_final ## 186 1466 186 1466
after crop g_in torch.Size([1, 3, 1280, 1280])
grad_out size torch.Size([1, 3, 1650, 1650])
crop [185, 185, 185, 185] [1095, 2744, 7495, 9144] [1280, 2559, 7680, 8959]
## 185 1465 185 1465
##############return grad_in in conv2d tensor([ 1.9207e-07,  1.0694e-07, -1.2849e-07, -1.1892e-07,  3.1512e-08,
         4.9267e-07,  1.1596e-07,  6.4303e-08, -3.2265e-07, -6.4781e-08],
       device='cuda:0')
[8, 8]
HREREERE torch.Size([1, 3, 10240, 10240])

############# Enter checkpointing bkward ####
inputs len 5 4
in customized Sequential 5
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 0]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1466])
myctx [[ 140046822727744[ PI( <6,0,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 0]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1464])
myctx [[ 140045015171856[ PI( <6,0,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 0]
shape input_tile_for_next
 torch.Size([1, 3, 822, 732])
myctx [[ 140045013800704[ PI( <6,0,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 0]
shape input_tile_for_next
 torch.Size([1, 3, 820, 730])
myctx [[ 140045013848944[ PI( <6,0,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 0]
shape input_tile_for_next
 torch.Size([1, 3, 408, 366])
myctx [[ 140045013206640[ PI( <6,0,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 0]
shape input_tile_for_next
 torch.Size([1, 3, 406, 364])
myctx [[ 140045013206784[ PI( <6,0,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 0]
shape input_tile_for_next
 torch.Size([1, 3, 404, 362])
myctx [[ 140045013206880[ PI( <6,0,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 0]
shape input_tile_for_next
 torch.Size([1, 3, 200, 182])
myctx [[ 140045013206352[ PI( <6,0,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 0]
shape input_tile_for_next
 torch.Size([1, 3, 198, 180])
myctx [[ 140045013206304[ PI( <6,0,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() [torch/csrc/autograd/python_engine.cpp] THPEngine_run_backward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 0]
shape input_tile_for_next
 torch.Size([1, 3, 196, 178])
myctx [[ 140045013207072[ PI( <6,0,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 0]
shape input_tile_for_next
 torch.Size([1, 3, 96, 90])
myctx [[ 140045013206400[ PI( <6,0,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 0]
shape input_tile_for_next
 torch.Size([1, 3, 94, 88])
myctx [[ 140045013206448[ PI( <6,0,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [6, 0]
shape input_tile_for_next
 torch.Size([1, 3, 92, 86])
myctx [[ 140045013206976[ PI( <6,0,>,

in customized Sequential 3
need to get existing

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 92, 86])
grad_out size torch.Size([1, 3, 320, 320])
arg size torch.Size([1, 3, 46, 43])
new_grad_out torch.Size([1, 3, 46, 43])
##############grad_in in maxp torch.Size([1, 3, 92, 86])
I am [6, 0]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 94, 88])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 92, 86])
grad_input torch.Size([1, 3, 94, 88])
padding info :: [1, 0, 0, 0]
grad_input old torch.Size([1, 3, 94, 88])
grad_input new torch.Size([1, 3, 94, 88])
new grad_input torch.Size([1, 3, 94, 88])
grad_out size torch.Size([1, 3, 92, 86])
crop [0, 6, 6, 6] [0, 85, 474, 565] [0, 79, 480, 559]
## 6 86 0 80
##############return grad_in in conv2d tensor([ 0.0000,  0.0000, -0.0007, -0.0020, -0.0007,  0.0006,  0.0004,  0.0008,
         0.0037, -0.0028], device='cuda:0')
I am [6, 0]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 96, 90])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 94, 88])
padding info :: [2, 0, 0, 0]
grad_input old torch.Size([1, 3, 96, 90])
grad_input new torch.Size([1, 3, 96, 90])
grad_input torch.Size([1, 3, 96, 90])
grad_out size torch.Size([1, 3, 94, 87])
crop [0, 7, 7, 7] [0, 86, 473, 566] [0, 79, 480, 559]
## 7 87 0 80
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00, -4.1984e-04,  5.5888e-04, -2.9044e-05,
         1.3528e-04,  6.0338e-04,  2.7660e-04, -1.5237e-04,  3.5382e-04],
       device='cuda:0')
I am [6, 0]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 98, 92])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 96, 90])
padding info :: [3, 0, 0, 0]
grad_input torch.Size([1, 3, 98, 89])
grad_out size torch.Size([1, 3, 96, 88])
crop [0, 8, 8, 8] [0, 87, 472, 567] [0, 79, 480, 559]
## 8 88 0 80
##############return grad_in in conv2d tensor([ 1.3327e-04,  1.3524e-04, -2.1065e-04,  5.2869e-05,  1.0283e-04,
        -2.6022e-04,  2.0900e-05,  3.0088e-05,  2.5256e-04, -1.0797e-04],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 196, 178])
grad_out size torch.Size([1, 3, 98, 89])
arg size torch.Size([1, 3, 98, 89])
##############grad_in in maxp torch.Size([1, 3, 196, 178])
I am [6, 0]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 198, 180])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 196, 178])
grad_input torch.Size([1, 3, 198, 180])
padding info :: [1, 0, 0, 0]
grad_input old torch.Size([1, 3, 198, 180])
grad_input new torch.Size([1, 3, 198, 180])
new grad_input torch.Size([1, 3, 198, 180])
grad_out size torch.Size([1, 3, 196, 178])
crop [0, 18, 18, 18] [0, 177, 942, 1137] [0, 159, 960, 1119]
## 18 178 0 160
##############return grad_in in conv2d tensor([ 0.0000e+00, -5.2301e-06, -2.8565e-05,  6.2931e-06, -2.5941e-05,
         1.5408e-06,  2.6781e-05,  1.0085e-05, -1.1670e-05, -4.6974e-06],
       device='cuda:0')
I am [6, 0]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 200, 182])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 198, 180])
padding info :: [2, 0, 0, 0]
grad_input old torch.Size([1, 3, 200, 182])
grad_input new torch.Size([1, 3, 200, 182])
grad_input torch.Size([1, 3, 200, 182])
grad_out size torch.Size([1, 3, 198, 179])
crop [0, 19, 19, 19] [0, 178, 941, 1138] [0, 159, 960, 1119]
## 19 179 0 160
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00, -4.0336e-06,  3.6670e-06, -1.7577e-05,
         1.1727e-05, -4.2305e-06,  7.6304e-06,  2.0639e-06,  6.9434e-06],
       device='cuda:0')
I am [6, 0]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 202, 184])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 200, 182])
padding info :: [3, 0, 0, 0]
grad_input torch.Size([1, 3, 202, 181])
grad_out size torch.Size([1, 3, 200, 180])
crop [0, 20, 20, 20] [0, 179, 940, 1139] [0, 159, 960, 1119]
## 20 180 0 160
##############return grad_in in conv2d tensor([ 2.3338e-06,  1.6084e-06,  3.2232e-09,  1.0780e-06, -2.3155e-06,
        -1.0788e-06,  2.7420e-07,  4.6443e-07,  2.3938e-06,  2.8771e-07],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 404, 362])
grad_out size torch.Size([1, 3, 202, 181])
arg size torch.Size([1, 3, 202, 181])
##############grad_in in maxp torch.Size([1, 3, 404, 362])
I am [6, 0]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 406, 364])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 404, 362])
grad_input torch.Size([1, 3, 406, 364])
padding info :: [1, 0, 0, 0]
grad_input old torch.Size([1, 3, 406, 364])
grad_input new torch.Size([1, 3, 406, 364])
new grad_input torch.Size([1, 3, 406, 364])
grad_out size torch.Size([1, 3, 404, 362])
crop [0, 42, 42, 42] [0, 361, 1878, 2281] [0, 319, 1920, 2239]
## 42 362 0 320
##############return grad_in in conv2d tensor([ 0.0000e+00,  3.1065e-07, -2.5954e-07, -1.2960e-07, -1.4983e-08,
         0.0000e+00,  4.2805e-07, -1.9585e-07, -1.2335e-06,  3.2383e-07],
       device='cuda:0')
I am [6, 0]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 408, 366])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 406, 364])
padding info :: [2, 0, 0, 0]
grad_input old torch.Size([1, 3, 408, 366])
grad_input new torch.Size([1, 3, 408, 366])
grad_input torch.Size([1, 3, 408, 366])
grad_out size torch.Size([1, 3, 406, 363])
crop [0, 43, 43, 43] [0, 362, 1877, 2282] [0, 319, 1920, 2239]
## 43 363 0 320
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  1.5370e-08,  1.2704e-07, -4.5787e-08,
        -6.9969e-08, -1.0616e-08, -5.8257e-08,  1.4079e-07,  1.6971e-07],
       device='cuda:0')
I am [6, 0]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 410, 368])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 408, 366])
padding info :: [3, 0, 0, 0]
grad_input torch.Size([1, 3, 410, 365])
grad_out size torch.Size([1, 3, 408, 364])
crop [0, 44, 44, 44] [0, 363, 1876, 2283] [0, 319, 1920, 2239]
## 44 364 0 320
##############return grad_in in conv2d [torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cCheckpointBackward
tensor([-3.9138e-08, -1.8801e-08,  2.0730e-09,  8.9744e-09,  2.2994e-08,
        -1.1308e-08, -5.1284e-08,  3.7713e-08,  3.9004e-08,  5.3775e-09],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 820, 730])
grad_out size torch.Size([1, 3, 410, 365])
arg size torch.Size([1, 3, 410, 365])
##############grad_in in maxp torch.Size([1, 3, 820, 730])
I am [6, 0]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 822, 732])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 820, 730])
grad_input torch.Size([1, 3, 822, 732])
padding info :: [1, 0, 0, 0]
grad_input old torch.Size([1, 3, 822, 732])
grad_input new torch.Size([1, 3, 822, 732])
new grad_input torch.Size([1, 3, 822, 732])
grad_out size torch.Size([1, 3, 820, 730])
crop [0, 90, 90, 90] [0, 729, 3750, 4569] [0, 639, 3840, 4479]
## 90 730 0 640
##############return grad_in in conv2d tensor([ 0.0000e+00,  1.6362e-09,  9.9712e-10, -1.8539e-09, -2.3526e-09,
         3.7839e-09,  3.9191e-11,  2.2351e-10, -8.4590e-10, -3.9755e-09],
       device='cuda:0')
I am [6, 0]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 824, 734])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 822, 732])
padding info :: [2, 0, 0, 0]
grad_input torch.Size([1, 3, 824, 732])
grad_out size torch.Size([1, 3, 822, 731])
crop [0, 91, 91, 91] [0, 730, 3749, 4570] [0, 639, 3840, 4479]
## 91 731 0 640
##############return grad_in in conv2d tensor([ 7.0932e-10,  6.1830e-10,  3.3904e-11, -6.4455e-10,  1.7381e-10,
         5.0978e-10, -7.4979e-11, -7.2438e-10, -7.1793e-10,  1.2287e-09],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 1648, 1464])
grad_out size torch.Size([1, 3, 824, 732])
arg size torch.Size([1, 3, 824, 732])
##############grad_in in maxp torch.Size([1, 3, 1648, 1464])
I am [6, 0]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 1650, 1466])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1648, 1464])
grad_input torch.Size([1, 3, 1650, 1466])
padding info :: [1, 0, 0, 0]
grad_input old torch.Size([1, 3, 1650, 1466])
grad_input new torch.Size([1, 3, 1650, 1466])
new grad_input torch.Size([1, 3, 1650, 1466])
grad_out size torch.Size([1, 3, 1648, 1464])
crop [0, 184, 184, 184] [0, 1463, 7496, 9143] [0, 1279, 7680, 8959]
## 184 1464 0 1280
##############return grad_in in conv2d tensor([ 0.0000e+00, -2.7208e-11, -6.1686e-11,  9.6069e-11, -9.9498e-11,
         2.0654e-12,  7.2844e-11,  0.0000e+00,  1.0614e-11,  1.6738e-10],
       device='cuda:0')
I am [6, 0]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 3, 1652, 1468])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1650, 1466])
final torch.Size([1, 3, 1652, 1468])
padding info :: [2, 0, 0, 0]
reshape_for_final ## 186 1466 0 1280
after crop g_in torch.Size([1, 3, 1280, 1280])
grad_out size torch.Size([1, 3, 1650, 1465])
crop [0, 185, 185, 185] [0, 1464, 7495, 9144] [0, 1279, 7680, 8959]
## 185 1465 0 1280
##############return grad_in in conv2d tensor([ 4.7802e-08,  6.2442e-08,  2.1448e-07, -3.3589e-07, -1.0582e-07,
        -2.4601e-07, -1.5885e-07,  2.2295e-07,  2.0683e-07, -5.6297e-07],
       device='cuda:0')
[8, 8]
HREREERE torch.Size([1, 3, 10240, 10240])

############# Enter checkpointing bkward ####
inputs len 5 4
in customized Sequential 5
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 7]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1466])
myctx [[ 140046822727744[ PI( <5,7,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 7]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1464])
myctx [[ 140045015171856[ PI( <5,7,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 7]
shape input_tile_for_next
 torch.Size([1, 3, 822, 732])
myctx [[ 140045013800704[ PI( <5,7,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 7]
shape input_tile_for_next
 torch.Size([1, 3, 820, 730])
myctx [[ 140045013848944[ PI( <5,7,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 7]
shape input_tile_for_next
 torch.Size([1, 3, 408, 366])
myctx [[ 140045013206640[ PI( <5,7,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 7]
shape input_tile_for_next
 torch.Size([1, 3, 406, 364])
myctx [[ 140045013206784[ PI( <5,7,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 7]
shape input_tile_for_next
 torch.Size([1, 3, 404, 362])
myctx [[ 140045013206880[ PI( <5,7,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 7]
shape input_tile_for_next
 torch.Size([1, 3, 200, 182])
myctx [[ 140045013206352[ PI( <5,7,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 7]
shape input_tile_for_next
 torch.Size([1, 3, 198, 180])
myctx [[ 140045013206304[ PI( <5,7,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 7]
shape input_tile_for_next
 torch.Size([1, 3, 196, 178])
myctx [[ 140045013207072[ PI( <5,7,>,

[torch/csrc/autograd/python_engine.cpp] THPEngine_run_backward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 7]
shape input_tile_for_next
 torch.Size([1, 3, 96, 90])
myctx [[ 140045013206400[ PI( <5,7,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 7]
shape input_tile_for_next
 torch.Size([1, 3, 94, 88])
myctx [[ 140045013206448[ PI( <5,7,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 7]
shape input_tile_for_next
 torch.Size([1, 3, 92, 86])
myctx [[ 140045013206976[ PI( <5,7,>,

in customized Sequential 3
need to get existing

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 92, 86])
grad_out size torch.Size([1, 3, 320, 320])
arg size torch.Size([1, 3, 46, 43])
new_grad_out torch.Size([1, 3, 46, 43])
##############grad_in in maxp torch.Size([1, 3, 92, 86])
I am [5, 7]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 94, 88])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 92, 86])
grad_input torch.Size([1, 3, 94, 88])
padding info :: [0, 1, 0, 0]
grad_input old torch.Size([1, 3, 94, 88])
grad_input new torch.Size([1, 3, 94, 88])
new grad_input torch.Size([1, 3, 94, 88])
grad_out size torch.Size([1, 3, 92, 86])
crop [6, 0, 6, 6] [554, 639, 394, 485] [560, 639, 400, 479]
## 6 86 6 86
##############return grad_in in conv2d tensor([ 0.0000,  0.0002,  0.0006,  0.0022, -0.0041, -0.0002,  0.0041, -0.0015,
        -0.0009, -0.0002], device='cuda:0')
I am [5, 7]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 96, 90])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 94, 88])
padding info :: [0, 2, 0, 0]
grad_input old torch.Size([1, 3, 96, 90])
grad_input new torch.Size([1, 3, 96, 90])
grad_input torch.Size([1, 3, 96, 90])
grad_out size torch.Size([1, 3, 94, 87])
crop [7, 0, 7, 7] [553, 639, 393, 486] [560, 639, 400, 479]
## 7 87 7 87
##############return grad_in in conv2d tensor([ 0.0000,  0.0001, -0.0002,  0.0005,  0.0003,  0.0006,  0.0009, -0.0003,
        -0.0011,  0.0005], device='cuda:0')
I am [5, 7]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 98, 92])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 96, 90])
padding info :: [0, 3, 0, 0]
grad_input torch.Size([1, 3, 98, 89])
grad_out size torch.Size([1, 3, 96, 88])
crop [8, 0, 8, 8] [552, 639, 392, 487] [560, 639, 400, 479]
## 8 88 8 88
##############return grad_in in conv2d tensor([ 0.0000e+00,  2.0852e-05, -4.0968e-05, -3.8139e-05,  1.8891e-04,
        -1.4043e-04,  1.2533e-04,  1.4048e-04, -4.2206e-04,  1.4712e-05],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 196, 178])
grad_out size torch.Size([1, 3, 98, 89])
arg size torch.Size([1, 3, 98, 89])
##############grad_in in maxp torch.Size([1, 3, 196, 178])
I am [5, 7]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 198, 180])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 196, 178])
grad_input torch.Size([1, 3, 198, 180])
padding info :: [0, 1, 0, 0]
grad_input old torch.Size([1, 3, 198, 180])
grad_input new torch.Size([1, 3, 198, 180])
new grad_input torch.Size([1, 3, 198, 180])
grad_out size torch.Size([1, 3, 196, 178])
crop [18, 0, 18, 18] [1102, 1279, 782, 977] [1120, 1279, 800, 959]
## 18 178 18 178
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  1.5583e-06, -1.3586e-07, -8.6050e-07,
        -2.0878e-06,  1.8153e-05, -1.8112e-06,  3.9670e-06,  2.7045e-05],
       device='cuda:0')
I am [5, 7]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 200, 182])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 198, 180])
padding info :: [0, 2, 0, 0]
grad_input old torch.Size([1, 3, 200, 182])
grad_input new torch.Size([1, 3, 200, 182])
grad_input torch.Size([1, 3, 200, 182])
grad_out size torch.Size([1, 3, 198, 179])
crop [19, 0, 19, 19] [1101, 1279, 781, 978] [1120, 1279, 800, 959]
## 19 179 19 179
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  4.5245e-07, -2.9935e-07,  2.6959e-07,
        -6.9665e-07,  4.8223e-06, -2.0381e-06,  6.6238e-06,  7.4402e-06],
       device='cuda:0')
I am [5, 7]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 202, 184])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 200, 182])
padding info :: [0, 3, 0, 0]
grad_input torch.Size([1, 3, 202, 181])
grad_out size torch.Size([1, 3, 200, 180])
crop [20, 0, 20, 20] [1100, 1279, 780, 979] [1120, 1279, 800, 959]
## 20 180 20 180
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00, -2.0099e-08, -7.8281e-08, -5.8003e-08,
         1.6707e-07, -1.5114e-07, -9.6129e-07, -3.8521e-07, -8.5906e-07],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 404, 362])
grad_out size torch.Size([1, 3, 202, 181])
arg size torch.Size([1, 3, 202, 181])
##############grad_in in maxp torch.Size([1, 3, 404, 362])
I am [5, 7]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 406, 364])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 404, 362])
grad_input torch.Size([1, 3, 406, 364])
padding info :: [0, 1, 0, 0]
grad_input old torch.Size([1, 3, 406, 364])
grad_input new torch.Size([1, 3, 406, 364])
new grad_input torch.Size([1, 3, 406, 364])
grad_out size torch.Size([1, 3, 404, 362])
crop [42, 0, 42, 42] [2198, 2559, 1558, 1961] [2240, 2559, 1600, 1919]
## 42 362 42 362
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
        -2.8505e-09, -1.2805e-08, -5.9725e-09,  5.2061e-09,  2.4463e-09],
       device='cuda:0')
I am [5, 7]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 408, 366])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 406, 364])
padding info :: [0, 2, 0, 0]
grad_input old torch.Size([1, 3, 408, 366])
grad_input new torch.Size([1, 3, 408, 366])
grad_input torch.Size([1, 3, 408, 366])
grad_out size torch.Size([1, 3, 406, 363])
crop [43, 0, 43, 43] [2197, 2559, 1557, 1962] [2240, 2559, 1600, 1919]
## 43 363 43 363
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
        -5.0846e-10, -3.0864e-09, -4.8301e-09, -2.8139e-09, -1.7411e-09],
       device='cuda:0')
I am [5, 7]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 410, 368])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 408, 366])
padding info :: [0, 3, 0, 0]
grad_input torch.Size([1, 3, 410, 365])
grad_out size torch.Size([1, 3, 408, 364])
crop [44, 0, 44, 44] [2196, 2559, 1556, 1963] [2240, 2559, 1600, 1919]
## 44 364 44 364
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
        -5.9095e-11, -4.6806e-11,  1.1086e-09,  2.0163e-09,  2.3544e-09],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 820, 730])
grad_out size torch.Size([1, 3, 410, 365])
arg size torch.Size([1, 3, 410, 365])
##############grad_in in maxp torch.Size([1, 3, 820, 730])
I am [5, 7]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 822, 732])
weight shape [torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cCheckpointBackward
torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 820, 730])
grad_input torch.Size([1, 3, 822, 732])
padding info :: [0, 1, 0, 0]
grad_input old torch.Size([1, 3, 822, 732])
grad_input new torch.Size([1, 3, 822, 732])
new grad_input torch.Size([1, 3, 822, 732])
grad_out size torch.Size([1, 3, 820, 730])
crop [90, 0, 90, 90] [4390, 5119, 3110, 3929] [4480, 5119, 3200, 3839]
## 90 730 90 730
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [5, 7]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 824, 734])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 822, 732])
padding info :: [0, 2, 0, 0]
grad_input torch.Size([1, 3, 824, 732])
grad_out size torch.Size([1, 3, 822, 731])
crop [91, 0, 91, 91] [4389, 5119, 3109, 3930] [4480, 5119, 3200, 3839]
## 91 731 91 731
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 1648, 1464])
grad_out size torch.Size([1, 3, 824, 732])
arg size torch.Size([1, 3, 824, 732])
##############grad_in in maxp torch.Size([1, 3, 1648, 1464])
I am [5, 7]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 1650, 1466])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1648, 1464])
grad_input torch.Size([1, 3, 1650, 1466])
padding info :: [0, 1, 0, 0]
grad_input old torch.Size([1, 3, 1650, 1466])
grad_input new torch.Size([1, 3, 1650, 1466])
new grad_input torch.Size([1, 3, 1650, 1466])
grad_out size torch.Size([1, 3, 1648, 1464])
crop [184, 0, 184, 184] [8776, 10239, 6216, 7863] [8960, 10239, 6400, 7679]
## 184 1464 184 1464
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [5, 7]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 3, 1652, 1468])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1650, 1466])
final torch.Size([1, 3, 1652, 1468])
padding info :: [0, 2, 0, 0]
reshape_for_final ## 186 1466 186 1466
after crop g_in torch.Size([1, 3, 1280, 1280])
grad_out size torch.Size([1, 3, 1650, 1465])
crop [185, 0, 185, 185] [8775, 10239, 6215, 7864] [8960, 10239, 6400, 7679]
## 185 1465 185 1465
##############return grad_in in conv2d tensor([-3.9899e-08, -3.6301e-07, -4.0168e-08,  2.0993e-07,  1.5971e-07,
        -1.8629e-07, -4.1726e-07, -3.9612e-07, -2.2771e-07, -2.3350e-07],
       device='cuda:0')
[8, 8]
HREREERE torch.Size([1, 3, 10240, 10240])

############# Enter checkpointing bkward ####
inputs len 5 4
in customized Sequential 5
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 6]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <5,6,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 6]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <5,6,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 6]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <5,6,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 6]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <5,6,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 6]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <5,6,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 6]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <5,6,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 6]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <5,6,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 6]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <5,6,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 6]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <5,6,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 6]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <5,6,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 6]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <5,6,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() [torch/csrc/autograd/python_engine.cpp] THPEngine_run_backward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 6]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <5,6,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 6]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <5,6,>,

in customized Sequential 3
need to get existing

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 92, 92])
grad_out size torch.Size([1, 3, 320, 320])
arg size torch.Size([1, 3, 46, 46])
new_grad_out torch.Size([1, 3, 46, 46])
##############grad_in in maxp torch.Size([1, 3, 92, 92])
I am [5, 6]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 94, 94])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 92, 92])
grad_input torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 94, 94])
grad_input new torch.Size([1, 3, 94, 94])
new grad_input torch.Size([1, 3, 94, 94])
grad_out size torch.Size([1, 3, 92, 92])
crop [6, 6, 6, 6] [474, 565, 394, 485] [480, 559, 400, 479]
## 6 86 6 86
##############return grad_in in conv2d tensor([ 1.8587e-05,  1.4994e-03, -2.1981e-03, -1.8857e-05,  0.0000e+00,
         1.8129e-03, -2.6349e-03, -5.6121e-03,  5.7041e-03,  6.5001e-03],
       device='cuda:0')
I am [5, 6]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 96, 96])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 96, 96])
grad_input new torch.Size([1, 3, 96, 96])
grad_input torch.Size([1, 3, 96, 96])
grad_out size torch.Size([1, 3, 94, 94])
crop [7, 7, 7, 7] [473, 566, 393, 486] [480, 559, 400, 479]
## 7 87 7 87
##############return grad_in in conv2d tensor([-2.0973e-06,  2.8088e-04,  1.4952e-04,  3.9999e-04,  6.3683e-04,
         1.9668e-04,  1.9043e-04,  1.7629e-04,  3.5420e-04,  1.2087e-03],
       device='cuda:0')
I am [5, 6]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 98, 98])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 96, 96])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 98, 98])
grad_out size torch.Size([1, 3, 96, 96])
crop [8, 8, 8, 8] [472, 567, 392, 487] [480, 559, 400, 479]
## 8 88 8 88
##############return grad_in in conv2d tensor([-6.5886e-07, -5.6291e-06,  7.8809e-05, -8.8577e-05,  1.5134e-04,
        -1.2656e-05, -4.4789e-05, -2.4876e-06, -1.3592e-04,  2.9194e-04],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 196, 196])
grad_out size torch.Size([1, 3, 98, 98])
arg size torch.Size([1, 3, 98, 98])
##############grad_in in maxp torch.Size([1, 3, 196, 196])
I am [5, 6]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 198, 198])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 196, 196])
grad_input torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 198, 198])
grad_input new torch.Size([1, 3, 198, 198])
new grad_input torch.Size([1, 3, 198, 198])
grad_out size torch.Size([1, 3, 196, 196])
crop [18, 18, 18, 18] [942, 1137, 782, 977] [960, 1119, 800, 959]
## 18 178 18 178
##############return grad_in in conv2d tensor([ 8.5371e-09, -1.1979e-07,  8.3339e-06, -5.9906e-07,  4.7523e-07,
         1.5321e-06, -6.6784e-07, -7.3250e-07,  3.4014e-05, -1.1999e-05],
       device='cuda:0')
I am [5, 6]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 200, 200])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 200, 200])
grad_input new torch.Size([1, 3, 200, 200])
grad_input torch.Size([1, 3, 200, 200])
grad_out size torch.Size([1, 3, 198, 198])
crop [19, 19, 19, 19] [941, 1138, 781, 978] [960, 1119, 800, 959]
## 19 179 19 179
##############return grad_in in conv2d tensor([ 2.4788e-09, -3.6367e-08,  2.5081e-06,  3.0597e-08,  2.0001e-06,
         1.2450e-06,  8.2880e-07,  1.5086e-07,  9.7454e-06, -7.3995e-06],
       device='cuda:0')
I am [5, 6]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 202, 202])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 200, 200])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 202, 202])
grad_out size torch.Size([1, 3, 200, 200])
crop [20, 20, 20, 20] [940, 1139, 780, 979] [960, 1119, 800, 959]
## 20 180 20 180
##############return grad_in in conv2d tensor([-1.1012e-10,  2.3007e-09, -1.8950e-07, -5.4304e-07, -2.0513e-08,
        -1.4576e-07, -6.1650e-07,  3.2662e-07, -4.4981e-07, -1.6501e-06],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 404, 404])
grad_out size torch.Size([1, 3, 202, 202])
arg size torch.Size([1, 3, 202, 202])
##############grad_in in maxp torch.Size([1, 3, 404, 404])
I am [5, 6]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 406, 406])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 404, 404])
grad_input torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 406, 406])
grad_input new torch.Size([1, 3, 406, 406])
new grad_input torch.Size([1, 3, 406, 406])
grad_out size torch.Size([1, 3, 404, 404])
crop [42, 42, 42, 42] [1878, 2281, 1558, 1961] [1920, 2239, 1600, 1919]
## 42 362 42 362
##############return grad_in in conv2d tensor([ 0.0000e+00, -1.5617e-11,  6.6717e-10, -6.9949e-10, -3.8165e-08,
         2.1199e-08, -5.9388e-08,  2.7531e-08,  2.0422e-09,  3.1283e-08],
       device='cuda:0')
I am [5, 6]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 408, 408])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 408, 408])
grad_input new torch.Size([1, 3, 408, 408])
grad_input torch.Size([1, 3, 408, 408])
grad_out size torch.Size([1, 3, 406, 406])
crop [43, 43, 43, 43] [1877, 2282, 1557, 1962] [1920, 2239, 1600, 1919]
## 43 363 43 363
##############return grad_in in conv2d tensor([ 0.0000e+00, -2.7856e-12, -9.5534e-11, -2.9232e-10, -8.6993e-09,
        -1.0183e-08,  1.8474e-08, -5.8283e-09, -6.8272e-09, -1.6795e-08],
       device='cuda:0')
I am [5, 6]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 410, 410])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 408, 408])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 410, 410])
grad_out size torch.Size([1, 3, 408, 408])
crop [44, 44, 44, 44] [1876, 2283, 1556, 1963] [1920, 2239, 1600, 1919]
## 44 364 44 364
##############return grad_in in conv2d tensor([ 0.0000e+00, -3.2376e-13, -2.3589e-12,  7.1873e-11, -9.6551e-10,
         3.3291e-09,  1.9196e-09,  2.5463e-09, -2.1459e-09,  1.3914e-09],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 820, 820])
grad_out size torch.Size([1, 3, 410, 410])
arg size torch.Size([1, 3, 410, 410])
##############grad_in in maxp torch.Size([1, 3, 820, 820])
I am [5, 6]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 822, 822])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 820, 820])
grad_input torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 822, 822])
grad_input new torch.Size([1, 3, 822, 822])
new grad_input torch.Size([1, 3, 822, 822])
grad_out size torch.Size([1, 3, 820, 820])
crop [90, 90, 90, 90] [3750, 4569, 3110, 3929] [3840, 4479, 3200, 3839]
## 90 730 90 730
##############return grad_in in conv2d [torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cCheckpointBackward
tensor([ 0.0000e+00,  0.0000e+00, -1.6082e-13, -9.8280e-14, -2.5274e-12,
        -1.3124e-12,  2.5750e-12,  4.9478e-12, -3.3205e-10, -3.4458e-10],
       device='cuda:0')
I am [5, 6]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 824, 824])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 824, 824])
grad_out size torch.Size([1, 3, 822, 822])
crop [91, 91, 91, 91] [3749, 4570, 3109, 3930] [3840, 4479, 3200, 3839]
## 91 731 91 731
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00, -1.3698e-14, -1.1959e-14, -3.6053e-13,
        -2.6364e-13, -1.1951e-12, -7.0032e-13, -4.0248e-11, -3.1889e-11],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 1648, 1648])
grad_out size torch.Size([1, 3, 824, 824])
arg size torch.Size([1, 3, 824, 824])
##############grad_in in maxp torch.Size([1, 3, 1648, 1648])
I am [5, 6]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 1650, 1650])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1648, 1648])
grad_input torch.Size([1, 3, 1650, 1650])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 1650, 1650])
grad_input new torch.Size([1, 3, 1650, 1650])
new grad_input torch.Size([1, 3, 1650, 1650])
grad_out size torch.Size([1, 3, 1648, 1648])
crop [184, 184, 184, 184] [7496, 9143, 6216, 7863] [7680, 8959, 6400, 7679]
## 184 1464 184 1464
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  3.4219e-15,
        -3.4306e-15, -3.4782e-15,  1.8657e-15,  1.2422e-14,  5.3729e-14],
       device='cuda:0')
I am [5, 6]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 3, 1652, 1652])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1650, 1650])
final torch.Size([1, 3, 1652, 1652])
padding info :: [0, 0, 0, 0]
reshape_for_final ## 186 1466 186 1466
after crop g_in torch.Size([1, 3, 1280, 1280])
grad_out size torch.Size([1, 3, 1650, 1650])
crop [185, 185, 185, 185] [7495, 9144, 6215, 7864] [7680, 8959, 6400, 7679]
## 185 1465 185 1465
##############return grad_in in conv2d tensor([ 1.0583e-07,  6.3749e-08,  1.0362e-07,  5.1945e-07,  1.4371e-07,
         5.0606e-08,  7.8084e-08,  1.5230e-07, -2.8454e-08,  1.5218e-09],
       device='cuda:0')
[8, 8]
HREREERE torch.Size([1, 3, 10240, 10240])

############# Enter checkpointing bkward ####
inputs len 5 4
in customized Sequential 5
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 5]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <5,5,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 5]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <5,5,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 5]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <5,5,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 5]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <5,5,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 5]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <5,5,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 5]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <5,5,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 5]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <5,5,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 5]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <5,5,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 5]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <5,5,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 5]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <5,5,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 5]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <5,5,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 5]
shape input_tile_for_next
 [torch/csrc/autograd/python_engine.cpp] THPEngine_run_backward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <5,5,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 5]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <5,5,>,

in customized Sequential 3
need to get existing

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 92, 92])
grad_out size torch.Size([1, 3, 320, 320])
arg size torch.Size([1, 3, 46, 46])
new_grad_out torch.Size([1, 3, 46, 46])
##############grad_in in maxp torch.Size([1, 3, 92, 92])
I am [5, 5]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 94, 94])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 92, 92])
grad_input torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 94, 94])
grad_input new torch.Size([1, 3, 94, 94])
new grad_input torch.Size([1, 3, 94, 94])
grad_out size torch.Size([1, 3, 92, 92])
crop [6, 6, 6, 6] [394, 485, 394, 485] [400, 479, 400, 479]
## 6 86 6 86
##############return grad_in in conv2d tensor([ 0.0000, -0.0017,  0.0015,  0.0036, -0.0017,  0.0041, -0.0040, -0.0023,
         0.0005,  0.0014], device='cuda:0')
I am [5, 5]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 96, 96])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 96, 96])
grad_input new torch.Size([1, 3, 96, 96])
grad_input torch.Size([1, 3, 96, 96])
grad_out size torch.Size([1, 3, 94, 94])
crop [7, 7, 7, 7] [393, 486, 393, 486] [400, 479, 400, 479]
## 7 87 7 87
##############return grad_in in conv2d tensor([ 0.0000, -0.0005,  0.0009, -0.0013, -0.0002, -0.0006, -0.0010, -0.0010,
        -0.0003, -0.0006], device='cuda:0')
I am [5, 5]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 98, 98])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 96, 96])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 98, 98])
grad_out size torch.Size([1, 3, 96, 96])
crop [8, 8, 8, 8] [392, 487, 392, 487] [400, 479, 400, 479]
## 8 88 8 88
##############return grad_in in conv2d tensor([ 0.0000e+00, -4.2122e-05,  1.4149e-04, -8.5579e-05, -3.4621e-04,
         5.3531e-05,  2.7212e-04, -8.7302e-05,  5.4929e-05, -6.8742e-06],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 196, 196])
grad_out size torch.Size([1, 3, 98, 98])
arg size torch.Size([1, 3, 98, 98])
##############grad_in in maxp torch.Size([1, 3, 196, 196])
I am [5, 5]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 198, 198])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 196, 196])
grad_input torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 198, 198])
grad_input new torch.Size([1, 3, 198, 198])
new grad_input torch.Size([1, 3, 198, 198])
grad_out size torch.Size([1, 3, 196, 196])
crop [18, 18, 18, 18] [782, 977, 782, 977] [800, 959, 800, 959]
## 18 178 18 178
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00, -5.7051e-06, -7.0135e-07,  7.5478e-06,
        -2.3685e-05, -4.0151e-06,  1.2548e-07, -6.9679e-06,  2.3796e-07],
       device='cuda:0')
I am [5, 5]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 200, 200])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 200, 200])
grad_input new torch.Size([1, 3, 200, 200])
grad_input torch.Size([1, 3, 200, 200])
grad_out size torch.Size([1, 3, 198, 198])
crop [19, 19, 19, 19] [781, 978, 781, 978] [800, 959, 800, 959]
## 19 179 19 179
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00, -1.7459e-06, -7.7328e-07,  8.9285e-07,
        -6.5609e-06,  4.8798e-06, -1.1209e-05, -1.4401e-06, -1.0853e-06],
       device='cuda:0')
I am [5, 5]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 202, 202])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 200, 200])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 202, 202])
grad_out size torch.Size([1, 3, 200, 200])
crop [20, 20, 20, 20] [780, 979, 780, 979] [800, 959, 800, 959]
## 20 180 20 180
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  1.5806e-07,  4.3126e-07, -1.6056e-07,
        -2.0045e-07,  1.1357e-06,  1.5630e-06,  2.7912e-07,  5.4332e-07],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 404, 404])
grad_out size torch.Size([1, 3, 202, 202])
arg size torch.Size([1, 3, 202, 202])
##############grad_in in maxp torch.Size([1, 3, 404, 404])
I am [5, 5]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 406, 406])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 404, 404])
grad_input torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 406, 406])
grad_input new torch.Size([1, 3, 406, 406])
new grad_input torch.Size([1, 3, 406, 406])
grad_out size torch.Size([1, 3, 404, 404])
crop [42, 42, 42, 42] [1558, 1961, 1558, 1961] [1600, 1919, 1600, 1919]
## 42 362 42 362
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  5.9184e-08,
        -2.4982e-08, -5.8356e-08,  7.2417e-08, -1.4786e-09, -1.0346e-07],
       device='cuda:0')
I am [5, 5]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 408, 408])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 408, 408])
grad_input new torch.Size([1, 3, 408, 408])
grad_input torch.Size([1, 3, 408, 408])
grad_out size torch.Size([1, 3, 406, 406])
crop [43, 43, 43, 43] [1557, 1962, 1557, 1962] [1600, 1919, 1600, 1919]
## 43 363 43 363
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -8.6476e-10,
        -7.0135e-09,  1.6992e-08,  2.2716e-08, -3.3835e-10,  9.3208e-09],
       device='cuda:0')
I am [5, 5]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 410, 410])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 408, 408])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 410, 410])
grad_out size torch.Size([1, 3, 408, 408])
crop [44, 44, 44, 44] [1556, 1963, 1556, 1963] [1600, 1919, 1600, 1919]
## 44 364 44 364
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  2.8190e-10,
         2.3480e-09, -1.3570e-09, -1.4474e-09, -3.1848e-09, -6.5494e-09],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 820, 820])
grad_out size torch.Size([1, 3, 410, 410])
arg size torch.Size([1, 3, 410, 410])
##############grad_in in maxp torch.Size([1, 3, 820, 820])
I am [5, 5]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 822, 822])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 820, 820])
grad_input torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 822, 822])
grad_input new torch.Size([1, 3, 822, 822])
new grad_input torch.Size([1, 3, 822, 822])
grad_out size torch.Size([1, 3, 820, 820])
crop [90, 90, 90, 90] [3110, 3929, 3110, 3929] [3200, 3839, 3200, 3839]
## 90 730 90 730
##############return grad_in in conv2d tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 5.5349e-11], device='cuda:0')
I am [5, 5]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 824, 824])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 824, 824])
grad_out size torch.Size([1, 3, 822, 822])
crop [91, 91, 91, 91] [torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cCheckpointBackward
[3109, 3930, 3109, 3930] [3200, 3839, 3200, 3839]
## 91 731 91 731
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.6577e-13],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 1648, 1648])
grad_out size torch.Size([1, 3, 824, 824])
arg size torch.Size([1, 3, 824, 824])
##############grad_in in maxp torch.Size([1, 3, 1648, 1648])
I am [5, 5]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 1650, 1650])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1648, 1648])
grad_input torch.Size([1, 3, 1650, 1650])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 1650, 1650])
grad_input new torch.Size([1, 3, 1650, 1650])
new grad_input torch.Size([1, 3, 1650, 1650])
grad_out size torch.Size([1, 3, 1648, 1648])
crop [184, 184, 184, 184] [6216, 7863, 6216, 7863] [6400, 7679, 6400, 7679]
## 184 1464 184 1464
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [5, 5]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 3, 1652, 1652])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1650, 1650])
final torch.Size([1, 3, 1652, 1652])
padding info :: [0, 0, 0, 0]
reshape_for_final ## 186 1466 186 1466
after crop g_in torch.Size([1, 3, 1280, 1280])
grad_out size torch.Size([1, 3, 1650, 1650])
crop [185, 185, 185, 185] [6215, 7864, 6215, 7864] [6400, 7679, 6400, 7679]
## 185 1465 185 1465
##############return grad_in in conv2d tensor([ 2.1608e-07, -4.0951e-07, -8.5774e-08, -4.9212e-07, -4.6803e-08,
         1.9713e-07,  6.8937e-08, -1.7293e-07,  5.6507e-08, -5.7593e-09],
       device='cuda:0')
[8, 8]
HREREERE torch.Size([1, 3, 10240, 10240])

############# Enter checkpointing bkward ####
inputs len 5 4
in customized Sequential 5
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 4]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <5,4,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 4]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <5,4,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 4]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <5,4,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 4]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <5,4,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 4]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <5,4,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 4]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <5,4,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 4]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <5,4,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 4]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <5,4,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 4]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <5,4,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 4]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <5,4,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 4]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <5,4,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 4]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <5,4,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 4]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <5,4,>,

in customized Sequential 3
need to get existing
[torch/csrc/autograd/python_engine.cpp] THPEngine_run_backward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 92, 92])
grad_out size torch.Size([1, 3, 320, 320])
arg size torch.Size([1, 3, 46, 46])
new_grad_out torch.Size([1, 3, 46, 46])
##############grad_in in maxp torch.Size([1, 3, 92, 92])
I am [5, 4]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 94, 94])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 92, 92])
grad_input torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 94, 94])
grad_input new torch.Size([1, 3, 94, 94])
new grad_input torch.Size([1, 3, 94, 94])
grad_out size torch.Size([1, 3, 92, 92])
crop [6, 6, 6, 6] [314, 405, 394, 485] [320, 399, 400, 479]
## 6 86 6 86
##############return grad_in in conv2d tensor([ 0.0007,  0.0010, -0.0031,  0.0043,  0.0049,  0.0011, -0.0075,  0.0002,
        -0.0017,  0.0024], device='cuda:0')
I am [5, 4]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 96, 96])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 96, 96])
grad_input new torch.Size([1, 3, 96, 96])
grad_input torch.Size([1, 3, 96, 96])
grad_out size torch.Size([1, 3, 94, 94])
crop [7, 7, 7, 7] [313, 406, 393, 486] [320, 399, 400, 479]
## 7 87 7 87
##############return grad_in in conv2d tensor([ 5.9893e-05,  3.9023e-07, -4.2557e-04,  1.0747e-03, -1.5964e-03,
         3.6382e-04, -1.0368e-03,  2.0587e-03, -2.0135e-04, -2.4953e-03],
       device='cuda:0')
I am [5, 4]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 98, 98])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 96, 96])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 98, 98])
grad_out size torch.Size([1, 3, 96, 96])
crop [8, 8, 8, 8] [312, 407, 392, 487] [320, 399, 400, 479]
## 8 88 8 88
##############return grad_in in conv2d tensor([ 1.3897e-06, -1.0191e-05, -7.1585e-06,  5.5029e-05, -1.0715e-04,
        -3.9182e-04,  2.8232e-04,  1.9231e-04,  7.0720e-04, -8.6698e-04],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 196, 196])
grad_out size torch.Size([1, 3, 98, 98])
arg size torch.Size([1, 3, 98, 98])
##############grad_in in maxp torch.Size([1, 3, 196, 196])
I am [5, 4]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 198, 198])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 196, 196])
grad_input torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 198, 198])
grad_input new torch.Size([1, 3, 198, 198])
new grad_input torch.Size([1, 3, 198, 198])
grad_out size torch.Size([1, 3, 196, 196])
crop [18, 18, 18, 18] [622, 817, 782, 977] [640, 799, 800, 959]
## 18 178 18 178
##############return grad_in in conv2d tensor([ 1.6000e-06, -7.8545e-07, -8.6150e-07, -6.1182e-07, -2.8099e-06,
         1.3655e-08,  1.9211e-05, -8.8145e-06, -9.9073e-06,  0.0000e+00],
       device='cuda:0')
I am [5, 4]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 200, 200])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 200, 200])
grad_input new torch.Size([1, 3, 200, 200])
grad_input torch.Size([1, 3, 200, 200])
grad_out size torch.Size([1, 3, 198, 198])
crop [19, 19, 19, 19] [621, 818, 781, 978] [640, 799, 800, 959]
## 19 179 19 179
##############return grad_in in conv2d tensor([ 4.6457e-07, -5.0245e-07,  2.6357e-07, -4.6071e-07, -1.1785e-06,
        -6.3640e-07,  5.2571e-06, -6.7417e-06,  3.0803e-06, -3.4844e-06],
       device='cuda:0')
I am [5, 4]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 202, 202])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 200, 200])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 202, 202])
grad_out size torch.Size([1, 3, 200, 200])
crop [20, 20, 20, 20] [620, 819, 780, 979] [640, 799, 800, 959]
## 20 180 20 180
##############return grad_in in conv2d tensor([-2.0638e-08, -6.4930e-08, -1.6516e-08,  1.4398e-07,  1.5821e-07,
         2.3337e-07, -3.1164e-07, -6.8585e-07, -2.4849e-08,  1.3245e-06],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 404, 404])
grad_out size torch.Size([1, 3, 202, 202])
arg size torch.Size([1, 3, 202, 202])
##############grad_in in maxp torch.Size([1, 3, 404, 404])
I am [5, 4]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 406, 406])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 404, 404])
grad_input torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 406, 406])
grad_input new torch.Size([1, 3, 406, 406])
new grad_input torch.Size([1, 3, 406, 406])
grad_out size torch.Size([1, 3, 404, 404])
crop [42, 42, 42, 42] [1238, 1641, 1558, 1961] [1280, 1599, 1600, 1919]
## 42 362 42 362
##############return grad_in in conv2d tensor([-1.9313e-09,  3.9595e-09, -2.0286e-08, -4.2758e-09,  2.8676e-09,
        -6.2257e-09,  3.2257e-08, -2.5818e-08, -1.0487e-08, -2.0759e-08],
       device='cuda:0')
I am [5, 4]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 408, 408])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 408, 408])
grad_input new torch.Size([1, 3, 408, 408])
grad_input torch.Size([1, 3, 408, 408])
grad_out size torch.Size([1, 3, 406, 406])
crop [43, 43, 43, 43] [1237, 1642, 1557, 1962] [1280, 1599, 1600, 1919]
## 43 363 43 363
##############return grad_in in conv2d tensor([-7.6739e-10, -4.9769e-10,  1.2500e-09, -6.0483e-09, -3.4773e-09,
        -2.6738e-09, -4.6163e-09, -9.8663e-09,  7.5060e-09, -4.7350e-09],
       device='cuda:0')
I am [5, 4]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 410, 410])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 408, 408])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 410, 410])
grad_out size torch.Size([1, 3, 408, 408])
crop [44, 44, 44, 44] [1236, 1643, 1556, 1963] [1280, 1599, 1600, 1919]
## 44 364 44 364
##############return grad_in in conv2d tensor([-8.2357e-11,  2.3125e-10, -3.6030e-10,  8.4571e-10,  1.4855e-09,
         1.9262e-09,  1.1838e-09,  3.2686e-09,  1.3913e-09,  2.9107e-09],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 820, 820])
grad_out size torch.Size([1, 3, 410, 410])
arg size torch.Size([1, 3, 410, 410])
##############grad_in in maxp torch.Size([1, 3, 820, 820])
I am [5, 4]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 822, 822])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 820, 820])
grad_input torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 822, 822])
grad_input new torch.Size([1, 3, 822, 822])
new grad_input torch.Size([1, 3, 822, 822])
grad_out size torch.Size([1, 3, 820, 820])
crop [90, 90, 90, 90] [2470, 3289, 3110, 3929] [2560, 3199, 3200, 3839]
## 90 730 90 730
##############return grad_in in conv2d tensor([-4.4843e-11, -2.7006e-11, -7.1384e-12, -2.8707e-11,  1.4503e-11,
        -3.3177e-12,  2.6898e-11,  0.0000e+00,  0.0000e+00,  2.7821e-10],
       device='cuda:0')
I am [5, 4]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 824, 824])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 824, 824])
grad_out size torch.Size([1, 3, 822, 822])
crop [91, 91, 91, 91] [2469, 3290, 3109, 3930] [2560, 3199, 3200, 3839]
## 91 731 91 731
##############return grad_in in conv2d tensor([-4.0368e-12, -3.4493e-12, -1.8190e-12, -4.1106e-12, -7.3829e-13,
         5.0164e-12,  6.7427e-12,  6.9347e-13,  1.9087e-12, -1.6865e-12],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 1648, 1648])
grad_out size torch.Size([1, 3, 824, 824])
arg size torch.Size([1, 3, 824, 824])
##############grad_in in maxp torch.Size([1, 3, 1648, 1648])
I am [5, 4]
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cCheckpointBackward
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 1650, 1650])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1648, 1648])
grad_input torch.Size([1, 3, 1650, 1650])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 1650, 1650])
grad_input new torch.Size([1, 3, 1650, 1650])
new grad_input torch.Size([1, 3, 1650, 1650])
grad_out size torch.Size([1, 3, 1648, 1648])
crop [184, 184, 184, 184] [4936, 6583, 6216, 7863] [5120, 6399, 6400, 7679]
## 184 1464 184 1464
##############return grad_in in conv2d tensor([ 1.5484e-13,  4.4045e-13,  9.2809e-13,  3.2919e-13,  5.1251e-14,
        -5.5418e-13,  1.2406e-12,  2.0659e-12,  1.0018e-12,  6.5498e-13],
       device='cuda:0')
I am [5, 4]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 3, 1652, 1652])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1650, 1650])
final torch.Size([1, 3, 1652, 1652])
padding info :: [0, 0, 0, 0]
reshape_for_final ## 186 1466 186 1466
after crop g_in torch.Size([1, 3, 1280, 1280])
grad_out size torch.Size([1, 3, 1650, 1650])
crop [185, 185, 185, 185] [4935, 6584, 6215, 7864] [5120, 6399, 6400, 7679]
## 185 1465 185 1465
##############return grad_in in conv2d tensor([-8.9365e-08,  2.1713e-07,  8.7749e-08,  4.1302e-07, -7.6028e-07,
        -2.8349e-07,  1.1988e-07, -8.4392e-09, -3.7716e-07,  2.3285e-07],
       device='cuda:0')
[8, 8]
HREREERE torch.Size([1, 3, 10240, 10240])

############# Enter checkpointing bkward ####
inputs len 5 4
in customized Sequential 5
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 3]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <5,3,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 3]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <5,3,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 3]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <5,3,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 3]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <5,3,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 3]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <5,3,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 3]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <5,3,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 3]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <5,3,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 3]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <5,3,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 3]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <5,3,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 3]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <5,3,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 3]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <5,3,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 3]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <5,3,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 3]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <5,3,>,

in customized Sequential 3
need to get existing

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 92, 92])
grad_out size torch.Size([1, 3, 320, 320])
arg size torch.Size([1, 3, 46, 46])
new_grad_out torch.Size([1, 3, 46, 46])
##############grad_in in maxp torch.Size([1, 3, 92, 92])
I am [5, 3]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 94, 94])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 92, 92])
[torch/csrc/autograd/python_engine.cpp] THPEngine_run_backward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
grad_input torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 94, 94])
grad_input new torch.Size([1, 3, 94, 94])
new grad_input torch.Size([1, 3, 94, 94])
grad_out size torch.Size([1, 3, 92, 92])
crop [6, 6, 6, 6] [234, 325, 394, 485] [240, 319, 400, 479]
## 6 86 6 86
##############return grad_in in conv2d tensor([ 3.1844e-03, -4.1073e-03,  4.1942e-05, -6.1132e-06, -4.0587e-04,
        -1.4964e-03,  3.4178e-04, -7.0402e-03, -3.0271e-04,  2.2985e-03],
       device='cuda:0')
I am [5, 3]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 96, 96])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 96, 96])
grad_input new torch.Size([1, 3, 96, 96])
grad_input torch.Size([1, 3, 96, 96])
grad_out size torch.Size([1, 3, 94, 94])
crop [7, 7, 7, 7] [233, 326, 393, 486] [240, 319, 400, 479]
## 7 87 7 87
##############return grad_in in conv2d tensor([ 5.9166e-04,  2.7572e-04,  9.1788e-04,  1.0634e-03, -2.3622e-04,
         1.7482e-05,  4.0285e-05, -2.1450e-03,  1.4161e-03, -1.6573e-03],
       device='cuda:0')
I am [5, 3]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 98, 98])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 96, 96])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 98, 98])
grad_out size torch.Size([1, 3, 96, 96])
crop [8, 8, 8, 8] [232, 327, 392, 487] [240, 319, 400, 479]
## 8 88 8 88
##############return grad_in in conv2d tensor([-1.4276e-05,  1.5030e-04, -1.5551e-04,  2.7925e-04, -3.2289e-05,
        -1.8701e-04, -2.5468e-05, -1.9552e-04,  2.0150e-04,  5.6714e-04],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 196, 196])
grad_out size torch.Size([1, 3, 98, 98])
arg size torch.Size([1, 3, 98, 98])
##############grad_in in maxp torch.Size([1, 3, 196, 196])
I am [5, 3]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 198, 198])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 196, 196])
grad_input torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 198, 198])
grad_input new torch.Size([1, 3, 198, 198])
new grad_input torch.Size([1, 3, 198, 198])
grad_out size torch.Size([1, 3, 196, 196])
crop [18, 18, 18, 18] [462, 657, 782, 977] [480, 639, 800, 959]
## 18 178 18 178
##############return grad_in in conv2d tensor([ 1.6525e-05, -1.4385e-05,  7.6960e-06,  8.2366e-06,  0.0000e+00,
        -4.0918e-05,  1.4786e-05,  3.8198e-05, -6.6309e-05,  8.9803e-06],
       device='cuda:0')
I am [5, 3]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 200, 200])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 200, 200])
grad_input new torch.Size([1, 3, 200, 200])
grad_input torch.Size([1, 3, 200, 200])
grad_out size torch.Size([1, 3, 198, 198])
crop [19, 19, 19, 19] [461, 658, 781, 978] [480, 639, 800, 959]
## 19 179 19 179
##############return grad_in in conv2d tensor([ 5.0174e-06, -3.8309e-06,  8.1150e-06,  7.4515e-07,  4.5027e-06,
        -1.0349e-05,  1.5488e-07, -3.8360e-06, -1.1069e-05,  2.1060e-05],
       device='cuda:0')
I am [5, 3]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 202, 202])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 200, 200])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 202, 202])
grad_out size torch.Size([1, 3, 200, 200])
crop [20, 20, 20, 20] [460, 659, 780, 979] [480, 639, 800, 959]
## 20 180 20 180
##############return grad_in in conv2d tensor([-3.9775e-07, -8.9119e-07,  5.2719e-07, -3.0318e-07, -2.2699e-06,
         1.6259e-06,  3.1964e-06, -8.4143e-07, -3.1701e-06,  3.8587e-06],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 404, 404])
grad_out size torch.Size([1, 3, 202, 202])
arg size torch.Size([1, 3, 202, 202])
##############grad_in in maxp torch.Size([1, 3, 404, 404])
I am [5, 3]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 406, 406])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 404, 404])
grad_input torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 406, 406])
grad_input new torch.Size([1, 3, 406, 406])
new grad_input torch.Size([1, 3, 406, 406])
grad_out size torch.Size([1, 3, 404, 404])
crop [42, 42, 42, 42] [918, 1321, 1558, 1961] [960, 1279, 1600, 1919]
## 42 362 42 362
##############return grad_in in conv2d tensor([ 0.0000e+00, -1.5321e-07,  6.7330e-08,  4.7121e-08, -1.7988e-07,
         1.8696e-07,  3.9478e-08,  2.4695e-08, -1.5698e-07, -3.6334e-07],
       device='cuda:0')
I am [5, 3]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 408, 408])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 408, 408])
grad_input new torch.Size([1, 3, 408, 408])
grad_input torch.Size([1, 3, 408, 408])
grad_out size torch.Size([1, 3, 406, 406])
crop [43, 43, 43, 43] [917, 1322, 1557, 1962] [960, 1279, 1600, 1919]
## 43 363 43 363
##############return grad_in in conv2d tensor([ 0.0000e+00,  2.7416e-09,  1.9476e-08, -5.8412e-08, -7.9723e-08,
        -1.6886e-08, -1.5564e-08,  1.6746e-08,  5.7978e-08, -7.9859e-08],
       device='cuda:0')
I am [5, 3]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 410, 410])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 408, 408])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 410, 410])
grad_out size torch.Size([1, 3, 408, 408])
crop [44, 44, 44, 44] [916, 1323, 1556, 1963] [960, 1279, 1600, 1919]
## 44 364 44 364
##############return grad_in in conv2d tensor([ 0.0000e+00, -6.8815e-10, -6.3494e-09,  1.4880e-09,  1.1612e-08,
         1.7910e-08,  2.3698e-08,  1.6515e-08,  1.2157e-09, -1.2223e-08],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 820, 820])
grad_out size torch.Size([1, 3, 410, 410])
arg size torch.Size([1, 3, 410, 410])
##############grad_in in maxp torch.Size([1, 3, 820, 820])
I am [5, 3]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 822, 822])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 820, 820])
grad_input torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 822, 822])
grad_input new torch.Size([1, 3, 822, 822])
new grad_input torch.Size([1, 3, 822, 822])
grad_out size torch.Size([1, 3, 820, 820])
crop [90, 90, 90, 90] [1830, 2649, 3110, 3929] [1920, 2559, 3200, 3839]
## 90 730 90 730
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  5.3533e-11, -1.1993e-09,
        -8.9715e-10, -1.3103e-09, -3.2072e-09, -1.1240e-09, -1.4444e-09],
       device='cuda:0')
I am [5, 3]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 824, 824])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 824, 824])
grad_out size torch.Size([1, 3, 822, 822])
crop [91, 91, 91, 91] [1829, 2650, 3109, 3930] [1920, 2559, 3200, 3839]
## 91 731 91 731
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  8.4011e-12,  2.3708e-11,
        -3.0094e-11, -6.1539e-10, -5.5235e-10, -6.5235e-10, -8.4782e-10],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 1648, 1648])
grad_out size torch.Size([1, 3, 824, 824])
arg size torch.Size([1, 3, 824, 824])
##############grad_in in maxp torch.Size([1, 3, 1648, 1648])
I am [5, 3]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 1650, 1650])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1648, 1648])
grad_input torch.Size([1, 3, 1650, 1650])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 1650, 1650])
grad_input new torch.Size([1, 3, 1650, 1650])
new grad_input torch.Size([1, 3, 1650, 1650])
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cCheckpointBackward
[torch/csrc/autograd/python_engine.cpp] THPEngine_run_backward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
grad_out size torch.Size([1, 3, 1648, 1648])
crop [184, 184, 184, 184] [3656, 5303, 6216, 7863] [3840, 5119, 6400, 7679]
## 184 1464 184 1464
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00, -7.9110e-13,  7.9313e-13,  9.1016e-13, -9.0936e-13],
       device='cuda:0')
I am [5, 3]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 3, 1652, 1652])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1650, 1650])
final torch.Size([1, 3, 1652, 1652])
padding info :: [0, 0, 0, 0]
reshape_for_final ## 186 1466 186 1466
after crop g_in torch.Size([1, 3, 1280, 1280])
grad_out size torch.Size([1, 3, 1650, 1650])
crop [185, 185, 185, 185] [3655, 5304, 6215, 7864] [3840, 5119, 6400, 7679]
## 185 1465 185 1465
##############return grad_in in conv2d tensor([ 6.8806e-07, -9.6809e-08, -8.8248e-08, -5.2162e-07,  3.8281e-07,
        -5.9149e-07,  1.1280e-07, -2.3080e-07, -4.8106e-07, -8.3167e-08],
       device='cuda:0')
[8, 8]
HREREERE torch.Size([1, 3, 10240, 10240])

############# Enter checkpointing bkward ####
inputs len 5 4
in customized Sequential 5
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 2]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <5,2,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 2]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <5,2,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 2]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <5,2,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 2]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <5,2,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 2]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <5,2,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 2]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <5,2,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 2]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <5,2,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 2]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <5,2,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 2]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <5,2,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 2]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <5,2,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 2]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <5,2,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 2]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <5,2,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 2]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <5,2,>,

in customized Sequential 3
need to get existing

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 92, 92])
grad_out size torch.Size([1, 3, 320, 320])
arg size torch.Size([1, 3, 46, 46])
new_grad_out torch.Size([1, 3, 46, 46])
##############grad_in in maxp torch.Size([1, 3, 92, 92])
I am [5, 2]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 94, 94])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 92, 92])
grad_input torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 94, 94])
grad_input new torch.Size([1, 3, 94, 94])
new grad_input torch.Size([1, 3, 94, 94])
grad_out size torch.Size([1, 3, 92, 92])
crop [6, 6, 6, 6] [154, 245, 394, 485] [160, 239, 400, 479]
## 6 86 6 86
##############return grad_in in conv2d [torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
tensor([ 0.0000, -0.0034, -0.0001, -0.0012,  0.0066,  0.0022,  0.0007,  0.0030,
         0.0019,  0.0031], device='cuda:0')
I am [5, 2]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 96, 96])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 96, 96])
grad_input new torch.Size([1, 3, 96, 96])
grad_input torch.Size([1, 3, 96, 96])
grad_out size torch.Size([1, 3, 94, 94])
crop [7, 7, 7, 7] [153, 246, 393, 486] [160, 239, 400, 479]
## 7 87 7 87
##############return grad_in in conv2d tensor([ 0.0000e+00,  2.3508e-04, -7.2983e-06,  5.5032e-05, -2.0633e-04,
        -1.1755e-03, -2.1607e-03, -2.2001e-03, -1.4057e-04, -5.6069e-04],
       device='cuda:0')
I am [5, 2]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 98, 98])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 96, 96])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 98, 98])
grad_out size torch.Size([1, 3, 96, 96])
crop [8, 8, 8, 8] [152, 247, 392, 487] [160, 239, 400, 479]
## 8 88 8 88
##############return grad_in in conv2d tensor([ 0.0000e+00,  1.2175e-04, -1.3485e-04, -5.7038e-05, -6.2532e-06,
         2.3297e-04, -4.1477e-04, -6.8108e-04,  1.9294e-04,  6.0467e-04],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 196, 196])
grad_out size torch.Size([1, 3, 98, 98])
arg size torch.Size([1, 3, 98, 98])
##############grad_in in maxp torch.Size([1, 3, 196, 196])
I am [5, 2]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 198, 198])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 196, 196])
grad_input torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 198, 198])
grad_input new torch.Size([1, 3, 198, 198])
new grad_input torch.Size([1, 3, 198, 198])
grad_out size torch.Size([1, 3, 196, 196])
crop [18, 18, 18, 18] [302, 497, 782, 977] [320, 479, 800, 959]
## 18 178 18 178
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -4.0594e-05,
         1.9928e-05,  2.5038e-05, -1.0863e-07, -4.5537e-08, -3.7541e-07],
       device='cuda:0')
I am [5, 2]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 200, 200])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 200, 200])
grad_input new torch.Size([1, 3, 200, 200])
grad_input torch.Size([1, 3, 200, 200])
grad_out size torch.Size([1, 3, 198, 198])
crop [19, 19, 19, 19] [301, 498, 781, 978] [320, 479, 800, 959]
## 19 179 19 179
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.1787e-05,
         1.2748e-05, -5.7265e-06,  7.5468e-06,  5.4638e-06,  3.2537e-07],
       device='cuda:0')
I am [5, 2]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 202, 202])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 200, 200])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 202, 202])
grad_out size torch.Size([1, 3, 200, 200])
crop [20, 20, 20, 20] [300, 499, 780, 979] [320, 479, 800, 959]
## 20 180 20 180
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  5.2361e-07,
         1.6474e-06,  3.4296e-07, -3.5398e-06, -4.0986e-07,  6.8254e-07],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 404, 404])
grad_out size torch.Size([1, 3, 202, 202])
arg size torch.Size([1, 3, 202, 202])
##############grad_in in maxp torch.Size([1, 3, 404, 404])
I am [5, 2]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 406, 406])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 404, 404])
grad_input torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 406, 406])
grad_input new torch.Size([1, 3, 406, 406])
new grad_input torch.Size([1, 3, 406, 406])
grad_out size torch.Size([1, 3, 404, 404])
crop [42, 42, 42, 42] [598, 1001, 1558, 1961] [640, 959, 1600, 1919]
## 42 362 42 362
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  4.8999e-08, -1.7471e-07],
       device='cuda:0')
I am [5, 2]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 408, 408])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 408, 408])
grad_input new torch.Size([1, 3, 408, 408])
grad_input torch.Size([1, 3, 408, 408])
grad_out size torch.Size([1, 3, 406, 406])
crop [43, 43, 43, 43] [597, 1002, 1557, 1962] [640, 959, 1600, 1919]
## 43 363 43 363
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  1.9470e-08, -6.1886e-10],
       device='cuda:0')
I am [5, 2]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 410, 410])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 408, 408])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 410, 410])
grad_out size torch.Size([1, 3, 408, 408])
crop [44, 44, 44, 44] [596, 1003, 1556, 1963] [640, 959, 1600, 1919]
## 44 364 44 364
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  2.0895e-09, -7.4066e-09],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 820, 820])
grad_out size torch.Size([1, 3, 410, 410])
arg size torch.Size([1, 3, 410, 410])
##############grad_in in maxp torch.Size([1, 3, 820, 820])
I am [5, 2]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 822, 822])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 820, 820])
grad_input torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 822, 822])
grad_input new torch.Size([1, 3, 822, 822])
new grad_input torch.Size([1, 3, 822, 822])
grad_out size torch.Size([1, 3, 820, 820])
crop [90, 90, 90, 90] [1190, 2009, 3110, 3929] [1280, 1919, 3200, 3839]
## 90 730 90 730
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [5, 2]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 824, 824])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 824, 824])
grad_out size torch.Size([1, 3, 822, 822])
crop [91, 91, 91, 91] [1189, 2010, 3109, 3930] [1280, 1919, 3200, 3839]
## 91 731 91 731
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 1648, 1648])
grad_out size torch.Size([1, 3, 824, 824])
arg size torch.Size([1, 3, 824, 824])
##############grad_in in maxp torch.Size([1, 3, 1648, 1648])
I am [5, 2]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 1650, 1650])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1648, 1648])
grad_input torch.Size([1, 3, 1650, 1650])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 1650, 1650])
grad_input new torch.Size([1, 3, 1650, 1650])
new grad_input torch.Size([1, 3, 1650, 1650])
grad_out size torch.Size([1, 3, 1648, 1648])
crop [184, 184, 184, 184] [2376, 4023, 6216, 7863] [2560, 3839, 6400, 7679]
## 184 1464 184 1464
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [5, 2]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 3, 1652, 1652])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1650, 1650])
final torch.Size([1, 3, 1652, 1652])
padding info :: [0, 0, 0, 0]
reshape_for_final ## 186 1466 186 1466
after crop g_in torch.Size([1, 3, 1280, 1280])
grad_out size [torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cCheckpointBackward
[torch/csrc/autograd/python_engine.cpp] THPEngine_run_backward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
torch.Size([1, 3, 1650, 1650])
crop [185, 185, 185, 185] [2375, 4024, 6215, 7864] [2560, 3839, 6400, 7679]
## 185 1465 185 1465
##############return grad_in in conv2d tensor([ 3.5962e-07,  6.3386e-07, -3.6069e-07,  6.6754e-07,  4.3503e-07,
        -4.5430e-07,  2.0538e-07, -3.7780e-07, -7.3368e-08, -1.7207e-07],
       device='cuda:0')
[8, 8]
HREREERE torch.Size([1, 3, 10240, 10240])

############# Enter checkpointing bkward ####
inputs len 5 4
in customized Sequential 5
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 1]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <5,1,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 1]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <5,1,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 1]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <5,1,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 1]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <5,1,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 1]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <5,1,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 1]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <5,1,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 1]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <5,1,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 1]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <5,1,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 1]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <5,1,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 1]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <5,1,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 1]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <5,1,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 1]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <5,1,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 1]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <5,1,>,

in customized Sequential 3
need to get existing

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 92, 92])
grad_out size torch.Size([1, 3, 320, 320])
arg size torch.Size([1, 3, 46, 46])
new_grad_out torch.Size([1, 3, 46, 46])
##############grad_in in maxp torch.Size([1, 3, 92, 92])
I am [5, 1]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 94, 94])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 92, 92])
grad_input torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 94, 94])
grad_input new torch.Size([1, 3, 94, 94])
new grad_input torch.Size([1, 3, 94, 94])
grad_out size torch.Size([1, 3, 92, 92])
crop [6, 6, 6, 6] [74, 165, 394, 485] [80, 159, 400, 479]
## 6 86 6 86
##############return grad_in in conv2d tensor([ 0.0000,  0.0000,  0.0000, -0.0007, -0.0019,  0.0006,  0.0000,  0.0000,
         0.0000,  0.0000], device='cuda:0')
I am [5, 1]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 96, 96])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 96, 96])
grad_input new torch.Size([1, 3, 96, 96])
grad_input torch.Size([1, 3, 96, 96])
grad_out size torch.Size([1, 3, 94, 94])
crop [7, 7, 7, 7] [73, 166, 393, 486] [80, 159, 400, 479]
## 7 87 7 87
##############return grad_in in conv2d [torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cCheckpointBackward
tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00, -3.9651e-04,  5.2782e-04,
        -1.6473e-04,  1.7064e-04, -1.3368e-05,  0.0000e+00,  0.0000e+00],
       device='cuda:0')
I am [5, 1]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 98, 98])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 96, 96])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 98, 98])
grad_out size torch.Size([1, 3, 96, 96])
crop [8, 8, 8, 8] [72, 167, 392, 487] [80, 159, 400, 479]
## 8 88 8 88
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00, -6.4062e-05,  1.2587e-04,
         8.4590e-05, -1.2480e-04, -2.3703e-05, -1.3627e-05,  1.1834e-05],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 196, 196])
grad_out size torch.Size([1, 3, 98, 98])
arg size torch.Size([1, 3, 98, 98])
##############grad_in in maxp torch.Size([1, 3, 196, 196])
I am [5, 1]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 198, 198])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 196, 196])
grad_input torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 198, 198])
grad_input new torch.Size([1, 3, 198, 198])
new grad_input torch.Size([1, 3, 198, 198])
grad_out size torch.Size([1, 3, 196, 196])
crop [18, 18, 18, 18] [142, 337, 782, 977] [160, 319, 800, 959]
## 18 178 18 178
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [5, 1]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 200, 200])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 200, 200])
grad_input new torch.Size([1, 3, 200, 200])
grad_input torch.Size([1, 3, 200, 200])
grad_out size torch.Size([1, 3, 198, 198])
crop [19, 19, 19, 19] [141, 338, 781, 978] [160, 319, 800, 959]
## 19 179 19 179
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [5, 1]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 202, 202])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 200, 200])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 202, 202])
grad_out size torch.Size([1, 3, 200, 200])
crop [20, 20, 20, 20] [140, 339, 780, 979] [160, 319, 800, 959]
## 20 180 20 180
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 404, 404])
grad_out size torch.Size([1, 3, 202, 202])
arg size torch.Size([1, 3, 202, 202])
##############grad_in in maxp torch.Size([1, 3, 404, 404])
I am [5, 1]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 406, 406])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 404, 404])
grad_input torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 406, 406])
grad_input new torch.Size([1, 3, 406, 406])
new grad_input torch.Size([1, 3, 406, 406])
grad_out size torch.Size([1, 3, 404, 404])
crop [42, 42, 42, 42] [278, 681, 1558, 1961] [320, 639, 1600, 1919]
## 42 362 42 362
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [5, 1]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 408, 408])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 408, 408])
grad_input new torch.Size([1, 3, 408, 408])
grad_input torch.Size([1, 3, 408, 408])
grad_out size torch.Size([1, 3, 406, 406])
crop [43, 43, 43, 43] [277, 682, 1557, 1962] [320, 639, 1600, 1919]
## 43 363 43 363
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [5, 1]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 410, 410])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 408, 408])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 410, 410])
grad_out size torch.Size([1, 3, 408, 408])
crop [44, 44, 44, 44] [276, 683, 1556, 1963] [320, 639, 1600, 1919]
## 44 364 44 364
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 820, 820])
grad_out size torch.Size([1, 3, 410, 410])
arg size torch.Size([1, 3, 410, 410])
##############grad_in in maxp torch.Size([1, 3, 820, 820])
I am [5, 1]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 822, 822])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 820, 820])
grad_input torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 822, 822])
grad_input new torch.Size([1, 3, 822, 822])
new grad_input torch.Size([1, 3, 822, 822])
grad_out size torch.Size([1, 3, 820, 820])
crop [90, 90, 90, 90] [550, 1369, 3110, 3929] [640, 1279, 3200, 3839]
## 90 730 90 730
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [5, 1]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 824, 824])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 824, 824])
grad_out size torch.Size([1, 3, 822, 822])
crop [91, 91, 91, 91] [549, 1370, 3109, 3930] [640, 1279, 3200, 3839]
## 91 731 91 731
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 1648, 1648])
grad_out size torch.Size([1, 3, 824, 824])
arg size torch.Size([1, 3, 824, 824])
##############grad_in in maxp torch.Size([1, 3, 1648, 1648])
I am [5, 1]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 1650, 1650])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1648, 1648])
grad_input torch.Size([1, 3, 1650, 1650])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 1650, 1650])
grad_input new torch.Size([1, 3, 1650, 1650])
new grad_input torch.Size([1, 3, 1650, 1650])
grad_out size torch.Size([1, 3, 1648, 1648])
crop [184, 184, 184, 184] [1096, 2743, 6216, 7863] [1280, 2559, 6400, 7679]
## 184 1464 184 1464
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [5, 1]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 3, 1652, 1652])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1650, 1650])
final torch.Size([1, 3, 1652, 1652])
padding info :: [0, 0, 0, 0]
reshape_for_final ## 186 1466 186 1466
after crop g_in torch.Size([1, 3, 1280, 1280])
grad_out size torch.Size([1, 3, 1650, 1650])
crop [185, 185, 185, 185] [1095, 2744, 6215, 7864] [1280, 2559, 6400, 7679]
## 185 1465 185 1465
##############return grad_in in conv2d tensor([-2.2659e-07, -1.3398e-07,  5.3092e-07, -3.2640e-07, -3.2204e-07,
        -1.7176e-07, -2.1093e-07, -1.4859e-07,  5.7362e-07,  5.5860e-07],
       device='cuda:0')
[8, 8]
HREREERE torch.Size([1, 3, 10240, 10240])

############# Enter checkpointing bkward ####
inputs len 5 4
in customized Sequential 5
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 0]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1466])
myctx [[ 140046822727744[ PI( <5,0,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
[torch/csrc/autograd/python_engine.cpp] THPEngine_run_backward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
need to get existing
current fwd info [5, 0]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1464])
myctx [[ 140045015171856[ PI( <5,0,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 0]
shape input_tile_for_next
 torch.Size([1, 3, 822, 732])
myctx [[ 140045013800704[ PI( <5,0,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 0]
shape input_tile_for_next
 torch.Size([1, 3, 820, 730])
myctx [[ 140045013848944[ PI( <5,0,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 0]
shape input_tile_for_next
 torch.Size([1, 3, 408, 366])
myctx [[ 140045013206640[ PI( <5,0,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 0]
shape input_tile_for_next
 torch.Size([1, 3, 406, 364])
myctx [[ 140045013206784[ PI( <5,0,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 0]
shape input_tile_for_next
 torch.Size([1, 3, 404, 362])
myctx [[ 140045013206880[ PI( <5,0,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 0]
shape input_tile_for_next
 torch.Size([1, 3, 200, 182])
myctx [[ 140045013206352[ PI( <5,0,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 0]
shape input_tile_for_next
 torch.Size([1, 3, 198, 180])
myctx [[ 140045013206304[ PI( <5,0,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 0]
shape input_tile_for_next
 torch.Size([1, 3, 196, 178])
myctx [[ 140045013207072[ PI( <5,0,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 0]
shape input_tile_for_next
 torch.Size([1, 3, 96, 90])
myctx [[ 140045013206400[ PI( <5,0,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 0]
shape input_tile_for_next
 torch.Size([1, 3, 94, 88])
myctx [[ 140045013206448[ PI( <5,0,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [5, 0]
shape input_tile_for_next
 torch.Size([1, 3, 92, 86])
myctx [[ 140045013206976[ PI( <5,0,>,

in customized Sequential 3
need to get existing

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 92, 86])
grad_out size torch.Size([1, 3, 320, 320])
arg size torch.Size([1, 3, 46, 43])
new_grad_out torch.Size([1, 3, 46, 43])
##############grad_in in maxp torch.Size([1, 3, 92, 86])
I am [5, 0]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 94, 88])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 92, 86])
grad_input torch.Size([1, 3, 94, 88])
padding info :: [1, 0, 0, 0]
grad_input old torch.Size([1, 3, 94, 88])
grad_input new torch.Size([1, 3, 94, 88])
new grad_input torch.Size([1, 3, 94, 88])
grad_out size torch.Size([1, 3, 92, 86])
crop [0, 6, 6, 6] [0, 85, 394, 485] [0, 79, 400, 479]
## 6 86 0 80
##############return grad_in in conv2d tensor([ 0.0000e+00, -4.1861e-03, -3.6011e-05, -2.7651e-04, -7.4281e-04,
         2.1993e-04,  1.7813e-04, -3.2177e-03,  4.4336e-03,  6.8343e-04],
       device='cuda:0')
I am [5, 0]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 96, 90])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 94, 88])
padding info :: [2, 0, 0, 0]
grad_input old torch.Size([1, 3, 96, 90])
grad_input new torch.Size([1, 3, 96, 90])
grad_input torch.Size([1, 3, 96, 90])
grad_out size torch.Size([1, 3, 94, 87])
crop [0, 7, 7, 7] [0, 86, 393, 486] [0, 79, 400, 479]
## 7 87 0 80
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  1.0037e-04,  1.0639e-03, -5.7599e-05,
        -6.5553e-05,  4.7803e-05, -5.8122e-04, -4.0739e-04, -6.8583e-04],
       device='cuda:0')
I am [5, 0]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 98, 92])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 96, 90])
padding info :: [3, 0, 0, 0]
grad_input torch.Size([1, 3, 98, 89])
grad_out size torch.Size([1, 3, 96, 88])
crop [0, 8, 8, 8] [0, 87, 392, 487] [0, 79, 400, 479]
## 8 88 0 80
##############return grad_in in conv2d tensor([ 2.5419e-04,  1.6656e-04, -2.3623e-04, -5.2623e-05,  1.5469e-05,
        -1.8003e-04,  1.7529e-04, -2.5860e-04, -7.0944e-05,  3.6069e-04],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 196, 178])
grad_out size torch.Size([1, 3, 98, 89])
arg size torch.Size([1, 3, 98, 89])
##############grad_in in maxp torch.Size([1, 3, 196, 178])
I am [5, 0]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 198, 180])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 196, 178])
grad_input torch.Size([1, 3, 198, 180])
padding info :: [1, 0, 0, 0]
grad_input old torch.Size([1, 3, 198, 180])
grad_input new torch.Size([1, 3, 198, 180])
new grad_input torch.Size([1, 3, 198, 180])
grad_out size torch.Size([1, 3, 196, 178])
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cCheckpointBackward
crop [0, 18, 18, 18] [0, 177, 782, 977] [0, 159, 800, 959]
## 18 178 0 160
##############return grad_in in conv2d tensor([ 0.0000e+00, -2.1620e-05, -4.7455e-05,  1.6124e-05,  1.7908e-05,
        -3.5041e-05,  1.7488e-05,  2.0482e-05,  8.2617e-06, -1.6312e-06],
       device='cuda:0')
I am [5, 0]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 200, 182])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 198, 180])
padding info :: [2, 0, 0, 0]
grad_input old torch.Size([1, 3, 200, 182])
grad_input new torch.Size([1, 3, 200, 182])
grad_input torch.Size([1, 3, 200, 182])
grad_out size torch.Size([1, 3, 198, 179])
crop [0, 19, 19, 19] [0, 178, 781, 978] [0, 159, 800, 959]
## 19 179 0 160
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00, -9.1916e-06,  3.4260e-06, -7.8448e-06,
        -2.5310e-06,  8.8022e-06, -9.9077e-06,  1.4441e-05, -3.8747e-07],
       device='cuda:0')
I am [5, 0]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 202, 184])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 200, 182])
padding info :: [3, 0, 0, 0]
grad_input torch.Size([1, 3, 202, 181])
grad_out size torch.Size([1, 3, 200, 180])
crop [0, 20, 20, 20] [0, 179, 780, 979] [0, 159, 800, 959]
## 20 180 0 160
##############return grad_in in conv2d tensor([ 4.9844e-06,  8.1775e-07, -2.7851e-06,  1.6703e-06,  1.5779e-06,
        -4.0120e-06, -6.9277e-07,  4.5529e-07, -1.7927e-07, -9.7439e-07],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 404, 362])
grad_out size torch.Size([1, 3, 202, 181])
arg size torch.Size([1, 3, 202, 181])
##############grad_in in maxp torch.Size([1, 3, 404, 362])
I am [5, 0]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 406, 364])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 404, 362])
grad_input torch.Size([1, 3, 406, 364])
padding info :: [1, 0, 0, 0]
grad_input old torch.Size([1, 3, 406, 364])
grad_input new torch.Size([1, 3, 406, 364])
new grad_input torch.Size([1, 3, 406, 364])
grad_out size torch.Size([1, 3, 404, 362])
crop [0, 42, 42, 42] [0, 361, 1558, 1961] [0, 319, 1600, 1919]
## 42 362 0 320
##############return grad_in in conv2d tensor([ 0.0000e+00,  1.1621e-07, -6.8639e-07,  3.6576e-07, -6.3784e-07,
        -2.1246e-07,  9.2954e-08,  4.9989e-07,  3.4018e-07,  2.8554e-07],
       device='cuda:0')
I am [5, 0]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 408, 366])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 406, 364])
padding info :: [2, 0, 0, 0]
grad_input old torch.Size([1, 3, 408, 366])
grad_input new torch.Size([1, 3, 408, 366])
grad_input torch.Size([1, 3, 408, 366])
grad_out size torch.Size([1, 3, 406, 363])
crop [0, 43, 43, 43] [0, 362, 1557, 1962] [0, 319, 1600, 1919]
## 43 363 0 320
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  3.8664e-08,  2.5216e-07, -1.7559e-07,
        -3.3246e-08,  2.4671e-07, -1.8530e-07, -7.7081e-08,  3.0330e-07],
       device='cuda:0')
I am [5, 0]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 410, 368])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 408, 366])
padding info :: [3, 0, 0, 0]
grad_input torch.Size([1, 3, 410, 365])
grad_out size torch.Size([1, 3, 408, 364])
crop [0, 44, 44, 44] [0, 363, 1556, 1963] [0, 319, 1600, 1919]
## 44 364 0 320
##############return grad_in in conv2d tensor([-7.8498e-08, -2.5230e-08, -6.0600e-09, -3.7347e-08, -1.3290e-08,
        -2.2383e-08, -5.0319e-08, -4.1004e-08, -5.1468e-08, -3.9795e-08],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 820, 730])
grad_out size torch.Size([1, 3, 410, 365])
arg size torch.Size([1, 3, 410, 365])
##############grad_in in maxp torch.Size([1, 3, 820, 730])
I am [5, 0]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 822, 732])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 820, 730])
grad_input torch.Size([1, 3, 822, 732])
padding info :: [1, 0, 0, 0]
grad_input old torch.Size([1, 3, 822, 732])
grad_input new torch.Size([1, 3, 822, 732])
new grad_input torch.Size([1, 3, 822, 732])
grad_out size torch.Size([1, 3, 820, 730])
crop [0, 90, 90, 90] [0, 729, 3110, 3929] [0, 639, 3200, 3839]
## 90 730 0 640
##############return grad_in in conv2d tensor([ 0.0000e+00, -2.4832e-09,  7.6348e-09, -1.7046e-08, -4.7149e-09,
        -7.5804e-09,  9.7490e-09,  7.1642e-09, -8.7814e-09, -1.4575e-08],
       device='cuda:0')
I am [5, 0]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 824, 734])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 822, 732])
padding info :: [2, 0, 0, 0]
grad_input torch.Size([1, 3, 824, 732])
grad_out size torch.Size([1, 3, 822, 731])
crop [0, 91, 91, 91] [0, 730, 3109, 3930] [0, 639, 3200, 3839]
## 91 731 0 640
##############return grad_in in conv2d tensor([-1.4156e-09, -4.2842e-09, -1.4964e-09, -1.7209e-09,  6.8526e-10,
        -5.9458e-10, -2.9553e-10, -1.1139e-09, -3.7670e-09, -3.9720e-09],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 1648, 1464])
grad_out size torch.Size([1, 3, 824, 732])
arg size torch.Size([1, 3, 824, 732])
##############grad_in in maxp torch.Size([1, 3, 1648, 1464])
I am [5, 0]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 1650, 1466])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1648, 1464])
grad_input torch.Size([1, 3, 1650, 1466])
padding info :: [1, 0, 0, 0]
grad_input old torch.Size([1, 3, 1650, 1466])
grad_input new torch.Size([1, 3, 1650, 1466])
new grad_input torch.Size([1, 3, 1650, 1466])
grad_out size torch.Size([1, 3, 1648, 1464])
crop [0, 184, 184, 184] [0, 1463, 6216, 7863] [0, 1279, 6400, 7679]
## 184 1464 0 1280
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.6433e-10,  3.6584e-10,
         3.6631e-11, -2.4838e-11, -7.7499e-11,  2.7465e-11,  1.6591e-10],
       device='cuda:0')
I am [5, 0]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 3, 1652, 1468])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1650, 1466])
final torch.Size([1, 3, 1652, 1468])
padding info :: [2, 0, 0, 0]
reshape_for_final ## 186 1466 0 1280
after crop g_in torch.Size([1, 3, 1280, 1280])
grad_out size torch.Size([1, 3, 1650, 1465])
crop [0, 185, 185, 185] [0, 1464, 6215, 7864] [0, 1279, 6400, 7679]
## 185 1465 0 1280
##############return grad_in in conv2d tensor([-1.1589e-07, -4.3318e-08, -7.0375e-08, -1.0975e-07, -1.1570e-07,
        -2.7520e-08, -9.4237e-08, -2.6806e-07,  1.7924e-07, -2.0618e-07],
       device='cuda:0')
[8, 8]
HREREERE torch.Size([1, 3, 10240, 10240])

############# Enter checkpointing bkward ####
inputs len 5 4
in customized Sequential 5
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 7]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1466])
myctx [[ 140046822727744[ PI( <4,7,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 7]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1464])
myctx [[ 140045015171856[ PI( <4,7,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() [torch/csrc/autograd/python_engine.cpp] THPEngine_run_backward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 7]
shape input_tile_for_next
 torch.Size([1, 3, 822, 732])
myctx [[ 140045013800704[ PI( <4,7,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 7]
shape input_tile_for_next
 torch.Size([1, 3, 820, 730])
myctx [[ 140045013848944[ PI( <4,7,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 7]
shape input_tile_for_next
 torch.Size([1, 3, 408, 366])
myctx [[ 140045013206640[ PI( <4,7,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 7]
shape input_tile_for_next
 torch.Size([1, 3, 406, 364])
myctx [[ 140045013206784[ PI( <4,7,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 7]
shape input_tile_for_next
 torch.Size([1, 3, 404, 362])
myctx [[ 140045013206880[ PI( <4,7,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 7]
shape input_tile_for_next
 torch.Size([1, 3, 200, 182])
myctx [[ 140045013206352[ PI( <4,7,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 7]
shape input_tile_for_next
 torch.Size([1, 3, 198, 180])
myctx [[ 140045013206304[ PI( <4,7,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 7]
shape input_tile_for_next
 torch.Size([1, 3, 196, 178])
myctx [[ 140045013207072[ PI( <4,7,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 7]
shape input_tile_for_next
 torch.Size([1, 3, 96, 90])
myctx [[ 140045013206400[ PI( <4,7,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 7]
shape input_tile_for_next
 torch.Size([1, 3, 94, 88])
myctx [[ 140045013206448[ PI( <4,7,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 7]
shape input_tile_for_next
 torch.Size([1, 3, 92, 86])
myctx [[ 140045013206976[ PI( <4,7,>,

in customized Sequential 3
need to get existing

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 92, 86])
grad_out size torch.Size([1, 3, 320, 320])
arg size torch.Size([1, 3, 46, 43])
new_grad_out torch.Size([1, 3, 46, 43])
##############grad_in in maxp torch.Size([1, 3, 92, 86])
I am [4, 7]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 94, 88])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 92, 86])
grad_input torch.Size([1, 3, 94, 88])
padding info :: [0, 1, 0, 0]
grad_input old torch.Size([1, 3, 94, 88])
grad_input new torch.Size([1, 3, 94, 88])
new grad_input torch.Size([1, 3, 94, 88])
grad_out size torch.Size([1, 3, 92, 86])
crop [6, 0, 6, 6] [554, 639, 314, 405] [560, 639, 320, 399]
## 6 86 6 86
##############return grad_in in conv2d tensor([ 3.2540e-04, -5.7435e-04, -4.1162e-05,  3.5983e-04, -4.5930e-03,
         4.4148e-03, -4.5309e-03,  2.9618e-03, -2.7803e-03,  4.9186e-03],
       device='cuda:0')
I am [4, 7]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 96, 90])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 94, 88])
padding info :: [0, 2, 0, 0]
grad_input old torch.Size([1, 3, 96, 90])
grad_input new torch.Size([1, 3, 96, 90])
grad_input torch.Size([1, 3, 96, 90])
grad_out size torch.Size([1, 3, 94, 87])
crop [7, 0, 7, 7] [553, 639, 313, 406] [560, 639, 320, 399]
## 7 87 7 87
##############return grad_in in conv2d tensor([ 6.0460e-05,  4.5623e-05,  3.3546e-05,  4.5597e-04, -3.9423e-04,
         1.2250e-04, -1.2688e-03, -9.6241e-04, -1.5371e-03, -1.9558e-03],
       device='cuda:0')
I am [4, 7]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 98, 92])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 96, 90])
padding info :: [0, 3, 0, 0]
grad_input torch.Size([1, 3, 98, 89])
grad_out size torch.Size([1, 3, 96, 88])
crop [8, 0, 8, 8] [552, 639, 312, 407] [560, 639, 320, 399]
## 8 88 8 88
##############return grad_in in conv2d tensor([-1.4589e-06,  2.0840e-05, -3.3010e-05,  6.1871e-05,  8.0524e-05,
        -2.7959e-04,  2.3452e-04, -3.0611e-04,  2.2259e-04, -5.0576e-04],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 196, 178])
grad_out size torch.Size([1, 3, 98, 89])
arg size torch.Size([1, 3, 98, 89])
##############grad_in in maxp torch.Size([1, 3, 196, 178])
I am [4, 7]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 198, 180])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 196, 178])
grad_input torch.Size([1, 3, 198, 180])
padding info :: [0, 1, 0, 0]
grad_input old torch.Size([1, 3, 198, 180])
grad_input new torch.Size([1, 3, 198, 180])
new grad_input torch.Size([1, 3, 198, 180])
grad_out size torch.Size([1, 3, 196, 178])
crop [18, 0, 18, 18] [1102, 1279, 622, 817] [1120, 1279, 640, 799]
## 18 178 18 178
##############return grad_in in conv2d [torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cCheckpointBackward
tensor([ 1.6887e-06,  9.3171e-08,  3.3751e-07,  1.0948e-06, -4.3507e-06,
        -3.1497e-06,  1.0725e-05,  5.5213e-07, -1.2661e-07,  3.0577e-06],
       device='cuda:0')
I am [4, 7]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 200, 182])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 198, 180])
padding info :: [0, 2, 0, 0]
grad_input old torch.Size([1, 3, 200, 182])
grad_input new torch.Size([1, 3, 200, 182])
grad_input torch.Size([1, 3, 200, 182])
grad_out size torch.Size([1, 3, 198, 179])
crop [19, 0, 19, 19] [1101, 1279, 621, 818] [1120, 1279, 640, 799]
## 19 179 19 179
##############return grad_in in conv2d tensor([ 5.1271e-07,  6.2412e-08,  4.3083e-07,  5.7274e-07, -6.8417e-07,
        -3.6323e-07,  2.1603e-07,  1.3404e-06,  1.2312e-06,  1.6551e-06],
       device='cuda:0')
I am [4, 7]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 202, 184])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 200, 182])
padding info :: [0, 3, 0, 0]
grad_input torch.Size([1, 3, 202, 181])
grad_out size torch.Size([1, 3, 200, 180])
crop [20, 0, 20, 20] [1100, 1279, 620, 819] [1120, 1279, 640, 799]
## 20 180 20 180
##############return grad_in in conv2d tensor([-4.0645e-08, -1.1123e-07, -1.3671e-08, -8.6249e-08, -1.6736e-07,
         4.0416e-07,  3.9957e-07, -1.0024e-06, -1.4220e-07,  2.0284e-07],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 404, 362])
grad_out size torch.Size([1, 3, 202, 181])
arg size torch.Size([1, 3, 202, 181])
##############grad_in in maxp torch.Size([1, 3, 404, 362])
I am [4, 7]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 406, 364])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 404, 362])
grad_input torch.Size([1, 3, 406, 364])
padding info :: [0, 1, 0, 0]
grad_input old torch.Size([1, 3, 406, 364])
grad_input new torch.Size([1, 3, 406, 364])
new grad_input torch.Size([1, 3, 406, 364])
grad_out size torch.Size([1, 3, 404, 362])
crop [42, 0, 42, 42] [2198, 2559, 1238, 1641] [2240, 2559, 1280, 1599]
## 42 362 42 362
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00, -4.6347e-09,  5.9413e-09,  6.0911e-11,
         3.5949e-09,  1.5989e-09, -8.5143e-09,  8.8860e-09, -1.7263e-08],
       device='cuda:0')
I am [4, 7]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 408, 366])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 406, 364])
padding info :: [0, 2, 0, 0]
grad_input old torch.Size([1, 3, 408, 366])
grad_input new torch.Size([1, 3, 408, 366])
grad_input torch.Size([1, 3, 408, 366])
grad_out size torch.Size([1, 3, 406, 363])
crop [43, 0, 43, 43] [2197, 2559, 1237, 1642] [2240, 2559, 1280, 1599]
## 43 363 43 363
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  6.1298e-10,  2.4193e-09, -2.0077e-09,
        -3.8130e-09,  3.0737e-09,  1.3153e-09,  4.1305e-09, -4.2370e-09],
       device='cuda:0')
I am [4, 7]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 410, 368])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 408, 366])
padding info :: [0, 3, 0, 0]
grad_input torch.Size([1, 3, 410, 365])
grad_out size torch.Size([1, 3, 408, 364])
crop [44, 0, 44, 44] [2196, 2559, 1236, 1643] [2240, 2559, 1280, 1599]
## 44 364 44 364
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  2.3042e-11, -4.3060e-10, -4.3449e-10,
         1.6662e-10, -2.6201e-11,  5.2031e-10, -9.2381e-10, -6.7154e-10],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 820, 730])
grad_out size torch.Size([1, 3, 410, 365])
arg size torch.Size([1, 3, 410, 365])
##############grad_in in maxp torch.Size([1, 3, 820, 730])
I am [4, 7]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 822, 732])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 820, 730])
grad_input torch.Size([1, 3, 822, 732])
padding info :: [0, 1, 0, 0]
grad_input old torch.Size([1, 3, 822, 732])
grad_input new torch.Size([1, 3, 822, 732])
new grad_input torch.Size([1, 3, 822, 732])
grad_out size torch.Size([1, 3, 820, 730])
crop [90, 0, 90, 90] [4390, 5119, 2470, 3289] [4480, 5119, 2560, 3199]
## 90 730 90 730
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  4.0585e-11,
         2.7431e-11,  2.8407e-11,  1.3959e-10, -1.1066e-10, -5.1976e-11],
       device='cuda:0')
I am [4, 7]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 824, 734])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 822, 732])
padding info :: [0, 2, 0, 0]
grad_input torch.Size([1, 3, 824, 732])
grad_out size torch.Size([1, 3, 822, 731])
crop [91, 0, 91, 91] [4389, 5119, 2469, 3290] [4480, 5119, 2560, 3199]
## 91 731 91 731
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  6.1005e-12,
         1.2207e-11,  6.4258e-12,  2.3476e-11,  6.8160e-12, -5.0594e-11],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 1648, 1464])
grad_out size torch.Size([1, 3, 824, 732])
arg size torch.Size([1, 3, 824, 732])
##############grad_in in maxp torch.Size([1, 3, 1648, 1464])
I am [4, 7]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 1650, 1466])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1648, 1464])
grad_input torch.Size([1, 3, 1650, 1466])
padding info :: [0, 1, 0, 0]
grad_input old torch.Size([1, 3, 1650, 1466])
grad_input new torch.Size([1, 3, 1650, 1466])
new grad_input torch.Size([1, 3, 1650, 1466])
grad_out size torch.Size([1, 3, 1648, 1464])
crop [184, 0, 184, 184] [8776, 10239, 4936, 6583] [8960, 10239, 5120, 6399]
## 184 1464 184 1464
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00, -6.7137e-14, -6.2063e-13],
       device='cuda:0')
I am [4, 7]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 3, 1652, 1468])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1650, 1466])
final torch.Size([1, 3, 1652, 1468])
padding info :: [0, 2, 0, 0]
reshape_for_final ## 186 1466 186 1466
after crop g_in torch.Size([1, 3, 1280, 1280])
grad_out size torch.Size([1, 3, 1650, 1465])
crop [185, 0, 185, 185] [8775, 10239, 4935, 6584] [8960, 10239, 5120, 6399]
## 185 1465 185 1465
##############return grad_in in conv2d tensor([-6.2008e-08, -2.2833e-07, -3.8203e-07,  3.0888e-07, -2.1775e-07,
        -3.1556e-07, -4.3248e-07, -4.5820e-09, -5.7703e-07, -1.9785e-07],
       device='cuda:0')
[8, 8]
HREREERE torch.Size([1, 3, 10240, 10240])

############# Enter checkpointing bkward ####
inputs len 5 4
in customized Sequential 5
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 6]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <4,6,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 6]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <4,6,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
[torch/csrc/autograd/python_engine.cpp] THPEngine_run_backward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
need to get existing
current fwd info [4, 6]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <4,6,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 6]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <4,6,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 6]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <4,6,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 6]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <4,6,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 6]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <4,6,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 6]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <4,6,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 6]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <4,6,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 6]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <4,6,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 6]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <4,6,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 6]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <4,6,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 6]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <4,6,>,

in customized Sequential 3
need to get existing

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 92, 92])
grad_out size torch.Size([1, 3, 320, 320])
arg size torch.Size([1, 3, 46, 46])
new_grad_out torch.Size([1, 3, 46, 46])
##############grad_in in maxp torch.Size([1, 3, 92, 92])
I am [4, 6]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 94, 94])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 92, 92])
grad_input torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 94, 94])
grad_input new torch.Size([1, 3, 94, 94])
new grad_input torch.Size([1, 3, 94, 94])
grad_out size torch.Size([1, 3, 92, 92])
crop [6, 6, 6, 6] [474, 565, 314, 405] [480, 559, 320, 399]
## 6 86 6 86
##############return grad_in in conv2d tensor([ 0.0012,  0.0033, -0.0010, -0.0005, -0.0035,  0.0035, -0.0014, -0.0015,
         0.0036, -0.0005], device='cuda:0')
I am [4, 6]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 96, 96])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 96, 96])
grad_input new torch.Size([1, 3, 96, 96])
grad_input torch.Size([1, 3, 96, 96])
grad_out size torch.Size([1, 3, 94, 94])
crop [7, 7, 7, 7] [473, 566, 313, 406] [480, 559, 320, 399]
## 7 87 7 87
##############return grad_in in conv2d tensor([ 6.9899e-04, -9.3047e-04,  2.9040e-04, -5.7881e-04, -9.0913e-06,
        -3.4317e-04, -2.8999e-04, -1.3837e-03,  6.5699e-04, -7.3604e-04],
       device='cuda:0')
I am [4, 6]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 98, 98])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 96, 96])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 98, 98])
grad_out size torch.Size([1, 3, 96, 96])
crop [8, 8, 8, 8] [472, 567, 312, 407] [480, 559, 320, 399]
## 8 88 8 88
##############return grad_in in conv2d tensor([ 1.1293e-04, -2.2188e-04, -1.4912e-04,  1.7508e-04,  1.3975e-04,
        -3.1544e-05,  6.8704e-05, -3.0896e-04, -3.2947e-05,  4.0117e-04],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 196, 196])
grad_out size torch.Size([1, 3, 98, 98])
arg size torch.Size([1, 3, 98, 98])
##############grad_in in maxp torch.Size([1, 3, 196, 196])
I am [4, 6]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 198, 198])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 196, 196])
grad_input torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 198, 198])
grad_input new torch.Size([1, 3, 198, 198])
new grad_input torch.Size([1, 3, 198, 198])
grad_out size torch.Size([1, 3, 196, 196])
crop [18, 18, 18, 18] [942, 1137, 622, 817] [960, 1119, 640, 799]
## 18 178 18 178
##############return grad_in in conv2d tensor([ 3.4072e-06,  8.3232e-06, -4.1918e-06, -4.5441e-06,  0.0000e+00,
         5.6066e-05, -2.0712e-05, -1.5792e-05, -4.2113e-05, -1.8827e-05],
       device='cuda:0')
I am [4, 6]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 200, 200])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 200, 200])
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cCheckpointBackward
grad_input new torch.Size([1, 3, 200, 200])
grad_input torch.Size([1, 3, 200, 200])
grad_out size torch.Size([1, 3, 198, 198])
crop [19, 19, 19, 19] [941, 1138, 621, 818] [960, 1119, 640, 799]
## 19 179 19 179
##############return grad_in in conv2d tensor([ 1.0290e-06,  2.5203e-06, -1.8803e-06,  1.8478e-06, -1.1891e-06,
         1.5525e-05, -1.8051e-05,  9.0138e-06, -1.3464e-05, -3.2250e-06],
       device='cuda:0')
I am [4, 6]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 202, 202])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 200, 200])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 202, 202])
grad_out size torch.Size([1, 3, 200, 200])
crop [20, 20, 20, 20] [940, 1139, 620, 819] [960, 1119, 640, 799]
## 20 180 20 180
##############return grad_in in conv2d tensor([-8.1484e-08, -3.3589e-07, -3.5921e-07, -1.4051e-07,  4.9163e-07,
        -4.8471e-07, -2.1154e-06, -5.9581e-07,  2.8838e-06,  1.5986e-06],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 404, 404])
grad_out size torch.Size([1, 3, 202, 202])
arg size torch.Size([1, 3, 202, 202])
##############grad_in in maxp torch.Size([1, 3, 404, 404])
I am [4, 6]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 406, 406])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 404, 404])
grad_input torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 406, 406])
grad_input new torch.Size([1, 3, 406, 406])
new grad_input torch.Size([1, 3, 406, 406])
grad_out size torch.Size([1, 3, 404, 404])
crop [42, 42, 42, 42] [1878, 2281, 1238, 1641] [1920, 2239, 1280, 1599]
## 42 362 42 362
##############return grad_in in conv2d tensor([-4.2484e-09,  3.5924e-09, -7.5060e-08, -3.1298e-08, -3.6178e-09,
        -6.7521e-08, -1.2444e-08,  1.9928e-08,  7.9008e-08,  1.7063e-08],
       device='cuda:0')
I am [4, 6]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 408, 408])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 408, 408])
grad_input new torch.Size([1, 3, 408, 408])
grad_input torch.Size([1, 3, 408, 408])
grad_out size torch.Size([1, 3, 406, 406])
crop [43, 43, 43, 43] [1877, 2282, 1237, 1642] [1920, 2239, 1280, 1599]
## 43 363 43 363
##############return grad_in in conv2d tensor([-1.6881e-09, -2.0076e-09, -4.1200e-09, -2.8044e-08, -2.3912e-08,
        -1.3562e-08, -1.5454e-10,  1.3460e-10, -1.6528e-08,  6.1592e-09],
       device='cuda:0')
I am [4, 6]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 410, 410])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 408, 408])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 410, 410])
grad_out size torch.Size([1, 3, 408, 408])
crop [44, 44, 44, 44] [1876, 2283, 1236, 1643] [1920, 2239, 1280, 1599]
## 44 364 44 364
##############return grad_in in conv2d tensor([-1.8117e-10,  4.0260e-10, -1.0310e-09,  3.9975e-09,  8.0945e-09,
         1.1774e-08,  1.0349e-08,  4.2459e-09,  1.7465e-09,  1.4964e-09],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 820, 820])
grad_out size torch.Size([1, 3, 410, 410])
arg size torch.Size([1, 3, 410, 410])
##############grad_in in maxp torch.Size([1, 3, 820, 820])
I am [4, 6]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 822, 822])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 820, 820])
grad_input torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 822, 822])
grad_input new torch.Size([1, 3, 822, 822])
new grad_input torch.Size([1, 3, 822, 822])
grad_out size torch.Size([1, 3, 820, 820])
crop [90, 90, 90, 90] [3750, 4569, 2470, 3289] [3840, 4479, 2560, 3199]
## 90 730 90 730
##############return grad_in in conv2d tensor([-3.3930e-11, -2.6411e-11, -5.6182e-11, -5.0676e-11, -8.4730e-11,
        -1.9309e-10,  5.9837e-10, -2.9208e-10, -1.1196e-09, -1.5740e-11],
       device='cuda:0')
I am [4, 6]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 824, 824])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 824, 824])
grad_out size torch.Size([1, 3, 822, 822])
crop [91, 91, 91, 91] [3749, 4570, 2469, 3290] [3840, 4479, 2560, 3199]
## 91 731 91 731
##############return grad_in in conv2d tensor([ 2.0567e-13, -8.8721e-13, -2.1422e-11, -3.5979e-11, -2.4737e-11,
        -5.3515e-12, -1.5600e-11, -1.7245e-10,  7.8656e-11,  1.9277e-10],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 1648, 1648])
grad_out size torch.Size([1, 3, 824, 824])
arg size torch.Size([1, 3, 824, 824])
##############grad_in in maxp torch.Size([1, 3, 1648, 1648])
I am [4, 6]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 1650, 1650])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1648, 1648])
grad_input torch.Size([1, 3, 1650, 1650])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 1650, 1650])
grad_input new torch.Size([1, 3, 1650, 1650])
new grad_input torch.Size([1, 3, 1650, 1650])
grad_out size torch.Size([1, 3, 1648, 1648])
crop [184, 184, 184, 184] [7496, 9143, 4936, 6583] [7680, 8959, 5120, 6399]
## 184 1464 184 1464
##############return grad_in in conv2d tensor([ 0.0000e+00,  8.0113e-13, -6.4313e-14, -2.8431e-13,  1.2701e-12,
         1.5276e-12, -3.5556e-13,  0.0000e+00,  0.0000e+00,  2.6262e-12],
       device='cuda:0')
I am [4, 6]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 3, 1652, 1652])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1650, 1650])
final torch.Size([1, 3, 1652, 1652])
padding info :: [0, 0, 0, 0]
reshape_for_final ## 186 1466 186 1466
after crop g_in torch.Size([1, 3, 1280, 1280])
grad_out size torch.Size([1, 3, 1650, 1650])
crop [185, 185, 185, 185] [7495, 9144, 4935, 6584] [7680, 8959, 5120, 6399]
## 185 1465 185 1465
##############return grad_in in conv2d tensor([ 1.7514e-07, -5.7634e-07,  6.2902e-08,  5.3147e-07,  5.1865e-07,
         5.4896e-07, -3.3425e-07,  2.2928e-07,  2.1822e-07, -1.2747e-07],
       device='cuda:0')
[8, 8]
HREREERE torch.Size([1, 3, 10240, 10240])

############# Enter checkpointing bkward ####
inputs len 5 4
in customized Sequential 5
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 5]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <4,5,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 5]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <4,5,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 5]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <4,5,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() [torch/csrc/autograd/python_engine.cpp] THPEngine_run_backward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 5]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <4,5,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 5]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <4,5,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 5]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <4,5,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 5]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <4,5,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 5]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <4,5,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 5]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <4,5,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 5]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <4,5,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 5]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <4,5,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 5]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <4,5,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 5]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <4,5,>,

in customized Sequential 3
need to get existing

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 92, 92])
grad_out size torch.Size([1, 3, 320, 320])
arg size torch.Size([1, 3, 46, 46])
new_grad_out torch.Size([1, 3, 46, 46])
##############grad_in in maxp torch.Size([1, 3, 92, 92])
I am [4, 5]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 94, 94])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 92, 92])
grad_input torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 94, 94])
grad_input new torch.Size([1, 3, 94, 94])
new grad_input torch.Size([1, 3, 94, 94])
grad_out size torch.Size([1, 3, 92, 92])
crop [6, 6, 6, 6] [394, 485, 314, 405] [400, 479, 320, 399]
## 6 86 6 86
##############return grad_in in conv2d tensor([ 0.0000, -0.0007, -0.0021,  0.0025, -0.0033,  0.0040, -0.0005, -0.0027,
         0.0039, -0.0032], device='cuda:0')
I am [4, 5]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 96, 96])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 96, 96])
grad_input new torch.Size([1, 3, 96, 96])
grad_input torch.Size([1, 3, 96, 96])
grad_out size torch.Size([1, 3, 94, 94])
crop [7, 7, 7, 7] [393, 486, 313, 406] [400, 479, 320, 399]
## 7 87 7 87
##############return grad_in in conv2d tensor([0.0000e+00, 7.3447e-05, 2.5349e-04, 4.3825e-04, 1.4666e-03, 7.8688e-04,
        1.3416e-03, 6.9556e-04, 5.3522e-04, 4.4322e-05], device='cuda:0')
I am [4, 5]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 98, 98])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 96, 96])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 98, 98])
grad_out size torch.Size([1, 3, 96, 96])
crop [8, 8, 8, 8] [392, 487, 312, 407] [400, 479, 320, 399]
## 8 88 8 88
##############return grad_in in conv2d tensor([ 0.0000e+00,  2.3073e-05,  4.7176e-05, -1.1502e-04,  3.1488e-04,
        -1.2834e-04, -2.3433e-05,  3.1889e-05, -1.3412e-04,  2.6508e-04],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 196, 196])
grad_out size torch.Size([1, 3, 98, 98])
arg size torch.Size([1, 3, 98, 98])
##############grad_in in maxp torch.Size([1, 3, 196, 196])
I am [4, 5]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 198, 198])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 196, 196])
grad_input torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 198, 198])
grad_input new torch.Size([1, 3, 198, 198])
new grad_input torch.Size([1, 3, 198, 198])
grad_out size torch.Size([1, 3, 196, 196])
crop [18, 18, 18, 18] [782, 977, 622, 817] [800, 959, 640, 799]
## 18 178 18 178
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  3.7493e-06, -1.0417e-05,
         1.2211e-05,  9.1941e-06, -3.0308e-07,  5.7016e-05, -3.4507e-05],
       device='cuda:0')
I am [4, 5]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 200, 200])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 200, 200])
grad_input new torch.Size([1, 3, 200, 200])
grad_input torch.Size([1, 3, 200, 200])
grad_out size torch.Size([1, 3, 198, 198])
crop [19, 19, 19, 19] [781, 978, 621, 818] [800, 959, 640, 799]
## 19 179 19 179
##############return grad_in in conv2d [torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cCheckpointBackward
tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0929e-06, -2.5974e-06,
         6.7454e-06, -6.7487e-07,  4.2079e-06,  1.9951e-05, -3.1864e-06],
       device='cuda:0')
I am [4, 5]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 202, 202])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 200, 200])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 202, 202])
grad_out size torch.Size([1, 3, 200, 200])
crop [20, 20, 20, 20] [780, 979, 620, 819] [800, 959, 640, 799]
## 20 180 20 180
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00, -8.8848e-08, -1.7430e-07,
         1.1815e-07, -3.9118e-07, -1.2156e-06, -1.5076e-06, -4.4880e-06],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 404, 404])
grad_out size torch.Size([1, 3, 202, 202])
arg size torch.Size([1, 3, 202, 202])
##############grad_in in maxp torch.Size([1, 3, 404, 404])
I am [4, 5]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 406, 406])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 404, 404])
grad_input torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 406, 406])
grad_input new torch.Size([1, 3, 406, 406])
new grad_input torch.Size([1, 3, 406, 406])
grad_out size torch.Size([1, 3, 404, 404])
crop [42, 42, 42, 42] [1558, 1961, 1238, 1641] [1600, 1919, 1280, 1599]
## 42 362 42 362
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00, -4.5726e-09, -8.4143e-09, -3.6855e-08],
       device='cuda:0')
I am [4, 5]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 408, 408])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 408, 408])
grad_input new torch.Size([1, 3, 408, 408])
grad_input torch.Size([1, 3, 408, 408])
grad_out size torch.Size([1, 3, 406, 406])
crop [43, 43, 43, 43] [1557, 1962, 1237, 1642] [1600, 1919, 1280, 1599]
## 43 363 43 363
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00, -1.8169e-09, -4.3515e-09, -5.4897e-11],
       device='cuda:0')
I am [4, 5]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 410, 410])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 408, 408])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 410, 410])
grad_out size torch.Size([1, 3, 408, 408])
crop [44, 44, 44, 44] [1556, 1963, 1236, 1643] [1600, 1919, 1280, 1599]
## 44 364 44 364
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00, -1.9499e-10,  1.7872e-10,  7.4316e-10],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 820, 820])
grad_out size torch.Size([1, 3, 410, 410])
arg size torch.Size([1, 3, 410, 410])
##############grad_in in maxp torch.Size([1, 3, 820, 820])
I am [4, 5]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 822, 822])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 820, 820])
grad_input torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 822, 822])
grad_input new torch.Size([1, 3, 822, 822])
new grad_input torch.Size([1, 3, 822, 822])
grad_out size torch.Size([1, 3, 820, 820])
crop [90, 90, 90, 90] [3110, 3929, 2470, 3289] [3200, 3839, 2560, 3199]
## 90 730 90 730
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [4, 5]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 824, 824])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 824, 824])
grad_out size torch.Size([1, 3, 822, 822])
crop [91, 91, 91, 91] [3109, 3930, 2469, 3290] [3200, 3839, 2560, 3199]
## 91 731 91 731
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 1648, 1648])
grad_out size torch.Size([1, 3, 824, 824])
arg size torch.Size([1, 3, 824, 824])
##############grad_in in maxp torch.Size([1, 3, 1648, 1648])
I am [4, 5]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 1650, 1650])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1648, 1648])
grad_input torch.Size([1, 3, 1650, 1650])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 1650, 1650])
grad_input new torch.Size([1, 3, 1650, 1650])
new grad_input torch.Size([1, 3, 1650, 1650])
grad_out size torch.Size([1, 3, 1648, 1648])
crop [184, 184, 184, 184] [6216, 7863, 4936, 6583] [6400, 7679, 5120, 6399]
## 184 1464 184 1464
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [4, 5]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 3, 1652, 1652])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1650, 1650])
final torch.Size([1, 3, 1652, 1652])
padding info :: [0, 0, 0, 0]
reshape_for_final ## 186 1466 186 1466
after crop g_in torch.Size([1, 3, 1280, 1280])
grad_out size torch.Size([1, 3, 1650, 1650])
crop [185, 185, 185, 185] [6215, 7864, 4935, 6584] [6400, 7679, 5120, 6399]
## 185 1465 185 1465
##############return grad_in in conv2d tensor([-2.3160e-07, -9.0235e-07,  1.2439e-07, -8.6375e-08,  1.1058e-06,
        -2.8532e-07, -6.6190e-07, -5.0416e-07, -1.4136e-08,  2.0091e-07],
       device='cuda:0')
[8, 8]
HREREERE torch.Size([1, 3, 10240, 10240])

############# Enter checkpointing bkward ####
inputs len 5 4
in customized Sequential 5
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 4]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <4,4,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 4]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <4,4,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 4]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <4,4,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 4]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <4,4,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
[torch/csrc/autograd/python_engine.cpp] THPEngine_run_backward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
need to get existing
current fwd info [4, 4]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <4,4,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 4]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <4,4,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 4]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <4,4,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 4]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <4,4,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 4]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <4,4,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 4]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <4,4,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 4]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <4,4,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 4]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <4,4,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 4]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <4,4,>,

in customized Sequential 3
need to get existing

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 92, 92])
grad_out size torch.Size([1, 3, 320, 320])
arg size torch.Size([1, 3, 46, 46])
new_grad_out torch.Size([1, 3, 46, 46])
##############grad_in in maxp torch.Size([1, 3, 92, 92])
I am [4, 4]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 94, 94])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 92, 92])
grad_input torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 94, 94])
grad_input new torch.Size([1, 3, 94, 94])
new grad_input torch.Size([1, 3, 94, 94])
grad_out size torch.Size([1, 3, 92, 92])
crop [6, 6, 6, 6] [314, 405, 314, 405] [320, 399, 320, 399]
## 6 86 6 86
##############return grad_in in conv2d tensor([ 1.8784e-03,  6.1235e-03, -4.0784e-03, -2.3238e-05,  3.8133e-04,
        -5.5424e-04, -4.7678e-06, -2.7539e-04,  3.8464e-03, -5.2219e-03],
       device='cuda:0')
I am [4, 4]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 96, 96])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 96, 96])
grad_input new torch.Size([1, 3, 96, 96])
grad_input torch.Size([1, 3, 96, 96])
grad_out size torch.Size([1, 3, 94, 94])
crop [7, 7, 7, 7] [313, 406, 313, 406] [320, 399, 320, 399]
## 7 87 7 87
##############return grad_in in conv2d tensor([ 9.0292e-04, -8.8503e-04,  4.6346e-04,  1.2671e-04,  8.5198e-04,
        -1.3265e-04,  1.0041e-04,  4.5992e-06,  1.0259e-03,  2.7826e-04],
       device='cuda:0')
I am [4, 4]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 98, 98])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 96, 96])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 98, 98])
grad_out size torch.Size([1, 3, 96, 96])
crop [8, 8, 8, 8] [312, 407, 312, 407] [320, 399, 320, 399]
## 8 88 8 88
##############return grad_in in conv2d tensor([ 1.4163e-04, -2.8874e-04, -1.1503e-04,  1.7673e-04,  2.6056e-04,
         3.9153e-05, -2.1805e-04,  1.4618e-05,  2.8335e-05,  1.9744e-04],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 196, 196])
grad_out size torch.Size([1, 3, 98, 98])
arg size torch.Size([1, 3, 98, 98])
##############grad_in in maxp torch.Size([1, 3, 196, 196])
I am [4, 4]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 198, 198])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 196, 196])
grad_input torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 198, 198])
grad_input new torch.Size([1, 3, 198, 198])
new grad_input torch.Size([1, 3, 198, 198])
grad_out size torch.Size([1, 3, 196, 196])
crop [18, 18, 18, 18] [622, 817, 622, 817] [640, 799, 640, 799]
## 18 178 18 178
##############return grad_in in conv2d tensor([ 1.1352e-05, -5.5729e-06, -6.1124e-06, -7.0053e-05,  6.2419e-05,
         6.0929e-05,  1.1895e-05,  2.8877e-06,  1.0604e-06,  3.1834e-05],
       device='cuda:0')
I am [4, 4]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 200, 200])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 200, 200])
grad_input new torch.Size([1, 3, 200, 200])
grad_input torch.Size([1, 3, 200, 200])
grad_out size torch.Size([1, 3, 198, 198])
crop [19, 19, 19, 19] [621, 818, 621, 818] [640, 799, 640, 799]
## 19 179 19 179
##############return grad_in in conv2d tensor([ 3.2962e-06, -3.5649e-06,  1.8701e-06, -2.1942e-05,  2.4838e-05,
        -1.5071e-05,  3.5500e-05,  5.5936e-06,  1.0702e-05,  1.3082e-05],
       device='cuda:0')
I am [4, 4]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 202, 202])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 200, 200])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 202, 202])
grad_out size torch.Size([1, 3, 200, 200])
crop [20, 20, 20, 20] [620, 819, 620, 819] [640, 799, 640, 799]
## 20 180 20 180
##############return grad_in in conv2d [torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cCheckpointBackward
tensor([-1.4643e-07, -4.6069e-07, -1.1719e-07,  1.8268e-06,  3.3124e-06,
        -1.7463e-07, -1.0417e-05, -1.5144e-06,  7.3398e-07, -3.4523e-06],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 404, 404])
grad_out size torch.Size([1, 3, 202, 202])
arg size torch.Size([1, 3, 202, 202])
##############grad_in in maxp torch.Size([1, 3, 404, 404])
I am [4, 4]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 406, 406])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 404, 404])
grad_input torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 406, 406])
grad_input new torch.Size([1, 3, 406, 406])
new grad_input torch.Size([1, 3, 406, 406])
grad_out size torch.Size([1, 3, 404, 404])
crop [42, 42, 42, 42] [1238, 1641, 1238, 1641] [1280, 1599, 1280, 1599]
## 42 362 42 362
##############return grad_in in conv2d tensor([-1.3703e-08,  2.6337e-09, -1.7946e-08,  7.2002e-08, -5.1376e-08,
         1.4650e-08,  1.6711e-07,  1.1449e-07,  7.3696e-08, -6.8490e-07],
       device='cuda:0')
I am [4, 4]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 408, 408])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 408, 408])
grad_input new torch.Size([1, 3, 408, 408])
grad_input torch.Size([1, 3, 408, 408])
grad_out size torch.Size([1, 3, 406, 406])
crop [43, 43, 43, 43] [1237, 1642, 1237, 1642] [1280, 1599, 1280, 1599]
## 43 363 43 363
##############return grad_in in conv2d tensor([-5.4447e-09,  6.2868e-09,  4.8334e-08, -2.4600e-08, -1.4307e-08,
         4.6342e-09,  8.1018e-08, -7.2608e-08, -4.2479e-07,  2.4470e-07],
       device='cuda:0')
I am [4, 4]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 410, 410])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 408, 408])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 410, 410])
grad_out size torch.Size([1, 3, 408, 408])
crop [44, 44, 44, 44] [1236, 1643, 1236, 1643] [1280, 1599, 1280, 1599]
## 44 364 44 364
##############return grad_in in conv2d tensor([-5.8433e-10,  2.3011e-09, -7.3021e-09, -2.7794e-09, -6.0324e-09,
        -9.0126e-10, -3.6279e-10, -2.0745e-08,  6.3533e-08,  1.9185e-08],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 820, 820])
grad_out size torch.Size([1, 3, 410, 410])
arg size torch.Size([1, 3, 410, 410])
##############grad_in in maxp torch.Size([1, 3, 820, 820])
I am [4, 4]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 822, 822])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 820, 820])
grad_input torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 822, 822])
grad_input new torch.Size([1, 3, 822, 822])
new grad_input torch.Size([1, 3, 822, 822])
grad_out size torch.Size([1, 3, 820, 820])
crop [90, 90, 90, 90] [2470, 3289, 2470, 3289] [2560, 3199, 2560, 3199]
## 90 730 90 730
##############return grad_in in conv2d tensor([-3.1816e-10, -1.9161e-10,  8.3812e-11,  5.9177e-10,  3.5568e-09,
         1.3913e-09,  1.3678e-09, -1.2410e-09, -7.5626e-10, -1.2505e-09],
       device='cuda:0')
I am [4, 4]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 824, 824])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 824, 824])
grad_out size torch.Size([1, 3, 822, 822])
crop [91, 91, 91, 91] [2469, 3290, 2469, 3290] [2560, 3199, 2560, 3199]
## 91 731 91 731
##############return grad_in in conv2d tensor([-2.8641e-11, -2.4473e-11,  5.9718e-12,  2.5927e-11,  4.9312e-10,
         8.3990e-10,  1.8626e-10, -2.0104e-10, -3.0863e-10, -4.5436e-10],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 1648, 1648])
grad_out size torch.Size([1, 3, 824, 824])
arg size torch.Size([1, 3, 824, 824])
##############grad_in in maxp torch.Size([1, 3, 1648, 1648])
I am [4, 4]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 1650, 1650])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1648, 1648])
grad_input torch.Size([1, 3, 1650, 1650])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 1650, 1650])
grad_input new torch.Size([1, 3, 1650, 1650])
new grad_input torch.Size([1, 3, 1650, 1650])
grad_out size torch.Size([1, 3, 1648, 1648])
crop [184, 184, 184, 184] [4936, 6583, 4936, 6583] [5120, 6399, 5120, 6399]
## 184 1464 184 1464
##############return grad_in in conv2d tensor([ 2.1616e-12,  8.1837e-12,  1.0370e-12,  1.7620e-12, -6.3526e-13,
        -4.2995e-13,  9.9119e-14, -1.3246e-11,  5.3194e-13, -4.0434e-11],
       device='cuda:0')
I am [4, 4]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 3, 1652, 1652])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1650, 1650])
final torch.Size([1, 3, 1652, 1652])
padding info :: [0, 0, 0, 0]
reshape_for_final ## 186 1466 186 1466
after crop g_in torch.Size([1, 3, 1280, 1280])
grad_out size torch.Size([1, 3, 1650, 1650])
crop [185, 185, 185, 185] [4935, 6584, 4935, 6584] [5120, 6399, 5120, 6399]
## 185 1465 185 1465
##############return grad_in in conv2d tensor([-3.9557e-07, -2.4991e-07, -8.7483e-08, -3.5979e-07, -2.2851e-07,
        -2.1470e-07, -7.2092e-08,  1.7474e-07,  1.6196e-07, -4.4305e-07],
       device='cuda:0')
[8, 8]
HREREERE torch.Size([1, 3, 10240, 10240])

############# Enter checkpointing bkward ####
inputs len 5 4
in customized Sequential 5
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 3]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <4,3,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 3]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <4,3,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 3]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <4,3,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 3]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <4,3,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 3]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <4,3,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() [torch/csrc/autograd/python_engine.cpp] THPEngine_run_backward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 3]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <4,3,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 3]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <4,3,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 3]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <4,3,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 3]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <4,3,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 3]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <4,3,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 3]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <4,3,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 3]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <4,3,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 3]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <4,3,>,

in customized Sequential 3
need to get existing

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 92, 92])
grad_out size torch.Size([1, 3, 320, 320])
arg size torch.Size([1, 3, 46, 46])
new_grad_out torch.Size([1, 3, 46, 46])
##############grad_in in maxp torch.Size([1, 3, 92, 92])
I am [4, 3]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 94, 94])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 92, 92])
grad_input torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 94, 94])
grad_input new torch.Size([1, 3, 94, 94])
new grad_input torch.Size([1, 3, 94, 94])
grad_out size torch.Size([1, 3, 92, 92])
crop [6, 6, 6, 6] [234, 325, 314, 405] [240, 319, 320, 399]
## 6 86 6 86
##############return grad_in in conv2d tensor([-0.0021, -0.0022,  0.0066, -0.0065,  0.0004,  0.0002, -0.0007, -0.0007,
        -0.0018,  0.0022], device='cuda:0')
I am [4, 3]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 96, 96])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 96, 96])
grad_input new torch.Size([1, 3, 96, 96])
grad_input torch.Size([1, 3, 96, 96])
grad_out size torch.Size([1, 3, 94, 94])
crop [7, 7, 7, 7] [233, 326, 313, 406] [240, 319, 320, 399]
## 7 87 7 87
##############return grad_in in conv2d tensor([-0.0005,  0.0007,  0.0009,  0.0006,  0.0007,  0.0018, -0.0003, -0.0004,
         0.0004,  0.0003], device='cuda:0')
I am [4, 3]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 98, 98])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 96, 96])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 98, 98])
grad_out size torch.Size([1, 3, 96, 96])
crop [8, 8, 8, 8] [232, 327, 312, 407] [240, 319, 320, 399]
## 8 88 8 88
##############return grad_in in conv2d tensor([-5.5332e-05,  1.1824e-04,  1.2352e-04,  1.4884e-04, -4.5539e-04,
         3.6652e-04,  1.4279e-04, -4.2585e-04,  5.1961e-05,  4.7265e-05],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 196, 196])
grad_out size torch.Size([1, 3, 98, 98])
arg size torch.Size([1, 3, 98, 98])
##############grad_in in maxp torch.Size([1, 3, 196, 196])
I am [4, 3]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 198, 198])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 196, 196])
grad_input torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 198, 198])
grad_input new torch.Size([1, 3, 198, 198])
new grad_input torch.Size([1, 3, 198, 198])
grad_out size torch.Size([1, 3, 196, 196])
crop [18, 18, 18, 18] [462, 657, 622, 817] [480, 639, 640, 799]
## 18 178 18 178
##############return grad_in in conv2d tensor([-5.8990e-06,  3.6430e-06,  7.4343e-06,  1.3430e-05, -1.6204e-05,
         3.9194e-06,  5.8253e-05, -1.3577e-05, -4.1932e-05,  1.4829e-05],
       device='cuda:0')
I am [4, 3]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 200, 200])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 200, 200])
grad_input new torch.Size([1, 3, 200, 200])
grad_input torch.Size([1, 3, 200, 200])
grad_out size torch.Size([1, 3, 198, 198])
crop [19, 19, 19, 19] [461, 658, 621, 818] [480, 639, 640, 799]
## 19 179 19 179
##############return grad_in in conv2d tensor([-1.6937e-06,  2.3997e-06, -6.4457e-08,  6.9074e-06, -2.7633e-06,
         1.4116e-06,  1.4256e-05,  5.6223e-07,  8.7526e-06,  1.0471e-05],
       device='cuda:0')
I am [4, 3]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 202, 202])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 200, 200])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 202, 202])
grad_out size torch.Size([1, 3, 200, 200])
crop [20, 20, 20, 20] [460, 659, 620, 819] [480, 639, 640, 799]
## 20 180 20 180
##############return grad_in in conv2d tensor([ 5.8068e-08,  1.9677e-07,  9.4878e-09, -1.3069e-06, -1.1284e-06,
         1.1310e-06, -3.2017e-07, -4.4597e-06, -2.8887e-07,  1.6071e-06],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 404, 404])
grad_out size torch.Size([1, 3, 202, 202])
arg size torch.Size([1, 3, 202, 202])
##############grad_in in maxp torch.Size([1, 3, 404, 404])
I am [4, 3]
@@@ using cudnn bkw
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cCheckpointBackward
local last ++ input shape torch.Size([1, 3, 406, 406])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 404, 404])
grad_input torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 406, 406])
grad_input new torch.Size([1, 3, 406, 406])
new grad_input torch.Size([1, 3, 406, 406])
grad_out size torch.Size([1, 3, 404, 404])
crop [42, 42, 42, 42] [918, 1321, 1238, 1641] [960, 1279, 1280, 1599]
## 42 362 42 362
##############return grad_in in conv2d tensor([ 0.0000e+00,  1.5290e-08, -2.0236e-08,  1.1052e-08,  2.0433e-08,
         1.6218e-08,  3.4751e-08, -4.6551e-08, -1.6003e-07, -8.4804e-08],
       device='cuda:0')
I am [4, 3]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 408, 408])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 408, 408])
grad_input new torch.Size([1, 3, 408, 408])
grad_input torch.Size([1, 3, 408, 408])
grad_out size torch.Size([1, 3, 406, 406])
crop [43, 43, 43, 43] [917, 1322, 1237, 1642] [960, 1279, 1280, 1599]
## 43 363 43 363
##############return grad_in in conv2d tensor([ 0.0000e+00,  4.2723e-09,  3.1067e-09, -1.0415e-08,  1.7307e-08,
        -9.1413e-09, -2.5476e-09,  3.1752e-08, -3.7401e-08, -6.8402e-08],
       device='cuda:0')
I am [4, 3]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 410, 410])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 408, 408])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 410, 410])
grad_out size torch.Size([1, 3, 408, 408])
crop [44, 44, 44, 44] [916, 1323, 1236, 1643] [960, 1279, 1280, 1599]
## 44 364 44 364
##############return grad_in in conv2d tensor([ 0.0000e+00,  4.7159e-10, -1.5962e-09,  2.7184e-10, -3.0489e-09,
        -6.1786e-10,  4.6295e-10, -3.8304e-09, -4.7536e-09,  7.6667e-09],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 820, 820])
grad_out size torch.Size([1, 3, 410, 410])
arg size torch.Size([1, 3, 410, 410])
##############grad_in in maxp torch.Size([1, 3, 820, 820])
I am [4, 3]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 822, 822])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 820, 820])
grad_input torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 822, 822])
grad_input new torch.Size([1, 3, 822, 822])
new grad_input torch.Size([1, 3, 822, 822])
grad_out size torch.Size([1, 3, 820, 820])
crop [90, 90, 90, 90] [1830, 2649, 2470, 3289] [1920, 2559, 2560, 3199]
## 90 730 90 730
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  2.4862e-10,  1.5048e-10,  3.9706e-11,
        -2.4414e-10, -6.7407e-10, -3.9001e-10,  5.3080e-10,  2.9143e-10],
       device='cuda:0')
I am [4, 3]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 824, 824])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 824, 824])
grad_out size torch.Size([1, 3, 822, 822])
crop [91, 91, 91, 91] [1829, 2650, 2469, 3290] [1920, 2559, 2560, 3199]
## 91 731 91 731
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  2.1970e-11,  1.8907e-11,  1.1557e-11,
         8.3514e-12, -8.9777e-11, -1.9393e-10, -7.7558e-12,  5.1600e-11],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 1648, 1648])
grad_out size torch.Size([1, 3, 824, 824])
arg size torch.Size([1, 3, 824, 824])
##############grad_in in maxp torch.Size([1, 3, 1648, 1648])
I am [4, 3]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 1650, 1650])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1648, 1648])
grad_input torch.Size([1, 3, 1650, 1650])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 1650, 1650])
grad_input new torch.Size([1, 3, 1650, 1650])
new grad_input torch.Size([1, 3, 1650, 1650])
grad_out size torch.Size([1, 3, 1648, 1648])
crop [184, 184, 184, 184] [3656, 5303, 4936, 6583] [3840, 5119, 5120, 6399]
## 184 1464 184 1464
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -5.3092e-12,
         5.3228e-12,  6.1082e-12, -2.5781e-13,  6.8445e-12, -6.7496e-12],
       device='cuda:0')
I am [4, 3]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 3, 1652, 1652])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1650, 1650])
final torch.Size([1, 3, 1652, 1652])
padding info :: [0, 0, 0, 0]
reshape_for_final ## 186 1466 186 1466
after crop g_in torch.Size([1, 3, 1280, 1280])
grad_out size torch.Size([1, 3, 1650, 1650])
crop [185, 185, 185, 185] [3655, 5304, 4935, 6584] [3840, 5119, 5120, 6399]
## 185 1465 185 1465
##############return grad_in in conv2d tensor([-2.2398e-07,  5.6080e-08, -2.3050e-07,  4.2888e-07,  7.3320e-08,
         4.0419e-07,  5.5650e-07, -2.1006e-07,  4.2903e-07,  1.6611e-07],
       device='cuda:0')
[8, 8]
HREREERE torch.Size([1, 3, 10240, 10240])

############# Enter checkpointing bkward ####
inputs len 5 4
in customized Sequential 5
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 2]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <4,2,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 2]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <4,2,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 2]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <4,2,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 2]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <4,2,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 2]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <4,2,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 2]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <4,2,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() [torch/csrc/autograd/python_engine.cpp] THPEngine_run_backward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 2]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <4,2,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 2]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <4,2,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 2]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <4,2,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 2]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <4,2,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 2]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <4,2,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 2]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <4,2,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 2]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <4,2,>,

in customized Sequential 3
need to get existing

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 92, 92])
grad_out size torch.Size([1, 3, 320, 320])
arg size torch.Size([1, 3, 46, 46])
new_grad_out torch.Size([1, 3, 46, 46])
##############grad_in in maxp torch.Size([1, 3, 92, 92])
I am [4, 2]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 94, 94])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 92, 92])
grad_input torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 94, 94])
grad_input new torch.Size([1, 3, 94, 94])
new grad_input torch.Size([1, 3, 94, 94])
grad_out size torch.Size([1, 3, 92, 92])
crop [6, 6, 6, 6] [154, 245, 314, 405] [160, 239, 320, 399]
## 6 86 6 86
##############return grad_in in conv2d tensor([ 0.0000,  0.0014,  0.0020, -0.0009, -0.0021, -0.0002, -0.0033,  0.0017,
        -0.0016, -0.0015], device='cuda:0')
I am [4, 2]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 96, 96])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 96, 96])
grad_input new torch.Size([1, 3, 96, 96])
grad_input torch.Size([1, 3, 96, 96])
grad_out size torch.Size([1, 3, 94, 94])
crop [7, 7, 7, 7] [153, 246, 313, 406] [160, 239, 320, 399]
## 7 87 7 87
##############return grad_in in conv2d tensor([ 0.0000, -0.0002, -0.0003, -0.0004, -0.0010,  0.0003,  0.0005, -0.0003,
         0.0017, -0.0010], device='cuda:0')
I am [4, 2]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 98, 98])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 96, 96])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 98, 98])
grad_out size torch.Size([1, 3, 96, 96])
crop [8, 8, 8, 8] [152, 247, 312, 407] [160, 239, 320, 399]
## 8 88 8 88
##############return grad_in in conv2d tensor([ 0.0000e+00, -5.0236e-05, -9.9011e-06,  7.5828e-05, -1.6317e-04,
        -2.7316e-05,  4.2626e-04, -2.2115e-04,  1.9965e-04, -3.8509e-05],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 196, 196])
grad_out size torch.Size([1, 3, 98, 98])
arg size torch.Size([1, 3, 98, 98])
##############grad_in in maxp torch.Size([1, 3, 196, 196])
I am [4, 2]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 198, 198])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 196, 196])
grad_input torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 198, 198])
grad_input new torch.Size([1, 3, 198, 198])
new grad_input torch.Size([1, 3, 198, 198])
grad_out size torch.Size([1, 3, 196, 196])
crop [18, 18, 18, 18] [302, 497, 622, 817] [320, 479, 640, 799]
## 18 178 18 178
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00, -3.0159e-06,  2.1600e-05,
        -1.5016e-05, -8.4862e-06,  2.4590e-06, -7.9800e-08, -3.3451e-08],
       device='cuda:0')
I am [4, 2]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 200, 200])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 200, 200])
grad_input new torch.Size([1, 3, 200, 200])
grad_input torch.Size([1, 3, 200, 200])
grad_out size torch.Size([1, 3, 198, 198])
crop [19, 19, 19, 19] [301, 498, 621, 818] [320, 479, 640, 799]
## 19 179 19 179
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00, -8.1739e-07,  5.6293e-06,
        -9.8707e-06,  5.7870e-06, -4.8836e-06, -3.0051e-06, -3.9078e-07],
       device='cuda:0')
I am [4, 2]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 202, 202])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 200, 200])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 202, 202])
grad_out size torch.Size([1, 3, 200, 200])
crop [20, 20, 20, 20] [300, 499, 620, 819] [320, 479, 640, 799]
## 20 180 20 180
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  6.3175e-08,  3.5072e-08,
        -4.8636e-07, -2.6295e-07,  1.3429e-06,  2.9137e-07,  6.5943e-08],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 404, 404])
grad_out size torch.Size([1, 3, 202, 202])
arg size torch.Size([1, 3, 202, 202])
##############grad_in in maxp torch.Size([1, 3, 404, 404])
I am [4, 2]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 406, 406])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 404, 404])
grad_input torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 406, 406])
grad_input new torch.Size([1, 3, 406, 406])
new grad_input torch.Size([1, 3, 406, 406])
grad_out size torch.Size([1, 3, 404, 404])
crop [42, 42, 42, 42] [598, 1001, 1238, 1641] [640, 959, 1280, 1599]
## 42 362 42 [torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cCheckpointBackward
362
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  3.5117e-09, -1.2521e-08,  1.6963e-08, -6.1432e-08],
       device='cuda:0')
I am [4, 2]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 408, 408])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 408, 408])
grad_input new torch.Size([1, 3, 408, 408])
grad_input torch.Size([1, 3, 408, 408])
grad_out size torch.Size([1, 3, 406, 406])
crop [43, 43, 43, 43] [597, 1002, 1237, 1642] [640, 959, 1280, 1599]
## 43 363 43 363
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  1.3954e-09, -4.4353e-11, -7.3248e-09,  1.1519e-08],
       device='cuda:0')
I am [4, 2]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 410, 410])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 408, 408])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 410, 410])
grad_out size torch.Size([1, 3, 408, 408])
crop [44, 44, 44, 44] [596, 1003, 1236, 1643] [640, 959, 1280, 1599]
## 44 364 44 364
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  1.4975e-10, -5.3082e-10,  6.5033e-10,  5.7625e-10],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 820, 820])
grad_out size torch.Size([1, 3, 410, 410])
arg size torch.Size([1, 3, 410, 410])
##############grad_in in maxp torch.Size([1, 3, 820, 820])
I am [4, 2]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 822, 822])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 820, 820])
grad_input torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 822, 822])
grad_input new torch.Size([1, 3, 822, 822])
new grad_input torch.Size([1, 3, 822, 822])
grad_out size torch.Size([1, 3, 820, 820])
crop [90, 90, 90, 90] [1190, 2009, 2470, 3289] [1280, 1919, 2560, 3199]
## 90 730 90 730
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [4, 2]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 824, 824])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 824, 824])
grad_out size torch.Size([1, 3, 822, 822])
crop [91, 91, 91, 91] [1189, 2010, 2469, 3290] [1280, 1919, 2560, 3199]
## 91 731 91 731
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 1648, 1648])
grad_out size torch.Size([1, 3, 824, 824])
arg size torch.Size([1, 3, 824, 824])
##############grad_in in maxp torch.Size([1, 3, 1648, 1648])
I am [4, 2]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 1650, 1650])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1648, 1648])
grad_input torch.Size([1, 3, 1650, 1650])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 1650, 1650])
grad_input new torch.Size([1, 3, 1650, 1650])
new grad_input torch.Size([1, 3, 1650, 1650])
grad_out size torch.Size([1, 3, 1648, 1648])
crop [184, 184, 184, 184] [2376, 4023, 4936, 6583] [2560, 3839, 5120, 6399]
## 184 1464 184 1464
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [4, 2]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 3, 1652, 1652])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1650, 1650])
final torch.Size([1, 3, 1652, 1652])
padding info :: [0, 0, 0, 0]
reshape_for_final ## 186 1466 186 1466
after crop g_in torch.Size([1, 3, 1280, 1280])
grad_out size torch.Size([1, 3, 1650, 1650])
crop [185, 185, 185, 185] [2375, 4024, 4935, 6584] [2560, 3839, 5120, 6399]
## 185 1465 185 1465
##############return grad_in in conv2d tensor([-1.4511e-07, -3.7162e-07, -1.3833e-07, -5.5977e-08, -8.9806e-07,
        -1.2682e-07, -5.8951e-07, -2.7406e-07,  1.5548e-07,  3.0480e-07],
       device='cuda:0')
[8, 8]
HREREERE torch.Size([1, 3, 10240, 10240])

############# Enter checkpointing bkward ####
inputs len 5 4
in customized Sequential 5
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 1]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <4,1,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 1]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <4,1,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 1]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <4,1,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 1]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <4,1,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 1]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <4,1,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 1]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <4,1,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 1]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <4,1,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 1]
[torch/csrc/autograd/python_engine.cpp] THPEngine_run_backward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <4,1,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 1]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <4,1,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 1]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <4,1,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 1]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <4,1,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 1]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <4,1,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 1]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <4,1,>,

in customized Sequential 3
need to get existing

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 92, 92])
grad_out size torch.Size([1, 3, 320, 320])
arg size torch.Size([1, 3, 46, 46])
new_grad_out torch.Size([1, 3, 46, 46])
##############grad_in in maxp torch.Size([1, 3, 92, 92])
I am [4, 1]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 94, 94])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 92, 92])
grad_input torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 94, 94])
grad_input new torch.Size([1, 3, 94, 94])
new grad_input torch.Size([1, 3, 94, 94])
grad_out size torch.Size([1, 3, 92, 92])
crop [6, 6, 6, 6] [74, 165, 314, 405] [80, 159, 320, 399]
## 6 86 6 86
##############return grad_in in conv2d tensor([ 2.0955e-03, -3.0456e-03, -2.6200e-05, -1.0798e-03, -5.2443e-03,
        -1.6693e-03,  1.9996e-03, -3.3130e-04, -2.2546e-04, -6.7639e-04],
       device='cuda:0')
I am [4, 1]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 96, 96])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 96, 96])
grad_input new torch.Size([1, 3, 96, 96])
grad_input torch.Size([1, 3, 96, 96])
grad_out size torch.Size([1, 3, 94, 94])
crop [7, 7, 7, 7] [73, 166, 313, 406] [80, 159, 320, 399]
## 7 87 7 87
##############return grad_in in conv2d tensor([ 0.0004,  0.0002,  0.0006,  0.0003,  0.0001,  0.0006,  0.0005,  0.0001,
        -0.0002, -0.0003], device='cuda:0')
I am [4, 1]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 98, 98])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 96, 96])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 98, 98])
grad_out size torch.Size([1, 3, 96, 96])
crop [8, 8, 8, 8] [72, 167, 312, 407] [80, 159, 320, 399]
## 8 88 8 88
##############return grad_in in conv2d tensor([-9.3946e-06,  1.1106e-04, -1.2225e-04,  1.0841e-04,  1.2628e-04,
         7.0982e-05, -3.9975e-06, -1.6334e-04, -2.0973e-04, -4.8945e-05],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 196, 196])
grad_out size torch.Size([1, 3, 98, 98])
arg size torch.Size([1, 3, 98, 98])
##############grad_in in maxp torch.Size([1, 3, 196, 196])
I am [4, 1]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 198, 198])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 196, 196])
grad_input torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 198, 198])
grad_input new torch.Size([1, 3, 198, 198])
new grad_input torch.Size([1, 3, 198, 198])
grad_out size torch.Size([1, 3, 196, 196])
crop [18, 18, 18, 18] [142, 337, 622, 817] [160, 319, 640, 799]
## 18 178 18 178
##############return grad_in in conv2d tensor([ 0.0000e+00, -1.0066e-05,  1.1609e-05,  1.4233e-05, -4.0873e-06,
        -2.9381e-07,  6.5081e-06,  1.4402e-05, -1.7066e-05,  6.6638e-06],
       device='cuda:0')
I am [4, 1]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 200, 200])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 200, 200])
grad_input new torch.Size([1, 3, 200, 200])
grad_input torch.Size([1, 3, 200, 200])
grad_out size torch.Size([1, 3, 198, 198])
crop [19, 19, 19, 19] [141, 338, 621, 818] [160, 319, 640, 799]
## 19 179 19 179
##############return grad_in in conv2d tensor([ 0.0000e+00, -2.9228e-06,  4.9682e-06,  2.5601e-06,  7.0742e-06,
        -1.4289e-06,  5.9648e-06,  7.7748e-06, -3.7418e-06,  3.3423e-06],
       device='cuda:0')
I am [4, 1]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 202, 202])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 200, 200])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 202, 202])
grad_out size torch.Size([1, 3, 200, 200])
crop [20, 20, 20, 20] [140, 339, 620, 819] [160, 319, 640, 799]
## 20 180 20 180
##############return grad_in in conv2d tensor([ 0.0000e+00,  1.2984e-07,  2.6884e-07, -9.2934e-07, -2.1284e-06,
         1.2695e-06, -3.2331e-07, -2.4513e-06,  1.6812e-07,  6.8709e-07],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 404, 404])
grad_out size torch.Size([1, 3, 202, 202])
arg size torch.Size([1, 3, 202, 202])
##############grad_in in maxp torch.Size([1, 3, 404, 404])
I am [4, 1]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 406, 406])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 404, 404])
grad_input torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 406, 406])
grad_input new torch.Size([1, 3, 406, 406])
new grad_input torch.Size([1, 3, 406, 406])
grad_out size torch.Size([1, 3, 404, 404])
crop [42, 42, 42, 42] [278, 681, 1238, 1641] [320, 639, 1280, 1599]
## 42 362 42 362
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.2151e-08, -8.2320e-08,
         2.1376e-07, -1.7648e-07, -1.6078e-07,  1.4081e-07, -1.2849e-07],
       device='cuda:0')
I am [4, 1]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 408, 408])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 408, 408])
grad_input new torch.Size([1, 3, 408, 408])
grad_input torch.Size([1, 3, 408, 408])
grad_out size torch.Size([1, 3, 406, 406])
crop [43, 43, 43, 43] [277, 682, 1237, 1642] [320, 639, 1280, 1599]
## 43 363 43 363
##############return grad_in in conv2d [torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cCheckpointBackward
tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  4.8280e-09,  5.9695e-09,
         1.3889e-08,  1.8092e-08, -5.3697e-08,  2.3202e-08, -3.8003e-08],
       device='cuda:0')
I am [4, 1]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 410, 410])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 408, 408])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 410, 410])
grad_out size torch.Size([1, 3, 408, 408])
crop [44, 44, 44, 44] [276, 683, 1236, 1643] [320, 639, 1280, 1599]
## 44 364 44 364
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  5.1815e-10, -1.6408e-09,
        -2.9293e-09, -1.3701e-08,  5.2206e-10, -1.6704e-09,  1.9510e-08],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 820, 820])
grad_out size torch.Size([1, 3, 410, 410])
arg size torch.Size([1, 3, 410, 410])
##############grad_in in maxp torch.Size([1, 3, 820, 820])
I am [4, 1]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 822, 822])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 820, 820])
grad_input torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 822, 822])
grad_input new torch.Size([1, 3, 822, 822])
new grad_input torch.Size([1, 3, 822, 822])
grad_out size torch.Size([1, 3, 820, 820])
crop [90, 90, 90, 90] [550, 1369, 2470, 3289] [640, 1279, 2560, 3199]
## 90 730 90 730
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  1.8509e-10,  4.4060e-10, -1.0653e-10],
       device='cuda:0')
I am [4, 1]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 824, 824])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 824, 824])
grad_out size torch.Size([1, 3, 822, 822])
crop [91, 91, 91, 91] [549, 1370, 2469, 3290] [640, 1279, 2560, 3199]
## 91 731 91 731
##############return grad_in in conv2d tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 2.5985e-11, 7.1329e-11, 7.9437e-11], device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 1648, 1648])
grad_out size torch.Size([1, 3, 824, 824])
arg size torch.Size([1, 3, 824, 824])
##############grad_in in maxp torch.Size([1, 3, 1648, 1648])
I am [4, 1]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 1650, 1650])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1648, 1648])
grad_input torch.Size([1, 3, 1650, 1650])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 1650, 1650])
grad_input new torch.Size([1, 3, 1650, 1650])
new grad_input torch.Size([1, 3, 1650, 1650])
grad_out size torch.Size([1, 3, 1648, 1648])
crop [184, 184, 184, 184] [1096, 2743, 4936, 6583] [1280, 2559, 5120, 6399]
## 184 1464 184 1464
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [4, 1]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 3, 1652, 1652])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1650, 1650])
final torch.Size([1, 3, 1652, 1652])
padding info :: [0, 0, 0, 0]
reshape_for_final ## 186 1466 186 1466
after crop g_in torch.Size([1, 3, 1280, 1280])
grad_out size torch.Size([1, 3, 1650, 1650])
crop [185, 185, 185, 185] [1095, 2744, 4935, 6584] [1280, 2559, 5120, 6399]
## 185 1465 185 1465
##############return grad_in in conv2d tensor([-5.6833e-08,  4.5257e-07,  3.1695e-07, -5.4189e-07,  1.8048e-07,
        -8.4740e-08,  2.3340e-07,  2.1693e-07,  2.6344e-07,  3.9771e-08],
       device='cuda:0')
[8, 8]
HREREERE torch.Size([1, 3, 10240, 10240])

############# Enter checkpointing bkward ####
inputs len 5 4
in customized Sequential 5
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 0]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1466])
myctx [[ 140046822727744[ PI( <4,0,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 0]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1464])
myctx [[ 140045015171856[ PI( <4,0,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 0]
shape input_tile_for_next
 torch.Size([1, 3, 822, 732])
myctx [[ 140045013800704[ PI( <4,0,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 0]
shape input_tile_for_next
 torch.Size([1, 3, 820, 730])
myctx [[ 140045013848944[ PI( <4,0,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 0]
shape input_tile_for_next
 torch.Size([1, 3, 408, 366])
myctx [[ 140045013206640[ PI( <4,0,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 0]
shape input_tile_for_next
 torch.Size([1, 3, 406, 364])
myctx [[ 140045013206784[ PI( <4,0,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 0]
shape input_tile_for_next
 torch.Size([1, 3, 404, 362])
myctx [[ 140045013206880[ PI( <4,0,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 0]
shape input_tile_for_next
 torch.Size([1, 3, 200, 182])
myctx [[ 140045013206352[ PI( <4,0,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 0]
shape input_tile_for_next
 torch.Size([1, 3, 198, 180])
myctx [torch/csrc/autograd/python_engine.cpp] THPEngine_run_backward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[[ 140045013206304[ PI( <4,0,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 0]
shape input_tile_for_next
 torch.Size([1, 3, 196, 178])
myctx [[ 140045013207072[ PI( <4,0,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 0]
shape input_tile_for_next
 torch.Size([1, 3, 96, 90])
myctx [[ 140045013206400[ PI( <4,0,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 0]
shape input_tile_for_next
 torch.Size([1, 3, 94, 88])
myctx [[ 140045013206448[ PI( <4,0,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [4, 0]
shape input_tile_for_next
 torch.Size([1, 3, 92, 86])
myctx [[ 140045013206976[ PI( <4,0,>,

in customized Sequential 3
need to get existing

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 92, 86])
grad_out size torch.Size([1, 3, 320, 320])
arg size torch.Size([1, 3, 46, 43])
new_grad_out torch.Size([1, 3, 46, 43])
##############grad_in in maxp torch.Size([1, 3, 92, 86])
I am [4, 0]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 94, 88])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 92, 86])
grad_input torch.Size([1, 3, 94, 88])
padding info :: [1, 0, 0, 0]
grad_input old torch.Size([1, 3, 94, 88])
grad_input new torch.Size([1, 3, 94, 88])
new grad_input torch.Size([1, 3, 94, 88])
grad_out size torch.Size([1, 3, 92, 86])
crop [0, 6, 6, 6] [0, 85, 314, 405] [0, 79, 320, 399]
## 6 86 0 80
##############return grad_in in conv2d tensor([ 0.0000e+00,  6.6915e-05,  4.9429e-04, -7.9851e-04, -2.1648e-03,
         1.6685e-03,  1.1332e-03,  3.0143e-03, -8.9248e-04,  0.0000e+00],
       device='cuda:0')
I am [4, 0]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 96, 90])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 94, 88])
padding info :: [2, 0, 0, 0]
grad_input old torch.Size([1, 3, 96, 90])
grad_input new torch.Size([1, 3, 96, 90])
grad_input torch.Size([1, 3, 96, 90])
grad_out size torch.Size([1, 3, 94, 87])
crop [0, 7, 7, 7] [0, 86, 313, 406] [0, 79, 320, 399]
## 7 87 0 80
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  4.6677e-05, -1.1216e-03,  4.7206e-04,
        -3.6258e-04,  5.2323e-04, -1.2381e-03,  3.4852e-04, -2.7555e-04],
       device='cuda:0')
I am [4, 0]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 98, 92])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 96, 90])
padding info :: [3, 0, 0, 0]
grad_input torch.Size([1, 3, 98, 89])
grad_out size torch.Size([1, 3, 96, 88])
crop [0, 8, 8, 8] [0, 87, 312, 407] [0, 79, 320, 399]
## 8 88 0 80
##############return grad_in in conv2d tensor([-2.5647e-04, -5.4109e-05,  2.2379e-04,  5.5005e-05, -2.8267e-04,
        -1.4362e-04,  2.9282e-04,  3.7243e-05,  3.2159e-05, -8.0515e-05],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 196, 178])
grad_out size torch.Size([1, 3, 98, 89])
arg size torch.Size([1, 3, 98, 89])
##############grad_in in maxp torch.Size([1, 3, 196, 178])
I am [4, 0]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 198, 180])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 196, 178])
grad_input torch.Size([1, 3, 198, 180])
padding info :: [1, 0, 0, 0]
grad_input old torch.Size([1, 3, 198, 180])
grad_input new torch.Size([1, 3, 198, 180])
new grad_input torch.Size([1, 3, 198, 180])
grad_out size torch.Size([1, 3, 196, 178])
crop [0, 18, 18, 18] [0, 177, 622, 817] [0, 159, 640, 799]
## 18 178 0 160
##############return grad_in in conv2d tensor([ 0.0000e+00, -1.5397e-05,  2.7044e-05,  7.8202e-06,  1.3435e-05,
        -3.8816e-05,  6.4684e-07,  3.3129e-05, -1.2241e-05, -1.3426e-05],
       device='cuda:0')
I am [4, 0]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 200, 182])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 198, 180])
padding info :: [2, 0, 0, 0]
grad_input old torch.Size([1, 3, 200, 182])
grad_input new torch.Size([1, 3, 200, 182])
grad_input torch.Size([1, 3, 200, 182])
grad_out size torch.Size([1, 3, 198, 179])
crop [0, 19, 19, 19] [0, 178, 621, 818] [0, 159, 640, 799]
## 19 179 0 160
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  4.5725e-06, -1.1081e-05,  2.0368e-05,
        -1.1746e-05,  1.4448e-05, -9.8628e-06, -2.1135e-06,  5.8864e-06],
       device='cuda:0')
I am [4, 0]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 202, 184])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 200, 182])
padding info :: [3, 0, 0, 0]
grad_input torch.Size([1, 3, 202, 181])
grad_out size torch.Size([1, 3, 200, 180])
crop [0, 20, 20, 20] [0, 179, 620, 819] [0, 159, 640, 799]
## 20 180 0 160
##############return grad_in in conv2d tensor([ 3.8679e-07, -3.6714e-06, -7.9269e-07,  1.5378e-06,  2.1104e-06,
        -2.3502e-06, -1.0950e-06,  2.8934e-06,  1.3008e-07, -8.1706e-07],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 404, 362])
grad_out size torch.Size([1, 3, 202, 181])
arg size torch.Size([1, 3, 202, 181])
##############grad_in in maxp torch.Size([1, 3, 404, 362])
I am [4, 0]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 406, 364])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 404, 362])
grad_input torch.Size([1, 3, 406, 364])
padding info :: [1, 0, 0, 0]
grad_input old torch.Size([1, 3, 406, 364])
grad_input new torch.Size([1, 3, 406, 364])
new grad_input torch.Size([1, 3, 406, 364])
grad_out size torch.Size([1, 3, 404, 362])
crop [0, 42, 42, 42] [0, 361, 1238, 1641] [0, 319, 1280, 1599]
## 42 362 0 320
##############return grad_in in conv2d tensor([ 0.0000e+00, -2.5057e-08, -4.8673e-07, -8.8742e-07,  8.1955e-07,
         7.1807e-07, -6.2329e-08,  1.6391e-08, -8.6232e-08,  9.5975e-07],
       device='cuda:0')
I am [4, 0]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 408, 366])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 406, 364])
padding info :: [2, 0, 0, 0]
grad_input old torch.Size([1, 3, 408, 366])
grad_input new torch.Size([1, 3, 408, 366])
grad_input torch.Size([1, 3, 408, 366])
grad_out size torch.Size([1, 3, 406, 363])
crop [0, 43, 43, 43] [0, 362, 1237, 1642] [0, 319, 1280, 1599]
## 43 363 0 320
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00, -1.0711e-07, -1.1904e-07,  1.4144e-07,
        -3.4713e-07, -7.5261e-09,  3.6989e-07,  2.1797e-07, -1.5118e-07],
       device='cuda:0')
I am [4, 0]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 410, 368])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 408, 366])
padding info :: [3, 0, 0, 0]
grad_input torch.Size([1, 3, 410, 365])
grad_out size torch.Size([1, 3, 408, 364])
crop [0, 44, 44, 44] [0, 363, 1236, 1643] [0, 319, 1280, 1599]
## 44 364 0 320
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cCheckpointBackward
##############return grad_in in conv2d tensor([ 3.6199e-08,  1.3381e-08,  7.0332e-08,  1.3150e-08, -2.8300e-08,
        -5.3973e-08, -3.4886e-08,  1.2103e-08, -3.9126e-08, -9.5543e-08],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 820, 730])
grad_out size torch.Size([1, 3, 410, 365])
arg size torch.Size([1, 3, 410, 365])
##############grad_in in maxp torch.Size([1, 3, 820, 730])
I am [4, 0]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 822, 732])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 820, 730])
grad_input torch.Size([1, 3, 822, 732])
padding info :: [1, 0, 0, 0]
grad_input old torch.Size([1, 3, 822, 732])
grad_input new torch.Size([1, 3, 822, 732])
new grad_input torch.Size([1, 3, 822, 732])
grad_out size torch.Size([1, 3, 820, 730])
crop [0, 90, 90, 90] [0, 729, 2470, 3289] [0, 639, 2560, 3199]
## 90 730 0 640
##############return grad_in in conv2d tensor([ 0.0000e+00,  5.2772e-09, -5.3901e-09,  2.5061e-09,  1.9508e-09,
        -1.0425e-08, -2.6763e-09,  4.7998e-09,  9.5468e-09,  1.4294e-08],
       device='cuda:0')
I am [4, 0]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 824, 734])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 822, 732])
padding info :: [2, 0, 0, 0]
grad_input torch.Size([1, 3, 824, 732])
grad_out size torch.Size([1, 3, 822, 731])
crop [0, 91, 91, 91] [0, 730, 2469, 3290] [0, 639, 2560, 3199]
## 91 731 0 640
##############return grad_in in conv2d tensor([ 5.2238e-10, -2.9395e-11, -1.0538e-09, -7.0505e-10, -3.6759e-09,
        -1.7108e-09,  4.0015e-09,  3.7091e-09,  3.1424e-09,  1.5145e-09],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 1648, 1464])
grad_out size torch.Size([1, 3, 824, 732])
arg size torch.Size([1, 3, 824, 732])
##############grad_in in maxp torch.Size([1, 3, 1648, 1464])
I am [4, 0]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 1650, 1466])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1648, 1464])
grad_input torch.Size([1, 3, 1650, 1466])
padding info :: [1, 0, 0, 0]
grad_input old torch.Size([1, 3, 1650, 1466])
grad_input new torch.Size([1, 3, 1650, 1466])
new grad_input torch.Size([1, 3, 1650, 1466])
grad_out size torch.Size([1, 3, 1648, 1464])
crop [0, 184, 184, 184] [0, 1463, 4936, 6583] [0, 1279, 5120, 6399]
## 184 1464 0 1280
##############return grad_in in conv2d tensor([ 0.0000e+00, -2.6319e-11, -4.2796e-10,  1.9418e-10,  4.4328e-11,
        -3.3735e-10,  3.1761e-10,  3.8260e-10,  3.5369e-11,  8.2945e-11],
       device='cuda:0')
I am [4, 0]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 3, 1652, 1468])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1650, 1466])
final torch.Size([1, 3, 1652, 1468])
padding info :: [2, 0, 0, 0]
reshape_for_final ## 186 1466 0 1280
after crop g_in torch.Size([1, 3, 1280, 1280])
grad_out size torch.Size([1, 3, 1650, 1465])
crop [0, 185, 185, 185] [0, 1464, 4935, 6584] [0, 1279, 5120, 6399]
## 185 1465 0 1280
##############return grad_in in conv2d tensor([-1.3329e-07, -2.2876e-09, -2.9441e-07, -1.4655e-07, -3.9479e-07,
        -1.5796e-07,  2.7550e-07, -8.3461e-08, -5.1932e-07,  3.1246e-07],
       device='cuda:0')
[8, 8]
HREREERE torch.Size([1, 3, 10240, 10240])

############# Enter checkpointing bkward ####
inputs len 5 4
in customized Sequential 5
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 7]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1466])
myctx [[ 140046822727744[ PI( <3,7,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 7]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1464])
myctx [[ 140045015171856[ PI( <3,7,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 7]
shape input_tile_for_next
 torch.Size([1, 3, 822, 732])
myctx [[ 140045013800704[ PI( <3,7,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 7]
shape input_tile_for_next
 torch.Size([1, 3, 820, 730])
myctx [[ 140045013848944[ PI( <3,7,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 7]
shape input_tile_for_next
 torch.Size([1, 3, 408, 366])
myctx [[ 140045013206640[ PI( <3,7,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 7]
shape input_tile_for_next
 torch.Size([1, 3, 406, 364])
myctx [[ 140045013206784[ PI( <3,7,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 7]
shape input_tile_for_next
 torch.Size([1, 3, 404, 362])
myctx [[ 140045013206880[ PI( <3,7,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 7]
shape input_tile_for_next
 torch.Size([1, 3, 200, 182])
myctx [[ 140045013206352[ PI( <3,7,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 7]
shape input_tile_for_next
 torch.Size([1, 3, 198, 180])
myctx [[ 140045013206304[ PI( <3,7,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 7]
shape input_tile_for_next
 torch.Size([1, 3, 196, 178])
myctx [torch/csrc/autograd/python_engine.cpp] THPEngine_run_backward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[[ 140045013207072[ PI( <3,7,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 7]
shape input_tile_for_next
 torch.Size([1, 3, 96, 90])
myctx [[ 140045013206400[ PI( <3,7,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 7]
shape input_tile_for_next
 torch.Size([1, 3, 94, 88])
myctx [[ 140045013206448[ PI( <3,7,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 7]
shape input_tile_for_next
 torch.Size([1, 3, 92, 86])
myctx [[ 140045013206976[ PI( <3,7,>,

in customized Sequential 3
need to get existing

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 92, 86])
grad_out size torch.Size([1, 3, 320, 320])
arg size torch.Size([1, 3, 46, 43])
new_grad_out torch.Size([1, 3, 46, 43])
##############grad_in in maxp torch.Size([1, 3, 92, 86])
I am [3, 7]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 94, 88])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 92, 86])
grad_input torch.Size([1, 3, 94, 88])
padding info :: [0, 1, 0, 0]
grad_input old torch.Size([1, 3, 94, 88])
grad_input new torch.Size([1, 3, 94, 88])
new grad_input torch.Size([1, 3, 94, 88])
grad_out size torch.Size([1, 3, 92, 86])
crop [6, 0, 6, 6] [554, 639, 234, 325] [560, 639, 240, 319]
## 6 86 6 86
##############return grad_in in conv2d tensor([-0.0007, -0.0011,  0.0010,  0.0042, -0.0072,  0.0020, -0.0007, -0.0001,
        -0.0016,  0.0007], device='cuda:0')
I am [3, 7]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 96, 90])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 94, 88])
padding info :: [0, 2, 0, 0]
grad_input old torch.Size([1, 3, 96, 90])
grad_input new torch.Size([1, 3, 96, 90])
grad_input torch.Size([1, 3, 96, 90])
grad_out size torch.Size([1, 3, 94, 87])
crop [7, 0, 7, 7] [553, 639, 233, 326] [560, 639, 240, 319]
## 7 87 7 87
##############return grad_in in conv2d tensor([-1.3296e-04,  1.6758e-04, -2.6423e-04,  1.3817e-03,  2.9618e-04,
         1.4859e-03,  2.5265e-03, -1.8226e-04,  7.4077e-04, -7.9967e-05],
       device='cuda:0')
I am [3, 7]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 98, 92])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 96, 90])
padding info :: [0, 3, 0, 0]
grad_input torch.Size([1, 3, 98, 89])
grad_out size torch.Size([1, 3, 96, 88])
crop [8, 0, 8, 8] [552, 639, 232, 327] [560, 639, 240, 319]
## 8 88 8 88
##############return grad_in in conv2d tensor([ 3.2081e-06,  3.8332e-05, -8.9339e-05,  4.3381e-05,  3.2163e-04,
        -4.4792e-04,  6.0262e-04, -4.6193e-05, -3.7244e-04, -7.2422e-05],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 196, 178])
grad_out size torch.Size([1, 3, 98, 89])
arg size torch.Size([1, 3, 98, 89])
##############grad_in in maxp torch.Size([1, 3, 196, 178])
I am [3, 7]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 198, 180])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 196, 178])
grad_input torch.Size([1, 3, 198, 180])
padding info :: [0, 1, 0, 0]
grad_input old torch.Size([1, 3, 198, 180])
grad_input new torch.Size([1, 3, 198, 180])
new grad_input torch.Size([1, 3, 198, 180])
grad_out size torch.Size([1, 3, 196, 178])
crop [18, 0, 18, 18] [1102, 1279, 462, 657] [1120, 1279, 480, 639]
## 18 178 18 178
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  4.0941e-06, -4.1032e-06, -6.5579e-06,
        -1.3868e-05,  1.6761e-05,  1.8054e-05, -6.2755e-06, -5.4323e-06],
       device='cuda:0')
I am [3, 7]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 200, 182])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 198, 180])
padding info :: [0, 2, 0, 0]
grad_input old torch.Size([1, 3, 200, 182])
grad_input new torch.Size([1, 3, 200, 182])
grad_input torch.Size([1, 3, 200, 182])
grad_out size torch.Size([1, 3, 198, 179])
crop [19, 0, 19, 19] [1101, 1279, 461, 658] [1120, 1279, 480, 639]
## 19 179 19 179
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  1.1651e-06, -6.4666e-07,  6.0765e-07,
        -7.4285e-06,  4.8508e-06,  4.1464e-06,  1.8356e-06,  1.5548e-06],
       device='cuda:0')
I am [3, 7]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 202, 184])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 200, 182])
padding info :: [0, 3, 0, 0]
grad_input torch.Size([1, 3, 202, 181])
grad_out size torch.Size([1, 3, 200, 180])
crop [20, 0, 20, 20] [1100, 1279, 460, 659] [1120, 1279, 480, 639]
## 20 180 20 180
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00, -9.1081e-08, -3.0668e-07,  1.2577e-07,
         1.2153e-06,  7.1781e-07, -1.3269e-06, -2.1609e-06,  5.1844e-07],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 404, 362])
grad_out size torch.Size([1, 3, 202, 181])
arg size torch.Size([1, 3, 202, 181])
##############grad_in in maxp torch.Size([1, 3, 404, 362])
I am [3, 7]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 406, 364])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 404, 362])
grad_input torch.Size([1, 3, 406, 364])
padding info :: [0, 1, 0, 0]
grad_input old torch.Size([1, 3, 406, 364])
grad_input new torch.Size([1, 3, 406, 364])
new grad_input torch.Size([1, 3, 406, 364])
grad_out size torch.Size([1, 3, 404, 362])
crop [42, 0, 42, 42] [2198, 2559, 918, 1321] [2240, 2559, 960, 1279]
## 42 362 42 362
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -2.2996e-08,
         2.4000e-08,  2.8652e-08, -5.2094e-08, -1.7008e-08,  4.8802e-08],
       device='cuda:0')
I am [3, 7]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 408, 366])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 406, 364])
padding info :: [0, 2, 0, 0]
grad_input old torch.Size([1, 3, 408, 366])
grad_input new torch.Size([1, 3, 408, 366])
grad_input torch.Size([1, 3, 408, 366])
grad_out size torch.Size([1, 3, 406, 363])
crop [43, 0, 43, 43] [2197, 2559, 917, 1322] [2240, 2559, 960, 1279]
## 43 363 43 363
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  3.0415e-09,
         9.8268e-09, -8.4981e-09, -1.0367e-08,  4.3904e-10, -1.4847e-08],
       device='cuda:0')
I am [3, 7]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 410, 368])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 408, 366])
padding info :: [0, 3, 0, 0]
grad_input torch.Size([1, 3, 410, 365])
grad_out size torch.Size([1, 3, 408, 364])
crop [44, 0, 44, 44] [2196, 2559, 916, 1323] [2240, 2559, 960, 1279]
## 44 364 44 364
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.1433e-10,
        -2.3702e-09, -1.1627e-09, -1.5435e-09, -1.1380e-10,  4.9234e-09],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 820, 730])
grad_out size torch.Size([1, 3, 410, 365])
arg size torch.Size([1, 3, 410, 365])
##############grad_in in maxp torch.Size([1, 3, 820, 730])
I am [3, 7]
@@@ using cudnn bkw
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cCheckpointBackward
local last ++ input shape torch.Size([1, 3, 822, 732])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 820, 730])
grad_input torch.Size([1, 3, 822, 732])
padding info :: [0, 1, 0, 0]
grad_input old torch.Size([1, 3, 822, 732])
grad_input new torch.Size([1, 3, 822, 732])
new grad_input torch.Size([1, 3, 822, 732])
grad_out size torch.Size([1, 3, 820, 730])
crop [90, 0, 90, 90] [4390, 5119, 1830, 2649] [4480, 5119, 1920, 2559]
## 90 730 90 730
##############return grad_in in conv2d tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0203e-10], device='cuda:0')
I am [3, 7]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 824, 734])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 822, 732])
padding info :: [0, 2, 0, 0]
grad_input torch.Size([1, 3, 824, 732])
grad_out size torch.Size([1, 3, 822, 731])
crop [91, 0, 91, 91] [4389, 5119, 1829, 2650] [4480, 5119, 1920, 2559]
## 91 731 91 731
##############return grad_in in conv2d tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1188e-11], device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 1648, 1464])
grad_out size torch.Size([1, 3, 824, 732])
arg size torch.Size([1, 3, 824, 732])
##############grad_in in maxp torch.Size([1, 3, 1648, 1464])
I am [3, 7]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 1650, 1466])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1648, 1464])
grad_input torch.Size([1, 3, 1650, 1466])
padding info :: [0, 1, 0, 0]
grad_input old torch.Size([1, 3, 1650, 1466])
grad_input new torch.Size([1, 3, 1650, 1466])
new grad_input torch.Size([1, 3, 1650, 1466])
grad_out size torch.Size([1, 3, 1648, 1464])
crop [184, 0, 184, 184] [8776, 10239, 3656, 5303] [8960, 10239, 3840, 5119]
## 184 1464 184 1464
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [3, 7]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 3, 1652, 1468])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1650, 1466])
final torch.Size([1, 3, 1652, 1468])
padding info :: [0, 2, 0, 0]
reshape_for_final ## 186 1466 186 1466
after crop g_in torch.Size([1, 3, 1280, 1280])
grad_out size torch.Size([1, 3, 1650, 1465])
crop [185, 0, 185, 185] [8775, 10239, 3655, 5304] [8960, 10239, 3840, 5119]
## 185 1465 185 1465
##############return grad_in in conv2d tensor([ 8.9630e-08, -6.5894e-08,  2.5706e-07,  1.5785e-07, -8.8550e-08,
        -1.5191e-07,  1.1260e-07, -1.2803e-07, -2.0022e-07,  1.4231e-07],
       device='cuda:0')
[8, 8]
HREREERE torch.Size([1, 3, 10240, 10240])

############# Enter checkpointing bkward ####
inputs len 5 4
in customized Sequential 5
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 6]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <3,6,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 6]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <3,6,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 6]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <3,6,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 6]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <3,6,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 6]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <3,6,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 6]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <3,6,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 6]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <3,6,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 6]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <3,6,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 6]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <3,6,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 6]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <3,6,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 6]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <3,6,>,

in customized Sequential 3[torch/csrc/autograd/python_engine.cpp] THPEngine_run_backward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward

id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 6]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <3,6,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 6]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <3,6,>,

in customized Sequential 3
need to get existing

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 92, 92])
grad_out size torch.Size([1, 3, 320, 320])
arg size torch.Size([1, 3, 46, 46])
new_grad_out torch.Size([1, 3, 46, 46])
##############grad_in in maxp torch.Size([1, 3, 92, 92])
I am [3, 6]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 94, 94])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 92, 92])
grad_input torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 94, 94])
grad_input new torch.Size([1, 3, 94, 94])
new grad_input torch.Size([1, 3, 94, 94])
grad_out size torch.Size([1, 3, 92, 92])
crop [6, 6, 6, 6] [474, 565, 234, 325] [480, 559, 240, 319]
## 6 86 6 86
##############return grad_in in conv2d tensor([-0.0016,  0.0025,  0.0004,  0.0010, -0.0015,  0.0022,  0.0012,  0.0033,
        -0.0011, -0.0032], device='cuda:0')
I am [3, 6]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 96, 96])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 96, 96])
grad_input new torch.Size([1, 3, 96, 96])
grad_input torch.Size([1, 3, 96, 96])
grad_out size torch.Size([1, 3, 94, 94])
crop [7, 7, 7, 7] [473, 566, 233, 326] [480, 559, 240, 319]
## 7 87 7 87
##############return grad_in in conv2d tensor([-3.3134e-04, -1.7851e-04, -2.7896e-04, -1.0114e-03,  6.9274e-06,
         2.0535e-05, -6.4637e-04, -3.2997e-05,  3.6021e-04,  4.6403e-04],
       device='cuda:0')
I am [3, 6]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 98, 98])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 96, 96])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 98, 98])
grad_out size torch.Size([1, 3, 96, 96])
crop [8, 8, 8, 8] [472, 567, 232, 327] [480, 559, 240, 319]
## 8 88 8 88
##############return grad_in in conv2d tensor([ 4.5960e-06, -8.6221e-05,  1.3089e-04, -2.4396e-04, -2.8693e-05,
         2.0181e-04,  7.9088e-06, -2.1481e-04,  2.0125e-04,  1.0539e-04],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 196, 196])
grad_out size torch.Size([1, 3, 98, 98])
arg size torch.Size([1, 3, 98, 98])
##############grad_in in maxp torch.Size([1, 3, 196, 196])
I am [3, 6]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 198, 198])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 196, 196])
grad_input torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 198, 198])
grad_input new torch.Size([1, 3, 198, 198])
new grad_input torch.Size([1, 3, 198, 198])
grad_out size torch.Size([1, 3, 196, 196])
crop [18, 18, 18, 18] [942, 1137, 462, 657] [960, 1119, 480, 639]
## 18 178 18 178
##############return grad_in in conv2d tensor([-1.4557e-06, -3.7632e-06, -5.3483e-06, -1.6287e-05,  1.1080e-06,
         8.1049e-06, -1.3805e-05, -3.9948e-06,  5.0246e-05, -2.4666e-05],
       device='cuda:0')
I am [3, 6]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 200, 200])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 200, 200])
grad_input new torch.Size([1, 3, 200, 200])
grad_input torch.Size([1, 3, 200, 200])
grad_out size torch.Size([1, 3, 198, 198])
crop [19, 19, 19, 19] [941, 1138, 461, 658] [960, 1119, 480, 639]
## 19 179 19 179
##############return grad_in in conv2d tensor([-5.3692e-07, -2.8224e-06, -1.1288e-06, -7.6091e-06, -2.5374e-06,
        -1.4759e-06, -4.4720e-06,  3.8524e-06,  6.0514e-06, -1.4076e-05],
       device='cuda:0')
I am [3, 6]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 202, 202])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 200, 200])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 202, 202])
grad_out size torch.Size([1, 3, 200, 200])
crop [20, 20, 20, 20] [940, 1139, 460, 659] [960, 1119, 480, 639]
## 20 180 20 180
##############return grad_in in conv2d tensor([ 1.2678e-07,  3.1447e-07, -2.5236e-08,  1.2795e-06,  1.7853e-06,
        -7.1592e-07, -3.2146e-07,  1.0794e-06,  3.8882e-07, -2.0381e-06],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 404, 404])
grad_out size torch.Size([1, 3, 202, 202])
arg size torch.Size([1, 3, 202, 202])
##############grad_in in maxp torch.Size([1, 3, 404, 404])
I am [3, 6]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 406, 406])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 404, 404])
grad_input torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 406, 406])
grad_input new torch.Size([1, 3, 406, 406])
new grad_input torch.Size([1, 3, 406, 406])
grad_out size torch.Size([1, 3, 404, 404])
crop [42, 42, 42, 42] [1878, 2281, 918, 1321] [1920, 2239, 960, 1279]
## 42 362 42 362
##############return grad_in in conv2d tensor([ 0.0000e+00,  2.3238e-08, -2.0878e-08, -4.0986e-08,  1.6300e-08,
        -4.7211e-08,  4.9272e-08,  1.0100e-07, -1.5040e-07,  2.0374e-07],
       device='cuda:0')
I am [3, 6]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 408, 408])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 408, 408])
grad_input new torch.Size([1, 3, 408, 408])
grad_input torch.Size([1, 3, 408, 408])
grad_out size torch.Size([1, 3, 406, 406])
crop [43, 43, 43, 43] [1877, 2282, 917, 1322] [1920, 2239, 960, 1279]
## 43 363 43 363
##############return grad_in in conv2d tensor([ 0.0000e+00, -3.0735e-09, -8.5892e-09,  8.5448e-09, -3.2524e-09,
        -4.2793e-09,  2.1798e-08, -6.8620e-10, -8.2249e-09, -5.9975e-08],
       device='cuda:0')
I am [3, 6]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 410, 410])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 408, 408])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 410, 410])
grad_out size torch.Size([1, 3, 408, 408])
crop [44, 44, 44, 44] [1876, 2283, 916, 1323] [1920, 2239, 960, 1279]
## 44 364 44 364
##############return grad_in in conv2d tensor([ 0.0000e+00, -1.1554e-10,  2.5391e-09,  6.6480e-10,  1.3167e-09,
         2.5410e-09, -3.7285e-09,  2.6446e-10, -7.9187e-09,  3.6546e-09],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 820, 820])
grad_out size torch.Size([1, 3, 410, 410])
arg size torch.Size([1, 3, 410, 410])
##############grad_in in maxp torch.Size([1, 3, 820, 820])
I am [3, 6]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 822, 822])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 820, 820])
grad_input torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 822, 822])
grad_input new torch.Size([1, 3, 822, 822])
new grad_input torch.Size([1, 3, 822, 822])
grad_out size torch.Size([1, 3, 820, 820])
crop [90, 90, 90, 90] [3750, 4569, 1830, 2649] [3840, 4479, 1920, 2559]
## 90 730 90 730
##############return grad_in in conv2d [torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cCheckpointBackward
tensor([ 0.0000e+00,  0.0000e+00, -2.1638e-11, -1.6843e-11,  1.7203e-11,
         4.7553e-10,  3.7015e-10, -2.5356e-10,  9.6917e-11, -4.2907e-10],
       device='cuda:0')
I am [3, 6]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 824, 824])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 824, 824])
grad_out size torch.Size([1, 3, 822, 822])
crop [91, 91, 91, 91] [3749, 4570, 1829, 2650] [3840, 4479, 1920, 2559]
## 91 731 91 731
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  1.3116e-13, -5.6580e-13, -5.3389e-12,
        -2.8372e-12,  1.6007e-11,  1.1658e-10,  2.2594e-12, -9.4131e-11],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 1648, 1648])
grad_out size torch.Size([1, 3, 824, 824])
arg size torch.Size([1, 3, 824, 824])
##############grad_in in maxp torch.Size([1, 3, 1648, 1648])
I am [3, 6]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 1650, 1650])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1648, 1648])
grad_input torch.Size([1, 3, 1650, 1650])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 1650, 1650])
grad_input new torch.Size([1, 3, 1650, 1650])
new grad_input torch.Size([1, 3, 1650, 1650])
grad_out size torch.Size([1, 3, 1648, 1648])
crop [184, 184, 184, 184] [7496, 9143, 3656, 5303] [7680, 8959, 3840, 5119]
## 184 1464 184 1464
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  4.1258e-13,
        -4.1364e-13, -3.4705e-13,  5.4349e-13,  4.4665e-13,  1.5117e-13],
       device='cuda:0')
I am [3, 6]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 3, 1652, 1652])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1650, 1650])
final torch.Size([1, 3, 1652, 1652])
padding info :: [0, 0, 0, 0]
reshape_for_final ## 186 1466 186 1466
after crop g_in torch.Size([1, 3, 1280, 1280])
grad_out size torch.Size([1, 3, 1650, 1650])
crop [185, 185, 185, 185] [7495, 9144, 3655, 5304] [7680, 8959, 3840, 5119]
## 185 1465 185 1465
##############return grad_in in conv2d tensor([-5.0692e-07,  1.6211e-07, -6.6770e-07, -5.5973e-07,  2.2715e-07,
        -4.2262e-07,  8.2504e-07, -4.0597e-07, -5.2653e-07, -5.4603e-07],
       device='cuda:0')
[8, 8]
HREREERE torch.Size([1, 3, 10240, 10240])

############# Enter checkpointing bkward ####
inputs len 5 4
in customized Sequential 5
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 5]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <3,5,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 5]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <3,5,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 5]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <3,5,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 5]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <3,5,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 5]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <3,5,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 5]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <3,5,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 5]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <3,5,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 5]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <3,5,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 5]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <3,5,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 5]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <3,5,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 5]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <3,5,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 5]
shape input_tile_for_next
 [torch/csrc/autograd/python_engine.cpp] THPEngine_run_backward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <3,5,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 5]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <3,5,>,

in customized Sequential 3
need to get existing

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 92, 92])
grad_out size torch.Size([1, 3, 320, 320])
arg size torch.Size([1, 3, 46, 46])
new_grad_out torch.Size([1, 3, 46, 46])
##############grad_in in maxp torch.Size([1, 3, 92, 92])
I am [3, 5]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 94, 94])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 92, 92])
grad_input torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 94, 94])
grad_input new torch.Size([1, 3, 94, 94])
new grad_input torch.Size([1, 3, 94, 94])
grad_out size torch.Size([1, 3, 92, 92])
crop [6, 6, 6, 6] [394, 485, 234, 325] [400, 479, 240, 319]
## 6 86 6 86
##############return grad_in in conv2d tensor([-4.9129e-03,  5.1724e-03, -3.1105e-03,  5.5144e-03,  4.7437e-05,
         0.0000e+00,  0.0000e+00,  1.1551e-03, -5.4695e-04, -3.7222e-04],
       device='cuda:0')
I am [3, 5]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 96, 96])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 96, 96])
grad_input new torch.Size([1, 3, 96, 96])
grad_input torch.Size([1, 3, 96, 96])
grad_out size torch.Size([1, 3, 94, 94])
crop [7, 7, 7, 7] [393, 486, 233, 326] [400, 479, 240, 319]
## 7 87 7 87
##############return grad_in in conv2d tensor([-3.1301e-04, -3.7586e-04, -5.0708e-04, -1.9383e-03, -4.1547e-04,
        -1.6093e-03,  3.5256e-04, -1.3034e-04,  4.0703e-05, -5.5363e-04],
       device='cuda:0')
I am [3, 5]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 98, 98])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 96, 96])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 98, 98])
grad_out size torch.Size([1, 3, 96, 96])
crop [8, 8, 8, 8] [392, 487, 232, 327] [400, 479, 240, 319]
## 8 88 8 88
##############return grad_in in conv2d tensor([ 8.4216e-05, -2.7632e-04,  3.0799e-04, -4.2592e-04,  4.8723e-05,
        -3.7504e-05, -4.3422e-05,  3.1458e-04,  6.5969e-05, -6.9899e-05],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 196, 196])
grad_out size torch.Size([1, 3, 98, 98])
arg size torch.Size([1, 3, 98, 98])
##############grad_in in maxp torch.Size([1, 3, 196, 196])
I am [3, 5]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 198, 198])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 196, 196])
grad_input torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 198, 198])
grad_input new torch.Size([1, 3, 198, 198])
new grad_input torch.Size([1, 3, 198, 198])
grad_out size torch.Size([1, 3, 196, 196])
crop [18, 18, 18, 18] [782, 977, 462, 657] [800, 959, 480, 639]
## 18 178 18 178
##############return grad_in in conv2d tensor([ 1.3030e-05, -8.9775e-06, -1.5635e-05, -2.4586e-06,  1.8490e-05,
         1.9802e-05, -3.5055e-05, -2.8149e-05,  1.4621e-05,  6.8421e-05],
       device='cuda:0')
I am [3, 5]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 200, 200])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 200, 200])
grad_input new torch.Size([1, 3, 200, 200])
grad_input torch.Size([1, 3, 200, 200])
grad_out size torch.Size([1, 3, 198, 198])
crop [19, 19, 19, 19] [781, 978, 461, 658] [800, 959, 480, 639]
## 19 179 19 179
##############return grad_in in conv2d tensor([ 3.7833e-06, -5.0279e-06,  4.8849e-07, -4.8865e-07, -3.0298e-06,
         9.9631e-06, -3.1102e-06, -1.1476e-05, -1.2116e-06,  1.4144e-05],
       device='cuda:0')
I am [3, 5]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 202, 202])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 200, 200])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 202, 202])
grad_out size torch.Size([1, 3, 200, 200])
crop [20, 20, 20, 20] [780, 979, 460, 659] [800, 959, 480, 639]
## 20 180 20 180
##############return grad_in in conv2d tensor([-1.6807e-07, -4.5203e-07, -3.6948e-08,  1.1545e-06,  9.7206e-07,
        -1.9965e-06, -2.4646e-06,  3.7803e-06,  3.8112e-06, -2.3283e-06],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 404, 404])
grad_out size torch.Size([1, 3, 202, 202])
arg size torch.Size([1, 3, 202, 202])
##############grad_in in maxp torch.Size([1, 3, 404, 404])
I am [3, 5]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 406, 406])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 404, 404])
grad_input torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 406, 406])
grad_input new torch.Size([1, 3, 406, 406])
new grad_input torch.Size([1, 3, 406, 406])
grad_out size torch.Size([1, 3, 404, 404])
crop [42, 42, 42, 42] [1558, 1961, 918, 1321] [1600, 1919, 960, 1279]
## 42 362 42 362
##############return grad_in in conv2d tensor([-9.2619e-08,  9.7212e-08,  1.1604e-10,  0.0000e+00,  0.0000e+00,
         1.7174e-08, -6.1235e-08,  8.2954e-08,  0.0000e+00,  3.6562e-08],
       device='cuda:0')
I am [3, 5]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 408, 408])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 408, 408])
grad_input new torch.Size([1, 3, 408, 408])
grad_input torch.Size([1, 3, 408, 408])
grad_out size torch.Size([1, 3, 406, 406])
crop [43, 43, 43, 43] [1557, 1962, 917, 1322] [1600, 1919, 960, 1279]
## 43 363 43 363
##############return grad_in in conv2d tensor([-3.4837e-09,  1.3621e-08,  8.2447e-09, -2.9986e-08,  2.8993e-08,
         6.8238e-09, -2.1690e-10, -3.5821e-08,  1.6598e-08,  3.4271e-09],
       device='cuda:0')
I am [3, 5]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 410, 410])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 408, 408])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 410, 410])
grad_out size torch.Size([1, 3, 408, 408])
crop [44, 44, 44, 44] [1556, 1963, 916, 1323] [1600, 1919, 960, 1279]
## 44 364 44 364
##############return grad_in in conv2d tensor([-9.0104e-10, -1.5580e-09, -2.3185e-09,  1.7664e-09, -5.9938e-09,
         3.3867e-10, -4.4026e-09,  3.1804e-09,  1.3244e-09,  2.9242e-09],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 820, 820])
grad_out size torch.Size([1, 3, 410, 410])
arg size torch.Size([1, 3, 410, 410])
##############grad_in in maxp torch.Size([1, 3, 820, 820])
I am [3, 5]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 822, 822])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 820, 820])
grad_input torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 822, 822])
grad_input new torch.Size([1, 3, 822, 822])
new grad_input torch.Size([1, 3, 822, 822])
grad_out size torch.Size([1, 3, 820, 820])
crop [90, 90, 90, 90] [3110, 3929, 1830, 2649] [3200, 3839, 1920, 2559]
## 90 730 90 730
##############return grad_in in conv2d tensor([ 0.0000e+00, -3.9636e-10, -2.2099e-10, -4.8841e-10,  5.3383e-11,
        -5.9210e-11, -3.3371e-10, -6.0508e-10,  5.5672e-10,  1.3609e-09],
       device='cuda:0')
I am [3, 5]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 824, 824])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cCheckpointBackward
grad_input torch.Size([1, 3, 824, 824])
grad_out size torch.Size([1, 3, 822, 822])
crop [91, 91, 91, 91] [3109, 3930, 1829, 2650] [3200, 3839, 1920, 2559]
## 91 731 91 731
##############return grad_in in conv2d tensor([ 0.0000e+00, -5.8787e-11, -1.0369e-10, -3.4880e-11,  2.2535e-11,
        -6.9246e-11, -5.5058e-11, -2.0009e-10,  6.9351e-11,  4.6216e-10],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 1648, 1648])
grad_out size torch.Size([1, 3, 824, 824])
arg size torch.Size([1, 3, 824, 824])
##############grad_in in maxp torch.Size([1, 3, 1648, 1648])
I am [3, 5]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 1650, 1650])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1648, 1648])
grad_input torch.Size([1, 3, 1650, 1650])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 1650, 1650])
grad_input new torch.Size([1, 3, 1650, 1650])
new grad_input torch.Size([1, 3, 1650, 1650])
grad_out size torch.Size([1, 3, 1648, 1648])
crop [184, 184, 184, 184] [6216, 7863, 3656, 5303] [6400, 7679, 3840, 5119]
## 184 1464 184 1464
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  3.9772e-12,
         7.4652e-12, -1.7210e-12,  1.3379e-12, -6.6582e-12,  1.0773e-11],
       device='cuda:0')
I am [3, 5]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 3, 1652, 1652])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1650, 1650])
final torch.Size([1, 3, 1652, 1652])
padding info :: [0, 0, 0, 0]
reshape_for_final ## 186 1466 186 1466
after crop g_in torch.Size([1, 3, 1280, 1280])
grad_out size torch.Size([1, 3, 1650, 1650])
crop [185, 185, 185, 185] [6215, 7864, 3655, 5304] [6400, 7679, 3840, 5119]
## 185 1465 185 1465
##############return grad_in in conv2d tensor([-1.1247e-07,  3.9007e-07,  1.4189e-07,  1.1365e-07, -1.6993e-07,
        -1.0705e-07, -1.5038e-07, -7.9339e-08, -1.7711e-08,  3.4640e-07],
       device='cuda:0')
[8, 8]
HREREERE torch.Size([1, 3, 10240, 10240])

############# Enter checkpointing bkward ####
inputs len 5 4
in customized Sequential 5
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 4]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <3,4,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 4]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <3,4,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 4]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <3,4,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 4]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <3,4,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 4]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <3,4,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 4]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <3,4,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 4]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <3,4,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 4]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <3,4,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 4]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <3,4,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 4]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <3,4,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 4]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <3,4,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 4]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <3,4,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() [torch/csrc/autograd/python_engine.cpp] THPEngine_run_backward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 4]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <3,4,>,

in customized Sequential 3
need to get existing

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 92, 92])
grad_out size torch.Size([1, 3, 320, 320])
arg size torch.Size([1, 3, 46, 46])
new_grad_out torch.Size([1, 3, 46, 46])
##############grad_in in maxp torch.Size([1, 3, 92, 92])
I am [3, 4]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 94, 94])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 92, 92])
grad_input torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 94, 94])
grad_input new torch.Size([1, 3, 94, 94])
new grad_input torch.Size([1, 3, 94, 94])
grad_out size torch.Size([1, 3, 92, 92])
crop [6, 6, 6, 6] [314, 405, 234, 325] [320, 399, 240, 319]
## 6 86 6 86
##############return grad_in in conv2d tensor([ 0.0000e+00, -9.9836e-05, -2.6820e-04, -6.7892e-04, -3.5063e-03,
        -5.4899e-03,  8.1958e-04,  3.0597e-03,  6.6705e-04, -2.6389e-04],
       device='cuda:0')
I am [3, 4]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 96, 96])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 96, 96])
grad_input new torch.Size([1, 3, 96, 96])
grad_input torch.Size([1, 3, 96, 96])
grad_out size torch.Size([1, 3, 94, 94])
crop [7, 7, 7, 7] [313, 406, 233, 326] [320, 399, 240, 319]
## 7 87 7 87
##############return grad_in in conv2d tensor([ 0.0000e+00, -5.6970e-05,  7.5836e-05, -4.5640e-04, -2.3777e-04,
         5.4461e-04, -4.2258e-04, -4.1677e-04, -3.3962e-04, -8.6188e-04],
       device='cuda:0')
I am [3, 4]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 98, 98])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 96, 96])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 98, 98])
grad_out size torch.Size([1, 3, 96, 96])
crop [8, 8, 8, 8] [312, 407, 232, 327] [320, 399, 240, 319]
## 8 88 8 88
##############return grad_in in conv2d tensor([ 0.0000e+00, -9.2044e-06,  1.8084e-05, -5.7761e-05, -1.6013e-05,
         3.6560e-04,  4.4891e-06, -3.6096e-04, -6.9003e-05, -1.1519e-04],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 196, 196])
grad_out size torch.Size([1, 3, 98, 98])
arg size torch.Size([1, 3, 98, 98])
##############grad_in in maxp torch.Size([1, 3, 196, 196])
I am [3, 4]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 198, 198])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 196, 196])
grad_input torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 198, 198])
grad_input new torch.Size([1, 3, 198, 198])
new grad_input torch.Size([1, 3, 198, 198])
grad_out size torch.Size([1, 3, 196, 196])
crop [18, 18, 18, 18] [622, 817, 462, 657] [640, 799, 480, 639]
## 18 178 18 178
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00, -8.3028e-07,  9.8006e-07,  2.8464e-07,
         3.6573e-06, -1.7954e-06, -1.9692e-06,  0.0000e+00,  1.7940e-05],
       device='cuda:0')
I am [3, 4]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 200, 200])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 200, 200])
grad_input new torch.Size([1, 3, 200, 200])
grad_input torch.Size([1, 3, 200, 200])
grad_out size torch.Size([1, 3, 198, 198])
crop [19, 19, 19, 19] [621, 818, 461, 658] [640, 799, 480, 639]
## 19 179 19 179
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00, -2.3363e-07,  1.5841e-07, -4.6043e-07,
         1.6249e-06, -1.2981e-06,  6.0247e-07, -6.7405e-07,  4.7817e-06],
       device='cuda:0')
I am [3, 4]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 202, 202])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 200, 200])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 202, 202])
grad_out size torch.Size([1, 3, 200, 200])
crop [20, 20, 20, 20] [620, 819, 460, 659] [640, 799, 480, 639]
## 20 180 20 180
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  1.8216e-08,  6.3459e-08, -3.9731e-09,
        -1.6449e-07, -1.2836e-07, -2.0801e-08,  2.7620e-07, -1.9589e-07],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 404, 404])
grad_out size torch.Size([1, 3, 202, 202])
arg size torch.Size([1, 3, 202, 202])
##############grad_in in maxp torch.Size([1, 3, 404, 404])
I am [3, 4]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 406, 406])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 404, 404])
grad_input torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 406, 406])
grad_input new torch.Size([1, 3, 406, 406])
new grad_input torch.Size([1, 3, 406, 406])
grad_out size torch.Size([1, 3, 404, 404])
crop [42, 42, 42, 42] [1238, 1641, 918, 1321] [1280, 1599, 960, 1279]
## 42 362 42 362
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  3.5731e-09,
        -1.9854e-09,  1.2698e-08,  7.0313e-09, -5.4957e-09,  3.4922e-10],
       device='cuda:0')
I am [3, 4]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 408, 408])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 408, 408])
grad_input new torch.Size([1, 3, 408, 408])
grad_input torch.Size([1, 3, 408, 408])
grad_out size torch.Size([1, 3, 406, 406])
crop [43, 43, 43, 43] [1237, 1642, 917, 1322] [1280, 1599, 960, 1279]
## 43 363 43 363
##############return grad_in in conv2d tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5406e-10, 9.9004e-10,
        7.7996e-11, 4.8958e-09, 1.0883e-09, 2.6113e-09], device='cuda:0')
I am [3, 4]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 410, 410])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 408, 408])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 410, 410])
grad_out size torch.Size([1, 3, 408, 408])
crop [44, 44, 44, 44] [1236, 1643, 916, 1323] [1280, 1599, 960, 1279]
## 44 364 44 364
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  9.5762e-11,
        -3.1577e-10,  1.4723e-11, -1.0049e-09, -1.0526e-09, -1.7256e-09],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 820, 820])
grad_out size torch.Size([1, 3, 410, 410])
arg size torch.Size([1, 3, 410, 410])
##############grad_in in maxp torch.Size([1, 3, 820, 820])
I am [3, 4]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 822, 822])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 820, 820])
grad_input torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 822, 822])
grad_input new torch.Size([1, 3, 822, 822])
new grad_input torch.Size([1, 3, 822, 822])
grad_out size torch.Size([1, 3, 820, 820])
crop [90, 90, 90, 90] [2470, 3289, 1830, 2649] [2560, 3199, 1920, 2559]
## 90 730 90 730
##############return grad_in in conv2d tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 6.9866e-11], device='cuda:0')
I am [3, 4]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 824, 824])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 824, 824])
grad_out size torch.Size([1, 3, 822, 822])
crop [91, 91, 91, 91] [2469, 3290, 1829, 2650] [2560, 3199, 1920, 2559]
## 91 731 91 731
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cCheckpointBackward
##############return grad_in in conv2d tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0441e-11], device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 1648, 1648])
grad_out size torch.Size([1, 3, 824, 824])
arg size torch.Size([1, 3, 824, 824])
##############grad_in in maxp torch.Size([1, 3, 1648, 1648])
I am [3, 4]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 1650, 1650])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1648, 1648])
grad_input torch.Size([1, 3, 1650, 1650])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 1650, 1650])
grad_input new torch.Size([1, 3, 1650, 1650])
new grad_input torch.Size([1, 3, 1650, 1650])
grad_out size torch.Size([1, 3, 1648, 1648])
crop [184, 184, 184, 184] [4936, 6583, 3656, 5303] [5120, 6399, 3840, 5119]
## 184 1464 184 1464
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [3, 4]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 3, 1652, 1652])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1650, 1650])
final torch.Size([1, 3, 1652, 1652])
padding info :: [0, 0, 0, 0]
reshape_for_final ## 186 1466 186 1466
after crop g_in torch.Size([1, 3, 1280, 1280])
grad_out size torch.Size([1, 3, 1650, 1650])
crop [185, 185, 185, 185] [4935, 6584, 3655, 5304] [5120, 6399, 3840, 5119]
## 185 1465 185 1465
##############return grad_in in conv2d tensor([ 3.3386e-07,  3.5326e-07,  4.3676e-07, -7.8243e-08,  1.7742e-07,
        -3.0042e-08,  1.1284e-07, -8.8518e-08, -1.6811e-07,  5.8805e-08],
       device='cuda:0')
[8, 8]
HREREERE torch.Size([1, 3, 10240, 10240])

############# Enter checkpointing bkward ####
inputs len 5 4
in customized Sequential 5
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 3]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <3,3,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 3]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <3,3,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 3]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <3,3,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 3]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <3,3,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 3]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <3,3,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 3]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <3,3,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 3]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <3,3,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 3]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <3,3,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 3]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <3,3,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 3]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <3,3,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 3]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <3,3,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 3]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <3,3,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 3]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <3,3,>,

in customized Sequential 3
need to get existing

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 92, 92])
grad_out size [torch/csrc/autograd/python_engine.cpp] THPEngine_run_backward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
torch.Size([1, 3, 320, 320])
arg size torch.Size([1, 3, 46, 46])
new_grad_out torch.Size([1, 3, 46, 46])
##############grad_in in maxp torch.Size([1, 3, 92, 92])
I am [3, 3]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 94, 94])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 92, 92])
grad_input torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 94, 94])
grad_input new torch.Size([1, 3, 94, 94])
new grad_input torch.Size([1, 3, 94, 94])
grad_out size torch.Size([1, 3, 92, 92])
crop [6, 6, 6, 6] [234, 325, 234, 325] [240, 319, 240, 319]
## 6 86 6 86
##############return grad_in in conv2d tensor([-0.0013, -0.0003,  0.0029, -0.0024,  0.0057, -0.0046, -0.0051,  0.0082,
         0.0030, -0.0015], device='cuda:0')
I am [3, 3]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 96, 96])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 96, 96])
grad_input new torch.Size([1, 3, 96, 96])
grad_input torch.Size([1, 3, 96, 96])
grad_out size torch.Size([1, 3, 94, 94])
crop [7, 7, 7, 7] [233, 326, 233, 326] [240, 319, 240, 319]
## 7 87 7 87
##############return grad_in in conv2d tensor([ 0.0001, -0.0002,  0.0004, -0.0007, -0.0004,  0.0001, -0.0013,  0.0016,
        -0.0025, -0.0022], device='cuda:0')
I am [3, 3]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 98, 98])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 96, 96])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 98, 98])
grad_out size torch.Size([1, 3, 96, 96])
crop [8, 8, 8, 8] [232, 327, 232, 327] [240, 319, 240, 319]
## 8 88 8 88
##############return grad_in in conv2d tensor([ 4.4321e-05, -7.2210e-05, -1.5162e-05,  1.7458e-04, -3.9905e-04,
         2.9761e-04, -1.9749e-04,  9.5006e-05,  3.1411e-04, -9.8365e-04],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 196, 196])
grad_out size torch.Size([1, 3, 98, 98])
arg size torch.Size([1, 3, 98, 98])
##############grad_in in maxp torch.Size([1, 3, 196, 196])
I am [3, 3]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 198, 198])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 196, 196])
grad_input torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 198, 198])
grad_input new torch.Size([1, 3, 198, 198])
new grad_input torch.Size([1, 3, 198, 198])
grad_out size torch.Size([1, 3, 196, 196])
crop [18, 18, 18, 18] [462, 657, 462, 657] [480, 639, 480, 639]
## 18 178 18 178
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00, -4.3351e-06, -8.5124e-06, -4.2724e-07,
         4.2978e-06,  1.7033e-06,  1.0943e-05, -5.6249e-05,  1.3252e-05],
       device='cuda:0')
I am [3, 3]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 200, 200])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 200, 200])
grad_input new torch.Size([1, 3, 200, 200])
grad_input torch.Size([1, 3, 200, 200])
grad_out size torch.Size([1, 3, 198, 198])
crop [19, 19, 19, 19] [461, 658, 461, 658] [480, 639, 480, 639]
## 19 179 19 179
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00, -1.1749e-06, -3.4779e-06, -2.6374e-06,
         1.2215e-06, -1.6106e-06,  1.4956e-06, -1.3896e-05,  1.9228e-05],
       device='cuda:0')
I am [3, 3]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 202, 202])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 200, 200])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 202, 202])
grad_out size torch.Size([1, 3, 200, 200])
crop [20, 20, 20, 20] [460, 659, 460, 659] [480, 639, 480, 639]
## 20 180 20 180
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  9.0809e-08,  6.3898e-07,  9.7229e-07,
        -3.2099e-07, -4.8899e-07,  4.3864e-07, -4.5948e-07,  1.3297e-06],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 404, 404])
grad_out size torch.Size([1, 3, 202, 202])
arg size torch.Size([1, 3, 202, 202])
##############grad_in in maxp torch.Size([1, 3, 404, 404])
I am [3, 3]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 406, 406])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 404, 404])
grad_input torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 406, 406])
grad_input new torch.Size([1, 3, 406, 406])
new grad_input torch.Size([1, 3, 406, 406])
grad_out size torch.Size([1, 3, 404, 404])
crop [42, 42, 42, 42] [918, 1321, 918, 1321] [960, 1279, 960, 1279]
## 42 362 42 362
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  2.3605e-08,
        -2.4635e-08, -1.9758e-08,  5.6203e-08,  1.0076e-07,  2.6932e-08],
       device='cuda:0')
I am [3, 3]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 408, 408])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 408, 408])
grad_input new torch.Size([1, 3, 408, 408])
grad_input torch.Size([1, 3, 408, 408])
grad_out size torch.Size([1, 3, 406, 406])
crop [43, 43, 43, 43] [917, 1322, 917, 1322] [960, 1279, 960, 1279]
## 43 363 43 363
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -3.1220e-09,
        -1.0087e-08,  1.2558e-08,  1.9888e-08,  1.0318e-09,  1.9582e-08],
       device='cuda:0')
I am [3, 3]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 410, 410])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 408, 408])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 410, 410])
grad_out size torch.Size([1, 3, 408, 408])
crop [44, 44, 44, 44] [916, 1323, 916, 1323] [960, 1279, 960, 1279]
## 44 364 44 364
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.1736e-10,
         2.4330e-09,  1.6050e-09,  1.1225e-09, -1.9631e-09, -1.1500e-08],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 820, 820])
grad_out size torch.Size([1, 3, 410, 410])
arg size torch.Size([1, 3, 410, 410])
##############grad_in in maxp torch.Size([1, 3, 820, 820])
I am [3, 3]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 822, 822])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 820, 820])
grad_input torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 822, 822])
grad_input new torch.Size([1, 3, 822, 822])
new grad_input torch.Size([1, 3, 822, 822])
grad_out size torch.Size([1, 3, 820, 820])
crop [90, 90, 90, 90] [1830, 2649, 1830, 2649] [1920, 2559, 1920, 2559]
## 90 730 90 730
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -2.2869e-10],
       device='cuda:0')
I am [3, 3]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 824, 824])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 824, 824])
grad_out size torch.Size([1, 3, 822, 822])
crop [91, 91, 91, 91] [1829, 2650, 1829, 2650] [1920, 2559, 1920, 2559]
## 91 731 91 731
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -3.0938e-11],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 1648, 1648])
grad_out size torch.Size([1, 3, 824, 824])
arg size torch.Size([1, 3, 824, 824])
##############grad_in in maxp torch.Size([1, 3, 1648, 1648])
I am [3, 3]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 1650, 1650])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape [torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cCheckpointBackward
torch.Size([1, 3, 1648, 1648])
grad_input torch.Size([1, 3, 1650, 1650])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 1650, 1650])
grad_input new torch.Size([1, 3, 1650, 1650])
new grad_input torch.Size([1, 3, 1650, 1650])
grad_out size torch.Size([1, 3, 1648, 1648])
crop [184, 184, 184, 184] [3656, 5303, 3656, 5303] [3840, 5119, 3840, 5119]
## 184 1464 184 1464
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [3, 3]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 3, 1652, 1652])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1650, 1650])
final torch.Size([1, 3, 1652, 1652])
padding info :: [0, 0, 0, 0]
reshape_for_final ## 186 1466 186 1466
after crop g_in torch.Size([1, 3, 1280, 1280])
grad_out size torch.Size([1, 3, 1650, 1650])
crop [185, 185, 185, 185] [3655, 5304, 3655, 5304] [3840, 5119, 3840, 5119]
## 185 1465 185 1465
##############return grad_in in conv2d tensor([-5.1074e-07, -5.1625e-07, -5.7796e-07, -1.6946e-07,  1.2355e-07,
         1.3988e-07, -3.7057e-07,  1.4799e-07, -1.2153e-07, -4.4310e-07],
       device='cuda:0')
[8, 8]
HREREERE torch.Size([1, 3, 10240, 10240])

############# Enter checkpointing bkward ####
inputs len 5 4
in customized Sequential 5
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 2]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <3,2,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 2]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <3,2,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 2]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <3,2,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 2]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <3,2,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 2]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <3,2,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 2]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <3,2,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 2]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <3,2,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 2]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <3,2,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 2]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <3,2,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 2]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <3,2,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 2]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <3,2,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 2]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <3,2,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 2]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <3,2,>,

in customized Sequential 3
need to get existing

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 92, 92])
grad_out size torch.Size([1, 3, 320, 320])
arg size torch.Size([1, 3, 46, 46])
new_grad_out torch.Size([1, 3, 46, 46])
##############grad_in in maxp torch.Size([1, 3, 92, 92])
I am [3, 2]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 94, 94])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 92, 92])
grad_input torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 94, 94])
grad_input new torch.Size([1, 3, 94, 94])
new grad_input torch.Size([1, 3, 94, 94])
grad_out size torch.Size([1, 3, 92, 92])
crop [torch/csrc/autograd/python_engine.cpp] THPEngine_run_backward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[6, 6, 6, 6] [154, 245, 234, 325] [160, 239, 240, 319]
## 6 86 6 86
##############return grad_in in conv2d tensor([ 8.4570e-05,  2.2719e-04, -6.7267e-05, -1.6602e-03,  2.4130e-03,
        -1.1392e-03,  2.6051e-03, -1.6741e-04, -4.1180e-04, -1.4242e-03],
       device='cuda:0')
I am [3, 2]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 96, 96])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 96, 96])
grad_input new torch.Size([1, 3, 96, 96])
grad_input torch.Size([1, 3, 96, 96])
grad_out size torch.Size([1, 3, 94, 94])
crop [7, 7, 7, 7] [153, 246, 233, 326] [160, 239, 240, 319]
## 7 87 7 87
##############return grad_in in conv2d tensor([ 4.8259e-05, -6.4240e-05,  2.0050e-05, -3.2923e-04, -1.7276e-04,
        -6.5269e-04, -6.0750e-04, -2.3418e-04, -2.3070e-04, -5.8726e-04],
       device='cuda:0')
I am [3, 2]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 98, 98])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 96, 96])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 98, 98])
grad_out size torch.Size([1, 3, 96, 96])
crop [8, 8, 8, 8] [152, 247, 232, 327] [160, 239, 240, 319]
## 8 88 8 88
##############return grad_in in conv2d tensor([ 7.7970e-06, -1.5319e-05, -1.0295e-05,  2.2632e-05, -8.5103e-05,
         1.0372e-04, -2.1980e-04,  6.8941e-05,  1.7970e-05, -4.8756e-05],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 196, 196])
grad_out size torch.Size([1, 3, 98, 98])
arg size torch.Size([1, 3, 98, 98])
##############grad_in in maxp torch.Size([1, 3, 196, 196])
I am [3, 2]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 198, 198])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 196, 196])
grad_input torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 198, 198])
grad_input new torch.Size([1, 3, 198, 198])
new grad_input torch.Size([1, 3, 198, 198])
grad_out size torch.Size([1, 3, 196, 196])
crop [18, 18, 18, 18] [302, 497, 462, 657] [320, 479, 480, 639]
## 18 178 18 178
##############return grad_in in conv2d tensor([ 7.0332e-07, -8.3020e-07, -9.8774e-08, -4.8610e-09,  4.1382e-06,
        -2.0325e-06, -8.7057e-07, -1.1482e-05, -3.7947e-07,  1.3021e-07],
       device='cuda:0')
I am [3, 2]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 200, 200])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 200, 200])
grad_input new torch.Size([1, 3, 200, 200])
grad_input torch.Size([1, 3, 200, 200])
grad_out size torch.Size([1, 3, 198, 198])
crop [19, 19, 19, 19] [301, 498, 461, 658] [320, 479, 480, 639]
## 19 179 19 179
##############return grad_in in conv2d tensor([ 1.9791e-07, -1.3419e-07,  4.3302e-07, -4.7403e-07,  1.3611e-06,
        -1.2811e-06,  1.0656e-06, -3.9136e-06,  3.0746e-07, -3.5315e-06],
       device='cuda:0')
I am [3, 2]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 202, 202])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 200, 200])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 202, 202])
grad_out size torch.Size([1, 3, 200, 200])
crop [20, 20, 20, 20] [300, 499, 460, 659] [320, 479, 480, 639]
## 20 180 20 180
##############return grad_in in conv2d tensor([-1.5431e-08, -5.3756e-08, -3.8565e-11,  8.9890e-08, -7.1091e-08,
        -1.8461e-07, -5.9556e-08,  4.5227e-07,  6.5895e-07,  2.6990e-07],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 404, 404])
grad_out size torch.Size([1, 3, 202, 202])
arg size torch.Size([1, 3, 202, 202])
##############grad_in in maxp torch.Size([1, 3, 404, 404])
I am [3, 2]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 406, 406])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 404, 404])
grad_input torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 406, 406])
grad_input new torch.Size([1, 3, 406, 406])
new grad_input torch.Size([1, 3, 406, 406])
grad_out size torch.Size([1, 3, 404, 404])
crop [42, 42, 42, 42] [598, 1001, 918, 1321] [640, 959, 960, 1279]
## 42 362 42 362
##############return grad_in in conv2d tensor([ 0.0000e+00, -3.9220e-09,  2.6918e-09,  6.3491e-09,  1.6604e-09,
         2.8274e-10, -3.8516e-10,  1.3299e-08, -4.4310e-09, -8.6997e-09],
       device='cuda:0')
I am [3, 2]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 408, 408])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 408, 408])
grad_input new torch.Size([1, 3, 408, 408])
grad_input torch.Size([1, 3, 408, 408])
grad_out size torch.Size([1, 3, 406, 406])
crop [43, 43, 43, 43] [597, 1002, 917, 1322] [640, 959, 960, 1279]
## 43 363 43 363
##############return grad_in in conv2d tensor([ 0.0000e+00,  5.1872e-10,  1.8613e-09, -8.5047e-10, -1.1898e-09,
         2.0971e-09,  9.9750e-10,  2.4140e-09,  3.1043e-09, -2.4125e-09],
       device='cuda:0')
I am [3, 2]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 410, 410])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 408, 408])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 410, 410])
grad_out size torch.Size([1, 3, 408, 408])
crop [44, 44, 44, 44] [596, 1003, 916, 1323] [640, 959, 960, 1279]
## 44 364 44 364
##############return grad_in in conv2d tensor([ 0.0000e+00,  1.9499e-11, -3.9727e-10, -3.4274e-10, -1.9115e-10,
        -3.7456e-10, -2.2648e-10,  9.7508e-11, -1.1813e-09, -1.0004e-09],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 820, 820])
grad_out size torch.Size([1, 3, 410, 410])
arg size torch.Size([1, 3, 410, 410])
##############grad_in in maxp torch.Size([1, 3, 820, 820])
I am [3, 2]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 822, 822])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 820, 820])
grad_input torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 822, 822])
grad_input new torch.Size([1, 3, 822, 822])
new grad_input torch.Size([1, 3, 822, 822])
grad_out size torch.Size([1, 3, 820, 820])
crop [90, 90, 90, 90] [1190, 2009, 1830, 2649] [1280, 1919, 1920, 2559]
## 90 730 90 730
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.3749e-11,  7.0099e-12,
         2.1043e-13,  0.0000e+00,  0.0000e+00, -3.5799e-11, -2.7866e-11],
       device='cuda:0')
I am [3, 2]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 824, 824])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 824, 824])
grad_out size torch.Size([1, 3, 822, 822])
crop [91, 91, 91, 91] [1189, 2010, 1829, 2650] [1280, 1919, 1920, 2559]
## 91 731 91 731
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.9303e-12,  1.4235e-12,
        -2.1720e-12, -3.7765e-13,  1.0119e-12,  2.1701e-13, -9.3609e-13],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 1648, 1648])
grad_out size torch.Size([1, 3, 824, 824])
arg size torch.Size([1, 3, 824, 824])
##############grad_in in maxp torch.Size([1, 3, 1648, 1648])
I am [3, 2]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 1650, 1650])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1648, 1648])
grad_input torch.Size([1, 3, 1650, 1650])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 1650, 1650])
grad_input new torch.Size([1, 3, 1650, 1650])
new grad_input torch.Size([1, 3, 1650, 1650])
grad_out size torch.Size([1, 3, 1648, 1648])
crop [184, 184, 184, 184] [2376, 4023, 3656, 5303] [2560, 3839, 3840, 5119]
## 184 1464 184 1464
##############return grad_in in conv2d [torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cCheckpointBackward
[torch/csrc/autograd/python_engine.cpp] THPEngine_run_backward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00, -3.5586e-14, -5.1586e-13,  2.0372e-13, -2.5037e-13],
       device='cuda:0')
I am [3, 2]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 3, 1652, 1652])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1650, 1650])
final torch.Size([1, 3, 1652, 1652])
padding info :: [0, 0, 0, 0]
reshape_for_final ## 186 1466 186 1466
after crop g_in torch.Size([1, 3, 1280, 1280])
grad_out size torch.Size([1, 3, 1650, 1650])
crop [185, 185, 185, 185] [2375, 4024, 3655, 5304] [2560, 3839, 3840, 5119]
## 185 1465 185 1465
##############return grad_in in conv2d tensor([-6.8817e-07, -8.4005e-07,  4.3678e-09, -6.3585e-07,  9.0955e-08,
         1.0484e-06, -1.1960e-06,  2.9185e-07,  7.4190e-09,  3.7027e-07],
       device='cuda:0')
[8, 8]
HREREERE torch.Size([1, 3, 10240, 10240])

############# Enter checkpointing bkward ####
inputs len 5 4
in customized Sequential 5
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 1]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <3,1,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 1]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <3,1,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 1]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <3,1,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 1]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <3,1,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 1]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <3,1,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 1]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <3,1,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 1]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <3,1,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 1]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <3,1,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 1]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <3,1,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 1]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <3,1,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 1]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <3,1,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 1]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <3,1,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 1]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <3,1,>,

in customized Sequential 3
need to get existing

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 92, 92])
grad_out size torch.Size([1, 3, 320, 320])
arg size torch.Size([1, 3, 46, 46])
new_grad_out torch.Size([1, 3, 46, 46])
##############grad_in in maxp torch.Size([1, 3, 92, 92])
I am [3, 1]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 94, 94])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 92, 92])
grad_input torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 94, 94])
grad_input new torch.Size([1, 3, 94, 94])
new grad_input torch.Size([1, 3, 94, 94])
grad_out size torch.Size([1, 3, 92, 92])
crop [6, 6, 6, 6] [74, 165, 234, 325] [80, 159, 240, 319]
## 6 86 6 86
##############return grad_in in conv2d tensor([-1.5868e-03,  4.0707e-03,  6.4803e-03, -3.5016e-03, -2.1212e-04,
        -1.2972e-04,  6.8757e-04,  2.3427e-03, -4.0996e-03, -3.3361e-05],
       device='cuda:0')
I am [3, 1]
@@@ using cudnn bkw
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
in the middle ++ input shape torch.Size([1, 3, 96, 96])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 96, 96])
grad_input new torch.Size([1, 3, 96, 96])
grad_input torch.Size([1, 3, 96, 96])
grad_out size torch.Size([1, 3, 94, 94])
crop [7, 7, 7, 7] [73, 166, 233, 326] [80, 159, 240, 319]
## 7 87 7 87
##############return grad_in in conv2d tensor([-2.9482e-04,  8.4016e-04, -1.4384e-03, -1.1944e-04,  1.8140e-04,
         5.7081e-04, -1.7577e-04,  4.5712e-04, -4.9268e-05,  8.0797e-04],
       device='cuda:0')
I am [3, 1]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 98, 98])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 96, 96])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 98, 98])
grad_out size torch.Size([1, 3, 96, 96])
crop [8, 8, 8, 8] [72, 167, 232, 327] [80, 159, 240, 319]
## 8 88 8 88
##############return grad_in in conv2d tensor([ 7.1139e-06,  7.8575e-05, -2.3475e-04, -2.9535e-04,  2.4516e-04,
         3.5527e-04, -8.7833e-06, -1.2524e-04,  8.8409e-05, -1.6737e-04],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 196, 196])
grad_out size torch.Size([1, 3, 98, 98])
arg size torch.Size([1, 3, 98, 98])
##############grad_in in maxp torch.Size([1, 3, 196, 196])
I am [3, 1]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 198, 198])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 196, 196])
grad_input torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 198, 198])
grad_input new torch.Size([1, 3, 198, 198])
new grad_input torch.Size([1, 3, 198, 198])
grad_out size torch.Size([1, 3, 196, 196])
crop [18, 18, 18, 18] [142, 337, 462, 657] [160, 319, 480, 639]
## 18 178 18 178
##############return grad_in in conv2d tensor([-8.2346e-06, -4.5433e-07,  1.0284e-05, -5.0939e-06,  2.6810e-05,
        -1.5199e-05,  6.5582e-06,  3.8302e-05,  3.3680e-05,  2.6051e-06],
       device='cuda:0')
I am [3, 1]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 200, 200])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 200, 200])
grad_input new torch.Size([1, 3, 200, 200])
grad_input torch.Size([1, 3, 200, 200])
grad_out size torch.Size([1, 3, 198, 198])
crop [19, 19, 19, 19] [141, 338, 461, 658] [160, 319, 480, 639]
## 19 179 19 179
##############return grad_in in conv2d tensor([-2.5002e-06, -3.0434e-07,  1.3630e-06, -4.8856e-06,  1.0655e-05,
        -5.0680e-06,  5.4877e-06, -5.2644e-07,  1.3198e-05,  1.2449e-05],
       device='cuda:0')
I am [3, 1]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 202, 202])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 200, 200])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 202, 202])
grad_out size torch.Size([1, 3, 200, 200])
crop [20, 20, 20, 20] [140, 339, 460, 659] [160, 319, 480, 639]
## 20 180 20 180
##############return grad_in in conv2d tensor([ 1.9820e-07,  5.4240e-07, -8.7210e-08, -1.9133e-07, -3.7574e-07,
        -1.3909e-06,  1.5066e-06,  6.1357e-07, -5.4550e-06, -3.7863e-06],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 404, 404])
grad_out size torch.Size([1, 3, 202, 202])
arg size torch.Size([1, 3, 202, 202])
##############grad_in in maxp torch.Size([1, 3, 404, 404])
I am [3, 1]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 406, 406])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 404, 404])
grad_input torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 406, 406])
grad_input new torch.Size([1, 3, 406, 406])
new grad_input torch.Size([1, 3, 406, 406])
grad_out size torch.Size([1, 3, 404, 404])
crop [42, 42, 42, 42] [278, 681, 918, 1321] [320, 639, 960, 1279]
## 42 362 42 362
##############return grad_in in conv2d tensor([ 0.0000e+00,  3.8411e-08,  2.6573e-09,  1.4408e-08, -2.8158e-08,
         0.0000e+00,  0.0000e+00,  2.1541e-08, -1.0824e-07, -3.1986e-08],
       device='cuda:0')
I am [3, 1]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 408, 408])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 408, 408])
grad_input new torch.Size([1, 3, 408, 408])
grad_input torch.Size([1, 3, 408, 408])
grad_out size torch.Size([1, 3, 406, 406])
crop [43, 43, 43, 43] [277, 682, 917, 1322] [320, 639, 960, 1279]
## 43 363 43 363
##############return grad_in in conv2d tensor([ 0.0000e+00,  9.1077e-09,  7.7887e-09, -2.5305e-08,  2.5551e-08,
         2.6393e-09, -1.3406e-08, -2.6207e-09, -2.4551e-08, -4.8717e-08],
       device='cuda:0')
I am [3, 1]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 410, 410])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 408, 408])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 410, 410])
grad_out size torch.Size([1, 3, 408, 408])
crop [44, 44, 44, 44] [276, 683, 916, 1323] [320, 639, 960, 1279]
## 44 364 44 364
##############return grad_in in conv2d tensor([ 0.0000e+00,  1.0221e-09, -3.4777e-09,  3.7343e-10, -3.4390e-09,
         7.4113e-10,  1.1712e-09,  5.0639e-10,  3.4902e-09,  6.0614e-09],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 820, 820])
grad_out size torch.Size([1, 3, 410, 410])
arg size torch.Size([1, 3, 410, 410])
##############grad_in in maxp torch.Size([1, 3, 820, 820])
I am [3, 1]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 822, 822])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 820, 820])
grad_input torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 822, 822])
grad_input new torch.Size([1, 3, 822, 822])
new grad_input torch.Size([1, 3, 822, 822])
grad_out size torch.Size([1, 3, 820, 820])
crop [90, 90, 90, 90] [550, 1369, 1830, 2649] [640, 1279, 1920, 2559]
## 90 730 90 730
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  3.3726e-10, -1.9301e-10,
        -3.5583e-10, -4.8789e-10, -4.4508e-10,  3.8979e-11, -6.9967e-10],
       device='cuda:0')
I am [3, 1]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 824, 824])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 824, 824])
grad_out size torch.Size([1, 3, 822, 822])
crop [91, 91, 91, 91] [549, 1370, 1829, 2650] [640, 1279, 1920, 2559]
## 91 731 91 731
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  4.7351e-11,  7.9072e-11,
        -4.0662e-11, -3.5702e-10, -8.6686e-11,  2.9001e-10,  4.8906e-11],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 1648, 1648])
grad_out size torch.Size([1, 3, 824, 824])
arg size torch.Size([1, 3, 824, 824])
##############grad_in in maxp torch.Size([1, 3, 1648, 1648])
I am [3, 1]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 1650, 1650])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1648, 1648])
grad_input torch.Size([1, 3, 1650, 1650])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 1650, 1650])
grad_input new torch.Size([1, 3, 1650, 1650])
new grad_input torch.Size([1, 3, 1650, 1650])
grad_out size torch.Size([1, 3, 1648, 1648])
crop [184, 184, 184, 184] [1096, 2743, 3656, 5303] [1280, 2559, 3840, 5119]
## 184 1464 184 1464
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00, -1.8163e-12, -3.4091e-12,  9.7768e-13,  4.6744e-12],
       device='cuda:0')
I am [3, 1]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 3, 1652, 1652])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1650, 1650])
final torch.Size([1, 3, 1652, 1652])
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cCheckpointBackward
[torch/csrc/autograd/python_engine.cpp] THPEngine_run_backward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
padding info :: [0, 0, 0, 0]
reshape_for_final ## 186 1466 186 1466
after crop g_in torch.Size([1, 3, 1280, 1280])
grad_out size torch.Size([1, 3, 1650, 1650])
crop [185, 185, 185, 185] [1095, 2744, 3655, 5304] [1280, 2559, 3840, 5119]
## 185 1465 185 1465
##############return grad_in in conv2d tensor([ 1.1501e-07, -1.6815e-07,  1.2517e-07, -3.7043e-08,  3.1535e-07,
         9.2066e-08, -3.0373e-08,  6.2240e-07, -6.2973e-08,  2.0547e-07],
       device='cuda:0')
[8, 8]
HREREERE torch.Size([1, 3, 10240, 10240])

############# Enter checkpointing bkward ####
inputs len 5 4
in customized Sequential 5
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 0]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1466])
myctx [[ 140046822727744[ PI( <3,0,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 0]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1464])
myctx [[ 140045015171856[ PI( <3,0,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 0]
shape input_tile_for_next
 torch.Size([1, 3, 822, 732])
myctx [[ 140045013800704[ PI( <3,0,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 0]
shape input_tile_for_next
 torch.Size([1, 3, 820, 730])
myctx [[ 140045013848944[ PI( <3,0,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 0]
shape input_tile_for_next
 torch.Size([1, 3, 408, 366])
myctx [[ 140045013206640[ PI( <3,0,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 0]
shape input_tile_for_next
 torch.Size([1, 3, 406, 364])
myctx [[ 140045013206784[ PI( <3,0,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 0]
shape input_tile_for_next
 torch.Size([1, 3, 404, 362])
myctx [[ 140045013206880[ PI( <3,0,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 0]
shape input_tile_for_next
 torch.Size([1, 3, 200, 182])
myctx [[ 140045013206352[ PI( <3,0,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 0]
shape input_tile_for_next
 torch.Size([1, 3, 198, 180])
myctx [[ 140045013206304[ PI( <3,0,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 0]
shape input_tile_for_next
 torch.Size([1, 3, 196, 178])
myctx [[ 140045013207072[ PI( <3,0,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 0]
shape input_tile_for_next
 torch.Size([1, 3, 96, 90])
myctx [[ 140045013206400[ PI( <3,0,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 0]
shape input_tile_for_next
 torch.Size([1, 3, 94, 88])
myctx [[ 140045013206448[ PI( <3,0,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [3, 0]
shape input_tile_for_next
 torch.Size([1, 3, 92, 86])
myctx [[ 140045013206976[ PI( <3,0,>,

in customized Sequential 3
need to get existing

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 92, 86])
grad_out size torch.Size([1, 3, 320, 320])
arg size torch.Size([1, 3, 46, 43])
new_grad_out torch.Size([1, 3, 46, 43])
##############grad_in in maxp torch.Size([1, 3, 92, 86])
I am [3, 0]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 94, 88])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 92, 86])
grad_input torch.Size([1, 3, 94, 88])
padding info :: [1, 0, 0, 0]
grad_input old torch.Size([1, 3, 94, 88])
grad_input new torch.Size([1, 3, 94, 88])
new grad_input torch.Size([1, 3, 94, 88])
grad_out size torch.Size([1, 3, 92, 86])
crop [0, 6, 6, 6] [0, 85, 234, 325] [0, 79, 240, 319]
## 6 86 0 80
##############return grad_in in conv2d tensor([ 0.0000, -0.0030,  0.0065, -0.0071, -0.0018,  0.0006,  0.0004,  0.0045,
        -0.0026,  0.0006], device='cuda:0')
I am [3, 0]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 96, 90])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 94, 88])
padding info :: [2, 0, 0, 0]
grad_input old torch.Size([1, 3, 96, 90])
grad_input new torch.Size([1, 3, 96, 90])
grad_input torch.Size([1, 3, 96, 90])
grad_out size torch.Size([1, 3, 94, 87])
crop [0, 7, 7, 7] [0, 86, 233, 326] [0, 79, 240, 319]
## 7 87 0 80
##############return grad_in in conv2d [torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
tensor([0.0000, 0.0000, 0.0014, 0.0013, 0.0015, 0.0024, 0.0001, 0.0010, 0.0002,
        0.0008], device='cuda:0')
I am [3, 0]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 98, 92])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 96, 90])
padding info :: [3, 0, 0, 0]
grad_input torch.Size([1, 3, 98, 89])
grad_out size torch.Size([1, 3, 96, 88])
crop [0, 8, 8, 8] [0, 87, 232, 327] [0, 79, 240, 319]
## 8 88 0 80
##############return grad_in in conv2d tensor([ 4.4390e-04, -2.5837e-04,  3.6424e-04,  1.0310e-04, -4.4918e-04,
        -3.6291e-05, -3.5633e-04,  4.5521e-04,  3.8794e-05, -3.5345e-04],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 196, 178])
grad_out size torch.Size([1, 3, 98, 89])
arg size torch.Size([1, 3, 98, 89])
##############grad_in in maxp torch.Size([1, 3, 196, 178])
I am [3, 0]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 198, 180])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 196, 178])
grad_input torch.Size([1, 3, 198, 180])
padding info :: [1, 0, 0, 0]
grad_input old torch.Size([1, 3, 198, 180])
grad_input new torch.Size([1, 3, 198, 180])
new grad_input torch.Size([1, 3, 198, 180])
grad_out size torch.Size([1, 3, 196, 178])
crop [0, 18, 18, 18] [0, 177, 462, 657] [0, 159, 480, 639]
## 18 178 0 160
##############return grad_in in conv2d tensor([ 0.0000e+00, -4.8980e-05, -8.1939e-05, -8.1190e-06,  6.1433e-05,
         6.9429e-06,  4.4105e-05, -1.2180e-05, -3.0659e-05,  2.5161e-05],
       device='cuda:0')
I am [3, 0]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 200, 182])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 198, 180])
padding info :: [2, 0, 0, 0]
grad_input old torch.Size([1, 3, 200, 182])
grad_input new torch.Size([1, 3, 200, 182])
grad_input torch.Size([1, 3, 200, 182])
grad_out size torch.Size([1, 3, 198, 179])
crop [0, 19, 19, 19] [0, 178, 461, 658] [0, 159, 480, 639]
## 19 179 0 160
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00, -1.7595e-06, -2.2273e-05,  1.0923e-05,
        -1.6116e-05,  3.1528e-05, -7.3790e-07,  8.7474e-06,  1.5869e-07],
       device='cuda:0')
I am [3, 0]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 202, 184])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 200, 182])
padding info :: [3, 0, 0, 0]
grad_input torch.Size([1, 3, 202, 181])
grad_out size torch.Size([1, 3, 200, 180])
crop [0, 20, 20, 20] [0, 179, 460, 659] [0, 159, 480, 639]
## 20 180 0 160
##############return grad_in in conv2d tensor([ 9.7769e-06, -2.4719e-06, -7.5737e-07, -4.1388e-07, -5.4989e-06,
        -2.3947e-07,  3.5123e-06, -1.6519e-06, -2.8359e-06, -2.5895e-06],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 404, 362])
grad_out size torch.Size([1, 3, 202, 181])
arg size torch.Size([1, 3, 202, 181])
##############grad_in in maxp torch.Size([1, 3, 404, 362])
I am [3, 0]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 406, 364])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 404, 362])
grad_input torch.Size([1, 3, 406, 364])
padding info :: [1, 0, 0, 0]
grad_input old torch.Size([1, 3, 406, 364])
grad_input new torch.Size([1, 3, 406, 364])
new grad_input torch.Size([1, 3, 406, 364])
grad_out size torch.Size([1, 3, 404, 362])
crop [0, 42, 42, 42] [0, 361, 918, 1321] [0, 319, 960, 1279]
## 42 362 0 320
##############return grad_in in conv2d tensor([ 0.0000e+00, -3.0308e-08, -1.9457e-06,  1.2119e-06, -1.6417e-06,
         0.0000e+00, -5.8695e-08, -3.5068e-08, -7.5526e-07, -3.4412e-07],
       device='cuda:0')
I am [3, 0]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 408, 366])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 406, 364])
padding info :: [2, 0, 0, 0]
grad_input old torch.Size([1, 3, 408, 366])
grad_input new torch.Size([1, 3, 408, 366])
grad_input torch.Size([1, 3, 408, 366])
grad_out size torch.Size([1, 3, 406, 363])
crop [0, 43, 43, 43] [0, 362, 917, 1322] [0, 319, 960, 1279]
## 43 363 0 320
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00, -3.9199e-08,  4.9560e-07, -7.5135e-08,
        -3.2848e-07, -1.7400e-07, -2.2778e-08, -1.5130e-07, -2.6935e-07],
       device='cuda:0')
I am [3, 0]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 410, 368])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 408, 366])
padding info :: [3, 0, 0, 0]
grad_input torch.Size([1, 3, 410, 365])
grad_out size torch.Size([1, 3, 408, 364])
crop [0, 44, 44, 44] [0, 363, 916, 1323] [0, 319, 960, 1279]
## 44 364 0 320
##############return grad_in in conv2d tensor([-1.2739e-07, -4.8171e-08, -3.9500e-08, -8.8463e-09,  9.4102e-09,
         5.7377e-09,  6.6582e-08,  9.3896e-08,  1.4494e-07,  4.3876e-08],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 820, 730])
grad_out size torch.Size([1, 3, 410, 365])
arg size torch.Size([1, 3, 410, 365])
##############grad_in in maxp torch.Size([1, 3, 820, 730])
I am [3, 0]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 822, 732])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 820, 730])
grad_input torch.Size([1, 3, 822, 732])
padding info :: [1, 0, 0, 0]
grad_input old torch.Size([1, 3, 822, 732])
grad_input new torch.Size([1, 3, 822, 732])
new grad_input torch.Size([1, 3, 822, 732])
grad_out size torch.Size([1, 3, 820, 730])
crop [0, 90, 90, 90] [0, 729, 1830, 2649] [0, 639, 1920, 2559]
## 90 730 0 640
##############return grad_in in conv2d tensor([ 0.0000e+00, -1.8571e-08,  1.8968e-08, -9.0217e-09, -1.4420e-08,
         1.4143e-09, -3.7159e-09, -5.8486e-09, -9.7789e-09, -1.2777e-08],
       device='cuda:0')
I am [3, 0]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 824, 734])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 822, 732])
padding info :: [2, 0, 0, 0]
grad_input torch.Size([1, 3, 824, 732])
grad_out size torch.Size([1, 3, 822, 731])
crop [0, 91, 91, 91] [0, 730, 1829, 2650] [0, 639, 1920, 2559]
## 91 731 0 640
##############return grad_in in conv2d tensor([-1.8383e-09,  1.0467e-10,  3.7477e-09, -2.4194e-09, -3.3125e-09,
        -2.6556e-09, -2.2801e-09, -3.0452e-09, -6.9575e-09, -4.8417e-09],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 1648, 1464])
grad_out size torch.Size([1, 3, 824, 732])
arg size torch.Size([1, 3, 824, 732])
##############grad_in in maxp torch.Size([1, 3, 1648, 1464])
I am [3, 0]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 1650, 1466])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1648, 1464])
grad_input torch.Size([1, 3, 1650, 1466])
padding info :: [1, 0, 0, 0]
grad_input old torch.Size([1, 3, 1650, 1466])
grad_input new torch.Size([1, 3, 1650, 1466])
new grad_input torch.Size([1, 3, 1650, 1466])
grad_out size torch.Size([1, 3, 1648, 1464])
crop [0, 184, 184, 184] [0, 1463, 3656, 5303] [0, 1279, 3840, 5119]
## 184 1464 0 1280
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  1.3496e-10,  1.5889e-09, -7.4089e-10,
        -1.0776e-09, -2.6982e-10,  1.5501e-10,  1.7419e-10,  8.6903e-11],
       device='cuda:0')
I am [3, 0]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 3, 1652, 1468])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1650, 1466])
final torch.Size([1, 3, 1652, 1468])
padding info :: [2, 0, 0, 0]
reshape_for_final ## 186 1466 0 1280
after crop g_in torch.Size([1, 3, 1280, 1280])
grad_out size torch.Size([1, 3, 1650, 1465])
crop [0, 185, 185, 185] [0, 1464, 3655, 5304] [0, 1279, 3840, 5119]
## 185 1465 0 1280
##############return grad_in in conv2d tensor([-5.1450e-08,  1.3455e-07, -1.2930e-07, -5.1327e-07, -3.8287e-08,
        -1.0550e-07, -1.0709e-06, -2.9089e-07,  6.7245e-07,  5.8441e-07],
       device='cuda:0')
[8, 8]
HREREERE torch.Size([1, 3, 10240, 10240])

############# Enter checkpointing bkward ####
inputs len 5 4
[torch/csrc/autograd/engine.cpp] call_function cCheckpointBackward
[torch/csrc/autograd/python_engine.cpp] THPEngine_run_backward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
in customized Sequential 5
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 7]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1466])
myctx [[ 140046822727744[ PI( <2,7,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 7]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1464])
myctx [[ 140045015171856[ PI( <2,7,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 7]
shape input_tile_for_next
 torch.Size([1, 3, 822, 732])
myctx [[ 140045013800704[ PI( <2,7,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 7]
shape input_tile_for_next
 torch.Size([1, 3, 820, 730])
myctx [[ 140045013848944[ PI( <2,7,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 7]
shape input_tile_for_next
 torch.Size([1, 3, 408, 366])
myctx [[ 140045013206640[ PI( <2,7,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 7]
shape input_tile_for_next
 torch.Size([1, 3, 406, 364])
myctx [[ 140045013206784[ PI( <2,7,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 7]
shape input_tile_for_next
 torch.Size([1, 3, 404, 362])
myctx [[ 140045013206880[ PI( <2,7,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 7]
shape input_tile_for_next
 torch.Size([1, 3, 200, 182])
myctx [[ 140045013206352[ PI( <2,7,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 7]
shape input_tile_for_next
 torch.Size([1, 3, 198, 180])
myctx [[ 140045013206304[ PI( <2,7,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 7]
shape input_tile_for_next
 torch.Size([1, 3, 196, 178])
myctx [[ 140045013207072[ PI( <2,7,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 7]
shape input_tile_for_next
 torch.Size([1, 3, 96, 90])
myctx [[ 140045013206400[ PI( <2,7,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 7]
shape input_tile_for_next
 torch.Size([1, 3, 94, 88])
myctx [[ 140045013206448[ PI( <2,7,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 7]
shape input_tile_for_next
 torch.Size([1, 3, 92, 86])
myctx [[ 140045013206976[ PI( <2,7,>,

in customized Sequential 3
need to get existing

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 92, 86])
grad_out size torch.Size([1, 3, 320, 320])
arg size torch.Size([1, 3, 46, 43])
new_grad_out torch.Size([1, 3, 46, 43])
##############grad_in in maxp torch.Size([1, 3, 92, 86])
I am [2, 7]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 94, 88])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 92, 86])
grad_input torch.Size([1, 3, 94, 88])
padding info :: [0, 1, 0, 0]
grad_input old torch.Size([1, 3, 94, 88])
grad_input new torch.Size([1, 3, 94, 88])
new grad_input torch.Size([1, 3, 94, 88])
grad_out size torch.Size([1, 3, 92, 86])
crop [6, 0, 6, 6] [554, 639, 154, 245] [560, 639, 160, 239]
## 6 86 6 86
##############return grad_in in conv2d tensor([ 2.5262e-03, -1.1962e-03, -1.6162e-03, -2.0221e-05,  3.1840e-04,
         0.0000e+00,  0.0000e+00,  4.6048e-04,  1.2370e-03, -7.1499e-04],
       device='cuda:0')
I am [2, 7]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 96, 90])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 94, 88])
padding info :: [0, 2, 0, 0]
grad_input old torch.Size([1, 3, 96, 90])
grad_input new torch.Size([1, 3, 96, 90])
grad_input torch.Size([1, 3, 96, 90])
grad_out size torch.Size([1, 3, 94, 87])
crop [7, 0, 7, 7] [553, 639, 153, 246] [560, 639, 160, 239]
## 7 87 7 87
##############return grad_in in conv2d tensor([-2.8505e-04,  8.9016e-05, -1.2068e-03,  4.5953e-04, -1.0075e-04,
        -7.2448e-05,  1.0307e-04,  2.6277e-04, -3.4979e-04, -8.9825e-05],
       device='cuda:0')
I am [2, 7]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 98, 92])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 96, 90])
padding info :: [0, 3, 0, 0]
grad_input torch.Size([1, 3, 98, 89])
grad_out size torch.Size([1, 3, 96, 88])
crop [8, 0, 8, 8] [552, 639, 152, 247] [560, 639, 160, 239]
## 8 88 8 88
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cCheckpointBackward
##############return grad_in in conv2d tensor([-8.9547e-05,  1.5393e-04, -1.4059e-04, -9.7972e-05,  2.8289e-04,
        -8.5505e-05, -2.1825e-06,  6.2460e-05, -9.8032e-05, -8.8208e-05],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 196, 178])
grad_out size torch.Size([1, 3, 98, 89])
arg size torch.Size([1, 3, 98, 89])
##############grad_in in maxp torch.Size([1, 3, 196, 178])
I am [2, 7]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 198, 180])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 196, 178])
grad_input torch.Size([1, 3, 198, 180])
padding info :: [0, 1, 0, 0]
grad_input old torch.Size([1, 3, 198, 180])
grad_input new torch.Size([1, 3, 198, 180])
new grad_input torch.Size([1, 3, 198, 180])
grad_out size torch.Size([1, 3, 196, 178])
crop [18, 0, 18, 18] [1102, 1279, 302, 497] [1120, 1279, 320, 479]
## 18 178 18 178
##############return grad_in in conv2d tensor([ 0.0000e+00, -5.3759e-06,  2.3256e-05, -1.3657e-05, -4.7592e-06,
         0.0000e+00, -2.4745e-05,  4.9958e-05,  1.1034e-06, -5.0819e-05],
       device='cuda:0')
I am [2, 7]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 200, 182])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 198, 180])
padding info :: [0, 2, 0, 0]
grad_input old torch.Size([1, 3, 200, 182])
grad_input new torch.Size([1, 3, 200, 182])
grad_input torch.Size([1, 3, 200, 182])
grad_out size torch.Size([1, 3, 198, 179])
crop [19, 0, 19, 19] [1101, 1279, 301, 498] [1120, 1279, 320, 479]
## 19 179 19 179
##############return grad_in in conv2d tensor([ 0.0000e+00, -1.4570e-06,  5.4820e-06, -6.5196e-06,  1.3524e-05,
        -1.0591e-05, -4.7962e-06,  1.2679e-05, -1.6134e-05,  5.2354e-06],
       device='cuda:0')
I am [2, 7]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 202, 184])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 200, 182])
padding info :: [0, 3, 0, 0]
grad_input torch.Size([1, 3, 202, 181])
grad_out size torch.Size([1, 3, 200, 180])
crop [20, 0, 20, 20] [1100, 1279, 300, 499] [1120, 1279, 320, 479]
## 20 180 20 180
##############return grad_in in conv2d tensor([ 0.0000e+00,  1.1261e-07,  1.3442e-07, -1.1084e-06, -1.1168e-06,
         1.9993e-06,  5.1916e-07,  7.0761e-07, -1.5067e-06, -2.7835e-06],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 404, 362])
grad_out size torch.Size([1, 3, 202, 181])
arg size torch.Size([1, 3, 202, 181])
##############grad_in in maxp torch.Size([1, 3, 404, 362])
I am [2, 7]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 406, 364])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 404, 362])
grad_input torch.Size([1, 3, 406, 364])
padding info :: [0, 1, 0, 0]
grad_input old torch.Size([1, 3, 406, 364])
grad_input new torch.Size([1, 3, 406, 364])
new grad_input torch.Size([1, 3, 406, 364])
grad_out size torch.Size([1, 3, 404, 362])
crop [42, 0, 42, 42] [2198, 2559, 598, 1001] [2240, 2559, 640, 959]
## 42 362 42 362
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  2.9272e-08, -3.0550e-08, -1.3364e-07,
         1.7450e-07, -3.0431e-08, -8.7766e-08,  1.1890e-07, -4.0715e-09],
       device='cuda:0')
I am [2, 7]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 408, 366])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 406, 364])
padding info :: [0, 2, 0, 0]
grad_input old torch.Size([1, 3, 408, 366])
grad_input new torch.Size([1, 3, 408, 366])
grad_input torch.Size([1, 3, 408, 366])
grad_out size torch.Size([1, 3, 406, 363])
crop [43, 0, 43, 43] [2197, 2559, 597, 1002] [2240, 2559, 640, 959]
## 43 363 43 363
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00, -3.8715e-09, -1.2509e-08,  8.3190e-09,
         3.4272e-08,  2.7673e-08, -3.9432e-08, -2.4839e-08,  2.2171e-08],
       device='cuda:0')
I am [2, 7]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 410, 368])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 408, 366])
padding info :: [0, 3, 0, 0]
grad_input torch.Size([1, 3, 410, 365])
grad_out size torch.Size([1, 3, 408, 364])
crop [44, 0, 44, 44] [2196, 2559, 596, 1003] [2240, 2559, 640, 959]
## 44 364 44 364
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00, -1.4553e-10,  3.0171e-09,  5.8302e-10,
        -1.7753e-09, -5.1877e-09, -7.3215e-09, -5.5424e-09,  6.5818e-10],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 820, 730])
grad_out size torch.Size([1, 3, 410, 365])
arg size torch.Size([1, 3, 410, 365])
##############grad_in in maxp torch.Size([1, 3, 820, 730])
I am [2, 7]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 822, 732])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 820, 730])
grad_input torch.Size([1, 3, 822, 732])
padding info :: [0, 1, 0, 0]
grad_input old torch.Size([1, 3, 822, 732])
grad_input new torch.Size([1, 3, 822, 732])
new grad_input torch.Size([1, 3, 822, 732])
grad_out size torch.Size([1, 3, 820, 730])
crop [90, 0, 90, 90] [4390, 5119, 1190, 2009] [4480, 5119, 1280, 1919]
## 90 730 90 730
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.5372e-10,
        -9.3677e-11, -6.2191e-10, -2.3723e-10,  4.1848e-10,  2.5936e-10],
       device='cuda:0')
I am [2, 7]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 824, 734])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 822, 732])
padding info :: [0, 2, 0, 0]
grad_input torch.Size([1, 3, 824, 732])
grad_out size torch.Size([1, 3, 822, 731])
crop [91, 0, 91, 91] [4389, 5119, 1189, 2010] [4480, 5119, 1280, 1919]
## 91 731 91 731
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -2.4124e-11,
        -6.6640e-11, -1.2141e-10, -6.0230e-11,  1.2918e-10,  1.9728e-10],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 1648, 1464])
grad_out size torch.Size([1, 3, 824, 732])
arg size torch.Size([1, 3, 824, 732])
##############grad_in in maxp torch.Size([1, 3, 1648, 1464])
I am [2, 7]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 1650, 1466])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1648, 1464])
grad_input torch.Size([1, 3, 1650, 1466])
padding info :: [0, 1, 0, 0]
grad_input old torch.Size([1, 3, 1650, 1466])
grad_input new torch.Size([1, 3, 1650, 1466])
new grad_input torch.Size([1, 3, 1650, 1466])
grad_out size torch.Size([1, 3, 1648, 1464])
crop [184, 0, 184, 184] [8776, 10239, 2376, 4023] [8960, 10239, 2560, 3839]
## 184 1464 184 1464
##############return grad_in in conv2d tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.5844e-13], device='cuda:0')
I am [2, 7]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 3, 1652, 1468])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1650, 1466])
final torch.Size([1, 3, 1652, 1468])
padding info :: [0, 2, 0, 0]
reshape_for_final ## 186 1466 186 1466
after crop g_in torch.Size([1, 3, 1280, 1280])
grad_out size torch.Size([1, 3, 1650, 1465])
crop [185, 0, 185, 185] [8775, 10239, 2375, 4024] [8960, 10239, 2560, 3839]
## 185 1465 185 1465
##############return grad_in in conv2d tensor([-3.4150e-07, -7.2749e-07,  1.4352e-08,  4.2481e-07,  5.8023e-07,
         4.5265e-07,  6.7058e-07, -2.8652e-07, -2.6096e-07, -2.5904e-07],
       device='cuda:0')
[8, 8]
HREREERE torch.Size([1, 3, 10240, 10240])

############# Enter checkpointing bkward ####
inputs len 5 4
in customized Sequential 5
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 6]
[torch/csrc/autograd/python_engine.cpp] THPEngine_run_backward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <2,6,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 6]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <2,6,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 6]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <2,6,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 6]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <2,6,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 6]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <2,6,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 6]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <2,6,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 6]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <2,6,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 6]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <2,6,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 6]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <2,6,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 6]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <2,6,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 6]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <2,6,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 6]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <2,6,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 6]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <2,6,>,

in customized Sequential 3
need to get existing

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 92, 92])
grad_out size torch.Size([1, 3, 320, 320])
arg size torch.Size([1, 3, 46, 46])
new_grad_out torch.Size([1, 3, 46, 46])
##############grad_in in maxp torch.Size([1, 3, 92, 92])
I am [2, 6]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 94, 94])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 92, 92])
grad_input torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 94, 94])
grad_input new torch.Size([1, 3, 94, 94])
new grad_input torch.Size([1, 3, 94, 94])
grad_out size torch.Size([1, 3, 92, 92])
crop [6, 6, 6, 6] [474, 565, 154, 245] [480, 559, 160, 239]
## 6 86 6 86
##############return grad_in in conv2d tensor([ 0.0000e+00,  2.1889e-03,  9.5272e-04,  3.8413e-03, -7.2252e-03,
         2.5845e-03,  2.2605e-05,  0.0000e+00,  0.0000e+00, -4.6414e-03],
       device='cuda:0')
I am [2, 6]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 96, 96])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 96, 96])
grad_input new torch.Size([1, 3, 96, 96])
grad_input torch.Size([1, 3, 96, 96])
grad_out size torch.Size([1, 3, 94, 94])
crop [7, 7, 7, 7] [473, 566, 153, 246] [480, 559, 160, 239]
## 7 87 7 87
##############return grad_in in conv2d tensor([ 0.0000e+00,  7.7681e-05, -2.1902e-05, -4.3671e-04,  2.8927e-04,
         3.4280e-04,  1.0010e-03, -1.0884e-03,  1.6800e-04,  3.6772e-04],
       device='cuda:0')
I am [2, 6]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 98, 98])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 96, 96])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 98, 98])
grad_out size torch.Size([1, 3, 96, 96])
crop [8, 8, 8, 8] [472, 567, 152, 247] [480, 559, 160, 239]
## 8 88 8 88
##############return grad_in in conv2d tensor([ 0.0000e+00, -1.6956e-05,  6.3390e-05, -2.6547e-04,  1.6328e-04,
        -4.0085e-05,  4.1778e-04, -1.4435e-04, -3.2786e-04,  3.2180e-04],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 196, 196])
grad_out size torch.Size([1, 3, 98, 98])
arg size torch.Size([1, 3, 98, 98])
##############grad_in in maxp [torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cCheckpointBackward
torch.Size([1, 3, 196, 196])
I am [2, 6]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 198, 198])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 196, 196])
grad_input torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 198, 198])
grad_input new torch.Size([1, 3, 198, 198])
new grad_input torch.Size([1, 3, 198, 198])
grad_out size torch.Size([1, 3, 196, 196])
crop [18, 18, 18, 18] [942, 1137, 302, 497] [960, 1119, 320, 479]
## 18 178 18 178
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  9.0649e-06,
        -2.6953e-06,  8.7787e-06, -1.8620e-05,  2.7809e-05,  2.2116e-05],
       device='cuda:0')
I am [2, 6]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 200, 200])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 200, 200])
grad_input new torch.Size([1, 3, 200, 200])
grad_input torch.Size([1, 3, 200, 200])
grad_out size torch.Size([1, 3, 198, 198])
crop [19, 19, 19, 19] [941, 1138, 301, 498] [960, 1119, 320, 479]
## 19 179 19 179
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  2.6198e-06,
         1.8799e-07,  6.4940e-06, -6.7979e-06,  8.2551e-06, -5.9991e-06],
       device='cuda:0')
I am [2, 6]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 202, 202])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 200, 200])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 202, 202])
grad_out size torch.Size([1, 3, 200, 200])
crop [20, 20, 20, 20] [940, 1139, 300, 499] [960, 1119, 320, 479]
## 20 180 20 180
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -2.0550e-07,
        -7.1381e-07, -4.6514e-07,  1.9117e-07,  1.2405e-06, -6.2898e-07],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 404, 404])
grad_out size torch.Size([1, 3, 202, 202])
arg size torch.Size([1, 3, 202, 202])
##############grad_in in maxp torch.Size([1, 3, 404, 404])
I am [2, 6]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 406, 406])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 404, 404])
grad_input torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 406, 406])
grad_input new torch.Size([1, 3, 406, 406])
new grad_input torch.Size([1, 3, 406, 406])
grad_out size torch.Size([1, 3, 404, 404])
crop [42, 42, 42, 42] [1878, 2281, 598, 1001] [1920, 2239, 640, 959]
## 42 362 42 362
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -9.1556e-08],
       device='cuda:0')
I am [2, 6]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 408, 408])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 408, 408])
grad_input new torch.Size([1, 3, 408, 408])
grad_input torch.Size([1, 3, 408, 408])
grad_out size torch.Size([1, 3, 406, 406])
crop [43, 43, 43, 43] [1877, 2282, 597, 1002] [1920, 2239, 640, 959]
## 43 363 43 363
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -2.7635e-09],
       device='cuda:0')
I am [2, 6]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 410, 410])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 408, 408])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 410, 410])
grad_out size torch.Size([1, 3, 408, 408])
crop [44, 44, 44, 44] [1876, 2283, 596, 1003] [1920, 2239, 640, 959]
## 44 364 44 364
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -8.1712e-10],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 820, 820])
grad_out size torch.Size([1, 3, 410, 410])
arg size torch.Size([1, 3, 410, 410])
##############grad_in in maxp torch.Size([1, 3, 820, 820])
I am [2, 6]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 822, 822])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 820, 820])
grad_input torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 822, 822])
grad_input new torch.Size([1, 3, 822, 822])
new grad_input torch.Size([1, 3, 822, 822])
grad_out size torch.Size([1, 3, 820, 820])
crop [90, 90, 90, 90] [3750, 4569, 1190, 2009] [3840, 4479, 1280, 1919]
## 90 730 90 730
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [2, 6]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 824, 824])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 824, 824])
grad_out size torch.Size([1, 3, 822, 822])
crop [91, 91, 91, 91] [3749, 4570, 1189, 2010] [3840, 4479, 1280, 1919]
## 91 731 91 731
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 1648, 1648])
grad_out size torch.Size([1, 3, 824, 824])
arg size torch.Size([1, 3, 824, 824])
##############grad_in in maxp torch.Size([1, 3, 1648, 1648])
I am [2, 6]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 1650, 1650])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1648, 1648])
grad_input torch.Size([1, 3, 1650, 1650])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 1650, 1650])
grad_input new torch.Size([1, 3, 1650, 1650])
new grad_input torch.Size([1, 3, 1650, 1650])
grad_out size torch.Size([1, 3, 1648, 1648])
crop [184, 184, 184, 184] [7496, 9143, 2376, 4023] [7680, 8959, 2560, 3839]
## 184 1464 184 1464
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [2, 6]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 3, 1652, 1652])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1650, 1650])
final torch.Size([1, 3, 1652, 1652])
padding info :: [0, 0, 0, 0]
reshape_for_final ## 186 1466 186 1466
after crop g_in torch.Size([1, 3, 1280, 1280])
grad_out size torch.Size([1, 3, 1650, 1650])
crop [185, 185, 185, 185] [7495, 9144, 2375, 4024] [7680, 8959, 2560, 3839]
## 185 1465 185 1465
##############return grad_in in conv2d tensor([-8.7476e-07, -2.2295e-07, -4.9023e-07, -2.9035e-07,  8.2258e-07,
        -4.5776e-07, -4.6346e-07,  3.3570e-07,  1.2855e-06,  2.0108e-07],
       device='cuda:0')
[8, 8]
HREREERE torch.Size([1, 3, 10240, 10240])

############# Enter checkpointing bkward ####
inputs len 5 4
in customized Sequential 5
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 5]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <2,5,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 5]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <2,5,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
[torch/csrc/autograd/python_engine.cpp] THPEngine_run_backward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 5]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <2,5,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 5]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <2,5,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 5]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <2,5,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 5]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <2,5,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 5]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <2,5,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 5]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <2,5,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 5]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <2,5,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 5]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <2,5,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 5]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <2,5,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 5]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <2,5,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 5]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <2,5,>,

in customized Sequential 3
need to get existing

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 92, 92])
grad_out size torch.Size([1, 3, 320, 320])
arg size torch.Size([1, 3, 46, 46])
new_grad_out torch.Size([1, 3, 46, 46])
##############grad_in in maxp torch.Size([1, 3, 92, 92])
I am [2, 5]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 94, 94])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 92, 92])
grad_input torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 94, 94])
grad_input new torch.Size([1, 3, 94, 94])
new grad_input torch.Size([1, 3, 94, 94])
grad_out size torch.Size([1, 3, 92, 92])
crop [6, 6, 6, 6] [394, 485, 154, 245] [400, 479, 160, 239]
## 6 86 6 86
##############return grad_in in conv2d tensor([-0.0003,  0.0010, -0.0003, -0.0002,  0.0000,  0.0006,  0.0015,  0.0006,
        -0.0003, -0.0015], device='cuda:0')
I am [2, 5]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 96, 96])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 96, 96])
grad_input new torch.Size([1, 3, 96, 96])
grad_input torch.Size([1, 3, 96, 96])
grad_out size torch.Size([1, 3, 94, 94])
crop [7, 7, 7, 7] [393, 486, 153, 246] [400, 479, 160, 239]
## 7 87 7 87
##############return grad_in in conv2d tensor([-5.0852e-05, -9.4596e-05, -5.1505e-05, -3.9579e-04,  1.1487e-04,
         2.3791e-04, -4.3802e-04,  1.3409e-05,  8.2187e-05, -1.1355e-03],
       device='cuda:0')
I am [2, 5]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 98, 98])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 96, 96])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 98, 98])
grad_out size torch.Size([1, 3, 96, 96])
crop [8, 8, 8, 8] [392, 487, 152, 247] [400, 479, 160, 239]
## 8 88 8 88
##############return grad_in in conv2d tensor([ 1.2270e-06, -3.5191e-05,  5.1526e-05, -6.2475e-05, -1.7710e-05,
         1.3115e-04, -1.2420e-04, -9.4440e-05,  2.0099e-04, -9.7371e-05],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 196, 196])
grad_out size torch.Size([1, 3, 98, 98])
arg size torch.Size([1, 3, 98, 98])
##############grad_in in maxp torch.Size([1, 3, 196, 196])
I am [2, 5]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 198, 198])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 196, 196])
grad_input torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 198, 198])
grad_input new torch.Size([1, 3, 198, 198])
new grad_input torch.Size([1, 3, 198, 198])
grad_out size torch.Size([1, 3, 196, 196])
crop [18, 18, 18, 18] [782, 977, 302, 497] [800, 959, 320, 479]
## 18 178 18 178
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00, -7.1808e-06,  3.8838e-06,  1.1456e-06,
         0.0000e+00, -8.5988e-06, -7.9989e-06,  9.2163e-06,  6.8799e-06],
       device='cuda:0')[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cCheckpointBackward

I am [2, 5]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 200, 200])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 200, 200])
grad_input new torch.Size([1, 3, 200, 200])
grad_input torch.Size([1, 3, 200, 200])
grad_out size torch.Size([1, 3, 198, 198])
crop [19, 19, 19, 19] [781, 978, 301, 498] [800, 959, 320, 479]
## 19 179 19 179
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00, -2.1032e-06,  5.2354e-07, -2.6657e-06,
         1.6147e-06, -3.4836e-06, -3.2689e-06,  8.3784e-07,  7.0534e-07],
       device='cuda:0')
I am [2, 5]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 202, 202])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 200, 200])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 202, 202])
grad_out size torch.Size([1, 3, 200, 200])
crop [20, 20, 20, 20] [780, 979, 300, 499] [800, 959, 320, 479]
## 20 180 20 180
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  1.6546e-07,  5.0958e-07,  4.4733e-09,
        -3.8574e-07,  5.0664e-07,  7.1854e-07,  5.0883e-07, -5.7526e-07],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 404, 404])
grad_out size torch.Size([1, 3, 202, 202])
arg size torch.Size([1, 3, 202, 202])
##############grad_in in maxp torch.Size([1, 3, 404, 404])
I am [2, 5]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 406, 406])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 404, 404])
grad_input torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 406, 406])
grad_input new torch.Size([1, 3, 406, 406])
new grad_input torch.Size([1, 3, 406, 406])
grad_out size torch.Size([1, 3, 404, 404])
crop [42, 42, 42, 42] [1558, 1961, 598, 1001] [1600, 1919, 640, 959]
## 42 362 42 362
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  4.1090e-08,
        -3.4104e-08, -9.1152e-08,  7.3263e-08, -3.6384e-08, -4.6111e-09],
       device='cuda:0')
I am [2, 5]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 408, 408])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 408, 408])
grad_input new torch.Size([1, 3, 408, 408])
grad_input torch.Size([1, 3, 408, 408])
grad_out size torch.Size([1, 3, 406, 406])
crop [43, 43, 43, 43] [1557, 1962, 597, 1002] [1600, 1919, 640, 959]
## 43 363 43 363
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -5.4345e-09,
        -1.4070e-08,  1.1635e-08, -1.1508e-08,  1.6417e-09, -5.9364e-09],
       device='cuda:0')
I am [2, 5]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 410, 410])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 408, 408])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 410, 410])
grad_out size torch.Size([1, 3, 408, 408])
crop [44, 44, 44, 44] [1556, 1963, 596, 1003] [1600, 1919, 640, 959]
## 44 364 44 364
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -2.0429e-10,
         4.6095e-09,  3.8133e-10,  4.1572e-09,  2.6817e-09,  2.3047e-09],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 820, 820])
grad_out size torch.Size([1, 3, 410, 410])
arg size torch.Size([1, 3, 410, 410])
##############grad_in in maxp torch.Size([1, 3, 820, 820])
I am [2, 5]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 822, 822])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 820, 820])
grad_input torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 822, 822])
grad_input new torch.Size([1, 3, 822, 822])
new grad_input torch.Size([1, 3, 822, 822])
grad_out size torch.Size([1, 3, 820, 820])
crop [90, 90, 90, 90] [3110, 3929, 1190, 2009] [3200, 3839, 1280, 1919]
## 90 730 90 730
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00, -3.8260e-11, -2.9782e-11],
       device='cuda:0')
I am [2, 5]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 824, 824])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 824, 824])
grad_out size torch.Size([1, 3, 822, 822])
crop [91, 91, 91, 91] [3109, 3930, 1189, 2010] [3200, 3839, 1280, 1919]
## 91 731 91 731
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  2.3192e-13, -1.0004e-12],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 1648, 1648])
grad_out size torch.Size([1, 3, 824, 824])
arg size torch.Size([1, 3, 824, 824])
##############grad_in in maxp torch.Size([1, 3, 1648, 1648])
I am [2, 5]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 1650, 1650])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1648, 1648])
grad_input torch.Size([1, 3, 1650, 1650])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 1650, 1650])
grad_input new torch.Size([1, 3, 1650, 1650])
new grad_input torch.Size([1, 3, 1650, 1650])
grad_out size torch.Size([1, 3, 1648, 1648])
crop [184, 184, 184, 184] [6216, 7863, 2376, 4023] [6400, 7679, 2560, 3839]
## 184 1464 184 1464
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [2, 5]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 3, 1652, 1652])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1650, 1650])
final torch.Size([1, 3, 1652, 1652])
padding info :: [0, 0, 0, 0]
reshape_for_final ## 186 1466 186 1466
after crop g_in torch.Size([1, 3, 1280, 1280])
grad_out size torch.Size([1, 3, 1650, 1650])
crop [185, 185, 185, 185] [6215, 7864, 2375, 4024] [6400, 7679, 2560, 3839]
## 185 1465 185 1465
##############return grad_in in conv2d tensor([ 1.8275e-08,  2.0765e-07, -5.1632e-08,  9.8574e-08,  1.0993e-07,
        -1.6177e-07, -2.3818e-08,  2.8858e-07, -3.2131e-07, -5.4317e-07],
       device='cuda:0')
[8, 8]
HREREERE torch.Size([1, 3, 10240, 10240])

############# Enter checkpointing bkward ####
inputs len 5 4
in customized Sequential 5
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 4]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <2,4,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 4]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <2,4,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 4]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <2,4,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() [torch/csrc/autograd/python_engine.cpp] THPEngine_run_backward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 4]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <2,4,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 4]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <2,4,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 4]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <2,4,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 4]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <2,4,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 4]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <2,4,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 4]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <2,4,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 4]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <2,4,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 4]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <2,4,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 4]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <2,4,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 4]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <2,4,>,

in customized Sequential 3
need to get existing

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 92, 92])
grad_out size torch.Size([1, 3, 320, 320])
arg size torch.Size([1, 3, 46, 46])
new_grad_out torch.Size([1, 3, 46, 46])
##############grad_in in maxp torch.Size([1, 3, 92, 92])
I am [2, 4]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 94, 94])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 92, 92])
grad_input torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 94, 94])
grad_input new torch.Size([1, 3, 94, 94])
new grad_input torch.Size([1, 3, 94, 94])
grad_out size torch.Size([1, 3, 92, 92])
crop [6, 6, 6, 6] [314, 405, 154, 245] [320, 399, 160, 239]
## 6 86 6 86
##############return grad_in in conv2d tensor([-8.5741e-05,  4.0598e-05, -1.1483e-05, -1.3976e-04, -2.0371e-03,
         3.0519e-03,  4.7104e-04,  7.3519e-03, -1.9418e-03, -1.3953e-03],
       device='cuda:0')
I am [2, 4]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 96, 96])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 96, 96])
grad_input new torch.Size([1, 3, 96, 96])
grad_input torch.Size([1, 3, 96, 96])
grad_out size torch.Size([1, 3, 94, 94])
crop [7, 7, 7, 7] [313, 406, 153, 246] [320, 399, 160, 239]
## 7 87 7 87
##############return grad_in in conv2d tensor([ 9.6747e-06, -3.0212e-06,  9.3745e-05, -3.2433e-06, -2.3805e-04,
        -1.8280e-04, -1.0374e-03, -2.0553e-03, -2.2457e-03, -1.6814e-03],
       device='cuda:0')
I am [2, 4]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 98, 98])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 96, 96])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 98, 98])
grad_out size torch.Size([1, 3, 96, 96])
crop [8, 8, 8, 8] [312, 407, 152, 247] [320, 399, 160, 239]
## 8 88 8 88
##############return grad_in in conv2d tensor([ 3.0393e-06, -5.2246e-06,  1.1576e-05, -9.5863e-07,  4.4406e-06,
        -8.6068e-05,  1.0341e-04, -4.2991e-04, -1.0409e-04, -1.3006e-04],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 196, 196])
grad_out size torch.Size([1, 3, 98, 98])
arg size torch.Size([1, 3, 98, 98])
##############grad_in in maxp torch.Size([1, 3, 196, 196])
I am [2, 4]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 198, 198])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 196, 196])
grad_input torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 198, 198])
grad_input new torch.Size([1, 3, 198, 198])
new grad_input torch.Size([1, 3, 198, 198])
grad_out size torch.Size([1, 3, 196, 196])
crop [18, 18, 18, 18] [622, 817, 302, 497] [640, 799, 320, 479]
## 18 178 18 178
##############return grad_in in conv2d tensor([ 4.9387e-07, -3.1313e-07, -3.9014e-07,  3.9572e-07,  8.5958e-07,
        -5.5755e-07, -6.7742e-07,  1.4545e-06,  1.4981e-05, -7.7477e-06],
       device='cuda:0')
I am [2, 4]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 200, 200])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 200, 200])
grad_input new torch.Size([1, 3, 200, 200])
grad_input torch.Size([1, 3, 200, 200])
grad_out size torch.Size([1, 3, 198, 198])
crop [19, 19, 19, 19] [621, 818, 301, 498] [torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cCheckpointBackward
[640, 799, 320, 479]
## 19 179 19 179
##############return grad_in in conv2d tensor([ 1.4396e-07, -3.4630e-08,  1.1909e-07, -9.7587e-08,  4.1318e-08,
         2.9119e-07,  2.0379e-07, -1.1609e-07,  4.4285e-06, -4.3874e-06],
       device='cuda:0')
I am [2, 4]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 202, 202])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 200, 200])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 202, 202])
grad_out size torch.Size([1, 3, 200, 200])
crop [20, 20, 20, 20] [620, 819, 300, 499] [640, 799, 320, 479]
## 20 180 20 180
##############return grad_in in conv2d tensor([-1.1703e-08, -3.6620e-08,  7.0170e-09,  6.0676e-08, -2.9477e-08,
        -1.2760e-07, -8.7501e-09,  1.2812e-07, -2.8761e-07, -6.5896e-07],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 404, 404])
grad_out size torch.Size([1, 3, 202, 202])
arg size torch.Size([1, 3, 202, 202])
##############grad_in in maxp torch.Size([1, 3, 404, 404])
I am [2, 4]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 406, 406])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 404, 404])
grad_input torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 406, 406])
grad_input new torch.Size([1, 3, 406, 406])
new grad_input torch.Size([1, 3, 406, 406])
grad_out size torch.Size([1, 3, 404, 404])
crop [42, 42, 42, 42] [1238, 1641, 598, 1001] [1280, 1599, 640, 959]
## 42 362 42 362
##############return grad_in in conv2d tensor([-6.0232e-10, -7.3330e-10, -1.2253e-09,  5.6032e-09,  7.7886e-10,
         1.6094e-09,  6.8740e-09,  5.1411e-09, -3.6037e-09,  0.0000e+00],
       device='cuda:0')
I am [2, 4]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 408, 408])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 408, 408])
grad_input new torch.Size([1, 3, 408, 408])
grad_input torch.Size([1, 3, 408, 408])
grad_out size torch.Size([1, 3, 406, 406])
crop [43, 43, 43, 43] [1237, 1642, 597, 1002] [1280, 1599, 640, 959]
## 43 363 43 363
##############return grad_in in conv2d tensor([-2.3933e-10,  3.8864e-10,  2.6623e-09, -8.2990e-10, -1.4397e-09,
        -4.2132e-10,  3.4450e-09,  3.8704e-09,  9.6443e-10,  2.2171e-09],
       device='cuda:0')
I am [2, 4]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 410, 410])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 408, 408])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 410, 410])
grad_out size torch.Size([1, 3, 408, 408])
crop [44, 44, 44, 44] [1236, 1643, 596, 1003] [1280, 1599, 640, 959]
## 44 364 44 364
##############return grad_in in conv2d tensor([-2.5685e-11,  1.0537e-10, -4.0191e-10, -3.0142e-10, -3.6422e-10,
        -2.2554e-11,  1.4019e-10, -5.4024e-10, -1.1228e-09, -1.6401e-09],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 820, 820])
grad_out size torch.Size([1, 3, 410, 410])
arg size torch.Size([1, 3, 410, 410])
##############grad_in in maxp torch.Size([1, 3, 820, 820])
I am [2, 4]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 822, 822])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 820, 820])
grad_input torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 822, 822])
grad_input new torch.Size([1, 3, 822, 822])
new grad_input torch.Size([1, 3, 822, 822])
grad_out size torch.Size([1, 3, 820, 820])
crop [90, 90, 90, 90] [2470, 3289, 1190, 2009] [2560, 3199, 1280, 1919]
## 90 730 90 730
##############return grad_in in conv2d tensor([-1.9756e-11, -1.1126e-11,  3.5406e-11,  3.0568e-11, -7.3944e-11,
         3.9728e-11,  1.0997e-10, -1.0636e-10, -7.4299e-11, -1.1605e-10],
       device='cuda:0')
I am [2, 4]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 824, 824])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 824, 824])
grad_out size torch.Size([1, 3, 822, 822])
crop [91, 91, 91, 91] [2469, 3290, 1189, 2010] [2560, 3199, 1280, 1919]
## 91 731 91 731
##############return grad_in in conv2d tensor([-2.9487e-12, -5.5370e-12,  1.4768e-12,  8.1526e-12,  8.3361e-12,
         1.2816e-11, -9.7145e-12, -3.1541e-11, -2.3687e-11, -1.6570e-11],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 1648, 1648])
grad_out size torch.Size([1, 3, 824, 824])
arg size torch.Size([1, 3, 824, 824])
##############grad_in in maxp torch.Size([1, 3, 1648, 1648])
I am [2, 4]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 1650, 1650])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1648, 1648])
grad_input torch.Size([1, 3, 1650, 1650])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 1650, 1650])
grad_input new torch.Size([1, 3, 1650, 1650])
new grad_input torch.Size([1, 3, 1650, 1650])
grad_out size torch.Size([1, 3, 1648, 1648])
crop [184, 184, 184, 184] [4936, 6583, 2376, 4023] [5120, 6399, 2560, 3839]
## 184 1464 184 1464
##############return grad_in in conv2d tensor([ 1.4776e-13,  7.7571e-13, -1.8264e-13, -1.9971e-14, -9.1902e-14,
        -9.7143e-13,  8.7095e-13,  8.4217e-14,  2.0899e-13,  4.8435e-12],
       device='cuda:0')
I am [2, 4]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 3, 1652, 1652])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1650, 1650])
final torch.Size([1, 3, 1652, 1652])
padding info :: [0, 0, 0, 0]
reshape_for_final ## 186 1466 186 1466
after crop g_in torch.Size([1, 3, 1280, 1280])
grad_out size torch.Size([1, 3, 1650, 1650])
crop [185, 185, 185, 185] [4935, 6584, 2375, 4024] [5120, 6399, 2560, 3839]
## 185 1465 185 1465
##############return grad_in in conv2d tensor([-3.2405e-07,  1.5691e-07,  3.2385e-07,  1.4937e-07, -1.2793e-07,
         5.8249e-07, -2.9316e-07, -1.6049e-07, -3.0332e-07, -2.3672e-07],
       device='cuda:0')
[8, 8]
HREREERE torch.Size([1, 3, 10240, 10240])

############# Enter checkpointing bkward ####
inputs len 5 4
in customized Sequential 5
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 3]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <2,3,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 3]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <2,3,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 3]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <2,3,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 3]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <2,3,>,

[torch/csrc/autograd/python_engine.cpp] THPEngine_run_backward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 3]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <2,3,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 3]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <2,3,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 3]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <2,3,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 3]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <2,3,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 3]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <2,3,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 3]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <2,3,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 3]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <2,3,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 3]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <2,3,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 3]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <2,3,>,

in customized Sequential 3
need to get existing

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 92, 92])
grad_out size torch.Size([1, 3, 320, 320])
arg size torch.Size([1, 3, 46, 46])
new_grad_out torch.Size([1, 3, 46, 46])
##############grad_in in maxp torch.Size([1, 3, 92, 92])
I am [2, 3]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 94, 94])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 92, 92])
grad_input torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 94, 94])
grad_input new torch.Size([1, 3, 94, 94])
new grad_input torch.Size([1, 3, 94, 94])
grad_out size torch.Size([1, 3, 92, 92])
crop [6, 6, 6, 6] [234, 325, 154, 245] [240, 319, 160, 239]
## 6 86 6 86
##############return grad_in in conv2d tensor([ 0.0000, -0.0012, -0.0031,  0.0003,  0.0009,  0.0032, -0.0065, -0.0020,
         0.0006,  0.0014], device='cuda:0')
I am [2, 3]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 96, 96])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 96, 96])
grad_input new torch.Size([1, 3, 96, 96])
grad_input torch.Size([1, 3, 96, 96])
grad_out size torch.Size([1, 3, 94, 94])
crop [7, 7, 7, 7] [233, 326, 153, 246] [240, 319, 160, 239]
## 7 87 7 87
##############return grad_in in conv2d tensor([ 0.0000, -0.0007,  0.0009, -0.0004,  0.0002,  0.0003, -0.0002,  0.0015,
         0.0013,  0.0007], device='cuda:0')
I am [2, 3]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 98, 98])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 96, 96])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 98, 98])
grad_out size torch.Size([1, 3, 96, 96])
crop [8, 8, 8, 8] [232, 327, 152, 247] [240, 319, 160, 239]
## 8 88 8 88
##############return grad_in in conv2d tensor([ 0.0000e+00, -1.0676e-04,  2.0976e-04,  1.4379e-04, -2.4126e-04,
        -4.3149e-05,  7.1192e-05,  5.6156e-07,  4.4962e-04, -3.2534e-05],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 196, 196])
grad_out size torch.Size([1, 3, 98, 98])
arg size torch.Size([1, 3, 98, 98])
##############grad_in in maxp torch.Size([1, 3, 196, 196])
I am [2, 3]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 198, 198])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 196, 196])
grad_input torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 198, 198])
grad_input new torch.Size([1, 3, 198, 198])
new grad_input torch.Size([1, 3, 198, 198])
grad_out size torch.Size([1, 3, 196, 196])
crop [18, 18, 18, 18] [462, 657, 302, 497] [480, 639, 320, 479]
## 18 178 18 178
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00, -9.6304e-06,  3.3893e-06,  7.2182e-06,
         4.2959e-06,  0.0000e+00,  4.2996e-07, -3.5058e-05,  2.1368e-05],
       device='cuda:0')
I am [2, 3]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 200, 200])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 200, 200])
grad_input new torch.Size([1, 3, 200, 200])
grad_input torch.Size([1, 3, 200, 200])
grad_out size torch.Size([1, 3, 198, 198])
crop [19, 19, 19, 19] [461, 658, 301, 498] [480, 639, 320, 479]
## 19 179 19 179
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00, -2.7099e-06, -4.7919e-07, -2.8351e-06,
         5.2164e-06, -2.6514e-07,  7.9497e-07, -8.3763e-06,  8.5435e-06],
       device='cuda:0')
I am [2, 3]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 202, 202])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 200, 200])
padding info :: [0, 0, 0, 0]
grad_input [torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cCheckpointBackward
torch.Size([1, 3, 202, 202])
grad_out size torch.Size([1, 3, 200, 200])
crop [20, 20, 20, 20] [460, 659, 300, 499] [480, 639, 320, 479]
## 20 180 20 180
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  2.1129e-07,  8.3897e-07,  2.7769e-07,
        -1.2784e-06, -4.2146e-07,  1.3448e-07,  1.5222e-07,  1.6494e-06],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 404, 404])
grad_out size torch.Size([1, 3, 202, 202])
arg size torch.Size([1, 3, 202, 202])
##############grad_in in maxp torch.Size([1, 3, 404, 404])
I am [2, 3]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 406, 406])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 404, 404])
grad_input torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 406, 406])
grad_input new torch.Size([1, 3, 406, 406])
new grad_input torch.Size([1, 3, 406, 406])
grad_out size torch.Size([1, 3, 404, 404])
crop [42, 42, 42, 42] [918, 1321, 598, 1001] [960, 1279, 640, 959]
## 42 362 42 362
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  2.9965e-08,
         7.1606e-08, -7.6267e-08,  1.2118e-08, -7.9096e-08, -8.8251e-08],
       device='cuda:0')
I am [2, 3]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 408, 408])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 408, 408])
grad_input new torch.Size([1, 3, 408, 408])
grad_input torch.Size([1, 3, 408, 408])
grad_out size torch.Size([1, 3, 406, 406])
crop [43, 43, 43, 43] [917, 1322, 597, 1002] [960, 1279, 640, 959]
## 43 363 43 363
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  5.3451e-09,
         4.5257e-09, -1.9768e-08,  2.0828e-08, -5.3021e-09, -2.9806e-08],
       device='cuda:0')
I am [2, 3]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 410, 410])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 408, 408])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 410, 410])
grad_out size torch.Size([1, 3, 408, 408])
crop [44, 44, 44, 44] [916, 1323, 596, 1003] [960, 1279, 640, 959]
## 44 364 44 364
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  6.2123e-10,
        -2.1944e-09,  1.0889e-09, -2.1136e-09,  3.5319e-09,  6.7956e-09],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 820, 820])
grad_out size torch.Size([1, 3, 410, 410])
arg size torch.Size([1, 3, 410, 410])
##############grad_in in maxp torch.Size([1, 3, 820, 820])
I am [2, 3]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 822, 822])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 820, 820])
grad_input torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 822, 822])
grad_input new torch.Size([1, 3, 822, 822])
new grad_input torch.Size([1, 3, 822, 822])
grad_out size torch.Size([1, 3, 820, 820])
crop [90, 90, 90, 90] [1830, 2649, 1190, 2009] [1920, 2559, 1280, 1919]
## 90 730 90 730
##############return grad_in in conv2d tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 4.3385e-10, 3.6160e-10], device='cuda:0')
I am [2, 3]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 824, 824])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 824, 824])
grad_out size torch.Size([1, 3, 822, 822])
crop [91, 91, 91, 91] [1829, 2650, 1189, 2010] [1920, 2559, 1280, 1919]
## 91 731 91 731
##############return grad_in in conv2d tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 6.4907e-11, 1.2394e-10], device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 1648, 1648])
grad_out size torch.Size([1, 3, 824, 824])
arg size torch.Size([1, 3, 824, 824])
##############grad_in in maxp torch.Size([1, 3, 1648, 1648])
I am [2, 3]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 1650, 1650])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1648, 1648])
grad_input torch.Size([1, 3, 1650, 1650])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 1650, 1650])
grad_input new torch.Size([1, 3, 1650, 1650])
new grad_input torch.Size([1, 3, 1650, 1650])
grad_out size torch.Size([1, 3, 1648, 1648])
crop [184, 184, 184, 184] [3656, 5303, 2376, 4023] [3840, 5119, 2560, 3839]
## 184 1464 184 1464
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [2, 3]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 3, 1652, 1652])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1650, 1650])
final torch.Size([1, 3, 1652, 1652])
padding info :: [0, 0, 0, 0]
reshape_for_final ## 186 1466 186 1466
after crop g_in torch.Size([1, 3, 1280, 1280])
grad_out size torch.Size([1, 3, 1650, 1650])
crop [185, 185, 185, 185] [3655, 5304, 2375, 4024] [3840, 5119, 2560, 3839]
## 185 1465 185 1465
##############return grad_in in conv2d tensor([-2.7386e-07, -4.7008e-07, -2.5320e-07, -2.5649e-07, -1.6068e-07,
         1.3330e-08, -2.2764e-07,  1.2416e-07,  6.9210e-08, -6.9759e-08],
       device='cuda:0')
[8, 8]
HREREERE torch.Size([1, 3, 10240, 10240])

############# Enter checkpointing bkward ####
inputs len 5 4
in customized Sequential 5
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 2]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <2,2,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 2]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <2,2,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 2]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <2,2,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 2]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <2,2,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 2]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <2,2,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() [torch/csrc/autograd/python_engine.cpp] THPEngine_run_backward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 2]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <2,2,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 2]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <2,2,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 2]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <2,2,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 2]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <2,2,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 2]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <2,2,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 2]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <2,2,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 2]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <2,2,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 2]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <2,2,>,

in customized Sequential 3
need to get existing

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 92, 92])
grad_out size torch.Size([1, 3, 320, 320])
arg size torch.Size([1, 3, 46, 46])
new_grad_out torch.Size([1, 3, 46, 46])
##############grad_in in maxp torch.Size([1, 3, 92, 92])
I am [2, 2]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 94, 94])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 92, 92])
grad_input torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 94, 94])
grad_input new torch.Size([1, 3, 94, 94])
new grad_input torch.Size([1, 3, 94, 94])
grad_out size torch.Size([1, 3, 92, 92])
crop [6, 6, 6, 6] [154, 245, 154, 245] [160, 239, 160, 239]
## 6 86 6 86
##############return grad_in in conv2d tensor([ 2.4581e-04, -1.1346e-04, -8.0134e-05,  4.1848e-04, -1.6374e-03,
         1.7275e-03,  4.3823e-04, -4.3242e-04, -2.1108e-04,  2.0159e-03],
       device='cuda:0')
I am [2, 2]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 96, 96])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 96, 96])
grad_input new torch.Size([1, 3, 96, 96])
grad_input torch.Size([1, 3, 96, 96])
grad_out size torch.Size([1, 3, 94, 94])
crop [7, 7, 7, 7] [153, 246, 153, 246] [160, 239, 160, 239]
## 7 87 7 87
##############return grad_in in conv2d tensor([-2.8630e-05,  8.4525e-06, -1.2003e-04,  1.1462e-04, -1.4775e-04,
         2.1753e-04, -6.0836e-04, -7.1039e-05, -7.3794e-05, -7.4625e-05],
       device='cuda:0')
I am [2, 2]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 98, 98])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 96, 96])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 98, 98])
grad_out size torch.Size([1, 3, 96, 96])
crop [8, 8, 8, 8] [152, 247, 152, 247] [160, 239, 160, 239]
## 8 88 8 88
##############return grad_in in conv2d tensor([-8.8058e-06,  1.5002e-05, -1.4881e-05, -1.0026e-05,  5.3067e-05,
        -5.4604e-05, -1.1606e-05, -7.5088e-05,  1.7527e-04, -1.0146e-04],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 196, 196])
grad_out size torch.Size([1, 3, 98, 98])
arg size torch.Size([1, 3, 98, 98])
##############grad_in in maxp torch.Size([1, 3, 196, 196])
I am [2, 2]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 198, 198])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 196, 196])
grad_input torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 198, 198])
grad_input new torch.Size([1, 3, 198, 198])
new grad_input torch.Size([1, 3, 198, 198])
grad_out size torch.Size([1, 3, 196, 196])
crop [18, 18, 18, 18] [302, 497, 302, 497] [320, 479, 320, 479]
## 18 178 18 178
##############return grad_in in conv2d tensor([-9.0558e-07, -2.7835e-08, -5.4464e-08,  4.2101e-07, -1.4378e-08,
        -8.9939e-07,  1.5151e-06,  1.4923e-06,  2.9788e-07,  3.4885e-06],
       device='cuda:0')
I am [2, 2]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 200, 200])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 200, 200])
grad_input new torch.Size([1, 3, 200, 200])
grad_input torch.Size([1, 3, 200, 200])
grad_out size torch.Size([1, 3, 198, 198])
crop [19, 19, 19, 19] [301, 498, 301, 498] [320, 479, 320, 479]
## 19 179 19 179
##############return grad_in in conv2d tensor([-2.7499e-07, -6.1595e-08, -2.1252e-07, -3.5431e-08, -1.1762e-07,
        -1.4699e-07,  3.2416e-07, -4.0767e-07,  6.6116e-07,  1.5659e-06],
       device='cuda:0')
I am [2, 2]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 202, 202])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 200, 200])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 202, 202])
grad_out size torch.Size([1, 3, 200, 200])
crop [20, 20, 20, 20] [300, 499, 300, 499] [320, 479, 320, 479]
## 20 180 20 180
##############return grad_in in conv2d tensor([ 2.3075e-08,  6.3693e-08,  3.7480e-09,  1.6685e-08,  3.3979e-08,
        -2.8174e-08,  6.9432e-08, -2.0907e-08, -1.8789e-07, -1.9950e-07],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 404, 404])
grad_out size torch.Size([1, 3, 202, 202])
arg size torch.Size([1, 3, 202, 202])
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cCheckpointBackward
##############grad_in in maxp torch.Size([1, 3, 404, 404])
I am [2, 2]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 406, 406])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 404, 404])
grad_input torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 406, 406])
grad_input new torch.Size([1, 3, 406, 406])
new grad_input torch.Size([1, 3, 406, 406])
grad_out size torch.Size([1, 3, 404, 404])
crop [42, 42, 42, 42] [598, 1001, 598, 1001] [640, 959, 640, 959]
## 42 362 42 362
##############return grad_in in conv2d tensor([ 0.0000e+00,  4.4067e-09, -2.6879e-09,  6.2425e-09, -2.8909e-09,
        -2.1248e-10, -2.8110e-10,  5.7723e-11,  2.1090e-09, -2.9053e-09],
       device='cuda:0')
I am [2, 2]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 408, 408])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 408, 408])
grad_input new torch.Size([1, 3, 408, 408])
grad_input torch.Size([1, 3, 408, 408])
grad_out size torch.Size([1, 3, 406, 406])
crop [43, 43, 43, 43] [597, 1002, 597, 1002] [640, 959, 640, 959]
## 43 363 43 363
##############return grad_in in conv2d tensor([ 0.0000e+00,  1.0344e-09,  1.0178e-09, -1.6782e-09,  3.1877e-09,
        -8.1563e-10, -1.4789e-10,  8.8706e-10, -2.6545e-10, -2.8177e-10],
       device='cuda:0')
I am [2, 2]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 410, 410])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 408, 408])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 410, 410])
grad_out size torch.Size([1, 3, 408, 408])
crop [44, 44, 44, 44] [596, 1003, 596, 1003] [640, 959, 640, 959]
## 44 364 44 364
##############return grad_in in conv2d tensor([ 0.0000e+00,  1.1621e-10, -4.0747e-10, -1.4932e-10, -6.4925e-10,
        -5.9051e-11, -1.6518e-10, -1.3153e-10, -9.7927e-12, -1.9252e-10],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 820, 820])
grad_out size torch.Size([1, 3, 410, 410])
arg size torch.Size([1, 3, 410, 410])
##############grad_in in maxp torch.Size([1, 3, 820, 820])
I am [2, 2]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 822, 822])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 820, 820])
grad_input torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 822, 822])
grad_input new torch.Size([1, 3, 822, 822])
new grad_input torch.Size([1, 3, 822, 822])
grad_out size torch.Size([1, 3, 820, 820])
crop [90, 90, 90, 90] [1190, 2009, 1190, 2009] [1280, 1919, 1280, 1919]
## 90 730 90 730
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  3.8272e-11,  6.5826e-11, -1.8068e-11,
         3.9164e-11,  1.0771e-10, -9.8101e-11,  8.5525e-11,  1.6895e-10],
       device='cuda:0')
I am [2, 2]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 824, 824])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 824, 824])
grad_out size torch.Size([1, 3, 822, 822])
crop [91, 91, 91, 91] [1189, 2010, 1189, 2010] [1280, 1919, 1280, 1919]
## 91 731 91 731
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  5.3732e-12,  1.1231e-11,  1.9113e-11,
         3.1306e-11,  1.8216e-12, -9.2148e-12,  2.5884e-11,  3.3287e-11],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 1648, 1648])
grad_out size torch.Size([1, 3, 824, 824])
arg size torch.Size([1, 3, 824, 824])
##############grad_in in maxp torch.Size([1, 3, 1648, 1648])
I am [2, 2]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 1650, 1650])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1648, 1648])
grad_input torch.Size([1, 3, 1650, 1650])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 1650, 1650])
grad_input new torch.Size([1, 3, 1650, 1650])
new grad_input torch.Size([1, 3, 1650, 1650])
grad_out size torch.Size([1, 3, 1648, 1648])
crop [184, 184, 184, 184] [2376, 4023, 2376, 4023] [2560, 3839, 2560, 3839]
## 184 1464 184 1464
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -9.6457e-13,
         2.9727e-13,  6.4740e-13, -2.6520e-12,  7.9696e-13,  1.0972e-12],
       device='cuda:0')
I am [2, 2]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 3, 1652, 1652])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1650, 1650])
final torch.Size([1, 3, 1652, 1652])
padding info :: [0, 0, 0, 0]
reshape_for_final ## 186 1466 186 1466
after crop g_in torch.Size([1, 3, 1280, 1280])
grad_out size torch.Size([1, 3, 1650, 1650])
crop [185, 185, 185, 185] [2375, 4024, 2375, 4024] [2560, 3839, 2560, 3839]
## 185 1465 185 1465
##############return grad_in in conv2d tensor([-9.9949e-08, -7.7530e-08,  1.3654e-07, -4.3857e-08,  1.4537e-07,
         2.5752e-07, -2.8990e-07,  3.4742e-07, -3.7336e-08, -4.3097e-08],
       device='cuda:0')
[8, 8]
HREREERE torch.Size([1, 3, 10240, 10240])

############# Enter checkpointing bkward ####
inputs len 5 4
in customized Sequential 5
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 1]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <2,1,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 1]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <2,1,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 1]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <2,1,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 1]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <2,1,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 1]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <2,1,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 1]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <2,1,>,

[torch/csrc/autograd/python_engine.cpp] THPEngine_run_backward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 1]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <2,1,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 1]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <2,1,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 1]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <2,1,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 1]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <2,1,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 1]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <2,1,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 1]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <2,1,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 1]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <2,1,>,

in customized Sequential 3
need to get existing

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 92, 92])
grad_out size torch.Size([1, 3, 320, 320])
arg size torch.Size([1, 3, 46, 46])
new_grad_out torch.Size([1, 3, 46, 46])
##############grad_in in maxp torch.Size([1, 3, 92, 92])
I am [2, 1]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 94, 94])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 92, 92])
grad_input torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 94, 94])
grad_input new torch.Size([1, 3, 94, 94])
new grad_input torch.Size([1, 3, 94, 94])
grad_out size torch.Size([1, 3, 92, 92])
crop [6, 6, 6, 6] [74, 165, 154, 245] [80, 159, 160, 239]
## 6 86 6 86
##############return grad_in in conv2d tensor([ 0.0002,  0.0010, -0.0004,  0.0000,  0.0000,  0.0003,  0.0008, -0.0010,
         0.0003,  0.0015], device='cuda:0')
I am [2, 1]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 96, 96])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 96, 96])
grad_input new torch.Size([1, 3, 96, 96])
grad_input torch.Size([1, 3, 96, 96])
grad_out size torch.Size([1, 3, 94, 94])
crop [7, 7, 7, 7] [73, 166, 153, 246] [80, 159, 160, 239]
## 7 87 7 87
##############return grad_in in conv2d tensor([-3.4218e-04, -7.0451e-05, -8.8163e-04, -2.7085e-04, -9.2942e-05,
         1.7709e-04, -2.3573e-04,  1.5363e-04, -1.0121e-04,  5.8636e-04],
       device='cuda:0')
I am [2, 1]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 98, 98])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 96, 96])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 98, 98])
grad_out size torch.Size([1, 3, 96, 96])
crop [8, 8, 8, 8] [72, 167, 152, 247] [80, 159, 160, 239]
## 8 88 8 88
##############return grad_in in conv2d tensor([-3.9153e-05,  1.8057e-05, -1.1725e-05, -1.4712e-04,  1.1423e-04,
         8.9157e-05, -2.6634e-05, -1.2628e-05,  1.2501e-05,  4.7724e-05],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 196, 196])
grad_out size torch.Size([1, 3, 98, 98])
arg size torch.Size([1, 3, 98, 98])
##############grad_in in maxp torch.Size([1, 3, 196, 196])
I am [2, 1]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 198, 198])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 196, 196])
grad_input torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 198, 198])
grad_input new torch.Size([1, 3, 198, 198])
new grad_input torch.Size([1, 3, 198, 198])
grad_out size torch.Size([1, 3, 196, 196])
crop [18, 18, 18, 18] [142, 337, 302, 497] [160, 319, 320, 479]
## 18 178 18 178
##############return grad_in in conv2d tensor([-7.5737e-06,  1.6605e-06, -1.0170e-05,  1.2262e-05, -5.7584e-06,
        -2.2635e-05, -1.2527e-05,  2.8589e-05,  6.7233e-06, -9.8681e-07],
       device='cuda:0')
I am [2, 1]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 200, 200])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 200, 200])
grad_input new torch.Size([1, 3, 200, 200])
grad_input torch.Size([1, 3, 200, 200])
grad_out size torch.Size([1, 3, 198, 198])
crop [19, 19, 19, 19] [141, 338, 301, 498] [160, 319, 320, 479]
## 19 179 19 179
##############return grad_in in conv2d tensor([-2.2820e-06, -1.3472e-06, -5.7641e-06,  3.3114e-06, -8.0305e-06,
        -3.7563e-06, -2.7776e-06, -1.7107e-06, -3.4082e-06,  1.0133e-05],
       device='cuda:0')
I am [2, 1]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 202, 202])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 200, 200])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 202, 202])
grad_out size torch.Size([1, 3, 200, 200])
crop [20, 20, 20, 20] [140, 339, 300, 499] [160, 319, 320, 479]
## 20 180 20 180
##############return grad_in in conv2d tensor([ 2.3798e-07,  6.9020e-07,  1.7249e-07,  5.3544e-07,  2.7490e-07,
        -2.9395e-07,  2.4893e-06,  1.7401e-06, -2.3597e-06, -2.3769e-06],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 404, 404])
grad_out size torch.Size([1, 3, 202, 202])
arg size torch.Size([1, 3, 202, 202])
##############grad_in in maxp torch.Size([1, 3, 404, 404])
I am [2, 1]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 406, 406])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 404, 404])
grad_input torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 406, 406])
grad_input new torch.Size([1, 3, 406, 406])
new grad_input [torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cCheckpointBackward
torch.Size([1, 3, 406, 406])
grad_out size torch.Size([1, 3, 404, 404])
crop [42, 42, 42, 42] [278, 681, 598, 1001] [320, 639, 640, 959]
## 42 362 42 362
##############return grad_in in conv2d tensor([ 5.3521e-08, -5.5857e-08, -7.2918e-08,  1.2012e-07,  5.2823e-08,
        -1.4545e-08,  2.3498e-08,  1.0253e-07,  7.1844e-09,  5.2106e-08],
       device='cuda:0')
I am [2, 1]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 408, 408])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 408, 408])
grad_input new torch.Size([1, 3, 408, 408])
grad_input torch.Size([1, 3, 408, 408])
grad_out size torch.Size([1, 3, 406, 406])
crop [43, 43, 43, 43] [277, 682, 597, 1002] [320, 639, 640, 959]
## 43 363 43 363
##############return grad_in in conv2d tensor([-7.0786e-09, -2.2870e-08,  1.7300e-08,  2.6259e-08,  2.3607e-08,
         2.8513e-08,  4.0699e-08, -9.8050e-09,  2.8952e-08,  5.1836e-08],
       device='cuda:0')
I am [2, 1]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 410, 410])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 408, 408])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 410, 410])
grad_out size torch.Size([1, 3, 408, 408])
crop [44, 44, 44, 44] [276, 683, 596, 1003] [320, 639, 640, 959]
## 44 364 44 364
##############return grad_in in conv2d tensor([-2.6609e-10,  5.5164e-09,  2.4400e-09,  4.5651e-09, -2.2314e-09,
        -1.3791e-08, -2.1334e-08, -1.3519e-08, -1.2282e-08, -1.2697e-08],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 820, 820])
grad_out size torch.Size([1, 3, 410, 410])
arg size torch.Size([1, 3, 410, 410])
##############grad_in in maxp torch.Size([1, 3, 820, 820])
I am [2, 1]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 822, 822])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 820, 820])
grad_input torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 822, 822])
grad_input new torch.Size([1, 3, 822, 822])
new grad_input torch.Size([1, 3, 822, 822])
grad_out size torch.Size([1, 3, 820, 820])
crop [90, 90, 90, 90] [550, 1369, 1190, 2009] [640, 1279, 1280, 1919]
## 90 730 90 730
##############return grad_in in conv2d tensor([-1.8762e-10, -1.4549e-10, -8.9236e-10,  2.5642e-10,  5.5802e-10,
        -7.5424e-10,  1.6958e-09,  1.5314e-09,  1.3940e-09,  3.0996e-09],
       device='cuda:0')
I am [2, 1]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 824, 824])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 824, 824])
grad_out size torch.Size([1, 3, 822, 822])
crop [91, 91, 91, 91] [549, 1370, 1189, 2010] [640, 1279, 1280, 1919]
## 91 731 91 731
##############return grad_in in conv2d tensor([-2.6341e-11, -1.9123e-11, -9.1100e-11, -1.6153e-10, -1.8156e-11,
         1.4415e-10, -1.9906e-11,  1.4120e-10,  7.8939e-10,  1.0732e-09],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 1648, 1648])
grad_out size torch.Size([1, 3, 824, 824])
arg size torch.Size([1, 3, 824, 824])
##############grad_in in maxp torch.Size([1, 3, 1648, 1648])
I am [2, 1]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 1650, 1650])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1648, 1648])
grad_input torch.Size([1, 3, 1650, 1650])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 1650, 1650])
grad_input new torch.Size([1, 3, 1650, 1650])
new grad_input torch.Size([1, 3, 1650, 1650])
grad_out size torch.Size([1, 3, 1648, 1648])
crop [184, 184, 184, 184] [1096, 2743, 2376, 4023] [1280, 2559, 2560, 3839]
## 184 1464 184 1464
##############return grad_in in conv2d tensor([ 4.2430e-12, -4.2538e-12, -4.8815e-12,  1.4498e-12,  2.2753e-11,
        -1.3659e-11, -1.5655e-11,  5.2811e-12,  4.9494e-13, -6.5141e-12],
       device='cuda:0')
I am [2, 1]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 3, 1652, 1652])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1650, 1650])
final torch.Size([1, 3, 1652, 1652])
padding info :: [0, 0, 0, 0]
reshape_for_final ## 186 1466 186 1466
after crop g_in torch.Size([1, 3, 1280, 1280])
grad_out size torch.Size([1, 3, 1650, 1650])
crop [185, 185, 185, 185] [1095, 2744, 2375, 4024] [1280, 2559, 2560, 3839]
## 185 1465 185 1465
##############return grad_in in conv2d tensor([-3.5401e-07, -1.5873e-07,  3.3656e-07,  1.3525e-07, -2.1019e-07,
        -1.7534e-07, -5.2508e-07, -8.1287e-08, -9.1759e-08,  4.0251e-07],
       device='cuda:0')
[8, 8]
HREREERE torch.Size([1, 3, 10240, 10240])

############# Enter checkpointing bkward ####
inputs len 5 4
in customized Sequential 5
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 0]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1466])
myctx [[ 140046822727744[ PI( <2,0,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 0]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1464])
myctx [[ 140045015171856[ PI( <2,0,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 0]
shape input_tile_for_next
 torch.Size([1, 3, 822, 732])
myctx [[ 140045013800704[ PI( <2,0,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 0]
shape input_tile_for_next
 torch.Size([1, 3, 820, 730])
myctx [[ 140045013848944[ PI( <2,0,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 0]
shape input_tile_for_next
 torch.Size([1, 3, 408, 366])
myctx [[ 140045013206640[ PI( <2,0,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 0]
shape input_tile_for_next
 torch.Size([1, 3, 406, 364])
myctx [[ 140045013206784[ PI( <2,0,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 0]
shape input_tile_for_next
 torch.Size([1, 3, 404, 362])
myctx [torch/csrc/autograd/python_engine.cpp] THPEngine_run_backward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[[ 140045013206880[ PI( <2,0,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 0]
shape input_tile_for_next
 torch.Size([1, 3, 200, 182])
myctx [[ 140045013206352[ PI( <2,0,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 0]
shape input_tile_for_next
 torch.Size([1, 3, 198, 180])
myctx [[ 140045013206304[ PI( <2,0,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 0]
shape input_tile_for_next
 torch.Size([1, 3, 196, 178])
myctx [[ 140045013207072[ PI( <2,0,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 0]
shape input_tile_for_next
 torch.Size([1, 3, 96, 90])
myctx [[ 140045013206400[ PI( <2,0,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 0]
shape input_tile_for_next
 torch.Size([1, 3, 94, 88])
myctx [[ 140045013206448[ PI( <2,0,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [2, 0]
shape input_tile_for_next
 torch.Size([1, 3, 92, 86])
myctx [[ 140045013206976[ PI( <2,0,>,

in customized Sequential 3
need to get existing

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 92, 86])
grad_out size torch.Size([1, 3, 320, 320])
arg size torch.Size([1, 3, 46, 43])
new_grad_out torch.Size([1, 3, 46, 43])
##############grad_in in maxp torch.Size([1, 3, 92, 86])
I am [2, 0]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 94, 88])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 92, 86])
grad_input torch.Size([1, 3, 94, 88])
padding info :: [1, 0, 0, 0]
grad_input old torch.Size([1, 3, 94, 88])
grad_input new torch.Size([1, 3, 94, 88])
new grad_input torch.Size([1, 3, 94, 88])
grad_out size torch.Size([1, 3, 92, 86])
crop [0, 6, 6, 6] [0, 85, 154, 245] [0, 79, 160, 239]
## 6 86 0 80
##############return grad_in in conv2d tensor([ 0.0000,  0.0071, -0.0025,  0.0011,  0.0051, -0.0010, -0.0017,  0.0008,
        -0.0030,  0.0016], device='cuda:0')
I am [2, 0]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 96, 90])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 94, 88])
padding info :: [2, 0, 0, 0]
grad_input old torch.Size([1, 3, 96, 90])
grad_input new torch.Size([1, 3, 96, 90])
grad_input torch.Size([1, 3, 96, 90])
grad_out size torch.Size([1, 3, 94, 87])
crop [0, 7, 7, 7] [0, 86, 153, 246] [0, 79, 160, 239]
## 7 87 0 80
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00, -1.4341e-05, -5.0664e-04, -9.0797e-04,
        -5.1702e-05, -2.3749e-04, -4.4410e-05,  1.1390e-03, -3.3820e-04],
       device='cuda:0')
I am [2, 0]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 98, 92])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 96, 90])
padding info :: [3, 0, 0, 0]
grad_input torch.Size([1, 3, 98, 89])
grad_out size torch.Size([1, 3, 96, 88])
crop [0, 8, 8, 8] [0, 87, 152, 247] [0, 79, 160, 239]
## 8 88 0 80
##############return grad_in in conv2d tensor([-7.2958e-05, -3.1823e-04, -7.5467e-06,  2.5656e-04,  5.4495e-05,
         2.6044e-04, -1.9271e-04,  7.3675e-05,  1.5450e-04, -3.4318e-04],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 196, 178])
grad_out size torch.Size([1, 3, 98, 89])
arg size torch.Size([1, 3, 98, 89])
##############grad_in in maxp torch.Size([1, 3, 196, 178])
I am [2, 0]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 198, 180])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 196, 178])
grad_input torch.Size([1, 3, 198, 180])
padding info :: [1, 0, 0, 0]
grad_input old torch.Size([1, 3, 198, 180])
grad_input new torch.Size([1, 3, 198, 180])
new grad_input torch.Size([1, 3, 198, 180])
grad_out size torch.Size([1, 3, 196, 178])
crop [0, 18, 18, 18] [0, 177, 302, 497] [0, 159, 320, 479]
## 18 178 0 160
##############return grad_in in conv2d tensor([ 0.0000e+00,  1.9265e-05,  6.3450e-06,  6.9892e-06,  7.6659e-06,
        -4.5306e-07,  1.6196e-05, -1.5372e-05, -5.2961e-05, -1.5443e-06],
       device='cuda:0')
I am [2, 0]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 200, 182])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 198, 180])
padding info :: [2, 0, 0, 0]
grad_input old torch.Size([1, 3, 200, 182])
grad_input new torch.Size([1, 3, 200, 182])
grad_input torch.Size([1, 3, 200, 182])
grad_out size torch.Size([1, 3, 198, 179])
crop [0, 19, 19, 19] [0, 178, 301, 498] [0, 159, 320, 479]
## 19 179 0 160
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00, -3.1450e-06,  8.2235e-06, -4.9465e-07,
         2.5012e-06,  5.9724e-06, -1.5754e-06, -3.9540e-06, -1.4868e-05],
       device='cuda:0')
I am [2, 0]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 202, 184])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 200, 182])
padding info :: [3, 0, 0, 0]
grad_input torch.Size([1, 3, 202, 181])
grad_out size torch.Size([1, 3, 200, 180])
crop [0, 20, 20, 20] [0, 179, 300, 499] [0, 159, 320, 479]
## 20 180 0 160
##############return grad_in in conv2d tensor([-2.4182e-06,  7.2043e-07, -1.4594e-06, -4.9780e-07, -1.1509e-06,
         4.7257e-07,  5.8406e-06,  4.5832e-07, -1.2703e-06,  3.8320e-06],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 404, 362])
grad_out size torch.Size([1, 3, 202, 181])
arg size torch.Size([1, 3, 202, 181])
##############grad_in in maxp torch.Size([1, 3, 404, 362])
I am [2, 0]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 406, 364])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 404, 362])
grad_input torch.Size([1, 3, 406, 364])
padding info :: [1, 0, 0, 0]
grad_input old torch.Size([1, 3, 406, 364])
grad_input new torch.Size([1, 3, 406, 364])
new grad_input torch.Size([1, 3, 406, 364])
grad_out size torch.Size([1, 3, 404, 362])
crop [0, 42, 42, 42] [0, 361, 598, 1001] [0, 319, 640, 959]
## 42 362 0 320
##############return grad_in in conv2d tensor([ 0.0000e+00,  2.4003e-08,  6.9653e-08,  3.7951e-07, -5.3872e-07,
        -2.2608e-07,  1.3814e-07, -5.9816e-08,  6.5222e-09,  4.8985e-08],
       device='cuda:0')
I am [2, 0]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 408, 366])
weight shape [torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cCheckpointBackward
torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 406, 364])
padding info :: [2, 0, 0, 0]
grad_input old torch.Size([1, 3, 408, 366])
grad_input new torch.Size([1, 3, 408, 366])
grad_input torch.Size([1, 3, 408, 366])
grad_out size torch.Size([1, 3, 406, 363])
crop [0, 43, 43, 43] [0, 362, 597, 1002] [0, 319, 640, 959]
## 43 363 0 320
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  3.1865e-08, -8.9778e-09, -9.6456e-08,
         2.1390e-07, -4.0991e-08, -2.4302e-07,  7.8241e-08,  6.6479e-09],
       device='cuda:0')
I am [2, 0]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 410, 368])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 408, 366])
padding info :: [3, 0, 0, 0]
grad_input torch.Size([1, 3, 410, 365])
grad_out size torch.Size([1, 3, 408, 364])
crop [0, 44, 44, 44] [0, 363, 596, 1003] [0, 319, 640, 959]
## 44 364 0 320
##############return grad_in in conv2d tensor([-8.0046e-09,  1.8434e-08, -2.0187e-08, -6.2920e-09,  2.4609e-08,
         5.2751e-09,  1.7788e-08, -1.9758e-09, -1.5979e-09, -2.3743e-08],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 820, 730])
grad_out size torch.Size([1, 3, 410, 365])
arg size torch.Size([1, 3, 410, 365])
##############grad_in in maxp torch.Size([1, 3, 820, 730])
I am [2, 0]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 822, 732])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 820, 730])
grad_input torch.Size([1, 3, 822, 732])
padding info :: [1, 0, 0, 0]
grad_input old torch.Size([1, 3, 822, 732])
grad_input new torch.Size([1, 3, 822, 732])
new grad_input torch.Size([1, 3, 822, 732])
grad_out size torch.Size([1, 3, 820, 730])
crop [0, 90, 90, 90] [0, 729, 1190, 2009] [0, 639, 1280, 1919]
## 90 730 0 640
##############return grad_in in conv2d tensor([ 0.0000e+00, -1.5059e-09, -4.9929e-09, -2.7719e-09, -5.0670e-09,
         1.0300e-08,  2.3219e-09,  1.8065e-09, -4.0121e-09,  1.5359e-09],
       device='cuda:0')
I am [2, 0]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 824, 734])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 822, 732])
padding info :: [2, 0, 0, 0]
grad_input torch.Size([1, 3, 824, 732])
grad_out size torch.Size([1, 3, 822, 731])
crop [0, 91, 91, 91] [0, 730, 1189, 2010] [0, 639, 1280, 1919]
## 91 731 0 640
##############return grad_in in conv2d tensor([-5.7657e-10, -1.0808e-09, -2.4329e-10,  1.7756e-09,  1.0495e-09,
        -1.4880e-09, -5.6134e-10,  6.0196e-10,  1.8376e-09,  2.6843e-09],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 1648, 1464])
grad_out size torch.Size([1, 3, 824, 732])
arg size torch.Size([1, 3, 824, 732])
##############grad_in in maxp torch.Size([1, 3, 1648, 1464])
I am [2, 0]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 1650, 1466])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1648, 1464])
grad_input torch.Size([1, 3, 1650, 1466])
padding info :: [1, 0, 0, 0]
grad_input old torch.Size([1, 3, 1650, 1466])
grad_input new torch.Size([1, 3, 1650, 1466])
new grad_input torch.Size([1, 3, 1650, 1466])
grad_out size torch.Size([1, 3, 1648, 1464])
crop [0, 184, 184, 184] [0, 1463, 2376, 4023] [0, 1279, 2560, 3839]
## 184 1464 0 1280
##############return grad_in in conv2d tensor([ 0.0000e+00,  2.0199e-10, -2.0264e-11, -9.2091e-11,  9.3321e-12,
         6.2688e-11,  2.5610e-10, -4.3729e-11, -1.2784e-10, -1.0365e-10],
       device='cuda:0')
I am [2, 0]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 3, 1652, 1468])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1650, 1466])
final torch.Size([1, 3, 1652, 1468])
padding info :: [2, 0, 0, 0]
reshape_for_final ## 186 1466 0 1280
after crop g_in torch.Size([1, 3, 1280, 1280])
grad_out size torch.Size([1, 3, 1650, 1465])
crop [0, 185, 185, 185] [0, 1464, 2375, 4024] [0, 1279, 2560, 3839]
## 185 1465 0 1280
##############return grad_in in conv2d tensor([ 2.0070e-07, -2.1570e-07,  2.2879e-08,  1.4281e-07,  3.7847e-07,
         7.2987e-08,  1.9829e-08, -7.8262e-08,  2.2684e-07,  5.9667e-07],
       device='cuda:0')
[8, 8]
HREREERE torch.Size([1, 3, 10240, 10240])

############# Enter checkpointing bkward ####
inputs len 5 4
in customized Sequential 5
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 7]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1466])
myctx [[ 140046822727744[ PI( <1,7,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 7]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1464])
myctx [[ 140045015171856[ PI( <1,7,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 7]
shape input_tile_for_next
 torch.Size([1, 3, 822, 732])
myctx [[ 140045013800704[ PI( <1,7,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 7]
shape input_tile_for_next
 torch.Size([1, 3, 820, 730])
myctx [[ 140045013848944[ PI( <1,7,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 7]
shape input_tile_for_next
 torch.Size([1, 3, 408, 366])
myctx [[ 140045013206640[ PI( <1,7,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 7]
shape input_tile_for_next
 torch.Size([1, 3, 406, 364])
myctx [[ 140045013206784[ PI( <1,7,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 7]
shape input_tile_for_next
 torch.Size([1, 3, 404, 362])
myctx [[ 140045013206880[ PI( <1,7,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 7]
shape input_tile_for_next
 torch.Size([1, 3, 200, 182])
myctx [torch/csrc/autograd/python_engine.cpp] THPEngine_run_backward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[[ 140045013206352[ PI( <1,7,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 7]
shape input_tile_for_next
 torch.Size([1, 3, 198, 180])
myctx [[ 140045013206304[ PI( <1,7,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 7]
shape input_tile_for_next
 torch.Size([1, 3, 196, 178])
myctx [[ 140045013207072[ PI( <1,7,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 7]
shape input_tile_for_next
 torch.Size([1, 3, 96, 90])
myctx [[ 140045013206400[ PI( <1,7,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 7]
shape input_tile_for_next
 torch.Size([1, 3, 94, 88])
myctx [[ 140045013206448[ PI( <1,7,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 7]
shape input_tile_for_next
 torch.Size([1, 3, 92, 86])
myctx [[ 140045013206976[ PI( <1,7,>,

in customized Sequential 3
need to get existing

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 92, 86])
grad_out size torch.Size([1, 3, 320, 320])
arg size torch.Size([1, 3, 46, 43])
new_grad_out torch.Size([1, 3, 46, 43])
##############grad_in in maxp torch.Size([1, 3, 92, 86])
I am [1, 7]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 94, 88])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 92, 86])
grad_input torch.Size([1, 3, 94, 88])
padding info :: [0, 1, 0, 0]
grad_input old torch.Size([1, 3, 94, 88])
grad_input new torch.Size([1, 3, 94, 88])
new grad_input torch.Size([1, 3, 94, 88])
grad_out size torch.Size([1, 3, 92, 86])
crop [6, 0, 6, 6] [554, 639, 74, 165] [560, 639, 80, 159]
## 6 86 6 86
##############return grad_in in conv2d tensor([-0.0010,  0.0014,  0.0010, -0.0031,  0.0002, -0.0020, -0.0047,  0.0016,
        -0.0004,  0.0024], device='cuda:0')
I am [1, 7]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 96, 90])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 94, 88])
padding info :: [0, 2, 0, 0]
grad_input old torch.Size([1, 3, 96, 90])
grad_input new torch.Size([1, 3, 96, 90])
grad_input torch.Size([1, 3, 96, 90])
grad_out size torch.Size([1, 3, 94, 87])
crop [7, 0, 7, 7] [553, 639, 73, 166] [560, 639, 80, 159]
## 7 87 7 87
##############return grad_in in conv2d tensor([-1.7965e-04, -1.3226e-04, -2.1907e-04,  8.4158e-05,  4.2895e-04,
        -4.8972e-04,  1.1171e-03, -5.2552e-04,  2.5058e-04, -3.7393e-04],
       device='cuda:0')
I am [1, 7]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 98, 92])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 96, 90])
padding info :: [0, 3, 0, 0]
grad_input torch.Size([1, 3, 98, 89])
grad_out size torch.Size([1, 3, 96, 88])
crop [8, 0, 8, 8] [552, 639, 72, 167] [560, 639, 80, 159]
## 8 88 8 88
##############return grad_in in conv2d tensor([ 4.3349e-06, -5.6205e-05,  7.1527e-06,  7.1499e-05,  8.4785e-06,
        -3.9444e-05,  2.4161e-04,  9.3044e-05, -2.8785e-04, -1.4902e-04],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 196, 178])
grad_out size torch.Size([1, 3, 98, 89])
arg size torch.Size([1, 3, 98, 89])
##############grad_in in maxp torch.Size([1, 3, 196, 178])
I am [1, 7]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 198, 180])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 196, 178])
grad_input torch.Size([1, 3, 198, 180])
padding info :: [0, 1, 0, 0]
grad_input old torch.Size([1, 3, 198, 180])
grad_input new torch.Size([1, 3, 198, 180])
new grad_input torch.Size([1, 3, 198, 180])
grad_out size torch.Size([1, 3, 196, 178])
crop [18, 0, 18, 18] [1102, 1279, 142, 337] [1120, 1279, 160, 319]
## 18 178 18 178
##############return grad_in in conv2d tensor([ 0.0000e+00, -5.0178e-06, -1.7322e-06,  6.5779e-07,  7.8358e-07,
         2.0194e-05,  8.7388e-06,  1.2845e-06, -3.8569e-05, -1.2101e-06],
       device='cuda:0')
I am [1, 7]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 200, 182])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 198, 180])
padding info :: [0, 2, 0, 0]
grad_input old torch.Size([1, 3, 200, 182])
grad_input new torch.Size([1, 3, 200, 182])
grad_input torch.Size([1, 3, 200, 182])
grad_out size torch.Size([1, 3, 198, 179])
crop [19, 0, 19, 19] [1101, 1279, 141, 338] [1120, 1279, 160, 319]
## 19 179 19 179
##############return grad_in in conv2d tensor([ 0.0000e+00, -1.5235e-06, -6.0801e-07, -5.4841e-07, -1.2312e-06,
         5.8426e-06,  3.4634e-06,  4.5654e-06, -5.6581e-06,  4.5844e-06],
       device='cuda:0')
I am [1, 7]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 202, 184])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 200, 182])
padding info :: [0, 3, 0, 0]
grad_input torch.Size([1, 3, 202, 181])
grad_out size torch.Size([1, 3, 200, 180])
crop [20, 0, 20, 20] [1100, 1279, 140, 339] [1120, 1279, 160, 319]
## 20 180 20 180
##############return grad_in in conv2d tensor([ 0.0000e+00,  1.2077e-07,  3.4929e-07,  8.7476e-08,  1.5503e-07,
        -2.9254e-07, -1.7875e-06, -6.8261e-07,  3.3650e-08,  1.0721e-06],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 404, 362])
grad_out size torch.Size([1, 3, 202, 181])
arg size torch.Size([1, 3, 202, 181])
##############grad_in in maxp torch.Size([1, 3, 404, 362])
I am [1, 7]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 406, 364])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 404, 362])
grad_input torch.Size([1, 3, 406, 364])
padding info :: [0, 1, 0, 0]
grad_input old torch.Size([1, 3, 406, 364])
grad_input new torch.Size([1, 3, 406, 364])
new grad_input torch.Size([1, 3, 406, 364])
grad_out size torch.Size([1, 3, 404, 362])
crop [42, 0, 42, 42] [2198, 2559, 278, 681] [2240, 2559, 320, 639]
## 42 362 42 362
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  3.5673e-08, -5.4588e-08,
         4.8675e-08,  2.2230e-08, -8.3397e-09,  7.4118e-09,  8.8115e-09],
       device='cuda:0')
I am [1, 7]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 408, 366])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 406, 364])
padding info :: [0, 2, 0, 0]
grad_input old torch.Size([1, 3, 408, 366])
grad_input new torch.Size([1, 3, 408, 366])
grad_input torch.Size([1, 3, 408, 366])
grad_out size torch.Size([1, 3, 406, 363])
crop [43, 0, 43, 43] [2197, 2559, 277, 682] [2240, 2559, 320, 639]
## 43 363 43 363
##############return grad_in in conv2d [torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cCheckpointBackward
tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00, -1.3931e-09, -1.3246e-08,
         6.6225e-09,  3.3260e-08, -3.3806e-09,  1.6843e-08, -9.8480e-09],
       device='cuda:0')
I am [1, 7]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 410, 368])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 408, 366])
padding info :: [0, 3, 0, 0]
grad_input torch.Size([1, 3, 410, 365])
grad_out size torch.Size([1, 3, 408, 364])
crop [44, 0, 44, 44] [2196, 2559, 276, 683] [2240, 2559, 320, 639]
## 44 364 44 364
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.2158e-10,  2.0157e-09,
         3.9062e-09, -2.1094e-09, -2.9464e-09, -9.9854e-09, -4.0194e-09],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 820, 730])
grad_out size torch.Size([1, 3, 410, 365])
arg size torch.Size([1, 3, 410, 365])
##############grad_in in maxp torch.Size([1, 3, 820, 730])
I am [1, 7]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 822, 732])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 820, 730])
grad_input torch.Size([1, 3, 822, 732])
padding info :: [0, 1, 0, 0]
grad_input old torch.Size([1, 3, 822, 732])
grad_input new torch.Size([1, 3, 822, 732])
new grad_input torch.Size([1, 3, 822, 732])
grad_out size torch.Size([1, 3, 820, 730])
crop [90, 0, 90, 90] [4390, 5119, 550, 1369] [4480, 5119, 640, 1279]
## 90 730 90 730
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  2.2770e-11,  1.7724e-11, -5.0798e-10, -5.0149e-10],
       device='cuda:0')
I am [1, 7]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 824, 734])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 822, 732])
padding info :: [0, 2, 0, 0]
grad_input torch.Size([1, 3, 824, 732])
grad_out size torch.Size([1, 3, 822, 731])
crop [91, 0, 91, 91] [4389, 5119, 549, 1370] [4480, 5119, 640, 1279]
## 91 731 91 731
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00, -1.3803e-13,  5.9540e-13, -6.3159e-11, -9.0272e-11],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 1648, 1464])
grad_out size torch.Size([1, 3, 824, 732])
arg size torch.Size([1, 3, 824, 732])
##############grad_in in maxp torch.Size([1, 3, 1648, 1464])
I am [1, 7]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 1650, 1466])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1648, 1464])
grad_input torch.Size([1, 3, 1650, 1466])
padding info :: [0, 1, 0, 0]
grad_input old torch.Size([1, 3, 1650, 1466])
grad_input new torch.Size([1, 3, 1650, 1466])
new grad_input torch.Size([1, 3, 1650, 1466])
grad_out size torch.Size([1, 3, 1648, 1464])
crop [184, 0, 184, 184] [8776, 10239, 1096, 2743] [8960, 10239, 1280, 2559]
## 184 1464 184 1464
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [1, 7]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 3, 1652, 1468])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1650, 1466])
final torch.Size([1, 3, 1652, 1468])
padding info :: [0, 2, 0, 0]
reshape_for_final ## 186 1466 186 1466
after crop g_in torch.Size([1, 3, 1280, 1280])
grad_out size torch.Size([1, 3, 1650, 1465])
crop [185, 0, 185, 185] [8775, 10239, 1095, 2744] [8960, 10239, 1280, 2559]
## 185 1465 185 1465
##############return grad_in in conv2d tensor([-1.5440e-07,  1.0301e-07,  4.5929e-08,  1.0596e-07, -4.5647e-07,
         1.9218e-07,  2.4514e-07,  5.1806e-07,  2.5957e-07, -2.7507e-07],
       device='cuda:0')
[8, 8]
HREREERE torch.Size([1, 3, 10240, 10240])

############# Enter checkpointing bkward ####
inputs len 5 4
in customized Sequential 5
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 6]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <1,6,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 6]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <1,6,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 6]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <1,6,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 6]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <1,6,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 6]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <1,6,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 6]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <1,6,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 6]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <1,6,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 6]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <1,6,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 6]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx[torch/csrc/autograd/python_engine.cpp] THPEngine_run_backward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
 [[ 140045013206304[ PI( <1,6,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 6]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <1,6,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 6]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <1,6,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 6]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <1,6,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 6]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <1,6,>,

in customized Sequential 3
need to get existing

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 92, 92])
grad_out size torch.Size([1, 3, 320, 320])
arg size torch.Size([1, 3, 46, 46])
new_grad_out torch.Size([1, 3, 46, 46])
##############grad_in in maxp torch.Size([1, 3, 92, 92])
I am [1, 6]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 94, 94])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 92, 92])
grad_input torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 94, 94])
grad_input new torch.Size([1, 3, 94, 94])
new grad_input torch.Size([1, 3, 94, 94])
grad_out size torch.Size([1, 3, 92, 92])
crop [6, 6, 6, 6] [474, 565, 74, 165] [480, 559, 80, 159]
## 6 86 6 86
##############return grad_in in conv2d tensor([-0.0003, -0.0012,  0.0008, -0.0002,  0.0002,  0.0006, -0.0004,  0.0001,
        -0.0004, -0.0013], device='cuda:0')
I am [1, 6]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 96, 96])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 96, 96])
grad_input new torch.Size([1, 3, 96, 96])
grad_input torch.Size([1, 3, 96, 96])
grad_out size torch.Size([1, 3, 94, 94])
crop [7, 7, 7, 7] [473, 566, 73, 166] [480, 559, 80, 159]
## 7 87 7 87
##############return grad_in in conv2d tensor([-5.7502e-05,  1.5542e-04, -1.4018e-04,  7.5154e-04, -2.3315e-04,
         6.6408e-04, -1.4857e-07,  2.3091e-04, -3.2345e-04,  4.4166e-04],
       device='cuda:0')
I am [1, 6]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 98, 98])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 96, 96])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 98, 98])
grad_out size torch.Size([1, 3, 96, 96])
crop [8, 8, 8, 8] [472, 567, 72, 167] [480, 559, 80, 159]
## 8 88 8 88
##############return grad_in in conv2d tensor([ 1.3875e-06,  4.2635e-05, -8.3430e-05,  9.6130e-05,  1.4732e-05,
        -8.7586e-05,  1.0462e-04, -1.0910e-04, -4.4792e-05,  7.2555e-05],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 196, 196])
grad_out size torch.Size([1, 3, 98, 98])
arg size torch.Size([1, 3, 98, 98])
##############grad_in in maxp torch.Size([1, 3, 196, 196])
I am [1, 6]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 198, 198])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 196, 196])
grad_input torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 198, 198])
grad_input new torch.Size([1, 3, 198, 198])
new grad_input torch.Size([1, 3, 198, 198])
grad_out size torch.Size([1, 3, 196, 196])
crop [18, 18, 18, 18] [942, 1137, 142, 337] [960, 1119, 160, 319]
## 18 178 18 178
##############return grad_in in conv2d tensor([ 0.0000e+00, -1.6061e-06,  6.2538e-06, -4.6430e-06, -1.3542e-06,
        -1.6988e-05,  2.4079e-05, -8.4796e-07, -2.1836e-06, -3.2066e-05],
       device='cuda:0')
I am [1, 6]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 200, 200])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 200, 200])
grad_input new torch.Size([1, 3, 200, 200])
grad_input torch.Size([1, 3, 200, 200])
grad_out size torch.Size([1, 3, 198, 198])
crop [19, 19, 19, 19] [941, 1138, 141, 338] [960, 1119, 160, 319]
## 19 179 19 179
##############return grad_in in conv2d tensor([ 0.0000e+00, -4.8763e-07,  1.7768e-06, -1.0044e-06,  2.3795e-06,
        -7.3585e-06,  1.1046e-05, -4.2617e-06,  9.9688e-06, -1.1922e-05],
       device='cuda:0')
I am [1, 6]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 202, 202])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 200, 200])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 202, 202])
grad_out size torch.Size([1, 3, 200, 200])
crop [20, 20, 20, 20] [940, 1139, 140, 339] [960, 1119, 160, 319]
## 20 180 20 180
##############return grad_in in conv2d tensor([ 0.0000e+00,  3.8657e-08, -3.8294e-08, -4.5120e-07,  5.0962e-08,
         8.1228e-07,  3.2463e-09, -8.9306e-07, -1.3882e-06,  1.2163e-06],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 404, 404])
grad_out size torch.Size([1, 3, 202, 202])
arg size torch.Size([1, 3, 202, 202])
##############grad_in in maxp torch.Size([1, 3, 404, 404])
I am [1, 6]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 406, 406])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 404, 404])
grad_input torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 406, 406])
grad_input new torch.Size([1, 3, 406, 406])
new grad_input torch.Size([1, 3, 406, 406])
grad_out size torch.Size([1, 3, 404, 404])
crop [42, 42, 42, 42] [1878, 2281, 278, 681] [1920, 2239, 320, 639]
## 42 362 42 362
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  7.4917e-09, -3.5502e-08,
         4.0403e-08,  2.2727e-08,  2.5614e-08, -8.5796e-09,  4.8895e-08],
       device='cuda:0')
I am [1, 6]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 408, 408])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 408, 408])
grad_input new torch.Size([1, 3, 408, 408])
grad_input torch.Size([1, 3, 408, 408])
grad_out size torch.Size([1, 3, 406, 406])
crop [43, 43, 43, 43] [1877, 2282, 277, 682] [1920, 2239, 320, 639]
## 43 363 43 363
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.7763e-09,  6.2831e-09,
         1.0456e-08, -6.1239e-09,  5.0337e-09,  1.2492e-08, -1.7820e-08],
       device='cuda:0')
I am [1, 6]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 410, 410])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 408, 408])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 410, 410])
grad_out size torch.Size([1, 3, 408, 408])
crop [44, 44, 44, 44] [1876, 2283, 276, 683] [1920, 2239, 320, 639]
## 44 364 44 364
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cCheckpointBackward
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.9935e-10, -4.9921e-10,
        -3.6397e-09, -2.4090e-09, -2.2940e-09, -5.0289e-09,  5.4634e-10],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 820, 820])
grad_out size torch.Size([1, 3, 410, 410])
arg size torch.Size([1, 3, 410, 410])
##############grad_in in maxp torch.Size([1, 3, 820, 820])
I am [1, 6]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 822, 822])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 820, 820])
grad_input torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 822, 822])
grad_input new torch.Size([1, 3, 822, 822])
new grad_input torch.Size([1, 3, 822, 822])
grad_out size torch.Size([1, 3, 820, 820])
crop [90, 90, 90, 90] [3750, 4569, 550, 1369] [3840, 4479, 640, 1279]
## 90 730 90 730
##############return grad_in in conv2d tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        7.9506e-11, 4.8451e-11, 3.1369e-10, 2.3042e-10], device='cuda:0')
I am [1, 6]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 824, 824])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 824, 824])
grad_out size torch.Size([1, 3, 822, 822])
crop [91, 91, 91, 91] [3749, 4570, 549, 1370] [3840, 4479, 640, 1279]
## 91 731 91 731
##############return grad_in in conv2d tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        1.2477e-11, 3.4467e-11, 6.5522e-11, 1.3324e-10], device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 1648, 1648])
grad_out size torch.Size([1, 3, 824, 824])
arg size torch.Size([1, 3, 824, 824])
##############grad_in in maxp torch.Size([1, 3, 1648, 1648])
I am [1, 6]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 1650, 1650])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1648, 1648])
grad_input torch.Size([1, 3, 1650, 1650])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 1650, 1650])
grad_input new torch.Size([1, 3, 1650, 1650])
new grad_input torch.Size([1, 3, 1650, 1650])
grad_out size torch.Size([1, 3, 1648, 1648])
crop [184, 184, 184, 184] [7496, 9143, 1096, 2743] [7680, 8959, 1280, 2559]
## 184 1464 184 1464
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [1, 6]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 3, 1652, 1652])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1650, 1650])
final torch.Size([1, 3, 1652, 1652])
padding info :: [0, 0, 0, 0]
reshape_for_final ## 186 1466 186 1466
after crop g_in torch.Size([1, 3, 1280, 1280])
grad_out size torch.Size([1, 3, 1650, 1650])
crop [185, 185, 185, 185] [7495, 9144, 1095, 2744] [7680, 8959, 1280, 2559]
## 185 1465 185 1465
##############return grad_in in conv2d tensor([-4.3015e-08, -2.5787e-07,  1.8889e-07,  6.5150e-08, -3.3953e-07,
         4.7673e-07,  3.3295e-08, -8.5087e-08,  3.4298e-07, -1.4916e-09],
       device='cuda:0')
[8, 8]
HREREERE torch.Size([1, 3, 10240, 10240])

############# Enter checkpointing bkward ####
inputs len 5 4
in customized Sequential 5
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 5]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <1,5,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 5]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <1,5,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 5]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <1,5,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 5]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <1,5,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 5]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <1,5,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 5]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <1,5,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 5]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <1,5,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 5]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <1,5,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 5]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <1,5,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 5]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <1,5,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id [torch/csrc/autograd/python_engine.cpp] THPEngine_run_backward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 5]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <1,5,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 5]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <1,5,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 5]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <1,5,>,

in customized Sequential 3
need to get existing

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 92, 92])
grad_out size torch.Size([1, 3, 320, 320])
arg size torch.Size([1, 3, 46, 46])
new_grad_out torch.Size([1, 3, 46, 46])
##############grad_in in maxp torch.Size([1, 3, 92, 92])
I am [1, 5]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 94, 94])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 92, 92])
grad_input torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 94, 94])
grad_input new torch.Size([1, 3, 94, 94])
new grad_input torch.Size([1, 3, 94, 94])
grad_out size torch.Size([1, 3, 92, 92])
crop [6, 6, 6, 6] [394, 485, 74, 165] [400, 479, 80, 159]
## 6 86 6 86
##############return grad_in in conv2d tensor([ 0.0000,  0.0014, -0.0021,  0.0002,  0.0006, -0.0002,  0.0000,  0.0000,
         0.0009,  0.0008], device='cuda:0')
I am [1, 5]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 96, 96])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 96, 96])
grad_input new torch.Size([1, 3, 96, 96])
grad_input torch.Size([1, 3, 96, 96])
grad_out size torch.Size([1, 3, 94, 94])
crop [7, 7, 7, 7] [393, 486, 73, 166] [400, 479, 80, 159]
## 7 87 7 87
##############return grad_in in conv2d tensor([ 0.0000e+00,  2.6737e-04,  1.5116e-04,  4.9973e-04,  4.4958e-04,
        -8.3532e-05, -5.1989e-05,  4.0730e-06,  5.2809e-04, -5.1536e-04],
       device='cuda:0')
I am [1, 5]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 98, 98])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 96, 96])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 98, 98])
grad_out size torch.Size([1, 3, 96, 96])
crop [8, 8, 8, 8] [392, 487, 72, 167] [400, 479, 80, 159]
## 8 88 8 88
##############return grad_in in conv2d tensor([ 0.0000e+00, -6.4515e-06,  7.6267e-05, -6.4437e-05,  1.0446e-04,
        -2.9503e-05, -9.6826e-05,  8.8970e-06,  8.9474e-05, -1.1230e-04],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 196, 196])
grad_out size torch.Size([1, 3, 98, 98])
arg size torch.Size([1, 3, 98, 98])
##############grad_in in maxp torch.Size([1, 3, 196, 196])
I am [1, 5]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 198, 198])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 196, 196])
grad_input torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 198, 198])
grad_input new torch.Size([1, 3, 198, 198])
new grad_input torch.Size([1, 3, 198, 198])
grad_out size torch.Size([1, 3, 196, 196])
crop [18, 18, 18, 18] [782, 977, 142, 337] [800, 959, 160, 319]
## 18 178 18 178
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  7.4679e-06,  2.0263e-06,
        -7.0819e-07, -1.6118e-05, -5.4462e-07, -1.0042e-06,  3.1397e-05],
       device='cuda:0')
I am [1, 5]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 200, 200])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 200, 200])
grad_input new torch.Size([1, 3, 200, 200])
grad_input torch.Size([1, 3, 200, 200])
grad_out size torch.Size([1, 3, 198, 198])
crop [19, 19, 19, 19] [781, 978, 141, 338] [800, 959, 160, 319]
## 19 179 19 179
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  2.2674e-06,  7.4473e-07,
         9.8940e-07, -2.8910e-06, -6.2959e-07, -3.7774e-06,  6.3213e-06],
       device='cuda:0')
I am [1, 5]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 202, 202])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 200, 200])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 202, 202])
grad_out size torch.Size([1, 3, 200, 200])
crop [20, 20, 20, 20] [780, 979, 140, 339] [800, 959, 160, 319]
## 20 180 20 180
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00, -1.7975e-07, -5.1272e-07,
        -1.0780e-07,  1.6534e-07,  7.5013e-07,  4.2076e-07, -1.7054e-07],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 404, 404])
grad_out size torch.Size([1, 3, 202, 202])
arg size torch.Size([1, 3, 202, 202])
##############grad_in in maxp torch.Size([1, 3, 404, 404])
I am [1, 5]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 406, 406])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 404, 404])
grad_input torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 406, 406])
grad_input new torch.Size([1, 3, 406, 406])
new grad_input torch.Size([1, 3, 406, 406])
grad_out size torch.Size([1, 3, 404, 404])
crop [42, 42, 42, 42] [1558, 1961, 278, 681] [1600, 1919, 320, 639]
## 42 362 42 362
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00, -4.3747e-08,  2.1523e-08,  7.2291e-08],
       device='cuda:0')
I am [1, 5]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 408, 408])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 408, 408])
grad_input new torch.Size([1, 3, 408, 408])
grad_input torch.Size([1, 3, 408, 408])
grad_out size torch.Size([1, 3, 406, 406])
crop [43, 43, 43, 43] [1557, 1962, 277, 682] [1600, 1919, 320, 639]
## 43 363 43 363
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  5.7860e-09,  2.3440e-08, -4.6361e-09],
       device='cuda:0')
I am [1, 5]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 410, 410])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 408, 408])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 410, 410])
grad_out size torch.Size([1, 3, 408, 408])
crop [44, 44, 44, 44] [1556, 1963, 276, 683] [1600, 1919, 320, 639]
## 44 364 44 364
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  2.1750e-10, -4.2493e-09, -5.4455e-09],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 820, 820])
grad_out size torch.Size([1, 3, 410, 410])
arg size torch.Size([1, 3, 410, 410])
##############grad_in in maxp torch.Size([1, 3, 820, 820])
I am [1, 5]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 822, 822])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape [torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cCheckpointBackward
torch.Size([1, 3, 820, 820])
grad_input torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 822, 822])
grad_input new torch.Size([1, 3, 822, 822])
new grad_input torch.Size([1, 3, 822, 822])
grad_out size torch.Size([1, 3, 820, 820])
crop [90, 90, 90, 90] [3110, 3929, 550, 1369] [3200, 3839, 640, 1279]
## 90 730 90 730
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [1, 5]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 824, 824])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 824, 824])
grad_out size torch.Size([1, 3, 822, 822])
crop [91, 91, 91, 91] [3109, 3930, 549, 1370] [3200, 3839, 640, 1279]
## 91 731 91 731
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 1648, 1648])
grad_out size torch.Size([1, 3, 824, 824])
arg size torch.Size([1, 3, 824, 824])
##############grad_in in maxp torch.Size([1, 3, 1648, 1648])
I am [1, 5]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 1650, 1650])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1648, 1648])
grad_input torch.Size([1, 3, 1650, 1650])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 1650, 1650])
grad_input new torch.Size([1, 3, 1650, 1650])
new grad_input torch.Size([1, 3, 1650, 1650])
grad_out size torch.Size([1, 3, 1648, 1648])
crop [184, 184, 184, 184] [6216, 7863, 1096, 2743] [6400, 7679, 1280, 2559]
## 184 1464 184 1464
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [1, 5]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 3, 1652, 1652])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1650, 1650])
final torch.Size([1, 3, 1652, 1652])
padding info :: [0, 0, 0, 0]
reshape_for_final ## 186 1466 186 1466
after crop g_in torch.Size([1, 3, 1280, 1280])
grad_out size torch.Size([1, 3, 1650, 1650])
crop [185, 185, 185, 185] [6215, 7864, 1095, 2744] [6400, 7679, 1280, 2559]
## 185 1465 185 1465
##############return grad_in in conv2d tensor([-3.2642e-07, -1.9234e-07,  2.8783e-08, -1.8066e-07,  1.1554e-07,
        -2.0171e-07, -1.3353e-07,  1.4167e-07,  7.2779e-08,  3.6114e-08],
       device='cuda:0')
[8, 8]
HREREERE torch.Size([1, 3, 10240, 10240])

############# Enter checkpointing bkward ####
inputs len 5 4
in customized Sequential 5
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 4]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <1,4,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 4]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <1,4,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 4]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <1,4,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 4]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <1,4,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 4]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <1,4,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 4]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <1,4,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 4]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <1,4,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 4]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <1,4,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 4]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <1,4,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 4]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <1,4,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 4]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <1,4,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() [torch/csrc/autograd/python_engine.cpp] THPEngine_run_backward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 4]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <1,4,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 4]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <1,4,>,

in customized Sequential 3
need to get existing

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 92, 92])
grad_out size torch.Size([1, 3, 320, 320])
arg size torch.Size([1, 3, 46, 46])
new_grad_out torch.Size([1, 3, 46, 46])
##############grad_in in maxp torch.Size([1, 3, 92, 92])
I am [1, 4]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 94, 94])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 92, 92])
grad_input torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 94, 94])
grad_input new torch.Size([1, 3, 94, 94])
new grad_input torch.Size([1, 3, 94, 94])
grad_out size torch.Size([1, 3, 92, 92])
crop [6, 6, 6, 6] [314, 405, 74, 165] [320, 399, 80, 159]
## 6 86 6 86
##############return grad_in in conv2d tensor([-5.4013e-05,  2.0218e-03, -2.8152e-04,  0.0000e+00,  0.0000e+00,
        -1.1655e-03,  3.2211e-03, -5.0569e-04,  4.1927e-03, -4.4278e-04],
       device='cuda:0')
I am [1, 4]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 96, 96])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 96, 96])
grad_input new torch.Size([1, 3, 96, 96])
grad_input torch.Size([1, 3, 96, 96])
grad_out size torch.Size([1, 3, 94, 94])
crop [7, 7, 7, 7] [313, 406, 73, 166] [320, 399, 80, 159]
## 7 87 7 87
##############return grad_in in conv2d tensor([ 0.0004, -0.0005,  0.0005, -0.0003,  0.0001,  0.0007, -0.0007,  0.0015,
        -0.0014,  0.0002], device='cuda:0')
I am [1, 4]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 98, 98])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 96, 96])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 98, 98])
grad_out size torch.Size([1, 3, 96, 96])
crop [8, 8, 8, 8] [312, 407, 72, 167] [320, 399, 80, 159]
## 8 88 8 88
##############return grad_in in conv2d tensor([ 8.2554e-05, -1.5622e-04, -3.5411e-05,  1.3511e-04, -3.9960e-05,
         1.8435e-04, -3.0329e-04,  1.1992e-04, -2.3620e-05, -2.3365e-04],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 196, 196])
grad_out size torch.Size([1, 3, 98, 98])
arg size torch.Size([1, 3, 98, 98])
##############grad_in in maxp torch.Size([1, 3, 196, 196])
I am [1, 4]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 198, 198])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 196, 196])
grad_input torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 198, 198])
grad_input new torch.Size([1, 3, 198, 198])
new grad_input torch.Size([1, 3, 198, 198])
grad_out size torch.Size([1, 3, 196, 196])
crop [18, 18, 18, 18] [622, 817, 142, 337] [640, 799, 160, 319]
## 18 178 18 178
##############return grad_in in conv2d tensor([ 4.0377e-06,  2.9739e-06, -1.0879e-05, -1.2074e-05,  5.9637e-05,
        -1.2407e-05, -1.9713e-05,  9.9186e-07,  0.0000e+00,  0.0000e+00],
       device='cuda:0')
I am [1, 4]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 200, 200])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 200, 200])
grad_input new torch.Size([1, 3, 200, 200])
grad_input torch.Size([1, 3, 200, 200])
grad_out size torch.Size([1, 3, 198, 198])
crop [19, 19, 19, 19] [621, 818, 141, 338] [640, 799, 160, 319]
## 19 179 19 179
##############return grad_in in conv2d tensor([ 1.1724e-06,  7.5248e-08, -8.0667e-07,  2.2697e-07,  9.4566e-06,
        -1.7990e-05,  1.8032e-05, -1.0149e-05, -1.9108e-06,  1.9940e-07],
       device='cuda:0')
I am [1, 4]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 202, 202])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 200, 200])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 202, 202])
grad_out size torch.Size([1, 3, 200, 200])
crop [20, 20, 20, 20] [620, 819, 140, 339] [640, 799, 160, 319]
## 20 180 20 180
##############return grad_in in conv2d tensor([-5.2082e-08, -2.6767e-07, -4.4488e-07,  5.7959e-07,  1.3486e-06,
        -2.0489e-06, -2.9821e-06,  3.7811e-06,  3.2908e-07, -1.5323e-06],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 404, 404])
grad_out size torch.Size([1, 3, 202, 202])
arg size torch.Size([1, 3, 202, 202])
##############grad_in in maxp torch.Size([1, 3, 404, 404])
I am [1, 4]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 406, 406])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 404, 404])
grad_input torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 406, 406])
grad_input new torch.Size([1, 3, 406, 406])
new grad_input torch.Size([1, 3, 406, 406])
grad_out size torch.Size([1, 3, 404, 404])
crop [42, 42, 42, 42] [1238, 1641, 278, 681] [1280, 1599, 320, 639]
## 42 362 42 362
##############return grad_in in conv2d tensor([ 0.0000e+00, -2.1315e-08,  3.4537e-08, -3.7033e-09, -6.0789e-08,
        -4.0815e-08,  1.9037e-07, -3.6212e-08, -1.4215e-07,  6.1246e-09],
       device='cuda:0')
I am [1, 4]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 408, 408])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 408, 408])
grad_input new torch.Size([1, 3, 408, 408])
grad_input torch.Size([1, 3, 408, 408])
grad_out size torch.Size([1, 3, 406, 406])
crop [43, 43, 43, 43] [1237, 1642, 277, 682] [1280, 1599, 320, 639]
## 43 363 43 363
##############return grad_in in conv2d tensor([ 0.0000e+00,  2.3796e-10,  7.0872e-09,  3.8334e-09, -1.8635e-08,
        -1.5729e-08, -9.8887e-09, -1.9603e-08,  6.1539e-08,  3.7005e-08],
       device='cuda:0')
I am [1, 4]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 410, 410])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 408, 408])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 410, 410])
grad_out size torch.Size([1, 3, 408, 408])
crop [44, 44, 44, 44] [1236, 1643, 276, 683] [1280, 1599, 320, 639]
## 44 364 44 364
##############return grad_in in conv2d tensor([ 0.0000e+00, -1.2609e-10, -9.5790e-10, -1.7614e-09, -2.0757e-09,
         1.6096e-09,  9.6287e-09,  1.4645e-08, -3.0969e-09, -1.2412e-08],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 820, 820])
grad_out size torch.Size([1, 3, 410, 410])
arg size torch.Size([1, 3, 410, 410])
##############grad_in in maxp torch.Size([1, 3, 820, 820])
I am [1, 4]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 822, 822])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 820, 820])
grad_input torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 822, 822])
grad_input new torch.Size([1, 3, 822, 822])
new grad_input torch.Size([1, 3, 822, 822])
grad_out size torch.Size([1, 3, 820, 820])
crop [90, 90, 90, 90] [2470, 3289, 550, 1369] [2560, 3199, 640, 1279]
## 90 730 90 730
##############return grad_in in conv2d [torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cCheckpointBackward
tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00, -2.3615e-11, -1.8382e-11,
        -1.6062e-10, -1.3965e-10,  1.4263e-10, -6.1837e-10, -7.0403e-10],
       device='cuda:0')
I am [1, 4]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 824, 824])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 824, 824])
grad_out size torch.Size([1, 3, 822, 822])
crop [91, 91, 91, 91] [2469, 3290, 549, 1370] [2560, 3199, 640, 1279]
## 91 731 91 731
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.4315e-13, -6.1750e-13,
        -4.7392e-12, -4.6416e-12, -4.0366e-11, -8.6442e-11, -3.2050e-11],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 1648, 1648])
grad_out size torch.Size([1, 3, 824, 824])
arg size torch.Size([1, 3, 824, 824])
##############grad_in in maxp torch.Size([1, 3, 1648, 1648])
I am [1, 4]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 1650, 1650])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1648, 1648])
grad_input torch.Size([1, 3, 1650, 1650])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 1650, 1650])
grad_input new torch.Size([1, 3, 1650, 1650])
new grad_input torch.Size([1, 3, 1650, 1650])
grad_out size torch.Size([1, 3, 1648, 1648])
crop [184, 184, 184, 184] [4936, 6583, 1096, 2743] [5120, 6399, 1280, 2559]
## 184 1464 184 1464
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [1, 4]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 3, 1652, 1652])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1650, 1650])
final torch.Size([1, 3, 1652, 1652])
padding info :: [0, 0, 0, 0]
reshape_for_final ## 186 1466 186 1466
after crop g_in torch.Size([1, 3, 1280, 1280])
grad_out size torch.Size([1, 3, 1650, 1650])
crop [185, 185, 185, 185] [4935, 6584, 1095, 2744] [5120, 6399, 1280, 2559]
## 185 1465 185 1465
##############return grad_in in conv2d tensor([ 1.7887e-07, -4.2299e-08,  3.0388e-07,  2.2478e-07,  1.1515e-07,
        -4.1344e-08, -5.3091e-08,  1.5555e-07,  2.1376e-07,  7.2701e-08],
       device='cuda:0')
[8, 8]
HREREERE torch.Size([1, 3, 10240, 10240])

############# Enter checkpointing bkward ####
inputs len 5 4
in customized Sequential 5
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 3]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <1,3,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 3]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <1,3,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 3]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <1,3,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 3]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <1,3,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 3]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <1,3,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 3]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <1,3,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 3]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <1,3,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 3]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <1,3,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 3]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <1,3,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 3]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <1,3,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 3]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <1,3,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 3]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <1,3,>,

in customized Sequential 3
id 140045013206976[torch/csrc/autograd/python_engine.cpp] THPEngine_run_backward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward

== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 3]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <1,3,>,

in customized Sequential 3
need to get existing

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 92, 92])
grad_out size torch.Size([1, 3, 320, 320])
arg size torch.Size([1, 3, 46, 46])
new_grad_out torch.Size([1, 3, 46, 46])
##############grad_in in maxp torch.Size([1, 3, 92, 92])
I am [1, 3]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 94, 94])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 92, 92])
grad_input torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 94, 94])
grad_input new torch.Size([1, 3, 94, 94])
new grad_input torch.Size([1, 3, 94, 94])
grad_out size torch.Size([1, 3, 92, 92])
crop [6, 6, 6, 6] [234, 325, 74, 165] [240, 319, 80, 159]
## 6 86 6 86
##############return grad_in in conv2d tensor([-0.0003, -0.0028,  0.0012,  0.0006, -0.0022, -0.0059,  0.0007, -0.0028,
         0.0008, -0.0009], device='cuda:0')
I am [1, 3]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 96, 96])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 96, 96])
grad_input new torch.Size([1, 3, 96, 96])
grad_input torch.Size([1, 3, 96, 96])
grad_out size torch.Size([1, 3, 94, 94])
crop [7, 7, 7, 7] [233, 326, 73, 166] [240, 319, 80, 159]
## 7 87 7 87
##############return grad_in in conv2d tensor([-0.0002,  0.0005, -0.0001,  0.0010, -0.0016,  0.0020, -0.0011,  0.0013,
        -0.0003, -0.0003], device='cuda:0')
I am [1, 3]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 98, 98])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 96, 96])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 98, 98])
grad_out size torch.Size([1, 3, 96, 96])
crop [8, 8, 8, 8] [232, 327, 72, 167] [240, 319, 80, 159]
## 8 88 8 88
##############return grad_in in conv2d tensor([-3.0922e-05,  1.2878e-04, -7.6114e-05,  5.5898e-05, -1.5370e-04,
         2.1980e-04,  2.4243e-04, -2.5615e-04,  5.0393e-05, -3.1431e-04],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 196, 196])
grad_out size torch.Size([1, 3, 98, 98])
arg size torch.Size([1, 3, 98, 98])
##############grad_in in maxp torch.Size([1, 3, 196, 196])
I am [1, 3]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 198, 198])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 196, 196])
grad_input torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 198, 198])
grad_input new torch.Size([1, 3, 198, 198])
new grad_input torch.Size([1, 3, 198, 198])
grad_out size torch.Size([1, 3, 196, 196])
crop [18, 18, 18, 18] [462, 657, 142, 337] [480, 639, 160, 319]
## 18 178 18 178
##############return grad_in in conv2d tensor([-9.3291e-07,  3.1860e-08,  7.7448e-06, -6.2923e-06, -4.1757e-06,
        -1.0432e-07,  1.3381e-05,  2.8938e-05, -1.9115e-05, -1.8939e-05],
       device='cuda:0')
I am [1, 3]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 200, 200])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 200, 200])
grad_input new torch.Size([1, 3, 200, 200])
grad_input torch.Size([1, 3, 200, 200])
grad_out size torch.Size([1, 3, 198, 198])
crop [19, 19, 19, 19] [461, 658, 141, 338] [480, 639, 160, 319]
## 19 179 19 179
##############return grad_in in conv2d tensor([-2.8175e-07, -1.9119e-08,  1.8846e-06, -2.2042e-07,  5.6131e-06,
        -6.7526e-06,  6.5916e-06,  1.0206e-05, -6.3652e-06,  3.4948e-06],
       device='cuda:0')
I am [1, 3]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 202, 202])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 200, 200])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 202, 202])
grad_out size torch.Size([1, 3, 200, 200])
crop [20, 20, 20, 20] [460, 659, 140, 339] [480, 639, 160, 319]
## 20 180 20 180
##############return grad_in in conv2d tensor([ 2.2311e-08,  6.2162e-08, -1.5737e-07, -7.8865e-07, -3.5609e-07,
         1.6281e-06, -4.7348e-07, -2.2200e-06, -8.0156e-07,  1.9520e-07],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 404, 404])
grad_out size torch.Size([1, 3, 202, 202])
arg size torch.Size([1, 3, 202, 202])
##############grad_in in maxp torch.Size([1, 3, 404, 404])
I am [1, 3]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 406, 406])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 404, 404])
grad_input torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 406, 406])
grad_input new torch.Size([1, 3, 406, 406])
new grad_input torch.Size([1, 3, 406, 406])
grad_out size torch.Size([1, 3, 404, 404])
crop [42, 42, 42, 42] [918, 1321, 278, 681] [960, 1279, 320, 639]
## 42 362 42 362
##############return grad_in in conv2d tensor([ 3.1641e-09,  7.3364e-09, -7.0089e-09,  1.3432e-09, -1.4601e-08,
        -2.9638e-08, -6.0153e-08, -1.4455e-07,  9.7821e-08, -2.3118e-08],
       device='cuda:0')
I am [1, 3]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 408, 408])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 408, 408])
grad_input new torch.Size([1, 3, 408, 408])
grad_input torch.Size([1, 3, 408, 408])
grad_out size torch.Size([1, 3, 406, 406])
crop [43, 43, 43, 43] [917, 1322, 277, 682] [960, 1279, 320, 639]
## 43 363 43 363
##############return grad_in in conv2d tensor([ 5.6440e-10,  5.0759e-10, -1.6695e-09,  4.1272e-09, -3.3939e-10,
        -1.2318e-08, -1.9204e-08, -3.2101e-08, -4.4102e-08, -2.4466e-08],
       device='cuda:0')
I am [1, 3]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 410, 410])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 408, 408])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 410, 410])
grad_out size torch.Size([1, 3, 408, 408])
crop [44, 44, 44, 44] [916, 1323, 276, 683] [960, 1279, 320, 639]
## 44 364 44 364
##############return grad_in in conv2d tensor([ 6.5597e-11, -2.3060e-10,  1.2636e-10, -1.7638e-10, -9.6582e-10,
         1.0679e-10,  9.8060e-10,  7.8753e-09,  1.7874e-08,  2.0122e-08],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 820, 820])
grad_out size torch.Size([1, 3, 410, 410])
arg size torch.Size([1, 3, 410, 410])
##############grad_in in maxp torch.Size([1, 3, 820, 820])
I am [1, 3]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 822, 822])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 820, 820])
grad_input torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 822, 822])
grad_input new torch.Size([1, 3, 822, 822])
new grad_input torch.Size([1, 3, 822, 822])
grad_out size torch.Size([1, 3, 820, 820])
crop [90, 90, 90, 90] [1830, 2649, 550, 1369] [1920, 2559, 640, 1279]
## 90 730 90 730
##############return grad_in in conv2d tensor([ 4.5811e-11,  2.5897e-11, -1.6882e-11,  9.5445e-13, -7.1993e-11,
        -5.6384e-11, -5.9720e-11,  0.0000e+00,  0.0000e+00,  0.0000e+00],
       device='cuda:0')
I am [1, 3]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 824, 824])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 824, 824])
grad_out size torch.Size([1, 3, 822, 822])
crop [91, 91, 91, 91] [1829, 2650, 549, 1370] [1920, 2559, 640, 1279]
## 91 731 91 731
##############return grad_in in conv2d [torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cCheckpointBackward
tensor([ 6.8537e-12,  1.3162e-11,  6.3631e-12,  5.3765e-12, -1.8107e-11,
        -3.0745e-11, -2.8148e-12, -1.3174e-13, -9.1410e-12,  0.0000e+00],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 1648, 1648])
grad_out size torch.Size([1, 3, 824, 824])
arg size torch.Size([1, 3, 824, 824])
##############grad_in in maxp torch.Size([1, 3, 1648, 1648])
I am [1, 3]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 1650, 1650])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1648, 1648])
grad_input torch.Size([1, 3, 1650, 1650])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 1650, 1650])
grad_input new torch.Size([1, 3, 1650, 1650])
new grad_input torch.Size([1, 3, 1650, 1650])
grad_out size torch.Size([1, 3, 1648, 1648])
crop [184, 184, 184, 184] [3656, 5303, 1096, 2743] [3840, 5119, 1280, 2559]
## 184 1464 184 1464
##############return grad_in in conv2d tensor([-8.3608e-13,  7.5939e-13,  3.0000e-13, -1.5193e-12,  1.6788e-13,
         1.4455e-13, -3.6618e-14, -8.6152e-13,  1.9183e-12,  2.5383e-12],
       device='cuda:0')
I am [1, 3]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 3, 1652, 1652])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1650, 1650])
final torch.Size([1, 3, 1652, 1652])
padding info :: [0, 0, 0, 0]
reshape_for_final ## 186 1466 186 1466
after crop g_in torch.Size([1, 3, 1280, 1280])
grad_out size torch.Size([1, 3, 1650, 1650])
crop [185, 185, 185, 185] [3655, 5304, 1095, 2744] [3840, 5119, 1280, 2559]
## 185 1465 185 1465
##############return grad_in in conv2d tensor([-3.5514e-07, -1.6603e-08,  2.6438e-07, -6.4224e-07, -1.9386e-07,
         1.3229e-07, -2.2979e-07,  1.2747e-07,  1.3956e-07, -3.6888e-07],
       device='cuda:0')
[8, 8]
HREREERE torch.Size([1, 3, 10240, 10240])

############# Enter checkpointing bkward ####
inputs len 5 4
in customized Sequential 5
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 2]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <1,2,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 2]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <1,2,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 2]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <1,2,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 2]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <1,2,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 2]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <1,2,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 2]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <1,2,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 2]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <1,2,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 2]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <1,2,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 2]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <1,2,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 2]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <1,2,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 2]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <1,2,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 2]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <1,2,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 2]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <1,2,>,

in customized Sequential 3
need to get existing
[torch/csrc/autograd/python_engine.cpp] THPEngine_run_backward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 92, 92])
grad_out size torch.Size([1, 3, 320, 320])
arg size torch.Size([1, 3, 46, 46])
new_grad_out torch.Size([1, 3, 46, 46])
##############grad_in in maxp torch.Size([1, 3, 92, 92])
I am [1, 2]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 94, 94])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 92, 92])
grad_input torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 94, 94])
grad_input new torch.Size([1, 3, 94, 94])
new grad_input torch.Size([1, 3, 94, 94])
grad_out size torch.Size([1, 3, 92, 92])
crop [6, 6, 6, 6] [154, 245, 74, 165] [160, 239, 80, 159]
## 6 86 6 86
##############return grad_in in conv2d tensor([ 4.2941e-04, -1.0645e-03, -1.3174e-03,  3.7657e-06, -3.4264e-03,
         8.7991e-04,  8.4770e-04,  3.7050e-04, -1.7543e-04, -9.7837e-05],
       device='cuda:0')
I am [1, 2]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 96, 96])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 96, 96])
grad_input new torch.Size([1, 3, 96, 96])
grad_input torch.Size([1, 3, 96, 96])
grad_out size torch.Size([1, 3, 94, 94])
crop [7, 7, 7, 7] [153, 246, 73, 166] [160, 239, 80, 159]
## 7 87 7 87
##############return grad_in in conv2d tensor([ 7.9785e-05, -2.0620e-04,  3.7399e-04,  1.7572e-04, -1.0130e-03,
         5.4001e-04, -9.0479e-04, -8.5123e-04,  2.2586e-04, -1.8001e-04],
       device='cuda:0')
I am [1, 2]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 98, 98])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 96, 96])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 98, 98])
grad_out size torch.Size([1, 3, 96, 96])
crop [8, 8, 8, 8] [152, 247, 72, 167] [160, 239, 80, 159]
## 8 88 8 88
##############return grad_in in conv2d tensor([-1.9251e-06, -1.7844e-05,  4.2828e-05,  1.1959e-04, -1.4792e-04,
        -2.5135e-05,  2.5842e-04, -4.4087e-04, -3.7529e-06,  1.9119e-04],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 196, 196])
grad_out size torch.Size([1, 3, 98, 98])
arg size torch.Size([1, 3, 98, 98])
##############grad_in in maxp torch.Size([1, 3, 196, 196])
I am [1, 2]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 198, 198])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 196, 196])
grad_input torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 198, 198])
grad_input new torch.Size([1, 3, 198, 198])
new grad_input torch.Size([1, 3, 198, 198])
grad_out size torch.Size([1, 3, 196, 196])
crop [18, 18, 18, 18] [302, 497, 142, 337] [320, 479, 160, 319]
## 18 178 18 178
##############return grad_in in conv2d tensor([-1.1557e-07,  2.5470e-06, -2.1348e-08,  1.8759e-06,  1.7798e-06,
         3.0726e-06, -4.5161e-06, -1.3059e-06, -2.9878e-05,  1.9378e-05],
       device='cuda:0')
I am [1, 2]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 200, 200])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 200, 200])
grad_input new torch.Size([1, 3, 200, 200])
grad_input torch.Size([1, 3, 200, 200])
grad_out size torch.Size([1, 3, 198, 198])
crop [19, 19, 19, 19] [301, 498, 141, 338] [320, 479, 160, 319]
## 19 179 19 179
##############return grad_in in conv2d tensor([-3.1324e-08,  7.4223e-07, -3.5139e-08,  1.2651e-06,  6.6891e-07,
         8.5151e-07,  8.0049e-07,  1.9387e-06, -1.1463e-05,  5.0079e-06],
       device='cuda:0')
I am [1, 2]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 202, 202])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 200, 200])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 202, 202])
grad_out size torch.Size([1, 3, 200, 200])
crop [20, 20, 20, 20] [300, 499, 140, 339] [320, 479, 160, 319]
## 20 180 20 180
##############return grad_in in conv2d tensor([ 2.4210e-09, -4.6655e-08, -1.5730e-07, -8.5796e-08, -1.4741e-07,
        -2.1066e-07, -4.0050e-07, -1.5066e-07,  1.3580e-06,  1.8723e-06],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 404, 404])
grad_out size torch.Size([1, 3, 202, 202])
arg size torch.Size([1, 3, 202, 202])
##############grad_in in maxp torch.Size([1, 3, 404, 404])
I am [1, 2]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 406, 406])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 404, 404])
grad_input torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 406, 406])
grad_input new torch.Size([1, 3, 406, 406])
new grad_input torch.Size([1, 3, 406, 406])
grad_out size torch.Size([1, 3, 404, 404])
crop [42, 42, 42, 42] [598, 1001, 278, 681] [640, 959, 320, 639]
## 42 362 42 362
##############return grad_in in conv2d tensor([ 0.0000e+00,  4.7791e-10, -3.4666e-09,  1.1887e-08, -3.7726e-08,
        -1.3328e-08, -5.2769e-09,  1.9972e-08,  4.2057e-09,  2.7656e-09],
       device='cuda:0')
I am [1, 2]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 408, 408])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 408, 408])
grad_input new torch.Size([1, 3, 408, 408])
grad_input torch.Size([1, 3, 408, 408])
grad_out size torch.Size([1, 3, 406, 406])
crop [43, 43, 43, 43] [597, 1002, 277, 682] [640, 959, 320, 639]
## 43 363 43 363
##############return grad_in in conv2d tensor([ 0.0000e+00,  1.1472e-10, -1.1368e-09, -1.6902e-10,  2.8970e-09,
        -1.1751e-08, -5.2291e-09, -2.7582e-10,  4.3001e-09, -2.7518e-09],
       device='cuda:0')
I am [1, 2]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 410, 410])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 408, 408])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 410, 410])
grad_out size torch.Size([1, 3, 408, 408])
crop [44, 44, 44, 44] [596, 1003, 276, 683] [640, 959, 320, 639]
## 44 364 44 364
##############return grad_in in conv2d tensor([ 0.0000e+00,  1.2857e-11, -1.7854e-10,  4.6021e-10, -1.1089e-09,
         1.1912e-09,  2.4629e-09,  3.2881e-09,  5.7695e-10, -2.8273e-10],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 820, 820])
grad_out size torch.Size([1, 3, 410, 410])
arg size torch.Size([1, 3, 410, 410])
##############grad_in in maxp torch.Size([1, 3, 820, 820])
I am [1, 2]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 822, 822])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 820, 820])
grad_input torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 822, 822])
grad_input new torch.Size([1, 3, 822, 822])
new grad_input torch.Size([1, 3, 822, 822])
grad_out size torch.Size([1, 3, 820, 820])
crop [90, 90, 90, 90] [1190, 2009, 550, 1369] [1280, 1919, 640, 1279]
## 90 730 90 730
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  2.4079e-12,  1.8743e-12,
        -3.5352e-11,  6.0161e-11,  8.4743e-11, -7.3079e-11,  1.7487e-10],
       device='cuda:0')
I am [1, 2]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 824, 824])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 824, 824])
grad_out size torch.Size([1, 3, 822, 822])
crop [91, 91, 91, 91] [1189, 2010, 549, 1370] [1280, 1919, 640, 1279]
## 91 731 91 731
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00, -1.4596e-14,  6.2962e-14,
         7.9681e-13, -1.4019e-12, -7.6483e-12,  2.0411e-11,  3.2206e-11],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 1648, 1648])
grad_out size torch.Size([1, 3, 824, 824])
arg size torch.Size([1, 3, 824, 824])
##############grad_in in maxp torch.Size([1, 3, 1648, 1648])
I am[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cCheckpointBackward
 [1, 2]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 1650, 1650])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1648, 1648])
grad_input torch.Size([1, 3, 1650, 1650])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 1650, 1650])
grad_input new torch.Size([1, 3, 1650, 1650])
new grad_input torch.Size([1, 3, 1650, 1650])
grad_out size torch.Size([1, 3, 1648, 1648])
crop [184, 184, 184, 184] [2376, 4023, 1096, 2743] [2560, 3839, 1280, 2559]
## 184 1464 184 1464
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00, -1.1502e-14, -6.6235e-14, -6.2075e-15,  2.3719e-14],
       device='cuda:0')
I am [1, 2]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 3, 1652, 1652])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1650, 1650])
final torch.Size([1, 3, 1652, 1652])
padding info :: [0, 0, 0, 0]
reshape_for_final ## 186 1466 186 1466
after crop g_in torch.Size([1, 3, 1280, 1280])
grad_out size torch.Size([1, 3, 1650, 1650])
crop [185, 185, 185, 185] [2375, 4024, 1095, 2744] [2560, 3839, 1280, 2559]
## 185 1465 185 1465
##############return grad_in in conv2d tensor([-4.0676e-07, -1.1950e-07,  1.1403e-06,  5.1502e-07, -1.1977e-07,
         8.3434e-08, -1.8160e-07, -1.2555e-07,  2.7175e-07,  1.0728e-07],
       device='cuda:0')
[8, 8]
HREREERE torch.Size([1, 3, 10240, 10240])

############# Enter checkpointing bkward ####
inputs len 5 4
in customized Sequential 5
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 1]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1650])
myctx [[ 140046822727744[ PI( <1,1,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 1]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1648])
myctx [[ 140045015171856[ PI( <1,1,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 1]
shape input_tile_for_next
 torch.Size([1, 3, 822, 822])
myctx [[ 140045013800704[ PI( <1,1,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 1]
shape input_tile_for_next
 torch.Size([1, 3, 820, 820])
myctx [[ 140045013848944[ PI( <1,1,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 1]
shape input_tile_for_next
 torch.Size([1, 3, 408, 408])
myctx [[ 140045013206640[ PI( <1,1,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 1]
shape input_tile_for_next
 torch.Size([1, 3, 406, 406])
myctx [[ 140045013206784[ PI( <1,1,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 1]
shape input_tile_for_next
 torch.Size([1, 3, 404, 404])
myctx [[ 140045013206880[ PI( <1,1,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 1]
shape input_tile_for_next
 torch.Size([1, 3, 200, 200])
myctx [[ 140045013206352[ PI( <1,1,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 1]
shape input_tile_for_next
 torch.Size([1, 3, 198, 198])
myctx [[ 140045013206304[ PI( <1,1,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 1]
shape input_tile_for_next
 torch.Size([1, 3, 196, 196])
myctx [[ 140045013207072[ PI( <1,1,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 1]
shape input_tile_for_next
 torch.Size([1, 3, 96, 96])
myctx [[ 140045013206400[ PI( <1,1,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 1]
shape input_tile_for_next
 torch.Size([1, 3, 94, 94])
myctx [[ 140045013206448[ PI( <1,1,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 1]
shape input_tile_for_next
 torch.Size([1, 3, 92, 92])
myctx [[ 140045013206976[ PI( <1,1,>,

in customized Sequential 3
need to get existing

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 92, 92])
grad_out size torch.Size([1, 3, 320, 320])
arg size torch.Size([1, 3, 46, 46])
new_grad_out torch.Size([1, 3, 46, 46])
##############grad_in in maxp torch.Size([1, 3, 92, 92])
I am [1, 1]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 94, 94])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 92, 92])[torch/csrc/autograd/python_engine.cpp] THPEngine_run_backward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward

grad_input torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 94, 94])
grad_input new torch.Size([1, 3, 94, 94])
new grad_input torch.Size([1, 3, 94, 94])
grad_out size torch.Size([1, 3, 92, 92])
crop [6, 6, 6, 6] [74, 165, 74, 165] [80, 159, 80, 159]
## 6 86 6 86
##############return grad_in in conv2d tensor([ 0.0000, -0.0008, -0.0021,  0.0004, -0.0006,  0.0002,  0.0000,  0.0006,
        -0.0013, -0.0009], device='cuda:0')
I am [1, 1]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 96, 96])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 94, 94])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 96, 96])
grad_input new torch.Size([1, 3, 96, 96])
grad_input torch.Size([1, 3, 96, 96])
grad_out size torch.Size([1, 3, 94, 94])
crop [7, 7, 7, 7] [73, 166, 73, 166] [80, 159, 80, 159]
## 7 87 7 87
##############return grad_in in conv2d tensor([ 0.0000e+00, -4.4672e-04,  5.9466e-04, -3.1814e-04,  3.6868e-04,
        -7.0127e-05,  5.7039e-05,  1.1361e-04, -1.3275e-04,  4.3291e-04],
       device='cuda:0')
I am [1, 1]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 98, 98])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 96, 96])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 98, 98])
grad_out size torch.Size([1, 3, 96, 96])
crop [8, 8, 8, 8] [72, 167, 72, 167] [80, 159, 80, 159]
## 8 88 8 88
##############return grad_in in conv2d tensor([ 0.0000e+00, -7.2176e-05,  1.4181e-04,  7.3888e-05, -9.8529e-05,
         1.5706e-06, -5.7069e-05,  2.5608e-06, -3.1071e-06,  3.0209e-05],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 196, 196])
grad_out size torch.Size([1, 3, 98, 98])
arg size torch.Size([1, 3, 98, 98])
##############grad_in in maxp torch.Size([1, 3, 196, 196])
I am [1, 1]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 198, 198])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 196, 196])
grad_input torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 198, 198])
grad_input new torch.Size([1, 3, 198, 198])
new grad_input torch.Size([1, 3, 198, 198])
grad_out size torch.Size([1, 3, 196, 196])
crop [18, 18, 18, 18] [142, 337, 142, 337] [160, 319, 160, 319]
## 18 178 18 178
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00, -5.3937e-06,  3.9839e-05,
        -2.7445e-05, -1.9720e-05,  4.4547e-06, -7.7913e-06,  2.5573e-06],
       device='cuda:0')
I am [1, 1]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 200, 200])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 198, 198])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 200, 200])
grad_input new torch.Size([1, 3, 200, 200])
grad_input torch.Size([1, 3, 200, 200])
grad_out size torch.Size([1, 3, 198, 198])
crop [19, 19, 19, 19] [141, 338, 141, 338] [160, 319, 160, 319]
## 19 179 19 179
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00, -1.5661e-06,  1.2328e-05,
        -1.2820e-05,  1.1818e-05, -1.2999e-05, -3.0012e-06,  4.5617e-06],
       device='cuda:0')
I am [1, 1]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 202, 202])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 200, 200])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 202, 202])
grad_out size torch.Size([1, 3, 200, 200])
crop [20, 20, 20, 20] [140, 339, 140, 339] [160, 319, 160, 319]
## 20 180 20 180
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  6.9572e-08, -3.2936e-07,
        -1.7692e-06, -5.6821e-07,  4.0866e-06, -2.2959e-08, -8.6461e-07],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 404, 404])
grad_out size torch.Size([1, 3, 202, 202])
arg size torch.Size([1, 3, 202, 202])
##############grad_in in maxp torch.Size([1, 3, 404, 404])
I am [1, 1]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 406, 406])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 404, 404])
grad_input torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 406, 406])
grad_input new torch.Size([1, 3, 406, 406])
new grad_input torch.Size([1, 3, 406, 406])
grad_out size torch.Size([1, 3, 404, 404])
crop [42, 42, 42, 42] [278, 681, 278, 681] [320, 639, 320, 639]
## 42 362 42 362
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  2.1963e-08, -1.6411e-08, -1.0195e-07,  1.6792e-07],
       device='cuda:0')
I am [1, 1]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 408, 408])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 406, 406])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 408, 408])
grad_input new torch.Size([1, 3, 408, 408])
grad_input torch.Size([1, 3, 408, 408])
grad_out size torch.Size([1, 3, 406, 406])
crop [43, 43, 43, 43] [277, 682, 277, 682] [320, 639, 320, 639]
## 43 363 43 363
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00, -2.9048e-09, -6.7981e-09, -1.2380e-08, -1.7684e-08],
       device='cuda:0')
I am [1, 1]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 410, 410])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 408, 408])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 410, 410])
grad_out size torch.Size([1, 3, 408, 408])
crop [44, 44, 44, 44] [276, 683, 276, 683] [320, 639, 320, 639]
## 44 364 44 364
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         0.0000e+00, -1.0919e-10,  2.5413e-09, -2.0645e-09,  8.6568e-09],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 820, 820])
grad_out size torch.Size([1, 3, 410, 410])
arg size torch.Size([1, 3, 410, 410])
##############grad_in in maxp torch.Size([1, 3, 820, 820])
I am [1, 1]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 822, 822])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 820, 820])
grad_input torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 822, 822])
grad_input new torch.Size([1, 3, 822, 822])
new grad_input torch.Size([1, 3, 822, 822])
grad_out size torch.Size([1, 3, 820, 820])
crop [90, 90, 90, 90] [550, 1369, 550, 1369] [640, 1279, 640, 1279]
## 90 730 90 730
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [1, 1]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 824, 824])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 822, 822])
padding info :: [0, 0, 0, 0]
grad_input torch.Size([1, 3, 824, 824])
grad_out size torch.Size([1, 3, 822, 822])
crop [91, 91, 91, 91] [549, 1370, 549, 1370] [640, 1279, 640, 1279]
## 91 731 91 731
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 1648, 1648])
grad_out size torch.Size([1, 3, 824, 824])
arg size torch.Size([1, 3, 824, 824])
##############grad_in in maxp torch.Size([1, 3, 1648, 1648])
I am [1, 1]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 1650, 1650])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1648, 1648])
grad_input torch.Size([1, 3, 1650, 1650])
padding info :: [0, 0, 0, 0]
grad_input old torch.Size([1, 3, 1650, 1650])
grad_input new torch.Size([1, 3, 1650, 1650])
new grad_input torch.Size([1, 3, 1650, 1650])
grad_out size torch.Size([1, 3, 1648, 1648])
crop [184, 184, 184, 184] [1096, 2743, 1096, 2743] [1280, 2559, 1280, 2559]
## 184 1464 184 1464
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [1, 1]
@@@ using cudnn bkw
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cCheckpointBackward
[torch/csrc/autograd/python_engine.cpp] THPEngine_run_backward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
input grad ++ input shape torch.Size([1, 3, 1652, 1652])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1650, 1650])
final torch.Size([1, 3, 1652, 1652])
padding info :: [0, 0, 0, 0]
reshape_for_final ## 186 1466 186 1466
after crop g_in torch.Size([1, 3, 1280, 1280])
grad_out size torch.Size([1, 3, 1650, 1650])
crop [185, 185, 185, 185] [1095, 2744, 1095, 2744] [1280, 2559, 1280, 2559]
## 185 1465 185 1465
##############return grad_in in conv2d tensor([ 3.3674e-07,  9.1774e-09,  1.2074e-07, -1.7855e-07,  9.9951e-09,
         7.3924e-08,  2.9203e-07,  1.2719e-07, -6.5273e-07, -5.3541e-07],
       device='cuda:0')
[8, 8]
HREREERE torch.Size([1, 3, 10240, 10240])

############# Enter checkpointing bkward ####
inputs len 5 4
in customized Sequential 5
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 0]
shape input_tile_for_next
 torch.Size([1, 3, 1650, 1466])
myctx [[ 140046822727744[ PI( <1,0,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 0]
shape input_tile_for_next
 torch.Size([1, 3, 1648, 1464])
myctx [[ 140045015171856[ PI( <1,0,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 0]
shape input_tile_for_next
 torch.Size([1, 3, 822, 732])
myctx [[ 140045013800704[ PI( <1,0,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 0]
shape input_tile_for_next
 torch.Size([1, 3, 820, 730])
myctx [[ 140045013848944[ PI( <1,0,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 0]
shape input_tile_for_next
 torch.Size([1, 3, 408, 366])
myctx [[ 140045013206640[ PI( <1,0,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 0]
shape input_tile_for_next
 torch.Size([1, 3, 406, 364])
myctx [[ 140045013206784[ PI( <1,0,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 0]
shape input_tile_for_next
 torch.Size([1, 3, 404, 362])
myctx [[ 140045013206880[ PI( <1,0,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 0]
shape input_tile_for_next
 torch.Size([1, 3, 200, 182])
myctx [[ 140045013206352[ PI( <1,0,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 0]
shape input_tile_for_next
 torch.Size([1, 3, 198, 180])
myctx [[ 140045013206304[ PI( <1,0,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 0]
shape input_tile_for_next
 torch.Size([1, 3, 196, 178])
myctx [[ 140045013207072[ PI( <1,0,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 0]
shape input_tile_for_next
 torch.Size([1, 3, 96, 90])
myctx [[ 140045013206400[ PI( <1,0,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 0]
shape input_tile_for_next
 torch.Size([1, 3, 94, 88])
myctx [[ 140045013206448[ PI( <1,0,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [1, 0]
shape input_tile_for_next
 torch.Size([1, 3, 92, 86])
myctx [[ 140045013206976[ PI( <1,0,>,

in customized Sequential 3
need to get existing

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 92, 86])
grad_out size torch.Size([1, 3, 320, 320])
arg size torch.Size([1, 3, 46, 43])
new_grad_out torch.Size([1, 3, 46, 43])
##############grad_in in maxp torch.Size([1, 3, 92, 86])
I am [1, 0]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 94, 88])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 92, 86])
grad_input torch.Size([1, 3, 94, 88])
padding info :: [1, 0, 0, 0]
grad_input old torch.Size([1, 3, 94, 88])
grad_input new torch.Size([1, 3, 94, 88])
new grad_input torch.Size([1, 3, 94, 88])
grad_out size torch.Size([1, 3, 92, 86])
crop [0, 6, 6, 6] [0, 85, 74, 165] [0, 79, 80, 159]
## 6 86 0 80
##############return grad_in in conv2d tensor([ 0.0000e+00,  2.6169e-03, -1.8782e-03,  5.3996e-03, -8.8092e-03,
        -7.5781e-05,  2.6445e-04,  1.1262e-03, -2.7261e-04,  0.0000e+00],
       device='cuda:0')
I am [1, 0]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 96, 90])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 94, 88])
padding info :: [2, 0, 0, 0]
grad_input old torch.Size([1, 3, 96, 90])
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
grad_input new torch.Size([1, 3, 96, 90])
grad_input torch.Size([1, 3, 96, 90])
grad_out size torch.Size([1, 3, 94, 87])
crop [0, 7, 7, 7] [0, 86, 73, 166] [0, 79, 80, 159]
## 7 87 0 80
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  6.9148e-05,  5.7699e-04,  8.1755e-04,
         1.2754e-03,  2.8117e-03, -8.6869e-04,  1.5696e-04, -1.1742e-04],
       device='cuda:0')
I am [1, 0]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 98, 92])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 96, 90])
padding info :: [3, 0, 0, 0]
grad_input torch.Size([1, 3, 98, 89])
grad_out size torch.Size([1, 3, 96, 88])
crop [0, 8, 8, 8] [0, 87, 72, 167] [0, 79, 80, 159]
## 8 88 0 80
##############return grad_in in conv2d tensor([-6.7952e-05,  3.6994e-04, -3.0407e-04,  5.5734e-04, -4.4498e-05,
        -6.0821e-04,  8.2339e-05,  2.9185e-05, -4.1839e-05, -9.3954e-05],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 196, 178])
grad_out size torch.Size([1, 3, 98, 89])
arg size torch.Size([1, 3, 98, 89])
##############grad_in in maxp torch.Size([1, 3, 196, 178])
I am [1, 0]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 198, 180])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 196, 178])
grad_input torch.Size([1, 3, 198, 180])
padding info :: [1, 0, 0, 0]
grad_input old torch.Size([1, 3, 198, 180])
grad_input new torch.Size([1, 3, 198, 180])
new grad_input torch.Size([1, 3, 198, 180])
grad_out size torch.Size([1, 3, 196, 178])
crop [0, 18, 18, 18] [0, 177, 142, 337] [0, 159, 160, 319]
## 18 178 0 160
##############return grad_in in conv2d tensor([ 0.0000e+00,  1.6562e-05,  3.0186e-05,  2.0720e-06, -3.9943e-05,
         1.3535e-06,  1.5346e-04, -6.3040e-06, -1.1255e-04,  1.5023e-05],
       device='cuda:0')
I am [1, 0]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 200, 182])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 198, 180])
padding info :: [2, 0, 0, 0]
grad_input old torch.Size([1, 3, 200, 182])
grad_input new torch.Size([1, 3, 200, 182])
grad_input torch.Size([1, 3, 200, 182])
grad_out size torch.Size([1, 3, 198, 179])
crop [0, 19, 19, 19] [0, 178, 141, 338] [0, 159, 160, 319]
## 19 179 0 160
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  4.7878e-06,  7.2331e-06, -3.4861e-07,
         6.7116e-06,  2.7844e-05, -2.8075e-05,  3.1019e-05,  1.1466e-05],
       device='cuda:0')
I am [1, 0]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 202, 184])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 200, 182])
padding info :: [3, 0, 0, 0]
grad_input torch.Size([1, 3, 202, 181])
grad_out size torch.Size([1, 3, 200, 180])
crop [0, 20, 20, 20] [0, 179, 140, 339] [0, 159, 160, 319]
## 20 180 0 160
##############return grad_in in conv2d tensor([-4.9102e-06, -2.0852e-06,  3.4781e-06,  5.9317e-07, -8.3863e-06,
        -8.1604e-06,  8.5745e-06,  7.0317e-06, -3.1002e-06, -2.3883e-06],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 404, 362])
grad_out size torch.Size([1, 3, 202, 181])
arg size torch.Size([1, 3, 202, 181])
##############grad_in in maxp torch.Size([1, 3, 404, 362])
I am [1, 0]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 406, 364])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 404, 362])
grad_input torch.Size([1, 3, 406, 364])
padding info :: [1, 0, 0, 0]
grad_input old torch.Size([1, 3, 406, 364])
grad_input new torch.Size([1, 3, 406, 364])
new grad_input torch.Size([1, 3, 406, 364])
grad_out size torch.Size([1, 3, 404, 362])
crop [0, 42, 42, 42] [0, 361, 278, 681] [0, 319, 320, 639]
## 42 362 0 320
##############return grad_in in conv2d tensor([ 0.0000e+00,  4.0467e-07,  5.9646e-07, -4.7889e-07,  8.4147e-07,
        -8.2597e-08, -3.6229e-07,  2.9053e-07, -1.2246e-06, -7.1682e-07],
       device='cuda:0')
I am [1, 0]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 408, 366])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 406, 364])
padding info :: [2, 0, 0, 0]
grad_input old torch.Size([1, 3, 408, 366])
grad_input new torch.Size([1, 3, 408, 366])
grad_input torch.Size([1, 3, 408, 366])
grad_out size torch.Size([1, 3, 406, 363])
crop [0, 43, 43, 43] [0, 362, 277, 682] [0, 319, 320, 639]
## 43 363 0 320
##############return grad_in in conv2d tensor([ 0.0000e+00,  0.0000e+00,  1.2790e-07, -5.4731e-08, -3.1102e-09,
         9.4600e-09,  1.3656e-07,  8.3710e-09, -2.7469e-07, -4.1841e-07],
       device='cuda:0')
I am [1, 0]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 410, 368])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 408, 366])
padding info :: [3, 0, 0, 0]
grad_input torch.Size([1, 3, 410, 365])
grad_out size torch.Size([1, 3, 408, 364])
crop [0, 44, 44, 44] [0, 363, 276, 683] [0, 319, 320, 639]
## 44 364 0 320
##############return grad_in in conv2d tensor([-1.1979e-08, -3.7353e-08,  2.0549e-08,  8.2628e-09,  1.7756e-09,
        -1.9681e-08,  7.1120e-08,  1.4269e-07,  2.9680e-07,  2.3588e-07],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 820, 730])
grad_out size torch.Size([1, 3, 410, 365])
arg size torch.Size([1, 3, 410, 365])
##############grad_in in maxp torch.Size([1, 3, 820, 730])
I am [1, 0]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 822, 732])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 820, 730])
grad_input torch.Size([1, 3, 822, 732])
padding info :: [1, 0, 0, 0]
grad_input old torch.Size([1, 3, 822, 732])
grad_input new torch.Size([1, 3, 822, 732])
new grad_input torch.Size([1, 3, 822, 732])
grad_out size torch.Size([1, 3, 820, 730])
crop [0, 90, 90, 90] [0, 729, 550, 1369] [0, 639, 640, 1279]
## 90 730 0 640
##############return grad_in in conv2d tensor([ 0.0000e+00, -2.2434e-09, -8.7419e-09, -5.4016e-09,  8.2370e-09,
         8.1592e-11,  4.9659e-09,  5.6395e-09,  1.3274e-09, -2.9582e-09],
       device='cuda:0')
I am [1, 0]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 824, 734])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 822, 732])
padding info :: [2, 0, 0, 0]
grad_input torch.Size([1, 3, 824, 732])
grad_out size torch.Size([1, 3, 822, 731])
crop [0, 91, 91, 91] [0, 730, 549, 1370] [0, 639, 640, 1279]
## 91 731 0 640
##############return grad_in in conv2d tensor([-1.6255e-11, -1.0283e-09, -2.8742e-09, -7.4370e-10,  2.9745e-09,
         6.8922e-10, -1.8628e-09, -8.1216e-11, -6.1709e-10, -8.5177e-10],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 1648, 1464])
grad_out size torch.Size([1, 3, 824, 732])
arg size torch.Size([1, 3, 824, 732])
##############grad_in in maxp torch.Size([1, 3, 1648, 1464])
I am [1, 0]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 1650, 1466])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1648, 1464])
grad_input torch.Size([1, 3, 1650, 1466])
padding info :: [1, 0, 0, 0]
grad_input old torch.Size([1, 3, 1650, 1466])
grad_input new torch.Size([1, 3, 1650, 1466])
new grad_input torch.Size([1, 3, 1650, 1466])
grad_out size torch.Size([1, 3, 1648, 1464])
crop [0, 184, 184, 184] [0, 1463, 1096, 2743] [0, 1279, 1280, 2559]
## 184 1464 0 1280
##############return grad_in in conv2d tensor([ 0.0000e+00,  1.1703e-12, -2.6980e-13,  3.9443e-11,  3.9273e-11,
        -2.1725e-10,  9.7658e-12,  5.3544e-11,  4.5417e-10, -3.7408e-10],
       device='cuda:0')
I am [1, 0]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 3, 1652, 1468])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1650, 1466])
final torch.Size([1, 3, 1652, 1468])
padding info :: [2, 0, 0, 0]
reshape_for_final ## 186 1466 0 1280
after crop g_in torch.Size([1, 3, 1280, 1280])
grad_out size torch.Size([1, 3, 1650, 1465])
crop [0, 185, 185, 185] [0, 1464, 1095, 2744] [0, 1279, 1280, 2559]
## 185 1465 0 1280
##############return grad_in in conv2d [torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cCheckpointBackward
[torch/csrc/autograd/python_engine.cpp] THPEngine_run_backward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
tensor([-6.9749e-09,  6.8453e-08,  5.7463e-08, -1.2519e-07,  1.4199e-08,
         3.0174e-07, -3.7346e-08,  1.4472e-07, -3.0017e-09,  1.1263e-07],
       device='cuda:0')
[8, 8]
HREREERE torch.Size([1, 3, 10240, 10240])

############# Enter checkpointing bkward ####
inputs len 5 4
in customized Sequential 5
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 7]
shape input_tile_for_next
 torch.Size([1, 3, 1466, 1466])
myctx [[ 140046822727744[ PI( <0,7,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 7]
shape input_tile_for_next
 torch.Size([1, 3, 1464, 1464])
myctx [[ 140045015171856[ PI( <0,7,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 7]
shape input_tile_for_next
 torch.Size([1, 3, 732, 732])
myctx [[ 140045013800704[ PI( <0,7,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 7]
shape input_tile_for_next
 torch.Size([1, 3, 730, 730])
myctx [[ 140045013848944[ PI( <0,7,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 7]
shape input_tile_for_next
 torch.Size([1, 3, 366, 366])
myctx [[ 140045013206640[ PI( <0,7,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 7]
shape input_tile_for_next
 torch.Size([1, 3, 364, 364])
myctx [[ 140045013206784[ PI( <0,7,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 7]
shape input_tile_for_next
 torch.Size([1, 3, 362, 362])
myctx [[ 140045013206880[ PI( <0,7,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 7]
shape input_tile_for_next
 torch.Size([1, 3, 182, 182])
myctx [[ 140045013206352[ PI( <0,7,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 7]
shape input_tile_for_next
 torch.Size([1, 3, 180, 180])
myctx [[ 140045013206304[ PI( <0,7,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 7]
shape input_tile_for_next
 torch.Size([1, 3, 178, 178])
myctx [[ 140045013207072[ PI( <0,7,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 7]
shape input_tile_for_next
 torch.Size([1, 3, 90, 90])
myctx [[ 140045013206400[ PI( <0,7,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 7]
shape input_tile_for_next
 torch.Size([1, 3, 88, 88])
myctx [[ 140045013206448[ PI( <0,7,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 7]
shape input_tile_for_next
 torch.Size([1, 3, 86, 86])
myctx [[ 140045013206976[ PI( <0,7,>,

in customized Sequential 3
need to get existing

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 86, 86])
grad_out size torch.Size([1, 3, 320, 320])
arg size torch.Size([1, 3, 43, 43])
new_grad_out torch.Size([1, 3, 43, 43])
##############grad_in in maxp torch.Size([1, 3, 86, 86])
I am [0, 7]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 88, 88])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 86, 86])
grad_input torch.Size([1, 3, 88, 88])
padding info :: [0, 1, 1, 0]
grad_input old torch.Size([1, 3, 88, 88])
grad_input new torch.Size([1, 3, 88, 88])
new grad_input torch.Size([1, 3, 88, 88])
grad_out size torch.Size([1, 3, 86, 86])
crop [6, 0, 0, 6] [554, 639, 0, 85] [560, 639, 0, 79]
## 0 80 6 86
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [0, 7]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 90, 90])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 88, 88])
padding info :: [0, 2, 2, 0]
grad_input old torch.Size([1, 3, 90, 90])
grad_input new torch.Size([1, 3, 90, 90])
grad_input torch.Size([1, 3, 90, 90])
grad_out size torch.Size([1, 3, 87, 87])
crop [7, 0, 0, 7] [553, 639, 0, 86] [560, 639, 0, 79]
## 0 80 7 87
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [0, 7]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 92, 92])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 90, 90])
padding info :: [0, 3, 3, 0]
grad_input torch.Size([1, 3, 89, 89])
grad_out size torch.Size([1, 3, 88, 88])
crop [torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cCheckpointBackward
[8, 0, 0, 8] [552, 639, 0, 87] [560, 639, 0, 79]
## 0 80 8 88
##############return grad_in in conv2d tensor([-1.7780e-05,  5.4471e-04, -1.1439e-03,  1.2314e-03,  6.1674e-04,
        -1.1665e-03,  1.6242e-04,  5.8343e-04, -5.5575e-04,  8.2733e-04],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 178, 178])
grad_out size torch.Size([1, 3, 89, 89])
arg size torch.Size([1, 3, 89, 89])
##############grad_in in maxp torch.Size([1, 3, 178, 178])
I am [0, 7]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 180, 180])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 178, 178])
grad_input torch.Size([1, 3, 180, 180])
padding info :: [0, 1, 1, 0]
grad_input old torch.Size([1, 3, 180, 180])
grad_input new torch.Size([1, 3, 180, 180])
new grad_input torch.Size([1, 3, 180, 180])
grad_out size torch.Size([1, 3, 178, 178])
crop [18, 0, 0, 18] [1102, 1279, 0, 177] [1120, 1279, 0, 159]
## 0 160 18 178
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [0, 7]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 182, 182])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 180, 180])
padding info :: [0, 2, 2, 0]
grad_input old torch.Size([1, 3, 182, 182])
grad_input new torch.Size([1, 3, 182, 182])
grad_input torch.Size([1, 3, 182, 182])
grad_out size torch.Size([1, 3, 179, 179])
crop [19, 0, 0, 19] [1101, 1279, 0, 178] [1120, 1279, 0, 159]
## 0 160 19 179
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [0, 7]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 184, 184])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 182, 182])
padding info :: [0, 3, 3, 0]
grad_input torch.Size([1, 3, 181, 181])
grad_out size torch.Size([1, 3, 180, 180])
crop [20, 0, 0, 20] [1100, 1279, 0, 179] [1120, 1279, 0, 159]
## 0 160 20 180
##############return grad_in in conv2d tensor([-9.2506e-07,  6.3848e-07,  2.2698e-06, -4.1265e-06, -6.1348e-06,
        -3.9007e-05,  3.6262e-05,  1.0005e-04, -6.3437e-05, -9.0814e-05],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 362, 362])
grad_out size torch.Size([1, 3, 181, 181])
arg size torch.Size([1, 3, 181, 181])
##############grad_in in maxp torch.Size([1, 3, 362, 362])
I am [0, 7]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 364, 364])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 362, 362])
grad_input torch.Size([1, 3, 364, 364])
padding info :: [0, 1, 1, 0]
grad_input old torch.Size([1, 3, 364, 364])
grad_input new torch.Size([1, 3, 364, 364])
new grad_input torch.Size([1, 3, 364, 364])
grad_out size torch.Size([1, 3, 362, 362])
crop [42, 0, 0, 42] [2198, 2559, 0, 361] [2240, 2559, 0, 319]
## 0 320 42 362
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [0, 7]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 366, 366])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 364, 364])
padding info :: [0, 2, 2, 0]
grad_input old torch.Size([1, 3, 366, 366])
grad_input new torch.Size([1, 3, 366, 366])
grad_input torch.Size([1, 3, 366, 366])
grad_out size torch.Size([1, 3, 363, 363])
crop [43, 0, 0, 43] [2197, 2559, 0, 362] [2240, 2559, 0, 319]
## 0 320 43 363
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [0, 7]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 368, 368])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 366, 366])
padding info :: [0, 3, 3, 0]
grad_input torch.Size([1, 3, 365, 365])
grad_out size torch.Size([1, 3, 364, 364])
crop [44, 0, 0, 44] [2196, 2559, 0, 363] [2240, 2559, 0, 319]
## 0 320 44 364
##############return grad_in in conv2d tensor([ 6.1100e-09, -1.8993e-08, -2.5262e-08, -1.1307e-07,  5.3273e-08,
         1.1557e-08,  3.4349e-07,  4.8642e-07,  3.0932e-07, -3.5106e-07],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 730, 730])
grad_out size torch.Size([1, 3, 365, 365])
arg size torch.Size([1, 3, 365, 365])
##############grad_in in maxp torch.Size([1, 3, 730, 730])
I am [0, 7]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 732, 732])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 730, 730])
grad_input torch.Size([1, 3, 732, 732])
padding info :: [0, 1, 1, 0]
grad_input old torch.Size([1, 3, 732, 732])
grad_input new torch.Size([1, 3, 732, 732])
new grad_input torch.Size([1, 3, 732, 732])
grad_out size torch.Size([1, 3, 730, 730])
crop [90, 0, 0, 90] [4390, 5119, 0, 729] [4480, 5119, 0, 639]
## 0 640 90 730
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [0, 7]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 734, 734])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 732, 732])
padding info :: [0, 2, 2, 0]
grad_input torch.Size([1, 3, 732, 732])
grad_out size torch.Size([1, 3, 731, 731])
crop [91, 0, 0, 91] [4389, 5119, 0, 730] [4480, 5119, 0, 639]
## 0 640 91 731
##############return grad_in in conv2d tensor([ 1.1662e-09, -1.2971e-09, -1.2219e-09,  2.1289e-09, -2.1538e-09,
        -5.9665e-09, -4.0466e-09,  3.0156e-09,  4.7863e-09,  5.6386e-09],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 1464, 1464])
grad_out size torch.Size([1, 3, 732, 732])
arg size torch.Size([1, 3, 732, 732])
##############grad_in in maxp torch.Size([1, 3, 1464, 1464])
I am [0, 7]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 1466, 1466])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1464, 1464])
grad_input torch.Size([1, 3, 1466, 1466])
padding info :: [0, 1, 1, 0]
grad_input old torch.Size([1, 3, 1466, 1466])
grad_input new torch.Size([1, 3, 1466, 1466])
new grad_input torch.Size([1, 3, 1466, 1466])
grad_out size torch.Size([1, 3, 1464, 1464])
crop [184, 0, 0, 184] [8776, 10239, 0, 1463] [8960, 10239, 0, 1279]
## 0 1280 184 1464
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [0, 7]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 3, 1468, 1468])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1466, 1466])
final torch.Size([1, 3, 1468, 1468])
padding info :: [0, 2, 2, 0]
reshape_for_final ## 0 1280 186 1466
after crop g_in torch.Size([1, 3, 1280, 1280])
grad_out size torch.Size([1, 3, 1465, 1465])
crop [185, 0, 0, 185] [8775, 10239, 0, 1464] [8960, 10239, 0, 1279]
## 0 1280 185 1465
##############return grad_in in conv2d tensor([-1.2579e-08,  3.6589e-09, -3.2012e-08,  7.9939e-09,  3.2915e-08,
         5.6601e-08, -1.4079e-08,  5.3890e-08, -2.0237e-08,  3.6903e-08],
       device='cuda:0')
[8, 8]
HREREERE torch.Size([1, 3, 10240, 10240])

############# Enter checkpointing bkward ####
inputs len 5 4
in customized Sequential 5
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 6]
shape input_tile_for_next
 torch.Size([1, 3, 1466, 1650])
myctx [[ 140046822727744[ PI( <0,6,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 6]
shape input_tile_for_next
 torch.Size([1, 3, 1464, 1648])
myctx [[ 140045015171856[ PI( <0,6,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id[torch/csrc/autograd/python_engine.cpp] THPEngine_run_backward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 6]
shape input_tile_for_next
 torch.Size([1, 3, 732, 822])
myctx [[ 140045013800704[ PI( <0,6,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 6]
shape input_tile_for_next
 torch.Size([1, 3, 730, 820])
myctx [[ 140045013848944[ PI( <0,6,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 6]
shape input_tile_for_next
 torch.Size([1, 3, 366, 408])
myctx [[ 140045013206640[ PI( <0,6,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 6]
shape input_tile_for_next
 torch.Size([1, 3, 364, 406])
myctx [[ 140045013206784[ PI( <0,6,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 6]
shape input_tile_for_next
 torch.Size([1, 3, 362, 404])
myctx [[ 140045013206880[ PI( <0,6,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 6]
shape input_tile_for_next
 torch.Size([1, 3, 182, 200])
myctx [[ 140045013206352[ PI( <0,6,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 6]
shape input_tile_for_next
 torch.Size([1, 3, 180, 198])
myctx [[ 140045013206304[ PI( <0,6,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 6]
shape input_tile_for_next
 torch.Size([1, 3, 178, 196])
myctx [[ 140045013207072[ PI( <0,6,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 6]
shape input_tile_for_next
 torch.Size([1, 3, 90, 96])
myctx [[ 140045013206400[ PI( <0,6,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 6]
shape input_tile_for_next
 torch.Size([1, 3, 88, 94])
myctx [[ 140045013206448[ PI( <0,6,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 6]
shape input_tile_for_next
 torch.Size([1, 3, 86, 92])
myctx [[ 140045013206976[ PI( <0,6,>,

in customized Sequential 3
need to get existing

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 86, 92])
grad_out size torch.Size([1, 3, 320, 320])
arg size torch.Size([1, 3, 43, 46])
new_grad_out torch.Size([1, 3, 43, 46])
##############grad_in in maxp torch.Size([1, 3, 86, 92])
I am [0, 6]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 88, 94])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 86, 92])
grad_input torch.Size([1, 3, 88, 94])
padding info :: [0, 0, 1, 0]
grad_input old torch.Size([1, 3, 88, 94])
grad_input new torch.Size([1, 3, 88, 94])
new grad_input torch.Size([1, 3, 88, 94])
grad_out size torch.Size([1, 3, 86, 92])
crop [6, 6, 0, 6] [474, 565, 0, 85] [480, 559, 0, 79]
## 0 80 6 86
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [0, 6]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 90, 96])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 88, 94])
padding info :: [0, 0, 2, 0]
grad_input old torch.Size([1, 3, 90, 96])
grad_input new torch.Size([1, 3, 90, 96])
grad_input torch.Size([1, 3, 90, 96])
grad_out size torch.Size([1, 3, 87, 94])
crop [7, 7, 0, 7] [473, 566, 0, 86] [480, 559, 0, 79]
## 0 80 7 87
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [0, 6]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 92, 98])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 90, 96])
padding info :: [0, 0, 3, 0]
grad_input torch.Size([1, 3, 89, 98])
grad_out size torch.Size([1, 3, 88, 96])
crop [8, 8, 0, 8] [472, 567, 0, 87] [480, 559, 0, 79]
## 0 80 8 88
##############return grad_in in conv2d tensor([-3.3431e-04,  1.4716e-04,  1.9372e-04,  2.4656e-05, -2.6718e-04,
        -1.0644e-03, -6.8902e-04,  2.4373e-03,  4.3099e-05, -1.2787e-03],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 178, 196])
grad_out size torch.Size([1, 3, 89, 98])
arg size torch.Size([1, 3, 89, 98])
##############grad_in in maxp torch.Size([1, 3, 178, 196])
I am [0, 6]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 180, 198])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 178, 196])
grad_input torch.Size([1, 3, 180, 198])
padding info :: [0, 0, 1, 0]
grad_input old torch.Size([1, 3, 180, 198])
grad_input new torch.Size([1, 3, 180, 198])
new grad_input torch.Size([1, 3, 180, 198])
grad_out size torch.Size([1, 3, 178, 196])
crop [18, 18, 0, 18] [942, 1137, 0, 177] [960, 1119, 0, 159]
## 0 160 18 178
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [0, 6]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 182, 200])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 180, 198])
padding info :: [0, 0, 2, 0]
grad_input old torch.Size([1, 3, 182, 200])
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cCheckpointBackward
grad_input new torch.Size([1, 3, 182, 200])
grad_input torch.Size([1, 3, 182, 200])
grad_out size torch.Size([1, 3, 179, 198])
crop [19, 19, 0, 19] [941, 1138, 0, 178] [960, 1119, 0, 159]
## 0 160 19 179
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [0, 6]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 184, 202])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 182, 200])
padding info :: [0, 0, 3, 0]
grad_input torch.Size([1, 3, 181, 202])
grad_out size torch.Size([1, 3, 180, 200])
crop [20, 20, 0, 20] [940, 1139, 0, 179] [960, 1119, 0, 159]
## 0 160 20 180
##############return grad_in in conv2d tensor([ 2.0805e-06,  7.3303e-06,  1.4907e-05, -1.1459e-05, -1.9758e-05,
         2.1916e-05, -2.7752e-05, -2.6674e-05,  1.3805e-04, -1.3076e-04],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 362, 404])
grad_out size torch.Size([1, 3, 181, 202])
arg size torch.Size([1, 3, 181, 202])
##############grad_in in maxp torch.Size([1, 3, 362, 404])
I am [0, 6]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 364, 406])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 362, 404])
grad_input torch.Size([1, 3, 364, 406])
padding info :: [0, 0, 1, 0]
grad_input old torch.Size([1, 3, 364, 406])
grad_input new torch.Size([1, 3, 364, 406])
new grad_input torch.Size([1, 3, 364, 406])
grad_out size torch.Size([1, 3, 362, 404])
crop [42, 42, 0, 42] [1878, 2281, 0, 361] [1920, 2239, 0, 319]
## 0 320 42 362
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [0, 6]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 366, 408])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 364, 406])
padding info :: [0, 0, 2, 0]
grad_input old torch.Size([1, 3, 366, 408])
grad_input new torch.Size([1, 3, 366, 408])
grad_input torch.Size([1, 3, 366, 408])
grad_out size torch.Size([1, 3, 363, 406])
crop [43, 43, 0, 43] [1877, 2282, 0, 362] [1920, 2239, 0, 319]
## 0 320 43 363
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [0, 6]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 368, 410])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 366, 408])
padding info :: [0, 0, 3, 0]
grad_input torch.Size([1, 3, 365, 410])
grad_out size torch.Size([1, 3, 364, 408])
crop [44, 44, 0, 44] [1876, 2283, 0, 363] [1920, 2239, 0, 319]
## 0 320 44 364
##############return grad_in in conv2d tensor([ 1.2193e-09, -3.8854e-08, -1.2348e-07,  1.1713e-07,  4.1225e-07,
        -1.1422e-06, -8.6626e-07, -3.8696e-07,  2.7532e-07, -1.2900e-06],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 730, 820])
grad_out size torch.Size([1, 3, 365, 410])
arg size torch.Size([1, 3, 365, 410])
##############grad_in in maxp torch.Size([1, 3, 730, 820])
I am [0, 6]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 732, 822])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 730, 820])
grad_input torch.Size([1, 3, 732, 822])
padding info :: [0, 0, 1, 0]
grad_input old torch.Size([1, 3, 732, 822])
grad_input new torch.Size([1, 3, 732, 822])
new grad_input torch.Size([1, 3, 732, 822])
grad_out size torch.Size([1, 3, 730, 820])
crop [90, 90, 0, 90] [3750, 4569, 0, 729] [3840, 4479, 0, 639]
## 0 640 90 730
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [0, 6]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 734, 824])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 732, 822])
padding info :: [0, 0, 2, 0]
grad_input torch.Size([1, 3, 732, 824])
grad_out size torch.Size([1, 3, 731, 822])
crop [91, 91, 0, 91] [3749, 4570, 0, 730] [3840, 4479, 0, 639]
## 0 640 91 731
##############return grad_in in conv2d tensor([-7.9628e-12, -6.4984e-10, -1.2767e-10, -2.2711e-09,  1.5370e-08,
         1.7486e-08, -1.7224e-08, -7.1703e-09, -1.9092e-08, -3.3551e-08],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 1464, 1648])
grad_out size torch.Size([1, 3, 732, 824])
arg size torch.Size([1, 3, 732, 824])
##############grad_in in maxp torch.Size([1, 3, 1464, 1648])
I am [0, 6]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 1466, 1650])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1464, 1648])
grad_input torch.Size([1, 3, 1466, 1650])
padding info :: [0, 0, 1, 0]
grad_input old torch.Size([1, 3, 1466, 1650])
grad_input new torch.Size([1, 3, 1466, 1650])
new grad_input torch.Size([1, 3, 1466, 1650])
grad_out size torch.Size([1, 3, 1464, 1648])
crop [184, 184, 0, 184] [7496, 9143, 0, 1463] [7680, 8959, 0, 1279]
## 0 1280 184 1464
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [0, 6]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 3, 1468, 1652])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1466, 1650])
final torch.Size([1, 3, 1468, 1652])
padding info :: [0, 0, 2, 0]
reshape_for_final ## 0 1280 186 1466
after crop g_in torch.Size([1, 3, 1280, 1280])
grad_out size torch.Size([1, 3, 1465, 1650])
crop [185, 185, 0, 185] [7495, 9144, 0, 1464] [7680, 8959, 0, 1279]
## 0 1280 185 1465
##############return grad_in in conv2d tensor([ 7.5838e-08,  5.1947e-08, -1.9730e-08,  5.5463e-08, -1.2451e-08,
         5.2495e-08, -1.3512e-07, -4.3551e-08,  1.9802e-07,  1.6509e-07],
       device='cuda:0')
[8, 8]
HREREERE torch.Size([1, 3, 10240, 10240])

############# Enter checkpointing bkward ####
inputs len 5 4
in customized Sequential 5
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 5]
shape input_tile_for_next
 torch.Size([1, 3, 1466, 1650])
myctx [[ 140046822727744[ PI( <0,5,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 5]
shape input_tile_for_next
 torch.Size([1, 3, 1464, 1648])
myctx [[ 140045015171856[ PI( <0,5,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 5]
shape input_tile_for_next
 torch.Size([1, 3, 732, 822])
myctx [[ 140045013800704[ PI( <0,5,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 5]
shape input_tile_for_next
 torch.Size([1, 3, 730, 820])
myctx [[ 140045013848944[ PI( <0,5,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 5]
[torch/csrc/autograd/python_engine.cpp] THPEngine_run_backward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
shape input_tile_for_next
 torch.Size([1, 3, 366, 408])
myctx [[ 140045013206640[ PI( <0,5,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 5]
shape input_tile_for_next
 torch.Size([1, 3, 364, 406])
myctx [[ 140045013206784[ PI( <0,5,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 5]
shape input_tile_for_next
 torch.Size([1, 3, 362, 404])
myctx [[ 140045013206880[ PI( <0,5,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 5]
shape input_tile_for_next
 torch.Size([1, 3, 182, 200])
myctx [[ 140045013206352[ PI( <0,5,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 5]
shape input_tile_for_next
 torch.Size([1, 3, 180, 198])
myctx [[ 140045013206304[ PI( <0,5,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 5]
shape input_tile_for_next
 torch.Size([1, 3, 178, 196])
myctx [[ 140045013207072[ PI( <0,5,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 5]
shape input_tile_for_next
 torch.Size([1, 3, 90, 96])
myctx [[ 140045013206400[ PI( <0,5,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 5]
shape input_tile_for_next
 torch.Size([1, 3, 88, 94])
myctx [[ 140045013206448[ PI( <0,5,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 5]
shape input_tile_for_next
 torch.Size([1, 3, 86, 92])
myctx [[ 140045013206976[ PI( <0,5,>,

in customized Sequential 3
need to get existing

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 86, 92])
grad_out size torch.Size([1, 3, 320, 320])
arg size torch.Size([1, 3, 43, 46])
new_grad_out torch.Size([1, 3, 43, 46])
##############grad_in in maxp torch.Size([1, 3, 86, 92])
I am [0, 5]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 88, 94])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 86, 92])
grad_input torch.Size([1, 3, 88, 94])
padding info :: [0, 0, 1, 0]
grad_input old torch.Size([1, 3, 88, 94])
grad_input new torch.Size([1, 3, 88, 94])
new grad_input torch.Size([1, 3, 88, 94])
grad_out size torch.Size([1, 3, 86, 92])
crop [6, 6, 0, 6] [394, 485, 0, 85] [400, 479, 0, 79]
## 0 80 6 86
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [0, 5]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 90, 96])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 88, 94])
padding info :: [0, 0, 2, 0]
grad_input old torch.Size([1, 3, 90, 96])
grad_input new torch.Size([1, 3, 90, 96])
grad_input torch.Size([1, 3, 90, 96])
grad_out size torch.Size([1, 3, 87, 94])
crop [7, 7, 0, 7] [393, 486, 0, 86] [400, 479, 0, 79]
## 0 80 7 87
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [0, 5]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 92, 98])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 90, 96])
padding info :: [0, 0, 3, 0]
grad_input torch.Size([1, 3, 89, 98])
grad_out size torch.Size([1, 3, 88, 96])
crop [8, 8, 0, 8] [392, 487, 0, 87] [400, 479, 0, 79]
## 0 80 8 88
##############return grad_in in conv2d tensor([ 0.0001, -0.0003, -0.0002,  0.0003, -0.0003, -0.0001,  0.0002,  0.0002,
        -0.0004,  0.0011], device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 178, 196])
grad_out size torch.Size([1, 3, 89, 98])
arg size torch.Size([1, 3, 89, 98])
##############grad_in in maxp torch.Size([1, 3, 178, 196])
I am [0, 5]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 180, 198])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 178, 196])
grad_input torch.Size([1, 3, 180, 198])
padding info :: [0, 0, 1, 0]
grad_input old torch.Size([1, 3, 180, 198])
grad_input new torch.Size([1, 3, 180, 198])
new grad_input torch.Size([1, 3, 180, 198])
grad_out size torch.Size([1, 3, 178, 196])
crop [18, 18, 0, 18] [782, 977, 0, 177] [800, 959, 0, 159]
## 0 160 18 178
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [0, 5]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 182, 200])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 180, 198])
padding info :: [0, 0, 2, 0]
grad_input old torch.Size([1, 3, 182, 200])
grad_input new torch.Size([1, 3, 182, 200])
grad_input torch.Size([1, 3, 182, 200])
grad_out size torch.Size([1, 3, 179, 198])
crop [19, 19, 0, 19] [781, 978, 0, 178] [800, 959, 0, 159]
## 0 160 19 179
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [0, 5]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 184, 202])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 182, 200])
padding info :: [0, 0, 3, 0]
grad_input torch.Size([1, 3, 181, 202])
grad_out size torch.Size([1, 3, 180, 200])
crop [20, 20, 0, 20] [780, 979, 0, 179] [800, 959, 0, 159]
## 0 160 20 180
##############return grad_in in conv2d tensor([ 4.7918e-08, -1.9342e-06, -7.8896e-06,  4.4488e-06,  2.4603e-05,
         9.6615e-07,  1.0182e-05,  6.1595e-05,  1.9876e-06,  1.2672e-05],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 362, 404])
grad_out size torch.Size([1, 3, 181, 202])
arg size torch.Size([1, 3, 181, 202])
##############grad_in in maxp torch.Size([1, 3, 362, 404])
I am [0, 5]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 364, 406])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 362, 404])
grad_input torch.Size([1, 3, 364, 406])
padding info :: [0, 0, 1, 0]
grad_input old [torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cCheckpointBackward
torch.Size([1, 3, 364, 406])
grad_input new torch.Size([1, 3, 364, 406])
new grad_input torch.Size([1, 3, 364, 406])
grad_out size torch.Size([1, 3, 362, 404])
crop [42, 42, 0, 42] [1558, 1961, 0, 361] [1600, 1919, 0, 319]
## 0 320 42 362
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [0, 5]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 366, 408])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 364, 406])
padding info :: [0, 0, 2, 0]
grad_input old torch.Size([1, 3, 366, 408])
grad_input new torch.Size([1, 3, 366, 408])
grad_input torch.Size([1, 3, 366, 408])
grad_out size torch.Size([1, 3, 363, 406])
crop [43, 43, 0, 43] [1557, 1962, 0, 362] [1600, 1919, 0, 319]
## 0 320 43 363
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [0, 5]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 368, 410])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 366, 408])
padding info :: [0, 0, 3, 0]
grad_input torch.Size([1, 3, 365, 410])
grad_out size torch.Size([1, 3, 364, 408])
crop [44, 44, 0, 44] [1556, 1963, 0, 363] [1600, 1919, 0, 319]
## 0 320 44 364
##############return grad_in in conv2d tensor([ 3.4781e-08, -2.1930e-08, -8.2169e-08, -1.8729e-08, -2.1903e-08,
         2.8450e-07,  2.8798e-07, -1.9781e-07,  6.5955e-07,  8.3559e-07],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 730, 820])
grad_out size torch.Size([1, 3, 365, 410])
arg size torch.Size([1, 3, 365, 410])
##############grad_in in maxp torch.Size([1, 3, 730, 820])
I am [0, 5]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 732, 822])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 730, 820])
grad_input torch.Size([1, 3, 732, 822])
padding info :: [0, 0, 1, 0]
grad_input old torch.Size([1, 3, 732, 822])
grad_input new torch.Size([1, 3, 732, 822])
new grad_input torch.Size([1, 3, 732, 822])
grad_out size torch.Size([1, 3, 730, 820])
crop [90, 90, 0, 90] [3110, 3929, 0, 729] [3200, 3839, 0, 639]
## 0 640 90 730
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [0, 5]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 734, 824])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 732, 822])
padding info :: [0, 0, 2, 0]
grad_input torch.Size([1, 3, 732, 824])
grad_out size torch.Size([1, 3, 731, 822])
crop [91, 91, 0, 91] [3109, 3930, 0, 730] [3200, 3839, 0, 639]
## 0 640 91 731
##############return grad_in in conv2d tensor([-7.3241e-11,  6.0541e-10,  2.2243e-09, -4.3874e-09, -5.5238e-09,
        -5.1842e-09, -7.8787e-10, -1.3592e-08, -1.0411e-08, -3.5749e-11],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 1464, 1648])
grad_out size torch.Size([1, 3, 732, 824])
arg size torch.Size([1, 3, 732, 824])
##############grad_in in maxp torch.Size([1, 3, 1464, 1648])
I am [0, 5]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 1466, 1650])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1464, 1648])
grad_input torch.Size([1, 3, 1466, 1650])
padding info :: [0, 0, 1, 0]
grad_input old torch.Size([1, 3, 1466, 1650])
grad_input new torch.Size([1, 3, 1466, 1650])
new grad_input torch.Size([1, 3, 1466, 1650])
grad_out size torch.Size([1, 3, 1464, 1648])
crop [184, 184, 0, 184] [6216, 7863, 0, 1463] [6400, 7679, 0, 1279]
## 0 1280 184 1464
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [0, 5]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 3, 1468, 1652])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1466, 1650])
final torch.Size([1, 3, 1468, 1652])
padding info :: [0, 0, 2, 0]
reshape_for_final ## 0 1280 186 1466
after crop g_in torch.Size([1, 3, 1280, 1280])
grad_out size torch.Size([1, 3, 1465, 1650])
crop [185, 185, 0, 185] [6215, 7864, 0, 1464] [6400, 7679, 0, 1279]
## 0 1280 185 1465
##############return grad_in in conv2d tensor([-2.1507e-08,  3.0015e-08,  1.4504e-08,  3.9477e-08, -1.8712e-08,
        -8.7815e-08,  4.9651e-08, -1.2611e-07,  4.5233e-09,  3.7847e-08],
       device='cuda:0')
[8, 8]
HREREERE torch.Size([1, 3, 10240, 10240])

############# Enter checkpointing bkward ####
inputs len 5 4
in customized Sequential 5
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 4]
shape input_tile_for_next
 torch.Size([1, 3, 1466, 1650])
myctx [[ 140046822727744[ PI( <0,4,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 4]
shape input_tile_for_next
 torch.Size([1, 3, 1464, 1648])
myctx [[ 140045015171856[ PI( <0,4,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 4]
shape input_tile_for_next
 torch.Size([1, 3, 732, 822])
myctx [[ 140045013800704[ PI( <0,4,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 4]
shape input_tile_for_next
 torch.Size([1, 3, 730, 820])
myctx [[ 140045013848944[ PI( <0,4,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 4]
shape input_tile_for_next
 torch.Size([1, 3, 366, 408])
myctx [[ 140045013206640[ PI( <0,4,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 4]
shape input_tile_for_next
 torch.Size([1, 3, 364, 406])
myctx [[ 140045013206784[ PI( <0,4,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 4]
shape input_tile_for_next
 torch.Size([1, 3, 362, 404])
myctx [[ 140045013206880[ PI( <0,4,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() [torch/csrc/autograd/python_engine.cpp] THPEngine_run_backward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 4]
shape input_tile_for_next
 torch.Size([1, 3, 182, 200])
myctx [[ 140045013206352[ PI( <0,4,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 4]
shape input_tile_for_next
 torch.Size([1, 3, 180, 198])
myctx [[ 140045013206304[ PI( <0,4,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 4]
shape input_tile_for_next
 torch.Size([1, 3, 178, 196])
myctx [[ 140045013207072[ PI( <0,4,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 4]
shape input_tile_for_next
 torch.Size([1, 3, 90, 96])
myctx [[ 140045013206400[ PI( <0,4,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 4]
shape input_tile_for_next
 torch.Size([1, 3, 88, 94])
myctx [[ 140045013206448[ PI( <0,4,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 4]
shape input_tile_for_next
 torch.Size([1, 3, 86, 92])
myctx [[ 140045013206976[ PI( <0,4,>,

in customized Sequential 3
need to get existing

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 86, 92])
grad_out size torch.Size([1, 3, 320, 320])
arg size torch.Size([1, 3, 43, 46])
new_grad_out torch.Size([1, 3, 43, 46])
##############grad_in in maxp torch.Size([1, 3, 86, 92])
I am [0, 4]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 88, 94])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 86, 92])
grad_input torch.Size([1, 3, 88, 94])
padding info :: [0, 0, 1, 0]
grad_input old torch.Size([1, 3, 88, 94])
grad_input new torch.Size([1, 3, 88, 94])
new grad_input torch.Size([1, 3, 88, 94])
grad_out size torch.Size([1, 3, 86, 92])
crop [6, 6, 0, 6] [314, 405, 0, 85] [320, 399, 0, 79]
## 0 80 6 86
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [0, 4]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 90, 96])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 88, 94])
padding info :: [0, 0, 2, 0]
grad_input old torch.Size([1, 3, 90, 96])
grad_input new torch.Size([1, 3, 90, 96])
grad_input torch.Size([1, 3, 90, 96])
grad_out size torch.Size([1, 3, 87, 94])
crop [7, 7, 0, 7] [313, 406, 0, 86] [320, 399, 0, 79]
## 0 80 7 87
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [0, 4]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 92, 98])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 90, 96])
padding info :: [0, 0, 3, 0]
grad_input torch.Size([1, 3, 89, 98])
grad_out size torch.Size([1, 3, 88, 96])
crop [8, 8, 0, 8] [312, 407, 0, 87] [320, 399, 0, 79]
## 0 80 8 88
##############return grad_in in conv2d tensor([-2.7687e-05,  1.1056e-04,  6.1772e-06,  6.5253e-04, -1.0390e-04,
         3.4851e-05, -6.5377e-04,  1.7812e-04, -2.2025e-05,  7.6413e-05],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 178, 196])
grad_out size torch.Size([1, 3, 89, 98])
arg size torch.Size([1, 3, 89, 98])
##############grad_in in maxp torch.Size([1, 3, 178, 196])
I am [0, 4]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 180, 198])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 178, 196])
grad_input torch.Size([1, 3, 180, 198])
padding info :: [0, 0, 1, 0]
grad_input old torch.Size([1, 3, 180, 198])
grad_input new torch.Size([1, 3, 180, 198])
new grad_input torch.Size([1, 3, 180, 198])
grad_out size torch.Size([1, 3, 178, 196])
crop [18, 18, 0, 18] [622, 817, 0, 177] [640, 799, 0, 159]
## 0 160 18 178
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [0, 4]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 182, 200])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 180, 198])
padding info :: [0, 0, 2, 0]
grad_input old torch.Size([1, 3, 182, 200])
grad_input new torch.Size([1, 3, 182, 200])
grad_input torch.Size([1, 3, 182, 200])
grad_out size torch.Size([1, 3, 179, 198])
crop [19, 19, 0, 19] [621, 818, 0, 178] [640, 799, 0, 159]
## 0 160 19 179
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [0, 4]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 184, 202])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 182, 200])
padding info :: [0, 0, 3, 0]
grad_input torch.Size([1, 3, 181, 202])
grad_out size torch.Size([1, 3, 180, 200])
crop [20, 20, 0, 20] [620, 819, 0, 179] [640, 799, 0, 159]
## 0 160 20 180
##############return grad_in in conv2d tensor([-1.9972e-07, -1.0254e-06,  1.1275e-06,  3.8120e-06, -3.6687e-06,
        -2.2691e-05,  7.7981e-06,  1.9508e-05, -8.0536e-05, -2.8988e-05],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 362, 404])
grad_out size torch.Size([1, 3, 181, 202])
arg size torch.Size([1, 3, 181, 202])
##############grad_in in maxp torch.Size([1, 3, 362, 404])
I am [0, 4]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 364, 406])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 362, 404])
grad_input torch.Size([1, 3, 364, 406])
padding info :: [0, 0, 1, 0]
grad_input old torch.Size([1, 3, 364, 406])
grad_input new torch.Size([1, 3, 364, 406])
new grad_input torch.Size([1, 3, 364, 406])
grad_out size torch.Size([1, 3, 362, 404])
crop [42, 42, 0, 42] [1238, 1641, 0, 361] [1280, 1599, 0, 319]
## 0 320 42 362
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [0, 4]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 366, 408])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 364, 406])
padding info :: [0, 0, 2, 0]
grad_input old torch.Size([1, 3, 366, 408])
grad_input new torch.Size([1, 3, 366, 408])
grad_input torch.Size([1, 3, 366, 408])
grad_out size torch.Size([1, 3, 363, 406])
crop [43, 43, 0, 43] [1237, 1642, 0, 362] [1280, 1599, 0, 319]
## 0 320 43 363
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [0, 4]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 368, 410])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 366, 408])
padding info :: [0, 0, 3, 0]
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cCheckpointBackward
grad_input torch.Size([1, 3, 365, 410])
grad_out size torch.Size([1, 3, 364, 408])
crop [44, 44, 0, 44] [1236, 1643, 0, 363] [1280, 1599, 0, 319]
## 0 320 44 364
##############return grad_in in conv2d tensor([-1.6383e-08, -9.9577e-09,  4.2304e-08,  2.9564e-08, -3.8846e-08,
         2.2597e-08, -3.8906e-08,  4.6748e-08, -1.1634e-07, -8.3652e-09],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 730, 820])
grad_out size torch.Size([1, 3, 365, 410])
arg size torch.Size([1, 3, 365, 410])
##############grad_in in maxp torch.Size([1, 3, 730, 820])
I am [0, 4]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 732, 822])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 730, 820])
grad_input torch.Size([1, 3, 732, 822])
padding info :: [0, 0, 1, 0]
grad_input old torch.Size([1, 3, 732, 822])
grad_input new torch.Size([1, 3, 732, 822])
new grad_input torch.Size([1, 3, 732, 822])
grad_out size torch.Size([1, 3, 730, 820])
crop [90, 90, 0, 90] [2470, 3289, 0, 729] [2560, 3199, 0, 639]
## 0 640 90 730
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [0, 4]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 734, 824])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 732, 822])
padding info :: [0, 0, 2, 0]
grad_input torch.Size([1, 3, 732, 824])
grad_out size torch.Size([1, 3, 731, 822])
crop [91, 91, 0, 91] [2469, 3290, 0, 730] [2560, 3199, 0, 639]
## 0 640 91 731
##############return grad_in in conv2d tensor([ 5.4345e-10,  2.2045e-09,  1.7674e-09,  9.1244e-10, -1.6549e-09,
        -1.1448e-09,  1.0176e-09,  4.1594e-09, -1.8783e-10,  4.4892e-09],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 1464, 1648])
grad_out size torch.Size([1, 3, 732, 824])
arg size torch.Size([1, 3, 732, 824])
##############grad_in in maxp torch.Size([1, 3, 1464, 1648])
I am [0, 4]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 1466, 1650])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1464, 1648])
grad_input torch.Size([1, 3, 1466, 1650])
padding info :: [0, 0, 1, 0]
grad_input old torch.Size([1, 3, 1466, 1650])
grad_input new torch.Size([1, 3, 1466, 1650])
new grad_input torch.Size([1, 3, 1466, 1650])
grad_out size torch.Size([1, 3, 1464, 1648])
crop [184, 184, 0, 184] [4936, 6583, 0, 1463] [5120, 6399, 0, 1279]
## 0 1280 184 1464
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [0, 4]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 3, 1468, 1652])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1466, 1650])
final torch.Size([1, 3, 1468, 1652])
padding info :: [0, 0, 2, 0]
reshape_for_final ## 0 1280 186 1466
after crop g_in torch.Size([1, 3, 1280, 1280])
grad_out size torch.Size([1, 3, 1465, 1650])
crop [185, 185, 0, 185] [4935, 6584, 0, 1464] [5120, 6399, 0, 1279]
## 0 1280 185 1465
##############return grad_in in conv2d tensor([ 1.4977e-08, -4.0640e-08,  1.4876e-08, -2.8801e-08, -1.4299e-08,
        -8.8582e-08,  1.6013e-08, -3.8504e-08,  2.0636e-08,  2.6504e-08],
       device='cuda:0')
[8, 8]
HREREERE torch.Size([1, 3, 10240, 10240])

############# Enter checkpointing bkward ####
inputs len 5 4
in customized Sequential 5
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 3]
shape input_tile_for_next
 torch.Size([1, 3, 1466, 1650])
myctx [[ 140046822727744[ PI( <0,3,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 3]
shape input_tile_for_next
 torch.Size([1, 3, 1464, 1648])
myctx [[ 140045015171856[ PI( <0,3,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 3]
shape input_tile_for_next
 torch.Size([1, 3, 732, 822])
myctx [[ 140045013800704[ PI( <0,3,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 3]
shape input_tile_for_next
 torch.Size([1, 3, 730, 820])
myctx [[ 140045013848944[ PI( <0,3,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 3]
shape input_tile_for_next
 torch.Size([1, 3, 366, 408])
myctx [[ 140045013206640[ PI( <0,3,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 3]
shape input_tile_for_next
 torch.Size([1, 3, 364, 406])
myctx [[ 140045013206784[ PI( <0,3,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 3]
shape input_tile_for_next
 torch.Size([1, 3, 362, 404])
myctx [[ 140045013206880[ PI( <0,3,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 3]
shape input_tile_for_next
 torch.Size([1, 3, 182, 200])
myctx [[ 140045013206352[ PI( <0,3,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 3]
shape input_tile_for_next
 torch.Size([1, 3, 180, 198])
myctx [[ 140045013206304[ PI( <0,3,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 3]
shape input_tile_for_next
 torch.Size([1, 3, 178, 196])
myctx [[ 140045013207072[ PI( <0,3,>,

in customized Sequential 3
[torch/csrc/autograd/python_engine.cpp] THPEngine_run_backward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 3]
shape input_tile_for_next
 torch.Size([1, 3, 90, 96])
myctx [[ 140045013206400[ PI( <0,3,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 3]
shape input_tile_for_next
 torch.Size([1, 3, 88, 94])
myctx [[ 140045013206448[ PI( <0,3,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 3]
shape input_tile_for_next
 torch.Size([1, 3, 86, 92])
myctx [[ 140045013206976[ PI( <0,3,>,

in customized Sequential 3
need to get existing

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 86, 92])
grad_out size torch.Size([1, 3, 320, 320])
arg size torch.Size([1, 3, 43, 46])
new_grad_out torch.Size([1, 3, 43, 46])
##############grad_in in maxp torch.Size([1, 3, 86, 92])
I am [0, 3]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 88, 94])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 86, 92])
grad_input torch.Size([1, 3, 88, 94])
padding info :: [0, 0, 1, 0]
grad_input old torch.Size([1, 3, 88, 94])
grad_input new torch.Size([1, 3, 88, 94])
new grad_input torch.Size([1, 3, 88, 94])
grad_out size torch.Size([1, 3, 86, 92])
crop [6, 6, 0, 6] [234, 325, 0, 85] [240, 319, 0, 79]
## 0 80 6 86
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [0, 3]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 90, 96])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 88, 94])
padding info :: [0, 0, 2, 0]
grad_input old torch.Size([1, 3, 90, 96])
grad_input new torch.Size([1, 3, 90, 96])
grad_input torch.Size([1, 3, 90, 96])
grad_out size torch.Size([1, 3, 87, 94])
crop [7, 7, 0, 7] [233, 326, 0, 86] [240, 319, 0, 79]
## 0 80 7 87
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [0, 3]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 92, 98])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 90, 96])
padding info :: [0, 0, 3, 0]
grad_input torch.Size([1, 3, 89, 98])
grad_out size torch.Size([1, 3, 88, 96])
crop [8, 8, 0, 8] [232, 327, 0, 87] [240, 319, 0, 79]
## 0 80 8 88
##############return grad_in in conv2d tensor([ 0.0002, -0.0001, -0.0015,  0.0007,  0.0005,  0.0012,  0.0006, -0.0010,
         0.0007, -0.0011], device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 178, 196])
grad_out size torch.Size([1, 3, 89, 98])
arg size torch.Size([1, 3, 89, 98])
##############grad_in in maxp torch.Size([1, 3, 178, 196])
I am [0, 3]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 180, 198])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 178, 196])
grad_input torch.Size([1, 3, 180, 198])
padding info :: [0, 0, 1, 0]
grad_input old torch.Size([1, 3, 180, 198])
grad_input new torch.Size([1, 3, 180, 198])
new grad_input torch.Size([1, 3, 180, 198])
grad_out size torch.Size([1, 3, 178, 196])
crop [18, 18, 0, 18] [462, 657, 0, 177] [480, 639, 0, 159]
## 0 160 18 178
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [0, 3]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 182, 200])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 180, 198])
padding info :: [0, 0, 2, 0]
grad_input old torch.Size([1, 3, 182, 200])
grad_input new torch.Size([1, 3, 182, 200])
grad_input torch.Size([1, 3, 182, 200])
grad_out size torch.Size([1, 3, 179, 198])
crop [19, 19, 0, 19] [461, 658, 0, 178] [480, 639, 0, 159]
## 0 160 19 179
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [0, 3]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 184, 202])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 182, 200])
padding info :: [0, 0, 3, 0]
grad_input torch.Size([1, 3, 181, 202])
grad_out size torch.Size([1, 3, 180, 200])
crop [20, 20, 0, 20] [460, 659, 0, 179] [480, 639, 0, 159]
## 0 160 20 180
##############return grad_in in conv2d tensor([-3.0233e-06, -3.8298e-06, -1.8258e-06,  9.3102e-06,  2.4675e-06,
         3.9580e-05,  1.2403e-04,  1.5759e-05, -8.3700e-05,  2.7875e-04],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 362, 404])
grad_out size torch.Size([1, 3, 181, 202])
arg size torch.Size([1, 3, 181, 202])
##############grad_in in maxp torch.Size([1, 3, 362, 404])
I am [0, 3]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 364, 406])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 362, 404])
grad_input torch.Size([1, 3, 364, 406])
padding info :: [0, 0, 1, 0]
grad_input old torch.Size([1, 3, 364, 406])
grad_input new torch.Size([1, 3, 364, 406])
new grad_input torch.Size([1, 3, 364, 406])
grad_out size torch.Size([1, 3, 362, 404])
crop [42, 42, 0, 42] [918, 1321, 0, 361] [960, 1279, 0, 319]
## 0 320 42 362
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [0, 3]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 366, 408])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 364, 406])
padding info :: [0, 0, 2, 0]
grad_input old torch.Size([1, 3, 366, 408])
grad_input new torch.Size([1, 3, 366, 408])
grad_input torch.Size([1, 3, 366, 408])
grad_out size torch.Size([1, 3, 363, 406])
crop [43, 43, 0, 43] [917, 1322, 0, 362] [960, 1279, 0, 319]
## 0 320 43 363
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [0, 3]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 368, 410])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 366, 408])
padding info :: [0, 0, 3, 0]
grad_input torch.Size([1, 3, 365, 410])
grad_out size torch.Size([1, 3, 364, 408])
crop [44, 44, 0, 44] [916, 1323, 0, 363] [960, 1279, 0, 319]
## 0 320 44 364
##############return grad_in in conv2d tensor([ 2.2664e-08,  8.9448e-08, -5.7222e-08,  1.6713e-07,  9.1558e-07,
         1.1978e-07, -2.4821e-07,  7.2954e-07,  1.0860e-06,  8.1288e-07],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 730, 820])
grad_out size torch.Size([1, 3, 365, 410])
arg size torch.Size([1, 3, 365, 410])
##############grad_in in maxp torch.Size([1, 3, 730, 820])
I am [0, 3]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 732, 822])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 730, 820])
grad_input torch.Size([1, 3, 732, 822])
padding info :: [0, 0, 1, 0]
grad_input old torch.Size([1, 3, 732, 822])
grad_input new torch.Size([1, 3, 732, 822])
new grad_input torch.Size([1, 3, 732, 822])
grad_out size torch.Size([1, 3, 730, 820])
crop [90, 90, 0, 90] [1830, 2649, 0, 729] [1920, 2559, 0, 639]
## 0 640 90 730
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [0, 3]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 734, 824])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 732, 822])
padding info ::[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cCheckpointBackward
 [0, 0, 2, 0]
grad_input torch.Size([1, 3, 732, 824])
grad_out size torch.Size([1, 3, 731, 822])
crop [91, 91, 0, 91] [1829, 2650, 0, 730] [1920, 2559, 0, 639]
## 0 640 91 731
##############return grad_in in conv2d tensor([-2.3008e-09,  4.8751e-10,  4.2341e-09, -1.4845e-09,  8.6158e-09,
         2.8518e-09,  1.5980e-09, -2.4805e-08, -6.1499e-08, -2.8152e-08],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 1464, 1648])
grad_out size torch.Size([1, 3, 732, 824])
arg size torch.Size([1, 3, 732, 824])
##############grad_in in maxp torch.Size([1, 3, 1464, 1648])
I am [0, 3]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 1466, 1650])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1464, 1648])
grad_input torch.Size([1, 3, 1466, 1650])
padding info :: [0, 0, 1, 0]
grad_input old torch.Size([1, 3, 1466, 1650])
grad_input new torch.Size([1, 3, 1466, 1650])
new grad_input torch.Size([1, 3, 1466, 1650])
grad_out size torch.Size([1, 3, 1464, 1648])
crop [184, 184, 0, 184] [3656, 5303, 0, 1463] [3840, 5119, 0, 1279]
## 0 1280 184 1464
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [0, 3]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 3, 1468, 1652])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1466, 1650])
final torch.Size([1, 3, 1468, 1652])
padding info :: [0, 0, 2, 0]
reshape_for_final ## 0 1280 186 1466
after crop g_in torch.Size([1, 3, 1280, 1280])
grad_out size torch.Size([1, 3, 1465, 1650])
crop [185, 185, 0, 185] [3655, 5304, 0, 1464] [3840, 5119, 0, 1279]
## 0 1280 185 1465
##############return grad_in in conv2d tensor([ 2.1137e-08, -3.4144e-09,  3.2534e-08,  1.3296e-08, -4.1697e-09,
        -4.1840e-08,  2.0674e-08, -2.6026e-09, -5.7015e-08,  3.7608e-08],
       device='cuda:0')
[8, 8]
HREREERE torch.Size([1, 3, 10240, 10240])

############# Enter checkpointing bkward ####
inputs len 5 4
in customized Sequential 5
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 2]
shape input_tile_for_next
 torch.Size([1, 3, 1466, 1650])
myctx [[ 140046822727744[ PI( <0,2,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 2]
shape input_tile_for_next
 torch.Size([1, 3, 1464, 1648])
myctx [[ 140045015171856[ PI( <0,2,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 2]
shape input_tile_for_next
 torch.Size([1, 3, 732, 822])
myctx [[ 140045013800704[ PI( <0,2,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 2]
shape input_tile_for_next
 torch.Size([1, 3, 730, 820])
myctx [[ 140045013848944[ PI( <0,2,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 2]
shape input_tile_for_next
 torch.Size([1, 3, 366, 408])
myctx [[ 140045013206640[ PI( <0,2,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 2]
shape input_tile_for_next
 torch.Size([1, 3, 364, 406])
myctx [[ 140045013206784[ PI( <0,2,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 2]
shape input_tile_for_next
 torch.Size([1, 3, 362, 404])
myctx [[ 140045013206880[ PI( <0,2,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 2]
shape input_tile_for_next
 torch.Size([1, 3, 182, 200])
myctx [[ 140045013206352[ PI( <0,2,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 2]
shape input_tile_for_next
 torch.Size([1, 3, 180, 198])
myctx [[ 140045013206304[ PI( <0,2,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 2]
shape input_tile_for_next
 torch.Size([1, 3, 178, 196])
myctx [[ 140045013207072[ PI( <0,2,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 2]
shape input_tile_for_next
 torch.Size([1, 3, 90, 96])
myctx [[ 140045013206400[ PI( <0,2,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 2]
shape input_tile_for_next
 torch.Size([1, 3, 88, 94])
myctx [[ 140045013206448[ PI( <0,2,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 2]
shape input_tile_for_next
 torch.Size([1, 3, 86, 92])
myctx [torch/csrc/autograd/python_engine.cpp] THPEngine_run_backward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[[ 140045013206976[ PI( <0,2,>,

in customized Sequential 3
need to get existing

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 86, 92])
grad_out size torch.Size([1, 3, 320, 320])
arg size torch.Size([1, 3, 43, 46])
new_grad_out torch.Size([1, 3, 43, 46])
##############grad_in in maxp torch.Size([1, 3, 86, 92])
I am [0, 2]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 88, 94])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 86, 92])
grad_input torch.Size([1, 3, 88, 94])
padding info :: [0, 0, 1, 0]
grad_input old torch.Size([1, 3, 88, 94])
grad_input new torch.Size([1, 3, 88, 94])
new grad_input torch.Size([1, 3, 88, 94])
grad_out size torch.Size([1, 3, 86, 92])
crop [6, 6, 0, 6] [154, 245, 0, 85] [160, 239, 0, 79]
## 0 80 6 86
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [0, 2]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 90, 96])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 88, 94])
padding info :: [0, 0, 2, 0]
grad_input old torch.Size([1, 3, 90, 96])
grad_input new torch.Size([1, 3, 90, 96])
grad_input torch.Size([1, 3, 90, 96])
grad_out size torch.Size([1, 3, 87, 94])
crop [7, 7, 0, 7] [153, 246, 0, 86] [160, 239, 0, 79]
## 0 80 7 87
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [0, 2]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 92, 98])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 90, 96])
padding info :: [0, 0, 3, 0]
grad_input torch.Size([1, 3, 89, 98])
grad_out size torch.Size([1, 3, 88, 96])
crop [8, 8, 0, 8] [152, 247, 0, 87] [160, 239, 0, 79]
## 0 80 8 88
##############return grad_in in conv2d tensor([ 5.7742e-05, -9.1420e-05,  7.3844e-06,  6.4480e-04, -1.1988e-03,
         3.6344e-04, -1.9267e-04, -6.5420e-04, -1.5072e-03,  2.2530e-03],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 178, 196])
grad_out size torch.Size([1, 3, 89, 98])
arg size torch.Size([1, 3, 89, 98])
##############grad_in in maxp torch.Size([1, 3, 178, 196])
I am [0, 2]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 180, 198])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 178, 196])
grad_input torch.Size([1, 3, 180, 198])
padding info :: [0, 0, 1, 0]
grad_input old torch.Size([1, 3, 180, 198])
grad_input new torch.Size([1, 3, 180, 198])
new grad_input torch.Size([1, 3, 180, 198])
grad_out size torch.Size([1, 3, 178, 196])
crop [18, 18, 0, 18] [302, 497, 0, 177] [320, 479, 0, 159]
## 0 160 18 178
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [0, 2]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 182, 200])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 180, 198])
padding info :: [0, 0, 2, 0]
grad_input old torch.Size([1, 3, 182, 200])
grad_input new torch.Size([1, 3, 182, 200])
grad_input torch.Size([1, 3, 182, 200])
grad_out size torch.Size([1, 3, 179, 198])
crop [19, 19, 0, 19] [301, 498, 0, 178] [320, 479, 0, 159]
## 0 160 19 179
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [0, 2]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 184, 202])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 182, 200])
padding info :: [0, 0, 3, 0]
grad_input torch.Size([1, 3, 181, 202])
grad_out size torch.Size([1, 3, 180, 200])
crop [20, 20, 0, 20] [300, 499, 0, 179] [320, 479, 0, 159]
## 0 160 20 180
##############return grad_in in conv2d tensor([ 9.1224e-07, -2.0719e-06, -4.5257e-06, -5.7091e-06,  7.5809e-06,
        -9.0996e-07,  1.1910e-05, -2.1658e-06, -3.7915e-05,  3.1456e-05],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 362, 404])
grad_out size torch.Size([1, 3, 181, 202])
arg size torch.Size([1, 3, 181, 202])
##############grad_in in maxp torch.Size([1, 3, 362, 404])
I am [0, 2]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 364, 406])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 362, 404])
grad_input torch.Size([1, 3, 364, 406])
padding info :: [0, 0, 1, 0]
grad_input old torch.Size([1, 3, 364, 406])
grad_input new torch.Size([1, 3, 364, 406])
new grad_input torch.Size([1, 3, 364, 406])
grad_out size torch.Size([1, 3, 362, 404])
crop [42, 42, 0, 42] [598, 1001, 0, 361] [640, 959, 0, 319]
## 0 320 42 362
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [0, 2]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 366, 408])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 364, 406])
padding info :: [0, 0, 2, 0]
grad_input old torch.Size([1, 3, 366, 408])
grad_input new torch.Size([1, 3, 366, 408])
grad_input torch.Size([1, 3, 366, 408])
grad_out size torch.Size([1, 3, 363, 406])
crop [43, 43, 0, 43] [597, 1002, 0, 362] [640, 959, 0, 319]
## 0 320 43 363
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [0, 2]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 368, 410])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 366, 408])
padding info :: [0, 0, 3, 0]
grad_input torch.Size([1, 3, 365, 410])
grad_out size torch.Size([1, 3, 364, 408])
crop [44, 44, 0, 44] [596, 1003, 0, 363] [640, 959, 0, 319]
## 0 320 44 364
##############return grad_in in conv2d tensor([-1.1109e-09, -6.4687e-09, -3.1604e-08, -1.4835e-07, -1.9268e-07,
         6.7391e-08,  2.9366e-07,  7.0601e-07,  4.0692e-07, -2.9142e-07],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 730, 820])
grad_out size torch.Size([1, 3, 365, 410])
arg size torch.Size([1, 3, 365, 410])
##############grad_in in maxp torch.Size([1, 3, 730, 820])
I am [0, 2]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 732, 822])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 730, 820])
grad_input torch.Size([1, 3, 732, 822])
padding info :: [0, 0, 1, 0]
grad_input old torch.Size([1, 3, 732, 822])
grad_input new torch.Size([1, 3, 732, 822])
new grad_input torch.Size([1, 3, 732, 822])
grad_out size torch.Size([1, 3, 730, 820])
crop [90, 90, 0, 90] [1190, 2009, 0, 729] [1280, 1919, 0, 639]
## 0 640 90 730
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [0, 2]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 734, 824])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 732, 822])
padding info :: [0, 0, 2, 0]
grad_input torch.Size([1, 3, 732, 824])
grad_out size torch.Size([1, 3, 731, 822])
crop [91, 91, 0, 91] [1189, 2010, 0, 730] [1280, 1919, 0, 639]
## 0 640 91 731
##############return grad_in in conv2d tensor([ 5.7585e-11,  1.3660e-10,  1.0732e-09,  2.1181e-10, -1.5306e-09,
         1.4384e-09, -2.0385e-09,  7.8688e-09,  2.5634e-09,  1.8784e-08],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 1464, 1648])
grad_out size torch.Size([1, 3, 732, 824])
arg size torch.Size([1, 3, 732, 824])
##############grad_in in maxp torch.Size([1, 3, 1464, 1648])
I am [0, 2]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 1466, 1650])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1464, 1648])
grad_input torch.Size([1, 3, 1466, 1650])
padding info :: [0, 0, 1, 0]
grad_input old torch.Size([1, 3, 1466, 1650])
grad_input new torch.Size([1, 3, 1466, 1650])
new grad_input torch.Size([1, 3, 1466, 1650])
grad_out size torch.Size([1, 3, 1464, 1648])
crop [184, 184, 0, 184] [2376, 4023, 0, 1463] [2560, 3839, 0, 1279]
## 0 1280 184 1464
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [0, 2]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 3, 1468, 1652])
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cCheckpointBackward
[torch/csrc/autograd/python_engine.cpp] THPEngine_run_backward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1466, 1650])
final torch.Size([1, 3, 1468, 1652])
padding info :: [0, 0, 2, 0]
reshape_for_final ## 0 1280 186 1466
after crop g_in torch.Size([1, 3, 1280, 1280])
grad_out size torch.Size([1, 3, 1465, 1650])
crop [185, 185, 0, 185] [2375, 4024, 0, 1464] [2560, 3839, 0, 1279]
## 0 1280 185 1465
##############return grad_in in conv2d tensor([ 2.3085e-08,  7.8012e-09, -1.0021e-07,  3.7126e-08,  2.3335e-08,
         2.9418e-08,  1.1341e-08,  5.2598e-08,  6.3631e-09,  1.0790e-08],
       device='cuda:0')
[8, 8]
HREREERE torch.Size([1, 3, 10240, 10240])

############# Enter checkpointing bkward ####
inputs len 5 4
in customized Sequential 5
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 1]
shape input_tile_for_next
 torch.Size([1, 3, 1466, 1650])
myctx [[ 140046822727744[ PI( <0,1,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 1]
shape input_tile_for_next
 torch.Size([1, 3, 1464, 1648])
myctx [[ 140045015171856[ PI( <0,1,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 1]
shape input_tile_for_next
 torch.Size([1, 3, 732, 822])
myctx [[ 140045013800704[ PI( <0,1,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 1]
shape input_tile_for_next
 torch.Size([1, 3, 730, 820])
myctx [[ 140045013848944[ PI( <0,1,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 1]
shape input_tile_for_next
 torch.Size([1, 3, 366, 408])
myctx [[ 140045013206640[ PI( <0,1,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 1]
shape input_tile_for_next
 torch.Size([1, 3, 364, 406])
myctx [[ 140045013206784[ PI( <0,1,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 1]
shape input_tile_for_next
 torch.Size([1, 3, 362, 404])
myctx [[ 140045013206880[ PI( <0,1,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 1]
shape input_tile_for_next
 torch.Size([1, 3, 182, 200])
myctx [[ 140045013206352[ PI( <0,1,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 1]
shape input_tile_for_next
 torch.Size([1, 3, 180, 198])
myctx [[ 140045013206304[ PI( <0,1,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 1]
shape input_tile_for_next
 torch.Size([1, 3, 178, 196])
myctx [[ 140045013207072[ PI( <0,1,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 1]
shape input_tile_for_next
 torch.Size([1, 3, 90, 96])
myctx [[ 140045013206400[ PI( <0,1,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 1]
shape input_tile_for_next
 torch.Size([1, 3, 88, 94])
myctx [[ 140045013206448[ PI( <0,1,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 1]
shape input_tile_for_next
 torch.Size([1, 3, 86, 92])
myctx [[ 140045013206976[ PI( <0,1,>,

in customized Sequential 3
need to get existing

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 86, 92])
grad_out size torch.Size([1, 3, 320, 320])
arg size torch.Size([1, 3, 43, 46])
new_grad_out torch.Size([1, 3, 43, 46])
##############grad_in in maxp torch.Size([1, 3, 86, 92])
I am [0, 1]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 88, 94])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 86, 92])
grad_input torch.Size([1, 3, 88, 94])
padding info :: [0, 0, 1, 0]
grad_input old torch.Size([1, 3, 88, 94])
grad_input new torch.Size([1, 3, 88, 94])
new grad_input torch.Size([1, 3, 88, 94])
grad_out size torch.Size([1, 3, 86, 92])
crop [6, 6, 0, 6] [74, 165, 0, 85] [80, 159, 0, 79]
## 0 80 6 86
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [0, 1]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 90, 96])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 88, 94])
padding info :: [0, 0, 2, 0]
grad_input old torch.Size([1, 3, 90, 96])
grad_input new torch.Size([1, 3, 90, 96])
grad_input torch.Size([1, 3, 90, 96])
grad_out size torch.Size([1, 3, 87, 94])
crop [7, 7, 0, 7] [73, 166, 0, 86] [80, 159, 0, 79]
## 0 80 7 87
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cCheckpointBackward
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [0, 1]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 92, 98])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 90, 96])
padding info :: [0, 0, 3, 0]
grad_input torch.Size([1, 3, 89, 98])
grad_out size torch.Size([1, 3, 88, 96])
crop [8, 8, 0, 8] [72, 167, 0, 87] [80, 159, 0, 79]
## 0 80 8 88
##############return grad_in in conv2d tensor([ 3.2827e-04, -9.2036e-05,  5.7463e-04, -8.5718e-04,  2.8707e-04,
         9.1741e-04, -8.1300e-04,  2.2631e-04,  1.2637e-03,  1.8269e-04],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 178, 196])
grad_out size torch.Size([1, 3, 89, 98])
arg size torch.Size([1, 3, 89, 98])
##############grad_in in maxp torch.Size([1, 3, 178, 196])
I am [0, 1]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 180, 198])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 178, 196])
grad_input torch.Size([1, 3, 180, 198])
padding info :: [0, 0, 1, 0]
grad_input old torch.Size([1, 3, 180, 198])
grad_input new torch.Size([1, 3, 180, 198])
new grad_input torch.Size([1, 3, 180, 198])
grad_out size torch.Size([1, 3, 178, 196])
crop [18, 18, 0, 18] [142, 337, 0, 177] [160, 319, 0, 159]
## 0 160 18 178
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [0, 1]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 182, 200])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 180, 198])
padding info :: [0, 0, 2, 0]
grad_input old torch.Size([1, 3, 182, 200])
grad_input new torch.Size([1, 3, 182, 200])
grad_input torch.Size([1, 3, 182, 200])
grad_out size torch.Size([1, 3, 179, 198])
crop [19, 19, 0, 19] [141, 338, 0, 178] [160, 319, 0, 159]
## 0 160 19 179
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [0, 1]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 184, 202])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 182, 200])
padding info :: [0, 0, 3, 0]
grad_input torch.Size([1, 3, 181, 202])
grad_out size torch.Size([1, 3, 180, 200])
crop [20, 20, 0, 20] [140, 339, 0, 179] [160, 319, 0, 159]
## 0 160 20 180
##############return grad_in in conv2d tensor([ 6.0234e-06, -1.1996e-05, -2.3264e-05,  1.2331e-05, -5.1275e-05,
        -1.8367e-05,  3.9486e-05, -8.1154e-05, -3.3790e-06,  8.6528e-06],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 362, 404])
grad_out size torch.Size([1, 3, 181, 202])
arg size torch.Size([1, 3, 181, 202])
##############grad_in in maxp torch.Size([1, 3, 362, 404])
I am [0, 1]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 364, 406])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 362, 404])
grad_input torch.Size([1, 3, 364, 406])
padding info :: [0, 0, 1, 0]
grad_input old torch.Size([1, 3, 364, 406])
grad_input new torch.Size([1, 3, 364, 406])
new grad_input torch.Size([1, 3, 364, 406])
grad_out size torch.Size([1, 3, 362, 404])
crop [42, 42, 0, 42] [278, 681, 0, 361] [320, 639, 0, 319]
## 0 320 42 362
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [0, 1]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 366, 408])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 364, 406])
padding info :: [0, 0, 2, 0]
grad_input old torch.Size([1, 3, 366, 408])
grad_input new torch.Size([1, 3, 366, 408])
grad_input torch.Size([1, 3, 366, 408])
grad_out size torch.Size([1, 3, 363, 406])
crop [43, 43, 0, 43] [277, 682, 0, 362] [320, 639, 0, 319]
## 0 320 43 363
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [0, 1]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 368, 410])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 366, 408])
padding info :: [0, 0, 3, 0]
grad_input torch.Size([1, 3, 365, 410])
grad_out size torch.Size([1, 3, 364, 408])
crop [44, 44, 0, 44] [276, 683, 0, 363] [320, 639, 0, 319]
## 0 320 44 364
##############return grad_in in conv2d tensor([-5.7474e-08, -2.0449e-07, -1.1589e-07,  2.0864e-07,  6.9261e-07,
         1.0409e-06, -1.1623e-07, -4.9868e-07, -3.3783e-07,  1.9469e-06],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 730, 820])
grad_out size torch.Size([1, 3, 365, 410])
arg size torch.Size([1, 3, 365, 410])
##############grad_in in maxp torch.Size([1, 3, 730, 820])
I am [0, 1]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 732, 822])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 730, 820])
grad_input torch.Size([1, 3, 732, 822])
padding info :: [0, 0, 1, 0]
grad_input old torch.Size([1, 3, 732, 822])
grad_input new torch.Size([1, 3, 732, 822])
new grad_input torch.Size([1, 3, 732, 822])
grad_out size torch.Size([1, 3, 730, 820])
crop [90, 90, 0, 90] [550, 1369, 0, 729] [640, 1279, 0, 639]
## 0 640 90 730
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [0, 1]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 734, 824])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 732, 822])
padding info :: [0, 0, 2, 0]
grad_input torch.Size([1, 3, 732, 824])
grad_out size torch.Size([1, 3, 731, 822])
crop [91, 91, 0, 91] [549, 1370, 0, 730] [640, 1279, 0, 639]
## 0 640 91 731
##############return grad_in in conv2d tensor([-4.2177e-09,  8.2828e-09, -9.3928e-10,  8.3431e-09,  1.4223e-08,
         3.0365e-08,  2.4599e-08,  1.9716e-08, -5.8107e-08, -7.8135e-08],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 1464, 1648])
grad_out size torch.Size([1, 3, 732, 824])
arg size torch.Size([1, 3, 732, 824])
##############grad_in in maxp torch.Size([1, 3, 1464, 1648])
I am [0, 1]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 1466, 1650])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1464, 1648])
grad_input torch.Size([1, 3, 1466, 1650])
padding info :: [0, 0, 1, 0]
grad_input old torch.Size([1, 3, 1466, 1650])
grad_input new torch.Size([1, 3, 1466, 1650])
new grad_input torch.Size([1, 3, 1466, 1650])
grad_out size torch.Size([1, 3, 1464, 1648])
crop [184, 184, 0, 184] [1096, 2743, 0, 1463] [1280, 2559, 0, 1279]
## 0 1280 184 1464
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [0, 1]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 3, 1468, 1652])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1466, 1650])
final torch.Size([1, 3, 1468, 1652])
padding info :: [0, 0, 2, 0]
reshape_for_final ## 0 1280 186 1466
after crop g_in torch.Size([1, 3, 1280, 1280])
grad_out size torch.Size([1, 3, 1465, 1650])
crop [185, 185, 0, 185] [1095, 2744, 0, 1464] [1280, 2559, 0, 1279]
## 0 1280 185 1465
##############return grad_in in conv2d tensor([ 1.8338e-08, -9.1225e-08,  1.5213e-08,  2.7960e-08,  2.3547e-08,
        -2.7248e-08,  1.0224e-07, -7.6428e-08, -4.1121e-08, -5.6596e-08],
       device='cuda:0')
[8, 8]
HREREERE torch.Size([1, 3, 10240, 10240])

############# Enter checkpointing bkward ####
inputs len 5 4
in customized Sequential 5
in customized Sequential 2
id 140046822727744
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 0]
shape input_tile_for_next
 torch.Size([1, 3, 1466, 1466])
myctx [[ 140046822727744[ PI( <0,0,>,

in customized Sequential 3
id 140045015171856
== tiled conv2d forward
myctx_dict.keys() [torch/csrc/autograd/python_engine.cpp] THPEngine_run_backward
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 0]
shape input_tile_for_next
 torch.Size([1, 3, 1464, 1464])
myctx [[ 140045015171856[ PI( <0,0,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013800704
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 0]
shape input_tile_for_next
 torch.Size([1, 3, 732, 732])
myctx [[ 140045013800704[ PI( <0,0,>,

in customized Sequential 3
id 140045013848944
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 0]
shape input_tile_for_next
 torch.Size([1, 3, 730, 730])
myctx [[ 140045013848944[ PI( <0,0,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206640
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 0]
shape input_tile_for_next
 torch.Size([1, 3, 366, 366])
myctx [[ 140045013206640[ PI( <0,0,>,

in customized Sequential 3
id 140045013206784
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 0]
shape input_tile_for_next
 torch.Size([1, 3, 364, 364])
myctx [[ 140045013206784[ PI( <0,0,>,

in customized Sequential 3
id 140045013206880
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 0]
shape input_tile_for_next
 torch.Size([1, 3, 362, 362])
myctx [[ 140045013206880[ PI( <0,0,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206352
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 0]
shape input_tile_for_next
 torch.Size([1, 3, 182, 182])
myctx [[ 140045013206352[ PI( <0,0,>,

in customized Sequential 3
id 140045013206304
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 0]
shape input_tile_for_next
 torch.Size([1, 3, 180, 180])
myctx [[ 140045013206304[ PI( <0,0,>,

in customized Sequential 3
id 140045013207072
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 0]
shape input_tile_for_next
 torch.Size([1, 3, 178, 178])
myctx [[ 140045013207072[ PI( <0,0,>,

in customized Sequential 3
need to get existing
in customized Sequential 3
id 140045013206400
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 0]
shape input_tile_for_next
 torch.Size([1, 3, 90, 90])
myctx [[ 140045013206400[ PI( <0,0,>,

in customized Sequential 3
id 140045013206448
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 0]
shape input_tile_for_next
 torch.Size([1, 3, 88, 88])
myctx [[ 140045013206448[ PI( <0,0,>,

in customized Sequential 3
id 140045013206976
== tiled conv2d forward
myctx_dict.keys() dict_keys([140046822727744, 140045015171856, 140045013800704, 140045013848944, 140045013206640, 140045013206784, 140045013206880, 140045013206352, 140045013206304, 140045013207072, 140045013206400, 140045013206448, 140045013206976])
need to get existing
current fwd info [0, 0]
shape input_tile_for_next
 torch.Size([1, 3, 86, 86])
myctx [[ 140045013206976[ PI( <0,0,>,

in customized Sequential 3
need to get existing

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 86, 86])
grad_out size torch.Size([1, 3, 320, 320])
arg size torch.Size([1, 3, 43, 43])
new_grad_out torch.Size([1, 3, 43, 43])
##############grad_in in maxp torch.Size([1, 3, 86, 86])
I am [0, 0]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 88, 88])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 86, 86])
grad_input torch.Size([1, 3, 88, 88])
padding info :: [1, 0, 1, 0]
grad_input old torch.Size([1, 3, 88, 88])
grad_input new torch.Size([1, 3, 88, 88])
new grad_input torch.Size([1, 3, 88, 88])
grad_out size torch.Size([1, 3, 86, 86])
crop [0, 6, 0, 6] [0, 85, 0, 85] [0, 79, 0, 79]
## 0 80 0 80
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [0, 0]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 90, 90])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 88, 88])
padding info :: [2, 0, 2, 0]
grad_input old torch.Size([1, 3, 90, 90])
grad_input new torch.Size([1, 3, 90, 90])
grad_input torch.Size([1, 3, 90, 90])
grad_out size torch.Size([1, 3, 87, 87])
crop [0, 7, 0, 7] [0, 86, 0, 86] [0, 79, 0, 79]
## 0 80 0 80
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [0, 0]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 92, 92])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 90, 90])
padding info :: [3, 0, 3, 0]
grad_input torch.Size([1, 3, 89, 89])
grad_out size torch.Size([1, 3, 88, 88])
crop [0, 8, 0, 8] [0, 87, 0, 87] [0, 79, 0, 79]
## 0 80 0 80
##############return grad_in in conv2d tensor([ 1.2910e-04, -3.8348e-04,  8.0719e-04,  5.9961e-04, -9.3477e-04,
         8.5272e-04,  5.1967e-05,  2.9669e-05, -2.3806e-04,  6.3220e-04],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 178, 178])
grad_out size torch.Size([1, 3, 89, 89])
arg size torch.Size([1, 3, 89, 89])
##############grad_in in maxp torch.Size([1, 3, 178, 178])
I am [0, 0]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 180, 180])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 178, 178])
grad_input torch.Size([1, 3, 180, 180])
padding info :: [1, 0, 1, 0]
grad_input old torch.Size([1, 3, 180, 180])
grad_input new torch.Size([1, 3, 180, 180])
new grad_input torch.Size([1, 3, 180, 180])
grad_out size [torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function cMaxPool2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledConv2dFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function TiledSplitFunctionBackward
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
[torch/csrc/autograd/engine.cpp] call_function torch::autograd::AccumulateGrad
torch.Size([1, 3, 178, 178])
crop [0, 18, 0, 18] [0, 177, 0, 177] [0, 159, 0, 159]
## 0 160 0 160
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [0, 0]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 182, 182])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 180, 180])
padding info :: [2, 0, 2, 0]
grad_input old torch.Size([1, 3, 182, 182])
grad_input new torch.Size([1, 3, 182, 182])
grad_input torch.Size([1, 3, 182, 182])
grad_out size torch.Size([1, 3, 179, 179])
crop [0, 19, 0, 19] [0, 178, 0, 178] [0, 159, 0, 159]
## 0 160 0 160
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [0, 0]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 184, 184])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 182, 182])
padding info :: [3, 0, 3, 0]
grad_input torch.Size([1, 3, 181, 181])
grad_out size torch.Size([1, 3, 180, 180])
crop [0, 20, 0, 20] [0, 179, 0, 179] [0, 159, 0, 159]
## 0 160 0 160
##############return grad_in in conv2d tensor([ 1.2139e-05,  4.4097e-05,  1.2515e-05, -1.2410e-05, -7.4653e-05,
        -9.9715e-08, -3.1740e-06,  1.2098e-05, -1.5040e-05, -5.8433e-05],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 362, 362])
grad_out size torch.Size([1, 3, 181, 181])
arg size torch.Size([1, 3, 181, 181])
##############grad_in in maxp torch.Size([1, 3, 362, 362])
I am [0, 0]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 364, 364])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 362, 362])
grad_input torch.Size([1, 3, 364, 364])
padding info :: [1, 0, 1, 0]
grad_input old torch.Size([1, 3, 364, 364])
grad_input new torch.Size([1, 3, 364, 364])
new grad_input torch.Size([1, 3, 364, 364])
grad_out size torch.Size([1, 3, 362, 362])
crop [0, 42, 0, 42] [0, 361, 0, 361] [0, 319, 0, 319]
## 0 320 0 320
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [0, 0]
@@@ using cudnn bkw
in the middle ++ input shape torch.Size([1, 3, 366, 366])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 364, 364])
padding info :: [2, 0, 2, 0]
grad_input old torch.Size([1, 3, 366, 366])
grad_input new torch.Size([1, 3, 366, 366])
grad_input torch.Size([1, 3, 366, 366])
grad_out size torch.Size([1, 3, 363, 363])
crop [0, 43, 0, 43] [0, 362, 0, 362] [0, 319, 0, 319]
## 0 320 0 320
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [0, 0]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 368, 368])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 366, 366])
padding info :: [3, 0, 3, 0]
grad_input torch.Size([1, 3, 365, 365])
grad_out size torch.Size([1, 3, 364, 364])
crop [0, 44, 0, 44] [0, 363, 0, 363] [0, 319, 0, 319]
## 0 320 0 320
##############return grad_in in conv2d tensor([ 6.0609e-07, -1.1711e-06, -3.4317e-06, -1.2202e-06, -4.1245e-06,
        -1.8994e-06, -8.6747e-07,  1.1685e-06,  4.0052e-06, -3.2697e-07],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 730, 730])
grad_out size torch.Size([1, 3, 365, 365])
arg size torch.Size([1, 3, 365, 365])
##############grad_in in maxp torch.Size([1, 3, 730, 730])
I am [0, 0]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 732, 732])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 730, 730])
grad_input torch.Size([1, 3, 732, 732])
padding info :: [1, 0, 1, 0]
grad_input old torch.Size([1, 3, 732, 732])
grad_input new torch.Size([1, 3, 732, 732])
new grad_input torch.Size([1, 3, 732, 732])
grad_out size torch.Size([1, 3, 730, 730])
crop [0, 90, 0, 90] [0, 729, 0, 729] [0, 639, 0, 639]
## 0 640 0 640
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [0, 0]
@@@ using cudnn bkw
in the local first ++ input shape torch.Size([1, 3, 734, 734])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 732, 732])
padding info :: [2, 0, 2, 0]
grad_input torch.Size([1, 3, 732, 732])
grad_out size torch.Size([1, 3, 731, 731])
crop [0, 91, 0, 91] [0, 730, 0, 730] [0, 639, 0, 639]
## 0 640 0 640
##############return grad_in in conv2d tensor([ 1.6603e-07,  5.4449e-08,  7.0401e-08,  2.4694e-07,  6.0391e-07,
         4.2307e-07,  2.8928e-07, -3.0105e-07, -4.6469e-08,  1.1747e-07],
       device='cuda:0')

^^^^^cMaxPool2dFunction bwd
input size torch.Size([1, 3, 1464, 1464])
grad_out size torch.Size([1, 3, 732, 732])
arg size torch.Size([1, 3, 732, 732])
##############grad_in in maxp torch.Size([1, 3, 1464, 1464])
I am [0, 0]
@@@ using cudnn bkw
local last ++ input shape torch.Size([1, 3, 1466, 1466])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1464, 1464])
grad_input torch.Size([1, 3, 1466, 1466])
padding info :: [1, 0, 1, 0]
grad_input old torch.Size([1, 3, 1466, 1466])
grad_input new torch.Size([1, 3, 1466, 1466])
new grad_input torch.Size([1, 3, 1466, 1466])
grad_out size torch.Size([1, 3, 1464, 1464])
crop [0, 184, 0, 184] [0, 1463, 0, 1463] [0, 1279, 0, 1279]
## 0 1280 0 1280
##############return grad_in in conv2d tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
I am [0, 0]
@@@ using cudnn bkw
input grad ++ input shape torch.Size([1, 3, 1468, 1468])
weight shape torch.Size([3, 3, 3, 3])
grad_output shape torch.Size([1, 3, 1466, 1466])
final torch.Size([1, 3, 1468, 1468])
padding info :: [2, 0, 2, 0]
reshape_for_final ## 0 1280 0 1280
after crop g_in torch.Size([1, 3, 1280, 1280])
grad_out size torch.Size([1, 3, 1465, 1465])
crop [0, 185, 0, 185] [0, 1464, 0, 1464] [0, 1279, 0, 1279]
## 0 1280 0 1280
##############return grad_in in conv2d tensor([ 1.3957e-08, -4.2105e-09,  2.5997e-08,  1.0122e-08, -3.2897e-09,
        -1.9740e-08,  5.8699e-08, -2.3773e-08, -4.4604e-08, -2.5939e-08],
       device='cuda:0')
[8, 8]
HREREERE torch.Size([1, 3, 10240, 10240])
==== our_bwd done ...
our_bwd_use 4.7GiB
our_bwd_use t 2.6GiB
avail our 4.6GiB
max our 5.2GiB 5602192384
