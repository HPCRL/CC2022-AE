========= 512 
## 512

&& 0.599658727645874, 0.07151651382446289, 0.6739122867584229


&& 0.5860984325408936, 0.07450461387634277, 0.663264274597168

==========ours 
## 512

&& 0.6108517646789551, 0.0941474437713623, 0.7078368663787842

## 512

&& 0.6277568340301514, 0.1353771686553955, 0.7659592628479004

## 512

&& 0.7141876220703125, 0.28462719917297363, 1.0015099048614502

## 512

&& 1.0071969032287598, 0.832221508026123, 1.8431673049926758

## 512

&& 2.1941661834716797, 2.942589521408081, 5.14006495475769

========= 1024 
## 1024

&& 0.5962979793548584, 0.09290194511413574, 0.7128779888153076


&& 0.5938220024108887, 0.10145092010498047, 0.705742597579956

==========ours 
## 1024

&& 0.6431849002838135, 0.11705493927001953, 0.7735621929168701

## 1024

&& 0.660236120223999, 0.23268413543701172, 0.9057648181915283

## 1024

&& 0.8039376735687256, 0.43245744705200195, 1.2492477893829346

## 1024

&& 1.2828457355499268, 1.0748488903045654, 2.3706769943237305

## 1024

&& 2.9964170455932617, 3.7278237342834473, 6.737561464309692

========= 2048 
## 2048

&& 0.664680004119873, 0.17049288749694824, 0.8854296207427979


&& 0.6298911571502686, 0.18784785270690918, 0.868445873260498

==========ours 
## 2048

&& 0.6663217544555664, 0.2650284767150879, 0.9836723804473877

## 2048

&& 0.7059776782989502, 0.3636460304260254, 1.121981143951416

## 2048

&& 0.8366422653198242, 0.7763769626617432, 1.6653869152069092

## 2048

&& 1.271287441253662, 1.831824779510498, 3.1556499004364014

## 2048

&& 2.6928961277008057, 5.738554000854492, 8.484137296676636

========= 3072 
## 3072

&& 0.7119224071502686, 0.24806952476501465, 1.0746071338653564


&& 0.6869826316833496, 0.32021069526672363, 1.1195390224456787

==========ours 
## 3072

&& 0.7229371070861816, 0.4601874351501465, 1.2985165119171143

## 3072

&& 0.7986445426940918, 0.7099394798278809, 1.6248340606689453

## 3072

&& 0.9821648597717285, 1.2333695888519287, 2.330805540084839

## 3072

&& 1.563605546951294, 2.907311201095581, 4.586659669876099

## 3072

&& 3.874762535095215, 8.83263373374939, 12.823241710662842

========= 4096 
## 4096

&& 0.7832448482513428, 0.48261213302612305, 1.6796083450317383


&& 0.7798640727996826, 0.5622353553771973, 1.5419790744781494

==========ours 
## 4096
Traceback (most recent call last):
  File "/home/yufan/labpytorch/uu/benchmarking/darknet19-our.py", line 246, in <module>
    main()
  File "/home/yufan/labpytorch/uu/benchmarking/darknet19-our.py", line 220, in main
    loss.backward()
  File "/home/yufan/labpytorch/torch/tensor.py", line 245, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/yufan/labpytorch/torch/autograd/__init__.py", line 146, in backward
    Variable._execution_engine.run_backward(
  File "/home/yufan/labpytorch/torch/autograd/function.py", line 89, in apply
    return self._forward_cls.backward(self, *args)  # type: ignore
  File "/home/yufan/labpytorch/uu/utils/checkpoint.py", line 148, in backward
    torch.autograd.backward(outputs_with_grad, args_with_grad)
  File "/home/yufan/labpytorch/torch/autograd/__init__.py", line 146, in backward
    Variable._execution_engine.run_backward(
  File "/home/yufan/labpytorch/torch/autograd/function.py", line 89, in apply
    return self._forward_cls.backward(self, *args)  # type: ignore
  File "/home/yufan/labpytorch/uu/layers/conv2d.py", line 201, in backward
    grad_input = padding_calc.resize_grad_in(f_info, grad_input)
  File "/home/yufan/labpytorch/uu/utils/padding_calc.py", line 314, in resize_grad_in
    grad_input = pd(grad_input)
  File "/home/yufan/labpytorch/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/yufan/labpytorch/torch/nn/modules/padding.py", line 23, in forward
    return F.pad(input, self.padding, 'constant', self.value)
  File "/home/yufan/labpytorch/torch/nn/functional.py", line 4001, in _pad
    return _VF.constant_pad_nd(input, pad, value)
RuntimeError: CUDA out of memory. Tried to allocate 130.00 MiB (GPU 0; 10.76 GiB total capacity; 9.41 GiB already allocated; 109.44 MiB free; 9.65 GiB reserved in total by PyTorch)
## 4096

&& 0.9092593193054199, 1.1201221942901611, 2.2320196628570557

## 4096

&& 1.0771667957305908, 1.7907142639160156, 3.0715508460998535

## 4096

&& 1.7402880191802979, 4.846768856048584, 6.790480613708496

## 4096

&& 4.141987562179565, 13.519831657409668, 17.866309881210327

