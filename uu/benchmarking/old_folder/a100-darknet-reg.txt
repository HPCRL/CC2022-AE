========= 512 

&& 1.2913258075714111, 0.5918693542480469, 1.8868968486785889


&& 1.3222837448120117, 0.604292631149292, 1.930267333984375

==========ours 

&& 1.3309531211853027, 0.5216858386993408, 1.8564295768737793


&& 1.3474078178405762, 0.6992831230163574, 2.05049729347229


&& 1.494859218597412, 1.0037286281585693, 2.502412796020508


&& 1.8858139514923096, 1.896371841430664, 3.7864556312561035


&& 3.131828784942627, 4.728508710861206, 7.864089250564575

========= 1024 

&& 1.3664684295654297, 0.5510110855102539, 1.9317808151245117


&& 1.3247294425964355, 0.5554168224334717, 1.8945207595825195

==========ours 

&& 1.3459441661834717, 0.6355276107788086, 1.9997432231903076


&& 1.3750276565551758, 0.8287615776062012, 2.2220726013183594


&& 1.519026756286621, 1.177353858947754, 2.7146167755126953


&& 2.03959584236145, 1.8379261493682861, 3.895803213119507


&& 4.034195899963379, 5.197640657424927, 9.250905752182007

========= 2048 

&& 1.3333680629730225, 0.5933709144592285, 2.001612424850464


&& 1.3284623622894287, 0.6031367778778076, 2.0075714588165283

==========ours 

&& 1.3534817695617676, 0.7039840221405029, 2.1328766345977783


&& 1.4168267250061035, 0.9935791492462158, 2.4857680797576904


&& 1.5264010429382324, 1.3605387210845947, 2.96238112449646


&& 1.9277307987213135, 3.0266261100769043, 5.029798269271851


&& 3.2504701614379883, 9.083021879196167, 12.408849477767944

========= 3072 

&& 1.3700542449951172, 0.6365807056427002, 2.175503730773926


&& 1.3692739009857178, 0.6694340705871582, 2.2067654132843018

==========ours 

&& 1.428135633468628, 0.8532314300537109, 2.4505550861358643


&& 1.513763427734375, 1.170781135559082, 2.853874921798706


&& 1.6650481224060059, 1.7764322757720947, 3.610602855682373


&& 2.148798704147339, 4.559399843215942, 6.877752304077148


&& 4.220954418182373, 13.37637209892273, 17.76609754562378

========= 4096 

&& 1.437929630279541, 0.6953644752502441, 2.431959629058838


&& 1.420670747756958, 0.7281346321105957, 2.4467506408691406

==========ours 

&& 1.4769680500030518, 1.0427405834197998, 2.818995475769043


&& 1.6093854904174805, 1.3569645881652832, 3.300710439682007


&& 1.7164287567138672, 2.5083978176116943, 4.524149417877197


&& 2.15224289894104, 6.573004484176636, 9.024767398834229


&& 4.397414207458496, 21.35983109474182, 26.05769395828247

========= 5120 

&& 1.482267141342163, 0.799454927444458, 2.74704647064209


&& 1.4965202808380127, 0.8019227981567383, 2.765040636062622

==========ours 

&& 1.5826983451843262, 1.2883293628692627, 3.339360475540161


&& 1.7202577590942383, 1.707263708114624, 3.896357297897339


&& 1.7839174270629883, 3.07452130317688, 5.325408220291138


&& 2.4274911880493164, 8.583244323730469, 11.474281311035156


&& 4.253001689910889, 25.824127435684204, 30.544583320617676

========= 6144 

&& 1.5753111839294434, 4.269718170166016, 6.521125793457031


&& 1.579181432723999, 4.001808166503906, 6.25523567199707

==========ours 

&& 1.6418952941894531, 4.66042947769165, 7.054535150527954


&& 1.8374414443969727, 2.1584763526916504, 4.671952962875366


&& 1.963202714920044, 3.8265016078948975, 6.466660976409912


&& 2.4161479473114014, 11.37917184829712, 14.469989776611328


&& 4.627923965454102, 33.001535177230835, 38.30198955535889

========= 7168 

&& 1.6382160186767578, 5.99603533744812, 8.549277305603027


&& 1.6694393157958984, 5.793862581253052, 8.382252931594849

==========ours 

&& 1.7436761856079102, 6.458303213119507, 9.117618083953857


&& 1.9110429286956787, 2.6853184700012207, 5.512498378753662


&& 2.057518482208252, 4.752786159515381, 7.727805137634277


&& 2.453831672668457, 14.457627534866333, 17.823702335357666


&& 4.1372692584991455, 41.09612226486206, 46.15069317817688

========= 8192 
Traceback (most recent call last):
  File "/home/yufan/labpytorch/uu/benchmarking/darknet19-ref-no-cp.py", line 222, in <module>
    main()
  File "/home/yufan/labpytorch/uu/benchmarking/darknet19-ref-no-cp.py", line 189, in main
    out_ref = model_ref(input_ref)
  File "/home/yufan/labpytorch/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/yufan/labpytorch/uu/benchmarking/darknet19-ref-no-cp.py", line 159, in forward
    out = self.block(x)
  File "/home/yufan/labpytorch/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/yufan/labpytorch/torch/nn/modules/container.py", line 119, in forward
    input = module(input)
  File "/home/yufan/labpytorch/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/yufan/labpytorch/torch/nn/modules/pooling.py", line 162, in forward
    return F.max_pool2d(input, self.kernel_size, self.stride,
  File "/home/yufan/labpytorch/torch/_jit_internal.py", line 365, in fn
    return if_false(*args, **kwargs)
  File "/home/yufan/labpytorch/torch/nn/functional.py", line 659, in _max_pool2d
    return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
RuntimeError: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 39.59 GiB total capacity; 37.58 GiB already allocated; 198.19 MiB free; 37.60 GiB reserved in total by PyTorch)
Traceback (most recent call last):
  File "/home/yufan/labpytorch/uu/benchmarking/darknet19-ref-sq-cp.py", line 231, in <module>
    main()
  File "/home/yufan/labpytorch/uu/benchmarking/darknet19-ref-sq-cp.py", line 204, in main
    loss.backward()
  File "/home/yufan/labpytorch/torch/tensor.py", line 245, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/yufan/labpytorch/torch/autograd/__init__.py", line 146, in backward
    Variable._execution_engine.run_backward(
  File "/home/yufan/labpytorch/torch/autograd/function.py", line 89, in apply
    return self._forward_cls.backward(self, *args)  # type: ignore
  File "/home/yufan/labpytorch/torch/utils/checkpoint.py", line 114, in backward
    torch.autograd.backward(outputs_with_grad, args_with_grad)
  File "/home/yufan/labpytorch/torch/autograd/__init__.py", line 146, in backward
    Variable._execution_engine.run_backward(
RuntimeError: CUDA out of memory. Tried to allocate 4.00 GiB (GPU 0; 39.59 GiB total capacity; 32.66 GiB already allocated; 2.18 GiB free; 35.35 GiB reserved in total by PyTorch)
==========ours 
Traceback (most recent call last):
  File "/home/yufan/labpytorch/uu/benchmarking/darknet19-our.py", line 246, in <module>
    main()
  File "/home/yufan/labpytorch/uu/benchmarking/darknet19-our.py", line 220, in main
    loss.backward()
  File "/home/yufan/labpytorch/torch/tensor.py", line 245, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/yufan/labpytorch/torch/autograd/__init__.py", line 146, in backward
    Variable._execution_engine.run_backward(
  File "/home/yufan/labpytorch/torch/autograd/function.py", line 89, in apply
    return self._forward_cls.backward(self, *args)  # type: ignore
  File "/home/yufan/labpytorch/uu/utils/checkpoint.py", line 148, in backward
    torch.autograd.backward(outputs_with_grad, args_with_grad)
  File "/home/yufan/labpytorch/torch/autograd/__init__.py", line 146, in backward
    Variable._execution_engine.run_backward(
  File "/home/yufan/labpytorch/torch/autograd/function.py", line 89, in apply
    return self._forward_cls.backward(self, *args)  # type: ignore
  File "/home/yufan/labpytorch/uu/layers/conv2d.py", line 219, in backward
    grad_input = padding_calc.resize_grad_in(f_info, grad_input)
  File "/home/yufan/labpytorch/uu/utils/padding_calc.py", line 314, in resize_grad_in
    grad_input = pd(grad_input)
  File "/home/yufan/labpytorch/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/yufan/labpytorch/torch/nn/modules/padding.py", line 23, in forward
    return F.pad(input, self.padding, 'constant', self.value)
  File "/home/yufan/labpytorch/torch/nn/functional.py", line 4001, in _pad
    return _VF.constant_pad_nd(input, pad, value)
RuntimeError: CUDA out of memory. Tried to allocate 518.00 MiB (GPU 0; 39.59 GiB total capacity; 36.33 GiB already allocated; 138.19 MiB free; 37.40 GiB reserved in total by PyTorch)

&& 2.081179618835449, 3.2791857719421387, 6.559886932373047


&& 2.250236749649048, 5.565469980239868, 9.012903928756714


&& 2.7832155227661133, 14.396159887313843, 18.377082109451294


&& 4.48291540145874, 61.541797399520874, 67.22258734703064

